[
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nI\nA\n.\ns\nc\n[\n \n \n1\nv\n2\n4\n7\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nTransferrable Plausibility Model - A\nProbabilistic Interpretation of Mathematical\nTheory of Evidence\n\nMieczys law Alo jzy K lopotek\n\nInstitute of Computer Science, Polish Academy of Sciences, Warszawa, Poland,\nklopotek@ipipan.waw.pl\n\nAbstract. This paper suggests a new interpretation of the Dempster-\nShafer theory in terms of probabilistic interpretation of plausibility. A\nnew rule of combination of independent evidence is shown and its preser-\nvation of interpretation is demonstrated. 1\n\n1\n\nIntroduction\n\nDempster Rule of Independent Evidence Combination has been criticized for\nits failure to conform to probabilistic interpretation ascribed to belief and plau-\nsibility function. Among those verifying DST (Dempster-Shafer-Theory, [2,12])\ncritically were Kyburg [7], Fagin [3], Halpern [6], Pearl [9], Provan [10], Cano\n[1], just to mention a few.\nAs a way out of those diculties, we proposed in a recent book co-authored\nby S.T.Wierzchon [18] three proposals for an empirical model of DST:\n\n the marginally correct approximation.\n the qualitative model\n the quantitative model\n\nThe marginally correct approximation assumes that the belief function shall\nconstitute lower bounds for frequencies, though only for the marginals, and not\nfor the joint distribution. Then, the reasoning process is expressed in terms\nof the so-called Cano et al. conditionals - a special class of conditional belief\nfunctions that are positive. This approach implies modication of the reasoning\nmechanism, because the correctness is maintained only by reasoning forward.\nDepending on the reasoning direction we need dierent Markov trees for the\nreasoning engine.\nNote that lower/upper bound interpretations have a long tradition for DST\n[2,7] and have been heavily criticized [6]. The one that we presented in our\n\n1 This is a preliminary version of the paper:\nM.A. Kopotek: Transferable Plausibility Model - A Probabilistic Interpretation\nof Mathematical Theory of Evidence O.Hryniewicz, J. Kacprzyk, J.Koronacki,\nS.Wierzcho:\nIssues\nin Intelligent Systems Paradigms Akademicka Ocyna\nWydawnicza EXIT, Warszawa 2005 ISBN 83-87674-90-7, pp.107118\n\n\fbook diers from the known ones signicantly as we insist on dierent reason-\ning schemes (hypertrees) depending on which are our target variables, whose\nvalues are to be inferred. This assures overcoming of the basic diculties with\nlower/upper bound interpretations.\nOur qualitative approach is based on the earlier rough set interpretations of\nDST, but makes a small and still signicant distinction. All computations are\ncarried out in a strictly relational way, that is, indistinguishable ob jects in a\ndatabase are merged (no ob ject identities). The behavior under reasoning ts\nstrictly the DST reasoning model. Factors of well established hypergraph repre-\nsentation (due to Shafer and Shenoy [14]) can be expressed by relational tables.\nConditional independence is well dened. However, there is no interpretation for\nconditional belief functions in this model.\nRough set interpretations [15] were primarily developed for interpreting the\nbelief function in terms of decision tables. However, the Dempster-rule of evi-\ndence combination was valid there only for the extended decision tables, not\neasily derived from the original ones. In our interpretation, both the original\ntables and the resultant tables dealt with when simulating Dempster-rule are\nconventional decision tables and the process of combining of decision tables is a\nnatural one (relational join operator).\nOur rough set based interpretation may be directly applied in the domain of\nmultiple decision tables: independence of decision variables or Shenoys condi-\ntional independence in the sense of DST may serve as an indication of possibility\nof decomposition of the decision table into smaller but equivalent tables.\nFurthermore, it may be applied in the area of Cooperative Query Answer-\ning [11]. The problem there is that a query posed to a local relational database\nsystem may contain an unknown attribute. But, possibly, other co-operating\ndatabase systems know it and may explain it to the queried system in terms\nof known attributes, shared by the various systems. The uncertainties studied\nin the decision tables arise here in a natural way and our interpretation may\nbe used to measure these uncertainties in terms of DST (as a diversity of sup-\nport). Furthermore, if several co-operating systems respond, then the queried\nsystem may calculate the overall uncertainty measure using DST combination\nof measures of individual responses.\nThe quantitative model assumed that the ob jects possess multivalued proper-\nties which are then lost in some physical properties and these physical processes\nare described by DST belief functions (see e.g. [8])..\nThe quantitative model assumes that during the reasoning process one at-\ntaches labels to ob jects hiding some of their properties. There is a full agreement\nwith the reasoning mechanism of DST. Conditional independence and condi-\ntional belief functions are well dened. We have also elaborated processes that\ncan give rise to well-controlled graphoidally structured belief functions. Thus,\nsample generation for DST is possible. We elaborated also learning procedures\nfor discovery of graphoidal structures from data.\nThe quantitative model seems to be the best tting model for belief functions\ncreated so far.\n\n\fThis frequency model diers from what was previously considered [16,17] in\nthat it assumes that reasoning in DST is connected with updating of variables\nfor individual cases. This is dierent from e.g. reasoning in probability where rea-\nsoning means only selection of cases. In this way, failures of previous approaches\ncould be overcome.\nMany authors [13,16] question the need for an empirical model for DST and\npoint rather to theoretical properties of DST considered within an axiomatic\nframework seeking parallels with the probability theory. Though it is true that\nthe probability theory may be applied within the framework of Kolmogorov ax-\nioms and quite useful results are derived in this way, one shall still point out that\nthe applicability of probability theory is signicantly connected with frequencies.\nBoth frequencies considered as naive probabilities, ore ones being probabilities\nin the limit. Statistics is clearly an important part of the probabilistic world.\nAll three interpretations share a common drawback they are not sensu stricto\nprobabilistic. In the current paper we make an attempt of a purely probabilistic\nvision of plausibility function.\n\n2 Basics of the Dempster-Shafer Theory\n\nWe understand DST measures in a standard way (see [12]). Let  be a nite set\nof elements called elementary events. Any subset of  is a composite event, or\nhypothesis.  be called also the frame of discernment.\n\nDenition 1. [12] Let  be a nite set of elements cal led elementary events.\nThe set  is cal led frame of discernment. Any subset of  be a composite event.\nA basic probability assignment (bpa) function is any function m:2  [0, 1] such\nthat\n\nm(A) = ON E\n\nm() = 0,\n\nA2\n\nX\nA2\n\n0  X\nAB\n\nm(B )\n\nWe say that a bpa is vacuous i m( ) = ON E and m(A) = 0 for every A 6=  .\n\nIf ONE is equal 1, then we say that the belief function is normalized, otherwise\nnot (but ONE must be positive).\n\nDenition 2. [12] Let a belief function be dened as Bel:2  [0, 1] so that\nB el(A) = PBA m(B ). Let a plausibility function be Pl:2  [0, 1] with A2 P l(A) =\nON E  B el(  A), a commonality function be Q:2  {}  [0, 1] with\nA2 {} Q(A) = PAB m(B ).\nDenition 3. [12] The Rule of Combination of two Independent Belief Func-\ntions B elE1 , B elE2 Over the Same Frame of Discernment (the so-cal led Dempster-\nRule), denoted\n\nB elE1 ,E2 = B elE1  B elE2\n\nis dened as fol lows: :\n\nmE1 ,E2 (A) = c  X\nB ,C ;A=BC\n\nmE1 (B )  mE2 (C )\n\n\f(c - constant normalizing the sum of m to 1).\n\nUnder multivariate settings  is a set of vectors in n-dimensional space\nspanned by the set of variables X={ X1 , X2 , . . . Xn}. If A   , then by pro jec-\ntion AY of the set A onto a subspace spanned by the set of variables Y  X we\nunderstand the set B of vectors from A pro jected onto Y. Then marginalization\noperator\nfollows:\nas\ndened\nis\nDST\nof\nmY (B ) = PA;B=AX m(A).\n\nDenition 4. (See [?]) Let B be a subset of  , cal led evidence, mB be a basic\nprobability assignment such that mB (B ) = 1 and mB (A) = 0 for any A dierent\nfrom B. Then the conditional belief function B el(.||B ) representing the belief\nfunction B el conditioned on evidence B is dened as: B el(.||B ) = B el  B elB .\n\n3 New Rule of Evidence Combination\n\nLet us suggest now a totally new approach to understanding belief functions.\nWe assume the following interpretation of the plausibility function: P l (A) is\nthe maximum probability that an element from the set of events A occurs, given\nthe evidence  , where we assume the apriorical probability of all elementary\nevents is equal. Let 1 and 2 be two independent bodies of evidence, which are\nrepresented numerically by plausibility functions P l1 and P l2 over some frame\nof discourse  . We would like to obtain such an evidence updating rule P l that\nP l3 = P l1 P l P l2 would have the semantics that under that interpretation\nP l3 (A) is the maximum probability that an element from the set of events A\noccurs, given the evidence P l1 , P l2 under the least conicting evidence.\nLet us study in detail this assumption. First of all we have to tell what we\nmean by independent evidence. Let  be an elementary event from the frame of\ndiscernment  . The body of evidence 1 is independent of the body 2 if, for\neach    , the probability of occurrence of evidence 1 is independent of the\noccurrence of evidence 2. So we say that P r(1  2| ) = P r(1| )  P r(2| ).\nHow shall we understand the evidence, however. For any A   ) should hold\nP l (A)  P r(A| ). Consequently, by the way, P l (A) + P l (/A)  1.\nNow observe that P r(1  2 | ) = P r(1 | ) + P r(2 | ). As a consequence,\nwe have always that P l ({1}) + P l ({2})  P l ({1 , 2}).\nLet us now turn to combining independent evidence.\n\nP r( |1  2) = P r(1  2| ) \n\nP r( )\nP r(1  2)\n\nP r(1| )  P r(2| ) \n\nP r( )\nP r(1  2)\n\nP r( |1)  P r( |2) \n\nP r(1)  P r(2)\nP r(1  2)  P r( )\n\nSo we can conclude that P l12 ( ) = P l1 ( )P l2 ( )c where c is a normalizing\nfactor (which needs to be chosen carefully).\n\n\fBut what about P r(1  2 |1  2) ? We know that P r(1  2 |1  2) =\nP r(1 |1  2) + P r(2 |1  2) hence\n\nP r(1  2 |1  2)\n\nP r(1 |1)P r(1 |2)\n\nP r(1)  P r(2)\nP r(1  2)  P r(1 )\n\n+P r(2 |1)P r(2 |2)\n\nP r(1)  P r(2)\nP r(1  2)  P r(2 )\n\nAs P r( ) is the same for all the  s, we get\n\nP r(1  2 |1  2)\n\n(P r(1 |1)  P r(1 |2) + P r(2 |1)  P r(2 |2)) \n\nP r(1)  P r(2)\nP r(1  2)  P r( )\n\nWe can easily check that this translates to:\n\nP l12 ({1 , 2}) =\n\nmax(P l1 (1 )  P l2 (1 ) + (P l1 ({1 , 2}  P l1(1 ))  (P l2 ({1 , 2}  P l2(1 ))\n\n, P l1 (1 )  (P l2 ({1 , 2}  P l2 (2 )) + (P l1 ({1 , 2}  P l1 (1 ))  P l2 (2 )\n\n, P l1 (2 )  (P l2 ({1 , 2}  P l2 (1 )) + (P l1 ({1 , 2}  P l1 (2 ))  P l2 (1 )\n\n, P l1 (2 )  P l2 (2 ) + (P l1 ({1 , 2}  P l1(2 ))  (P l2 ({1 , 2}  P l2(2 )))  c\n\nwhere c is the normalizing factor mentioned earlier.\nThese formulas easily generalize for subsets of  with higher cardinality. The\nnormalizing factor should be chosen in such a way that P l12 ( ) = 1.\nThe generalization of P l for frames of discourse with cardinality higher than\n3 runs along the following lines. To combine P l1 with P l2 we calculate:\n\n for each subset X of \nP lresult (X ) = P LX\n1 V P l2 X ;\n\nThe operator  X does only a change of the domain of the P l function\nkeeping the values of P l for each subset of X and presuming that the discourse\nframe consists only of X . In this way we get unnormalized P ls here, which are\nnot normalized during this operation.\nThe operator V , returning a numerical value, attempts identify such com-\nbinations of mass assignments ma and mb to singleton sets that will not violate\nthe constraints imposed by plausibility functions P l1 and P l2 resp. and such\nthat the sum PX ;X asingleton ma (X )  mb (X ) is maximal.\nThis is done by the operation of so-called pushing down the plausibilities\nto singleton sets. Independently for P l1 and P l2 candidate ma and mb are ob-\ntaining via pushing-down recursively a singleton  of  . A candidate ma is\nobtained if all singletons are pushed down. Dierent candidates are obtained\nby dierent sequences of pushing down. It is easy to imagine that the process\nis time-consuming and its complexity grows exponentially with the number of\nelements of a set. Nonetheless for small domains the operation is feasible.\n\n\fThe idea of the push-down operator  + is as follows: Let P l be a plausibil-\nity function. If A does not contain  , P l+ (A) = min(P l(A), P l(A  {}) \nP l({})), and otherwise P l+ (A) = P l(A).\nUnder these conditions it is obvious that we do not seek actually the max-\nimum product over the whole domain, but rather in some corner points. We\nwill give a formal proof elsewhere that this check is in fact sucient to establish\nthe maximum. Here we only want to draw attention to the analogy with linear\nprogramming, where we seek the maximum sub ject to linear constraints. When-\never we x pushdown of one of the plausibility distributions, we in fact have\na linear optimization case with the other. If found, we can do the same with the\nother.\nThe P l operator is characterized by commutativity and associativity. The\ncommutativity is easily seen because all the operations are in fact symmetrical\nwith respect to left and right hand of the operators. The associativity is more\ndicult to grasp, and a formal proof will be sub ject of another publication.\nNonetheless we can give here brief common-sense guidelines how it can be es-\ntablished. We can essentially concentrate on the associative properties of the\nmaximum operator. Starting with the expression of combination of all the three\nplausibility functions, we can show that we can equivalently denote the same\noptimization task when drawing behind braces the rst or the third operand.\nIn the next section we show some properties of the new operator compared\nwith Dempster rule of combination for some illustrative examples.\n\n4 Examples\n\nLet us consider the bodies of evidence in the tables 1, 2, 3.\n\nTable 1. mass function for the body of evidence a\n\nm value for the set of elements\n0.0 { }\n0.25 { red }\n0.25 { blue }\n0.25 { red , blue }\n0.25 { green }\n0.0 { red , green }\n0.0 { blue , green }\n0.0 { red , blue , green }\n\nWe can check the commutativity and obtain the results as in the table 4.\nThe associativity has been veried in table 5.\nIt is worth noting, that the new operator is dierent from Dempster rule,\ncompare tables 4 and 6\n\n\fTable 2. mass function for the body of evidence  b\n\nm value for the set of elements\n0.0 { }\n0.2 { red }\n0.4 { blue }\n0.1 { red , blue }\n0.0 { green }\n0.0 { red , green }\n0.3 { blue , green }\n0.0 { red , blue , green }\n\nTable 3. mass function for the body of evidence  c\n\nm value for the set of elements\n0.0 { }\n0.0 { red }\n0.15 { blue }\n0.25 { red , blue }\n0.35 { green }\n0.25 { red , green }\n0.0 { blue , green }\n0.0 { red , blue , green }\n\nTable 4. mass function for the body of evidence (a P L  b) =( b P L a)\n\nm value for the set of elements\n0.0 { }\n0.11111111111111105 { red }\n0.4999999999999999 { blue }\n0.22222222222222232 { red , blue }\n0.0 { green }\n0.0 { red , green }\n0.16666666666666674 { blue , green }\n0.0 { red , blue , green }\n\n\fTable 5. mass function for the body of evidence ((aP L  b)P L  c) =(aP L ( bP L\n c))\n\nm value for the set of elements\n0.0 { }\n0.0 { red }\n0.4214285714285715 { blue }\n0.3214285714285714 { red , blue }\n0.014285714285714124 { green }\n0.07142857142857151 { red , green }\n0.1357142857142858 { blue , green }\n0.0357142857142857 { red , blue , green }\n\nTable 6. mass function for the body of evidence (a   b)\n\nm value for the set of elements\n0.0 { }\n0.20833333333333337 { red }\n0.6249999999999999 { blue }\n0.04166666666666663 { red , blue }\n0.12500000000000003 { green }\n0 { red , green }\n0 { blue , green }\n0 { red , blue , green }\n\n\fWith this and other experiments we see clearly the tendency of Dempster\nrule to move mass downwards to singleton sets, whereas the new rule is much\nmore cautious here and in fact does not introduce the feeling of certainty where\nit is not justied.\n\n5 Conclusions\n\nWe have introduced in this paper a new DST operator for combining independent\nevidence providing a clear probabilistic denition of the plausibility function,\nwhich is preserved under this rule of combination.\nWe have also provided several toy examples to give an impression what results\nare returned by the new operator.\nThough the strict theoretical proof of properties like cummutativeness, asso-\nciativeness is still to be provided, the computations for test examples show that\nthe properties really hold. It is also obvious from the examples that the new rule\ndiers from the Dempster rule of evidence combination. An interested reader is\ninvited\nto\nvisit\nthe\nWeb\npage\nhttp://www.ipipan.waw.pl/klopotek/DSTnew/DSTdemo.html to try out him-\nself.\n\nReferences\n\n1. Cano J., Delgado M., Moral S.: An axiomatic framework for propagating uncer-\ntainty in directed acyclic networks, Int. J. of Approximate Reasoning. 1993:8, 253-\n280.\n2. A.P.Dempster: Upper and lower probabilities induced by a multi-valued mapping.\nAnn. Math. Stat. 38 (1967), 325-339\n3. R.Fagin, J.Y.Halpern: Uncertainty, belief, and probability, Proc. Int. Joint Conf.\nAI, IJCAI89, Detroit, 1989, 1161-1167.\n4. R.Fagin, J.Y.Halpern: A new approach to updating beliefs, in:J.F. Lemmer, L.N.\nKanal eds: Uncertainty in Articial Intel ligence 6 (North-Holland Amsterdam,\n1991), 347-374.\n5. R. Fagin, J.Y. Halpern: Uncertainty, belief, and probability. Comput. Intell. 71991,\n160-173.\n6. J.Y. Halpern, R. Fagin: Two views of belief: belief as generalized probability and\nbelief as evidence.Articial Intel ligence 541992, 275-317\n7. H.E. Kyburg Jr: Bayesian and non-Bayesian evidential updating.\n8. M.A.Kopotek, S.T.Wierzcho: Quest on New Applications for Dempster-Shafer\nTheory: Risk Analysis in Pro ject Protability Calculus. In: P. Grzegorzewski,\nO.Hryiewicz, M.A.Gil Eds.: Soft Methods in Probability, Statistics and Data Anal-\nysis , Advances in Soft Computing Series, Physica-Verlag/Springer Verlag, 2002,\nISBN 3-7908-1526-8, pp. 302-309\n9. Pearl, 1990 J. Pearl: Bayesian and Belief-Function formalisms for evidential rea-\nsoning:A conceptual analysis. in:G. Shafer, J. Pearl (eds): Readings in Uncertain\nReasoning, (ISBN 1-55860-125-2, Morgan Kaufmann Publishers Inc., San Mateo,\nCalifornia, 1990), 540-569.\n\n\f10. Provan, 1990 G.M.Provan: A logic-based analysis of Dempster-Shafer Theory, In-\nternational Journal of Approximate Reasoning 41990, 451-495.\n11. Z.W. Ras: Query processing in distributed information systems, Fundamenta In-\nformaticae Journal, Special Issue on Logics for Articial Intelligence, IOS Press,\nVol. XV, No. 3/4, 1991, 381-397\n12. G.Shafer: A Mathematical Theory of Evidence , Princeton University Press, Prince-\nton, 1976\n13. G.Shafer: Perspectives on the theory and practice of belief functions, International\nJournal of Approximate Reasoning, 1990:4, 323-362.\n14. P.P.Shenoy: Valuation networks and conditional independence. International Jour-\nnal of Uncertainty, Fuzziness and Know ledge-Based Systems, Vol.2, No.2, June\n1994.\n15. A.Skowron, J.W.Grzyma la-Busse: From rough set theory to evidence theory.\nin:R.R.Yager, J.Kasprzyk and M.Fedrizzi, eds, Advances in the Dempster-Shafer\nTheory of Evidence J. Wiley, New York (1994), 193-236.\n16. Smets, 1992 Ph.Smets: Resolving misunderstandings about belief functions, Inter-\nnational Journal of Approximate Reasoning 1992:6:321-344.\n17. Smets, Kenne, 1994 Ph.Smets, R.Kennes: The tranferable belief model, Articial\nIntel ligence 66 (1994), 191-234\n18. S.T.Wierzcho, M.A.Kopotek: Evidential Reasoning. An Interpretative Investiga-\ntion. Wydawnictwo Akademii Podlaskiej, Siedlce, 2002 PL ISSN 0860-2719, 304\npages,\n\n\f", 
        "tag": "Artificial Intelligence", 
        "link": "https://arxiv.org/list/cs.AI/new"
    }, 
    {
        "text": "Multitask Learning with Low-Level Auxiliary Tasks\nfor Encoder-Decoder Based Speech Recognition\n\nShubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu\n\nToyota Technological Institute at Chicago\n{shtoshni, haotang, llu, klivescu}@ttic.edu\n\n7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nL\nC\n.\ns\nc\n[\n \n \n1\nv\n1\n3\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstract\nEnd-to-end training of deep learning-based models allows for\nimplicit learning of intermediate representations based on the\nnal task loss. However, the end-to-end approach ignores the\nuseful domain knowledge encoded in explicit intermediate-level\nsupervision. We hypothesize that using intermediate representa-\ntions as auxiliary supervision at lower levels of deep networks\nmay be a good way of combining the advantages of end-to-end\ntraining and more traditional pipeline approaches. We present\nexperiments on conversational speech recognition where we use\nlower-level tasks, such as phoneme recognition, in a multitask\ntraining approach with an encoder-decoder model for direct char-\nacter transcription. We compare multiple types of lower-level\ntasks and analyze the effects of the auxiliary tasks. Our results\non the Switchboard corpus show that this approach improves\nrecognition accuracy over a standard encoder-decoder model on\nthe Eval2000 test set.\nIndex Terms: speech recognition, multitask learning, encoder-\ndecoder, CTC, LSTM\n\n1. Introduction\nAutomatic speech recognition (ASR) has historically been ad-\ndressed with modular approaches, in which multiple parts of the\nsystem are trained separately. For example, traditional ASR sys-\ntems include components like frame classiers, phonetic acoustic\nmodels, lexicons (which may or may not be learned from data),\nand language models [1]. These components typically corre-\nspond to different levels of representation, such as frame-level\ntriphone states, phones, and words. Breaking up the task into\nsuch modules makes it easy to train each of them separately, pos-\nsibly on different data sets, and to study the effect of modifying\neach component separately.\nOver time, ASR research has moved increasingly toward\ntraining multiple components of ASR systems jointly. Typically,\nsuch approaches involve training initial separate modules, fol-\nlowed by joint ne-tuning using sequence-level losses [2, 3].\nRecently, completely integrated end-to-end training approaches,\nwhere all parameters are learned jointly using a loss at the nal\noutput level, have become viable and popular. End-to-end train-\ning is especially natural for deep neural network-based models,\nwhere the nal loss gradient can be backpropagated through all\nlayers. Typical end-to-end models are based on recurrent neural\nnetwork (RNN) encoder-decoders [4, 5, 6, 7] or connectionist\ntemporal classication (CTC)-based models [8, 9].\nEnd-to-end training is appealing because it is conceptually\nsimple and allows all model parameters to contribute to the same\nnal goal, and to do so in the context of all other model pa-\nrameters. End-to-end approaches have also achieved impressive\nresults in ASR [4, 9, 10] as well as other domains [11, 12, 13].\nOn the other hand, end-to-end training has some drawbacks:\nOptimization can be challenging; the intermediate learned repre-\n\nsentations are not interpretable, making the system hard to debug;\nand the approach ignores potentially useful domain-specic in-\nformation about intermediate representations, as well as existing\nintermediate levels of supervision.\nPrior work on analyzing deep end-to-end models has found\nthat different layers tend to specialize for different sub-tasks,\nwith lower layers focusing on lower-level tasks and higher ones\non higher-level tasks. This effect has been found in systems for\nspeech processing [14, 15] as well as computer vision [16, 17].\nWe propose an approach for deep neural ASR that aims to\nmaintain the advantages of end-to-end approaches, while also\nincluding the domain knowledge and intermediate supervision\nused in modular systems. We use a multitask learning approach\nthat combines the nal task loss (in our case, log loss on the out-\nput labels) with losses corresponding to lower-level tasks (such\nas phonetic recognition) applied on lower layers. This approach\nis intended to encapsulate the intuitive and empirical observa-\ntion that different layers encode different levels of information,\nand to encourage this effect more explicitly. In other words,\nwhile we want the end-to-end system to take input acoustics and\nproduce output text, we also believe that at some appropriate\nintermediate layer, the network should do a good job at distin-\nguishing more basic units like states or phones. Similarly, while\nend-to-end training need not require supervision at intermedi-\nate (state/phone) levels, if they are available then our multitask\napproach can take advantage of them.\nWe demonstrate this approach on a neural attention-based\nencoder-decoder character-level ASR model. Our baseline\nmodel is inspired by prior work [18, 8, 19, 4, 7], and our lower-\nlevel auxiliary tasks are based on phonetic recognition and frame-\nlevel state classication. We nd that applying an auxiliary loss\nat an appropriate intermediate layer of the encoder improves\nperformance over the baseline.\n\n2. Related Work\nMultitask training has been studied extensively in the machine\nlearning literature [21]. Its application to deep neural networks\nhas been successful in a variety of settings in speech and lan-\nguage processing [22, 23, 24, 25, 26, 27]. Most prior work\ncombines multiple losses applied at the nal output layer of the\nmodel, such as joint Mandarin character and phonetic recog-\nnition in [26] and joint CTC and attention-based training for\nEnglish ASR [25]. Our work differs from this prior work in that\nour losses relate to different types of supervision and are applied\ndifferent levels of the model.\nThe idea of using low-level supervision at lower levels was,\nto our knowledge, rst introduced by Sgaard & Goldberg [28]\nfor natural language processing tasks, and has since been ex-\ntended by [29]. The closest work to ours is the approach of Rao\nand Sak [30] using phoneme labels for training a multi-accent\nCTC-based ASR system in a multitask setting. Here we study\n\n\fthe approach in the context of encoder-decoder models, and we\ncompare a number of low-level auxiliary losses.\n\nFigure 1: Sketch of our training-time model with multiple losses applied at different layers. The encoder is a pyramidal bidirectional\nLSTM (our experiments use 4 layers; we show 3 layers for simplicity). Different hidden state layers of this encoder are used for predicting\nHMM state label si , phone sequence z (using either CTC or a LSTM decoder), and nally the output character sequence y via a LSTM\ndecoder. The dotted line in the character decoder denotes the use of (sampled) model predictions [20] during training (for the phone\ndecoder only the ground-truth prior phone is used in training). At test time, only the character decoder is used for transcription.\n for j = 2, 3, 4\nwhere f (j ) and b(j ) denote the forward and backward running\nLSTMs at layer j . Following [4], we use pyramidal layers\nto reduces the time resolution of the nal state sequence h(4)\nby a factor of 23 = 8. This reduction brings down the input\nsequence length, initially T = |x|, where |  | denotes the length\nof a sequence of vectors, close to the output sequence length2 ,\nK = |y |. For simplicity, we will refer to h(4) as h.\n\n3. Models\nThe multitask approach we propose can in principle be applied to\nany type of deep end-to-end model. Here we study the approach\nin the context of attention-based deep RNNs. Below we describe\nthe baseline model, followed by the auxiliary low-level training\ntasks.\n\ncessed as follows:\n\n2i1 ; h(j1)\ni = f (j ) ([h(j1)\nh(j )\n\n2i\ni = b(j ) ([h(j1)\n2i1 ; h(j1)\nh(j )\n\n\n],\n2i\nh(j )\nh(j )\nh(j )\ni )\n;\ni = (\ni\n\n\nh(j )\n\ni1 )\nh(j )\ni+1 )\n\n3.1. Baseline Model\n\n],\n\nThe model is based on attention-enabled encoder-decoder RNNs,\nproposed by [19]. The speech encoder reads in acoustic features\nx = (x1 , . . . , xT ) and outputs a sequence of high-level features\n(hidden states) h which the character decoder attends to in\ngenerating the output character sequence y = (y1 , . . . , yK ), as\nshown in Figure 1 (the attention mechanism and a pyramidal\nLSTM layer are not shown in the gure for simplicity).\n\n3.1.1. Speech Encoder\n\nThe speech encoder is a deep pyramidal bidirectional Long Short-\nTerm Memory [31] (BiLSTM) network [4]. In the rst layer,\na BiLSTM reads in acoustic features x and outputs h(1) =\n1 , . . . , h(1)\n(h(1)\nT ) given by:\n\n\nh(1)\nh(1)\ni = f (1) (xi ,\ni1 )\n\n\nh(1)\ni+1 )\n\n\nh(1)\ni = b(1) (xi ,\n\n\nh(1)\nh(1)\n;\ni\ni\n\n)\n\nh(1)\ni = (\n\nwhere i  {1, . . . , T } denotes the index of the timestep; f (1) ()\nand b(1) () denote the rst layer forward and backward LSTMs\nrespectively1 .\n1 , . . . , h(1)\nThe rst layer output h(1) = (h(1)\nT ) is then pro-\n\n3.1.2. Character Decoder\nThe character decoder is a single-layer LSTM that predicts a\nK(cid:89)\nsequence of characters y as follows:\nt=1\n\nP (y |x) = P (y |h) =\n\nP (yt |h, y<t ).\n\nThe conditional dependence on the encoder state vectors h is\nrepresented by context vector ct , which is a function of the\ncurrent decoder hidden state and the encoder state sequence:\n(cid:62) tanh(W1hi + W2dt + ba )\n|h|(cid:88)\ni=1\n\nt = softmax(ut )\n\nuit = v\n\nithi\n\nct =\n\nwhere the vectors v , ba and the matrices W1 , W2 are learnable\nparameters; dt is the hidden state of the decoder at time step\nt. The time complexity of calculating the context vector ct for\nevery time step is O(|h|); reducing the resolution on encoder\nside is crucial to reducing this runtime.\nThe hidden state of the decoder, dt , which captures the\nprevious character context y<t , is given by:\n\ndt = g( y t1 , dt1 , ct1 )\n\n1 For brevity we exclude the LSTM equations. The details can be\nfound, e.g., in Zaremba et al. [32].\n\n2 For Switchboard, the average of number of frames per character is\nabout 7.\n\ns8s7s6s5s4s3s2x8x7x6x5x4x3x2x1GOGOy1y2xTsTs1PhoneCTC(LCTCp)State(Ls)CharDec(Lc)PhoneDec(LDecp)z1z2z1\fwhere g() is the transformation of the single-layer LSTM, dt1\nis the previous hidden state of the decoder, and y t1 is a char-\nacter embedding vector for yt1 , as is typical practice in RNN-\nbased language models. Finally, the posterior distribution of the\noutput at time step t is given by:\nP (yt |h, y<t ) = softmax(Ws [ct ; dt ] + bs ),\nand the character decoder loss function is then dened as\nLc =  log P (y |x).\n\n3.2. Low-Level Auxiliary Tasks\nAs shown in Figure 1, we explore multiple types of auxiliary\ntasks in our multitask approach. We explore two types of auxil-\niary labels for multitask learning: phonemes and sub-phonetic\nstates. We hypothesize that the intermediate representations\nneeded for sub-phonetic state classication are learned at the\nlowest layers of the encoder, while representations for phonetic\nprediction may be learned at a somewhat higher level.\n\n3.2.2. State-Level Auxiliary Task\nSub-phonetic state labels provide another type of low-level su-\npervision that can be borrowed from traditional modular HMM-\nbased approaches. We apply this type of supervision at the frame\nlevel, as shown in Figure 1, using state alignments obtained from\na standard HMM-based system. We apply this auxiliary task at\nlayer 2 of the speech encoder. The probability of a sequence of\nM(cid:89)\nM(cid:89)\nstates s is dened as\nm=1\nm=1\n\nP (sm |h(2)\nm ),\n\nP (sm |x) =\n\nP (s|x) =\n\nwhere P (sm |h(2)\nm ) is computed by a softmax function, and M\nis the number of frames at layer 2 (in this case (cid:100)T /2(cid:101)). Since we\nuse this task at layer 2, we subsample the state labels to match\nthe reduced resolution. The nal state-level loss is\nLs =  log P (s|x).\n\n3.2.1. Phoneme-Based Auxiliary Tasks\nWe use phoneme-level supervision obtained from the word-level\ntranscriptions and pronunciation dictionary. We consider two\ntypes of phoneme transcription loss: Phoneme Decoder Loss:\nSimilar to the character decoder described above, we can attach\na phoneme decoder to the speech encoder as well. The phoneme\ndecoder has exactly the same mathematical form as the character\ndecoder, but with a phoneme label vocabulary at the output.\nSpecically, the phoneme decoder loss is dened as\np =  log P (z |x),\nLDec\nwhere z is the target phoneme sequence. Since this decoder\ncan be attached at any depth of the four-layer encoder described\nabove, we have four depths to choose from. We attach the\nphoneme decoder to layer 3 of the speech encoder, and also\ncompare this choice to attaching it to layer 4 (the nal layer) for\ncomparison with a more typical multitask training approach.\nCTC Loss: A CTC [33] output layer can also be added to\nvarious layers of the speech encoder [30]. This involves adding\nan extra softmax output layer on top of the chosen intermediate\nlayer of the encoder, and applying the CTC loss to the output\nof this softmax layer. Specically, let z be the target phoneme\nsequence, and k be the speech encoder layer where the loss is\n(cid:88)\n(cid:88)\nJ(cid:89)\napplied. The probability of z given the input sequence is\nB1 (z)\nB1 (z)\nj=1\nwhere B() removes repetitive symbols and blank symbols, B1\nis Bs pre-image, J is the number of frames at layer k and\nP (j |h(k)\n) is computed by a softmax function. The nal CTC\nj\nobjective is\np =  log P (z |x).\nLCTC\nThe CTC objective computation requires the output length to be\nless than the input length, i.e., |z | < J . In our case the encoder\nreduces the time resolution by a factor of 8 between the input\nand the top layer, making the top layer occasionally shorter than\nthe number of phonemes in an utterance. We therefore cannot\napply this loss to the topmost layer, and use it only at the third\nlayer.3\n\nP ( |h(k) ) =\n\nP (j |h(k)\nj\n\nP (z |x) =\n\n),\n\n3 In fact, even at the third layer we nd occasional instances (about 10\nutterances in our training set) where the hidden state sequence is shorter\n\nL =\n\n3.2.3. Training Loss\nThe nal loss function that we minimize is the average of the\nlosses involved. For example, in the case where we use the\ncharacter and phoneme decoder losses and the state-level loss,\nthe loss would be\n\n(Lc + LDec\np + Ls ).\n\n1\n3\n4. Experiments\nWe use the Switchboard corpus (LDC97S62) [34], which con-\ntains roughly 300 hours of conversational telephone speech, as\nour training set. We reserve the rst 4K utterances as a develop-\nment set. Since the training set has a large number of repetitions\nof short utterances (yeah, uh-huh, etc.), we remove dupli-\ncates beyond a count threshold of 300. The nal training set\nhas about 192K utterances. For evaluation, we use the HUB5\nEval2000 data set (LDC2002S09), consisting of two subsets:\nSwitchboard (SWB), which is similar in style to the training set,\nand CallHome (CHE), which contains unscripted conversations\nbetween close friends and family.\nFor input features, we use 40-dimensional log-mel lterbank\nfeatures along with their deltas, normalized with per-speaker\nmean and variance normalization. The phoneme labels for the\nauxiliary task are generated by mapping words to their canoni-\ncal pronunciations, using the lexicon in the Kaldi Switchboard\ntraining recipe. The HMM state labels were obtained via forced\nalignment using a baseline HMM/DNN hybrid system using\nthe Kaldi NNet1 recipe. The HMM/DNN has 8396 tied states,\nwhich makes the frame-level softmax costly for multitask learn-\ning. We use the importance sampling technique described in [35]\nto reduce this cost.\n4.1. Model Details and Inference\nThe speech encoder is a 4-layer pyramidal bidirectional LSTM,\nresulting in a 8-fold reduction in time resolution. We use 256\nhidden units in each direction of each layer. The decoder for\nall tasks is a single-layer LSTM with 256 hidden units. We\nrepresent the decoders output symbols (both characters and,\nat training time, phonemes) using 256-dimensional embedding\nvectors. At test time, we use a greedy decoder (beam size =\n\nthan the input sequence, due to sequences of phonemes of duration less\nthan 4 frames each. Anecdotally, these examples appear to correspond\nto incorrect training utterance alignments\n\n\fTable 1: Character error rate (CER, %) and word error rate\n(WER, %) results on development data.\nDev CER\nModel\n14.6\nEnc-Dec (baseline)\n13.8\nEnc-Dec + PhoneDec-3\n14.5\nEnc-Dec + PhoneDec-4\n14.0\nEnc-Dec + PhoneCTC-3\nEnc-Dec + State-2\n13.6\n13.4\nEnc-Dec + PhoneDec-3 + State-2\n\nDev WER\n26.0\n24.9\n25.9\n25.3\n24.1\n24.1\n\nTable 2: WER (%) on Eval2000 for different end-to-end models.\nPhoneDec-n refers to a phoneme decoder applied at layer n\nof the encoder. Similarly, PhoneCTC-3 means phoneme CTC\nloss applied at layer 3 and State-2 means state-label supervision\napplied at layer 2 of the encoder.\nModel\nOur models\nEnc-Dec (baseline)\nEnc-Dec + PhoneDec-3\nEnc-Dec + PhoneDec-4\nEnc-Dec + PhoneCTC-3\nEnc-Dec + State-2\nEnc-Dec + PhoneDec-3 + State-2\nLu et al.[7]\nEnc-Dec\nEnc-Dec (word) + 3-gram\nMaas et al. [8]\nCTC\nCTC + 3-layer RNN LM\nZweig et al. [9]\nIterated CTC\nCTC + Char Ngram\nCTC + Dictionary + Word Ngram\n\n25.0\n24.5\n25.4\n24.6\n24.7\n23.1\n\n42.4\n40.6\n41.9\n41.3\n42.0\n40.8\n\n56.1\n40.2\n\n37.1\n32.1\n25.3\n\n38.0\n21.4\n\n24.7\n19.8\n14.0\n\n48.2\n46.0\n\n33.7\n32.6\n33.7\n33.0\n33.4\n32.0\n\n37.8\n36.0\n\n47.1\n30.8\n\n27.3\n25.8\n\nSWB\n\nCHE\n\nFull\n\n\n\n\n\nFigure 2: Log-likelihood of train data (per character) for differ-\nent model variations.\n\n1) to generate the character sequence. The character with the\nmaximum posterior probability is chosen at every time step and\nfed as input into the next time step. The decoder stops after\nencountering the EOS (end-of-sentence) symbol. We use no\nexplicit language model.\nWe train all models using Adam [36] with a minibatch size of\n64 utterances. The initial learning rate is 1e-3 and is decayed by\na factor of 0.95, whenever there is an increase in log-likelihood\nof the development data, calculated after every 1K updates, over\nits previous value. All models are trained for 75K gradient\nupdates (about 25 epochs) and early stopping. To further control\novertting we: (a) use dropout [37] at a rate of 0.1 on the output\nof all LSTM layers (b) sample the previous steps prediction [20]\nin the character decoder, with a constant probability of 0.1 as\nin [4].\n4.2. Results\nWe evaluate performance using word error rate (WER). We\nreport results on the combined Eval2000 test set as well as sepa-\nrately on the SWB and CHE subsets. We also report character\nerror rates (CER) on the development set.\nDevelopment set results are shown in Table 1. We refer to\nthe baseline model as Enc-Dec and the models with multi-\ntask training as Enc-Dec + [auxiliary task]-[layer]. Adding\nphoneme recognition as an auxiliary task at layer 3, either with a\nseparate LSTM decoder or with CTC, reduces both the character\nerror rates and the nal word error rates.\nIn order to determine whether the improved performance is\na basic multitask training effect or is specic to the low-level\napplication of the loss, we compare these results to those of\nadding the phoneme decoder at the topmost layer (Enc-Dec +\nPhoneDec-4). The top-layer application of the phoneme loss\nproduces worse performance than having the supervision at the\nlower (third) layer. Finally, we obtain the best results by adding\nboth phoneme decoder supervision at the third layer and frame-\nlevel state supervision at the second layer (Enc-Dec + PhoneDec-\n3 + State-2). The results support the hypothesis that lower-level\nsupervision is best provided at lower layers. Table 2 provides test\nset results, showing the same pattern of improvement on both\n\nthe SWB and CHE subsets. For comparison, we also include a\nvariety of other recent results with neural end-to-end approaches\non this task. Our baseline model has better performance than\nthe most similar previous encoder-decoder result [7]. With the\naddition of the low-level auxiliary task training, our models are\ncompetitive with all of the previous end-to-end systems that do\nnot use a language model.\nFigure 2 shows the training set log-likelihood for the baseline\nmodel and two multitask variants. The plot suggests that mul-\ntitask training helps with optimization (improving the training\nerror). Training error is very similar for both multitask models,\nwhile the development set performance is better for one of them\n(see Table 1), suggesting that there may also be an improved\ngeneralization effect and not only improved optimization.\n5. Conclusion\nWe have presented a multitask training approach for deep end-\nto-end ASR models in which lower-level task losses are applied\nat lower levels, and we have explored this approach in the con-\ntext of attention-based encoder-decoder models. Results on\nSwitchboard and CallHome show consistent improvements over\nbaseline attention-based models and support the hypothesis that\nlower-level supervision is more effective when applied at lower\nlayers of the deep model. We have compared several types of\nauxiliary tasks, obtaining the best performance with a combina-\ntion of a phoneme decoder and frame-level state loss. Analysis\nof model training and performance suggests that the addition of\nauxiliary tasks can help in either optimization or generalization.\nFuture work includes studying a broader range of auxiliary\ntasks and model congurations. For example, it would be in-\nteresting to study even deeper models and word-level output,\nwhich would allow for more options of intermediate tasks and\nplacements of the auxiliary losses. Viewing the approach more\nbroadly, it may be fruitful to also consider higher-level task su-\npervision, incorporating syntactic or semantic labels, and to view\nthe ASR output as an intermediate output in a more general\nhierarchy of tasks.\n6. Acknowledgements\nWe are grateful to William Chan for helpful discussions, and to the speech\ngroup at TTIC, especially Shane Settle, Herman Kamper, Qingming\nTang, and Bowen Shi for sharing their data processing code. This\nresearch was supported by a Google faculty research award.\n\n\f[20] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, Scheduled sam-\npling for sequence prediction with recurrent neural networks, in\nNeural Information Processing Systems (NIPS), 2015.\n[21] R. Caruana, Multitask learning, Machine Learning, 1997.\n[22] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa, Natural language processing (almost) from scratch,\nJournal of Machine Learning Research (JMLR), 2011.\n[23] M. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser, Multi-\ntask sequence to sequence learning, in International Conference\non Learning Representations (ICLR), 2016.\n[24] A. Eriguchi, Y. Tsuruoka, and K. Cho, Learning to parse\nand translate improves neural machine translation, CoRR, vol.\nabs/1702.03525, 2017.\n[25] S. Kim, T. Hori, and S. Watanabe, Joint CTC-attention based\nend-to-end speech recognition using multi-task learning, CoRR,\nvol. abs/1609.06773, 2016.\n[26] W. Chan and I. Lane, On online attention-based speech recogni-\ntion and joint Mandarin character-Pinyin training, in Interspeech,\n2016.\n[27] Z. Wu, C. Valentini-Botinhao, O. Watts, and S. King, Deep neural\nnetworks employing multi-task learning and stacked bottleneck\nfeatures for speech synthesis, in International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2015.\n[28] A. Sgaard and Y. Goldberg, Deep multi-task learning with low\nlevel tasks supervised at lower layers, in Annual Meeting of the\nAssociation for Computational Linguistics (ACL), 2016.\n[29] K. Hashimoto, C. Xiong, Y. Tsuruoka, and R. Socher, A joint\nmany-task model: Growing a neural network for multiple NLP\ntasks, CoRR, vol. abs/1611.01587, 2016.\n[30] K. Rao and H. Sak, Multi-accent speech recognition with hierar-\nchical grapheme based models, in International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP), 2017.\n[31] S. Hochreiter and J. Schmidhuber, Long short-term memory,\nNeural Computation, vol. 9, 1997.\n[32] W. Zaremba, I. Sutskever, and O. Vinyals, Recurrent neural net-\nwork regularization, CoRR, vol. abs/1409.2329, 2014.\n[33] A. Graves, S. Fern andez, and F. Gomez, Connectionist temporal\nclassication: Labelling unsegmented sequence data with recur-\nrent neural networks, in International Conference on Machine\nLearning (ICML), 2006.\n[34] J. J. Godfrey, E. C. Holliman, and J. McDaniel, SWITCHBOARD:\nTelephone speech corpus for research and development, in Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 1992.\n[35] S. Jean, K. Cho, R. Memisevic, and Y. Bengio, On using very\nlarge target vocabulary for neural machine translation, in Annual\nMeeting of the Association for Computational Linguistics (ACL),\n2015.\n[36] J. Duchi, E. Hazan, and Y. Singer, Adaptive subgradient meth-\nods for online learning and stochastic optimization, Journal of\nMachine Learning Research (JMLR), vol. 12, 2011.\n[37] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, Dropout\nimproves recurrent neural networks for handwriting recognition,\nin International Conference on Frontiers in Handwriting Recogni-\ntion (ICFHR), 2014.\n\n7. References\n[1] M. Gales and S. Young, The application of hidden markov models\nin speech recognition, Foundations and trends in signal process-\ning, vol. 1, 2008.\n[2] K. Vesel `y, A. Ghoshal, L. Burget, and D. Povey, Sequence-\ndiscriminative training of deep neural networks. in Interspeech,\n2013.\n[3] D. Povey and B. Kingsbury, Evaluation of proposed modications\nto mpe for large scale discriminative training, in IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2007.\n[4] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, Listen, attend and\nspell: A neural network for large vocabulary conversational speech\nrecognition, in International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP), 2016.\n[5] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\ngio, Attention-based models for speech recognition, in Neural\nInformation Processing Systems (NIPS), 2015.\n[6] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\nEnd-to-end attention-based large vocabulary speech recognition,\nin International Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP), 2016.\n[7] L. Lu, X. Zhang, and S. Renals, On training the recurrent neural\nnetwork encoder-decoder for large vocabulary end-to-end speech\nrecognition, in International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP), 2016.\n[8] A. L. Maas, Z. Xie, D. Jurafsky, and A. Y. Ng, Lexicon-free\nconversational speech recognition with neural networks, in North\nAmerican Chapter of the Association for Computational Linguistics\non Human Language Technology (NAACL HLT), 2015.\n[9] G. Zweig, C. Yu, J. Droppo, and A. Stolcke, Advances in all-\nneural speech recognition, CoRR, vol. abs/1609.05935, 2016.\n[10] Y. Miao, M. Gowayyed, and F. Metze, EESEN: End-to-end speech\nrecognition using deep RNN models and WFST-based decoding,\nin IEEE Workshop on Automatic Speech Recognition and Under-\nstanding (ASRU), 2015.\n[11] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, Deep\nnetworks with stochastic depth, in European Conference on Com-\nputer Vision (ECCV), 2016.\n[12] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. E.\nHinton, Grammar as a foreign language, in Neural Information\nProcessing Systems (NIPS), 2015.\n[13] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen,\nN. Thorat, F. B. Vi egas, M. Wattenberg, G. Corrado, M. Hughes,\nand J. Dean, Googles multilingual neural machine translation sys-\ntem: Enabling zero-shot translation, CoRR, vol. abs/1611.04558,\n2016.\n[14] A.-r. Mohamed, G. E. Hinton, and G. Penn, Understanding how\ndeep belief networks perform acoustic modelling, in International\nConference on Acoustics, Speech, and Signal Processing (ICASSP),\n2012.\n[15] T. Nagamine, M. L. Seltzer, and N. Mesgarani, On the role of\nnonlinear transformations in deep neural network acoustic models,\nInterspeech, 2016.\n[16] M. D. Zeiler and R. Fergus, Visualizing and understanding convo-\nlutional networks, in European Conference on Computer Vision\n(ECCV), 2014.\n[17] R. Girshick, J. Donahue, T. Darrell, and J. Malik, Rich feature hi-\nerarchies for accurate object detection and semantic segmentation,\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2014.\n[18] I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence\nlearning with neural networks, in Neural Information Processing\nSystems (NIPS), 2014.\n[19] D. Bahdanau, K. Cho, and Y. Bengio, Neural machine transla-\ntion by jointly learning to align and translate, in International\nConference on Learning Representations (ICLR), 2015.\n\n\f", 
        "tag": "Computation and Language", 
        "link": "https://arxiv.org/list/cs.CL/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nC\nC\n.\ns\nc\n[\n \n \n1\nv\n7\n5\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nA Complexity Trichotomy for the Six-Vertex Model\n\nJin-Yi Cai\njyc@cs.wisc.edu\n\nZhiguo Fu\nfuzg@jlu.edu.cn\n\nShuai Shao\nsh@cs.wisc.edu\n\nAbstract\n\nWe prove a complexity classication theorem that divides the six-vertex model into exactly\nthree types. For every setting of the parameters of the model, the computation of the partition\nfunction is precisely: (1) Solvable in polynomial time for every graph, or (2) #P-hard for general\ngraphs but solvable in polynomial time for planar graphs, or (3) #P-hard even for planar\ngraphs. The classication has an explicit criterion. In addition to matchgates and matchgates-\ntransformable signatures, we discover previously unknown families of planar-tractable partition\nfunctions by a non-local connection to #CSP, dened in terms of a loop space. For the proof\nof #P-hardness, we introduce the use of Mobius transformations as a powerful new tool to prove\nthat certain complexity reductions succeed in polynomial time.\n\n1 Introduction\n\nPartition functions are Sum-of-Product computations. In physics, one considers a set of particles\nconnected by some bonds. Then physical considerations impose various local constraints, each with\na suitable weight. Given a conguration satisfying all the local constraints, the product of local\nweights is the weight of the conguration, and its sum over all congurations is the value of the\npartition function. It encodes much information about the physical system.\nThis is essentially the same set-up as counting constraint satisfaction problems (#CSP). Take\na set of constraint functions F , the problem #CSP(F ) is as follows: The input is a bipartite graph\nG = (U, V , E ), where U are the variables (spins), V is labeled by constraint functions from F ,\nand E describes how the constraints are applied on the variables. The output is the sum, over all\nassignments to variables in U , of the product of constraint function evaluations in V . Note that\neach function in F has a xed arity, and in general takes values in C (not just {0, 1}). A spin\nsystem is the special case of #CSP where the constraints are binary functions (in which case each\nv  V has degree 2 and can be replaced by an edge).\nBy denition, a partition function is an exponential sized sum. But in some cases, clever algo-\nrithms exist that can compute it in polynomial time. Well-known examples of partition functions\nfrom physics that have been investigated intensively in complexity theory include the Ising model,\nPotts model, hardcore gas and Ice model [14, 10, 11, 18, 25]. Most of these are spin systems. If\nparticles take (+) or () spins, each can be modeled by a Boolean variable, and local constraints\nDepartment of Computer Sciences, University of Wisconsin-Madison. Supported by NSF CCF-1217549.\nSchool of Mathematics, Jilin University. Supported by NSF CCF-1217549.\n\n1\n\n\fare expressed by edge (binary) constraint functions. These are nicely modeled by the #CSP frame-\nwork. Some physical systems are more naturally described as orientation problems, and these can\nbe modeled by Holant problems, of which #CSP is a special case. Roughly speaking, Holant prob-\nlems [7] (see Section 2 for denitions) are tensor networks where edges of a graph are variables while\nvertices are local constraint functions. Spin systems can be simulated easily as Holant problems,\nbut Freedman, Lovasz and Schrijver proved that simulation in the reverse direction is generally not\npossible [8]. In this paper we study a family of partition functions that t the Holant problems\nnaturally, but not as a spin system. This is the six-vertex model.\nIn physics, the six-vertex model concerns crystal lattices with hydrogen bonds. Remarkably it\ncan be expressed perfectly as a family of Holant problems with 6 parameters, although in physics\npeople are more focused on regular planar structures such as lattice graphs, and asymptotic limit.\nPreviously, without being able to account for the planar restriction, it has been proved [5] that\nthere is a complexity dichotomy where computing the partition function ZSix is either in P or\n#P-hard. However the more interesting problem is what happens on planar structures where\nphysicists had discovered some remarkable algorithms, such as Kasteleyns algorithm for planar\nperfect matchings [21, 15, 16]. Concomitantly, and also probably because of that, to achieve a\ncomplete complexity classication in the planar case is more challenging. It must isolate precisely\nthose problems that are #P-hard in general graphs but P-time computable on planar graphs.\nIn this paper we prove a complexity trichotomy theorem for the six-vertex models: According\nto the 6 parameters from C, the partition function ZSix is either computable in P-time, or #P-\nhard on general graphs but computable in P-time on planar graphs, or remains #P-hard on planar\ngraphs. The classication has an explicit criterion. In addition to matchgates and matchgates-\ntransformable signatures, we discover previously unknown families of planar-tractable ZSix by a\nnon-local connection to #CSP, dened in terms of a loop space.\nLinus Pauling in 1935 rst introduced the six-vertex models to account for the residual entropy\nof water ice [20]. We have a large number of oxygen and hydrogen atoms in a 1 to 2 ratio. Each\noxygen atom (O) is connected by a bond to four other neighboring oxygen atoms (O), and each\nbond is occupied by one hydrogen atom (H). Physical constraint requires that each (H) is closer to\neither one or the other of the two neighboring (O), but never in the middle of the bond. Pauling\nargued [20] that, furthermore, the allowed congurations are such that at each oxygen (O) site,\nexactly two hydrogen (H) are closer to it, and the other two are farther away. The placement of\noxygen and hydrogen atoms can be naturally represented by vertices and edges of a 4-regular graph.\nThe constraint on the placement of hydrogen atoms (H) can be represented by an orientation of the\nSince there are (cid:0)4\n(cid:1) = 6 local valid congurations, this is called the six-vertex model. In addition\nedges of the graph, such that at every vertex (O), exactly two edges are oriented toward the vertex,\nand exactly two edges are oriented away from it. In other words, this is an Eulerian orientation.\n2\nto water ice, potassium dihydrogen phosphate KH2PO4 (KDP) also satises this model.\nThe valid local congurations of the six-vertex model are illustrated in Figure 1. The energy E\nThen the partition function is ZSix = (cid:80) eE /kB T , where the sum is over all valid congurations,\nof the system is determined by six parameters \u00011 , \u00012 , . . . , \u00016 associated with each type of the local\nconguration. If there are ni sites in local congurations of type i, then E = n1 \u00011 + n2 \u00012 + . . . + n6 \u00016 .\nkB is Boltzmanns constant, and T is the systems temperature. Mathematically, this is a sum-of-\nproduct computation where the sum is over all Eulerian orientations of the graph, and the product\nis over all vertices where each vertex contributes a factor ci = c\u0001i if it is in conguration i (1  i  6)\nfor some constant c.\n\n2\n\n\f(cid:78)\n(cid:73) (cid:73)\n(cid:78)\n\n(cid:72)\n(cid:74) (cid:74)\n(cid:72)\n\n(cid:72)\n(cid:73) (cid:73)\n(cid:72)\n\n(cid:78)\n(cid:74) (cid:74)\n(cid:78)\n\n(cid:78)\n(cid:73) (cid:74)\n(cid:72)\n\n(cid:72)\n(cid:74) (cid:73)\n(cid:78)\n\nFigure 1: Valid congurations of the six-vertex model\n\nSome choices of the parameters are well-studied. On the square lattice graph, when modeling\napproaches (cid:0) 4\n(cid:1)3/2  1.5396007 . . . (Liebs square ice constant). This matched experimental data\nice one takes \u00011 = \u00012 = . . . = \u00016 = 0.\nIn 1967, Elliott Lieb [19] famously showed that, as the\nnumber N of vertices approaches , the value of the partition function per vertex W = Z 1/N\n1.540  0.001 so well that it is considered a triumph. Other well-known six-vertex models include:\n3\nthe KDP model of a ferroelectric (\u00011 = \u00012 = 0, and \u00013 = \u00014 = \u00015 = \u00016 > 0), the Rys F model of an\nantiferroelectric (\u00011 = \u00012 = \u00013 = \u00014 > 0, and \u00015 = \u00016 = 0). Historically these are widely considered\namong the most signicant applications ever made of statistical mechanics to real substances. In\nclassical statistical mechanics the parameters are all real numbers while in quantum theory the\nparameters are complex numbers in general.\nDisregarding the planarity restriction, [5] proved that computing the partition function ZSix\nis either in P or #P-hard. However known cases of planar P-time computable ZSix (but #P-\nhard on general graphs) are all #P-hard in this classication. In this paper we tackle the more\ndicult planar case, and prove a complexity trichotomy theorem. The most interesting part is\nthe classication of those ZSix which are #P-hard in general but P-time computable on planar\nstructures. The classication is valid for all parameter values c1 , c2 , . . . , c6  C. (To state our\ntheorem in strict Turing machine model, we take c1 , c2 , . . . , c6 to be algebraic numbers.) The\ndependence of this trichotomy on the values c1 , c2 , . . . , c6 is explicit.\nare transformable by a holographic transformation to matchgates (denoted by (cid:99)M ) do constitute a\nWe show that constraints that are expressible as matchgates (denoted by M ) or those that\nfamily of ZSix which are #P-hard in general but P-time computable on planar structures. This is\nas expected and is known before (Kasteleyns algorithm for planar perfect matchings, and Valiants\nholographic algorithms based on matchgates [23, 24]). However we also discover an additional\nfamily of such ZSix which are not transformable to matchgates. The P-time tractability on planar\ngraphs is via a non-local transformation to #CSP, where the variables in #CSP correspond to\ncertain loops in the six-vertex model graph. The fact that the #CSP instance is P-time tractable\ndepends heavily on the global topological constraint imposed by the planar structure.\nAfter carving out this last tractable family, we set about to prove that everything else is #P-\nhard, even for the planar case. A powerful tool in such proofs is the interpolation technique [22].\nTypically an interpolation proof can succeed when certain quantities (such as eigenvalues) are not\nroots of unity, lest the iteration repeat after a bounded number of steps. A sucient condition is\nthat these quantities have complex norm (cid:54)= 1. However for some constraint functions, we can show\nthat all constructions necessarily produce only relevant quantities of unit norm. In this case we\nintroduce Mobius transformations z (cid:55) az+b\ncz+d . It turns out that in this case the constraint function\ndenes a natural Mobius transformation that maps unit circle to unit circle on C. By exploiting\nthe conformal mapping property we can obtain a suitable Mobius transformation which generates\na group of innite order. This allows us to show that our interpolation proof succeeds.\n\n3\n\n\f2 Preliminaries and Notations\n1, a square root of 1.\n\nIn this paper, i denotes\n\n2.1 Denitions and Notations\nA constraint function f of arity k is a map {0, 1}k  C. Fix a set F of constraint functions. A\nsignature grid  = (G, ) is a tuple, where G = (V , E ) is a graph,  labels each v  V with a\nconsider all 0-1 edge assignments  , each gives an evaluation (cid:81)\nfunction fv  F of arity deg(v), and the incident edges E (v) at v with input variables of fv . We\nfv ( |E (v) ), where  |E (v) denotes\nvV\n(cid:88)\n(cid:89)\nthe restriction of  to E (v). The counting problem on the instance  is to compute\nfv ( |E (v) ).\nHolant(; F ) =\nvV\n :E{0,1}\nThe Holant problem parameterized by the set F is denoted by Holant(F ). If F = {f } is a single\nset, for simplicity, we write {f } as f directly, and also we write {f , g} as f , g . When G is a planar\ngraph, the corresponding signature grid is called a planer grid. We use Holant (F | G ) to denote the\nHolant problem over signature grids with a bipartite graph H = (U, V , E ), where each vertex in U\nor V is assigned a signature in F or G respectively. Signatures in F are considered as row vectors\n(or covariant tensors); signatures in G are considered as column vectors (or contravariant tensors).\nSimilarly, Pl-Holant (F | G ) denotes the Holant problem over signature grids with a planar bipartite\n(cid:34) f0000 f0010 f0001 f0011\n(cid:35)\ngraph.\nA constraint function is also called a signature. A signature f of arity 4 has the signature matrix\nf0100 f0110 f0101 f0111\nM (f ) = Mx1 x2 ,x4 x3 (f ) =\n. If (i, j, k , (cid:96)) is a permutation of (1, 2, 3, 4), then\nf1000 f1010 f1001 f1011\nf1100 f1110 f1101 f1111\nthe 4  4 matrix Mxi xj ,x(cid:96) xk (f ) lists the 16 values with row index xixj  {0, 1}2 and column index\n(cid:20) 0 0 0 a\n(cid:21)\nx(cid:96)xk  {0, 1}2 in lexicographic order. Without other specication, M (f ) denotes Mx1 x2 ,x4 x3 (f ).\n(cid:104) M (f )2,2 M (f )2,3\n(cid:105)\n(cid:3) denoted by MIn (f ) the inner matrix of M (f ), and the submatrix\n= (cid:2) b c\nThe planar six-vertex model is Pl-Holant((cid:54)=2 | f ), where M (f ) =\n0 b c 0\n. We call the subma-\n0 z y 0\n(cid:104) M (f )1,1 M (f )1,4\n(cid:105)\nx 0 0 0\ntrix\nz y\nM (f )3,2 M (f )3,3\n= [ 0 a\nx 0 ] denoted by MOut (f ) the outer matrix of M (f ). A binary signature g\nM (f )4,1 M (f )4,4\n(cid:20) 0 0 0 1\n(cid:21)\nhas the signature matrix M (g) = Mx1 ,x2 (g) = [ g00 g01\ng10 g11 ] . Without other specication, M (g) denotes\nMx1 ,x2 (g). We use ( (cid:54)=2 ) to denote binary Disequality signature (0, 1, 1, 0)T . It has the signature\nmatrix M ((cid:54)=2 ) = [ 0 1\n1 0 ]  [ 0 1\n. Note that N is the double Disequality,\n0 0 1 0\n1 0 ]. Let N = [ 0 1\n1 0 ] =\n0 1 0 0\nwhich is the function of connecting two pairs of edges by ((cid:54)=2 ). A function is symmetric if its value\n1 0 0 0\ndepends only on the Hamming weight of its input. A symmetric function f on k Boolean variables\ncan be expressed as [f0 , f1 , . . . , fk ], where fw is the value of f on inputs of Hamming weight w.\nFor example, (=k ) is the Equality signature [1, 0, . . . , 0, 1] (with k  1 many 0s) of arity k . The\nsupport of a function f is the set of inputs on which f is nonzero.\nCounting constraint satisfaction problems (#CSP) can be dened as a special case of Holant\nproblems. An instance of #CSP(F ) is presented as a bipartite graph. There is one node for each\nvariable and for each occurrence of constraint functions respectively. Connect a constraint node to\na variable node if the variable appears in that occurrence of constraint, with a labeling on the edges\n\n4\n\n\ffor the order of these variables. This bipartite graph is also known as the constraint graph. If we\nattach each variable node with an Equality function, and consider every edge as a variable, then\nthe #CSP is just the Holant problem on this bipartite graph. Thus #CSP(F ) T Holant (E Q | F ),\nwhere E Q = {=1 , =2 , =3 , . . . } is the set of Equality signatures of all arities. By restricting to\nplanar constraint graphs, we have the planar #CSP framework, which we denote by Pl-#CSP. The\nconstruction above also shows that Pl-#CSP(F ) T Pl-Holant (E Q | F ).\n\n2.2 Gadget Construction\n\nOne basic notion used throughout the paper is gadget construction. We say a signature f is\nconstructible or realizable from a signature set F if there is a gadget with some dangling edges\nsuch that each vertex is assigned a signature from F , and the resulting graph, when viewed as a\nblack-box signature with inputs on the dangling edges, is exactly f . If f is realizable from a set F ,\nthen we can freely add f into F while preserving the complexity.\n\nf (y1 , . . . , yn ) =\n\nFigure 2: An F -gate with 5 dangling edges.\nFormally, this notion is dened by an F -gate. An F -gate is similar to a signature grid (G, )\nfor Holant(F ) except that G = (V , E , D) is a graph with some dangling edges D. The dangling\nedges dene external variables for the F -gate (See Figure 2 for an example). We denote the regular\nedges in E by 1, 2, . . . , m and the dangling edges in D by m + 1, . . . , m + n. Then we can dene a\n(cid:88)\nfunction f for this F -gate as\nx1 ,...,xm{0,1}\nwhere (y1 , . . . , yn )  {0, 1}n is an assignment on the dangling edges and H (x1 , . . . , xm , y1 , . . . , yn )\nis the value of the signature grid on an assignment of all edges in G, which is the product of\nevaluations at all vertices in V . We also call this function f the signature of the F -gate.\nAn F -gate is planar if the underlying graph G is a planar graph, and the dangling edges, ordered\ncounterclockwise corresponding to the order of the input variables, are in the outer face in a planar\nembedding. A planar F -gate can be used in a planar signature grid as if it is just a single vertex\nwith the particular signature.\nUsing planar F -gates, we can reduce one planar Holant problem to another. Suppose g is the\nsignature of some planar F -gate. Then Pl-Holant(F , g) T Pl-Holant(F ). The reduction is simple.\nGiven an instance of Pl-Holant(F , g), by replacing every occurrence of g by the F -gate, we get an\ninstance of Pl-Holant(F ). Since the signature of the F -gate is g , the Holant values for these two\nsignature grids are identical. There are three common gadgets we will use in this paper.\n\nH (x1 , . . . , xm , y1 , . . . , yn ),\n\n5\n\n\fSuppose f1 and f2 have signature matrices Mxi xj ,x(cid:96) xk (f1 ) and Mxs xt ,xv xu (f2 ), where (i, j, k , (cid:96))\nand (s, t, u, v) are permutations of (1, 2, 3, 4). By connecting x(cid:96) with xs , xk with xt , both using Dise-\nquality ( (cid:54)=2 ), we get a signature of arity 4 with the signature matrix Mxi xj ,x(cid:96) xk (f1 )N Mxs xt ,xv xu (f2 )\nby matrix product with row index xixj and column index xv xu (See Figure 3). In this paper, we\n\nFigure 3: Connect variables x(cid:96) , xk of f1 with variables xs , xt of f2 both using ( (cid:54)=2 ).\n\nMx4 x1 ,x3 x2 (f ) =\n\n. Note that no matter in which signature matrix, only the pair (c, z ) is\n\nfocus on planar graphs, and we assume the edges incident to a vertex are ordered counterclockwise.\nWhen connecting two signatures, we need to keep the counterclockwise order of the edges incident\nto each vertex. In order to satisfy this planar property, (i, j, k , (cid:96)) and (s, t, u, v) both have to be\ncyclic permutations of (1, 2, 3, 4). In this paper, given a signature f of arity 4 with the signature\n(cid:20) 0 0 0 a\n(cid:21)\n(cid:20) 0 0 0 y\n(cid:21)\n(cid:20) 0 0 0 x\n(cid:21)\nmatrix Mxi xj ,x(cid:96) xk (f ), we always assume (i, j, k , (cid:96)) is a cyclic permutation of (1, 2, 3, 4). There are\nfour cyclic permutations of (1, 2, 3, 4), so correspondingly, a signature f has four 4  4 signature\n(cid:21)\n(cid:20) 0 0 0 b\n0 y c 0\n0 b c 0\n0 a z 0\nmatrices Mx1 x2 ,x4 x3 (f ) =\n, Mx2 x3 ,x1 x4 (f ) =\n, Mx3 x4 ,x1 x2 (f ) =\n, and\n0 z y 0\n0 c x 0\n0 z b 0\nx 0 0 0\nb 0 0 0\na 0 0 0\n0 x z 0\n0 c a 0\ny 0 0 0\nalways in the inner matrix. We call (c, z ) the inner pair, and (a, x), (b, y) outer pairs. On the other\nhand, given a signature f (cid:48) of arity 4 with the signature matrix Mx1 x2 ,x4 x3 (f (cid:48) ) = Mxi xj ,xk x(cid:96) (f ), we\nmay relabel the variables x1 , x2 , x3 , x4 of f (cid:48) by xi , xj , x(cid:96) , xk . Then, the signature f (cid:48) is exactly the\nsignature f . In fact, f (cid:48) can be viewed as a rotation form of f . The four rotation forms of f are\n3\n\n2 , f  and f\n2 . Once we get one form, all the four rotation forms can be freely\ndenoted by f , f\nused. In the proof, after one construction, we may use this property to get a similar construction\nand conclusion by quoting this rotational symmetry.\nA binary signature g has the signature vector g(x1 , x2 ) = (g00 , g01 , g10 , g11 )T , and also g(x2 , x1 ) =\n(g00 , g10 , g01 , g11 )T . Without other specication, g denotes g(x1 , x2 ). Let f be a signature of\narity 4 with the signature matrix Mxi xj ,x(cid:96) xk (f ) and (s, t) be a permutation of (1, 2). By con-\nnecting x(cid:96) with xs and xk with xt , both using Disequality ( (cid:54)=2 ), we get a binary signature\nwith the signature matrix Mxi xj ,xk x(cid:96) N g(xs , xt ) by matrix product with index xixj (See Figure 4).\nIf g00 = g11 , then N (g00 , g01 , g10 , g11 )T = (g11 , g10 , g01 , g00 )T = (g00 , g10 , g01 , g11 )T and similarly,\nN (g00 , g10 , g01 , g11 )T = (g00 , g01 , g10 , g11 )T . Therefore, Mxi xj ,x(cid:96) xk N g(xs , xt ) = Mxi xj ,x(cid:96) xk g(xt , xs ),\nwhich means connecting variables x(cid:96) , xk of the signature f with variables xs , xt of the signature\ng using double Disequality N are equivalent to connecting variables x(cid:96) , xk of the signature f\nwith variables xt , xs of the signature g directly. In this paper, we use Mxi xj ,x(cid:96) xk g(xt , xs ) instead of\nMxi xj ,x(cid:96) xk N g(xs , xt ) to represent connecting f and g using double Disequality N when g00 = g11 .\nSince g is a binary signature, we can rotate it by 180 degree. Then the variables xs and xt change\ntheir positions with each other, and that rotation do not destroy the planar graph. That is, both\ng(xs , xt ) and g(xt , xs ) can be freely used once we get one of them.\n\n6\n\n\fFigure 4: Connect variables x(cid:96) , xk of f with variables xs , xt of g both using ( (cid:54)=2 ).\nA binary signature g also has the 2  2 signature matrix M (g) = Mx1 ,x2 (g) = [ g00 g01\n(cid:105)\n(cid:104) f0000 f0010 f0001 f0011 f0100 f0110 f0101 f0111\ng10 g11 ]. Without\nother specication, M (g) denotes Mx1 ,x2 (g). A signature f of arity 4 also has the 2  8 signature\n. Suppose the signature matrix of\nmatrix Mx1 ,x2 x4 x3 (f ) =\nf1000 f1010 f1001 f1011 f1100 f1110 f1101 f1111\ng is Mxs ,xt (g) and the signature matrix of f is Mxi ,xj x(cid:96) xk (f ). By connecting xt with xi using Dise-\nqulity ((cid:54)=2 ), we get a signature h of arity 4 with the signature matrix Mxs ,xt (g)Z T ZMxi ,xj x(cid:96) xk (f )\nby matrix product with row index xs and column index xj x(cid:96)xk (See Figure 5). We may change\n\nFigure 5: Connect variable xt of g with variable xi of f using ( (cid:54)=2 ).\n\n=\n\n=\n\nIn particular, if\nthis form to a signature matrix with row index xsxj and column index x(cid:96)xk .\nMxs ,xt (g) = [ 0 1\nt 0 ], then\n(cid:21) (cid:20)f0000 f0010 f0001 f0011 f0100 f0110 f0101 f0111\n(cid:21)\n(cid:21) (cid:20)0 1\n(cid:20)0 1\nMxs ,xj x(cid:96) xk (h) = Mxs ,xt (g)M ( (cid:54)=2 )Mxi ,xj x(cid:96) xk (f )\n(cid:21)\n(cid:20) f0000\n1 0\nf1000 f1010 f1001 f1011 f1100 f1110 f1101 f1111\nt 0\nf0111\nf0101\nf0110\nf0100\nf0011\nf0001\nf0010\n(cid:34) f0000 f0010 f0001 f0011\n.\ntf1000\ntf1010\ntf1001\ntf1011\ntf1100\ntf1110\ntf1101\ntf1111\nf0100 f0110 f0101 f0111\ntf1000 tf1010 tf1001 tf1011\ntf1100 tf1110 tf1101 tf1111\nThat is, when connecting the variable xt of signature g with the variable xi of signature f , let the\nentries of Mxi xj ,x(cid:96) xk (f ) with index xi = 1 multiply by t and that will give the signature matrix of\nthe constructed signature h. Similarly, when connecting the variable xs of g with the variable xi of\nf , let the entries of Mxi xj ,x(cid:96) xk (f ) with index xi = 0 multiply by t and that will give Mxi xj ,x(cid:96) xk (h).\nA constant scalar C (cid:54)= 0 does not change the complexity of a Holant problem. That is,\nHolant(F  {f }) T Holant(F  {C f }) for any signature set F , signature f and constant scalar\n\nWe change the name of the index xs in Mxs ,xj x(cid:96) xk (h) by xi , then Mxi xj ,x(cid:96) xk (h) =\n\n(cid:35)\n\n.\n\n7\n\n\fC (cid:54)= 0. In planar instances, that is Pl-Holant(F  {f }) T Pl-Holant(F  {C f }). In other words,\nC f is realizable by f for any C (cid:54)= 0. Hence, we can pick a nonzero entry of the signature ma-\ntrix M (f ) and divide all entries in M (f ) by the nonzero entry we picked. We call this operation\nnormalization. By normalization, we may assume the nonzero entry we picked is of value 1.\n\n2.3 Holographic Transformation\n\nTo introduce the idea of holographic transformation, it is convenient to consider bipartite graphs.\nFor a general graph, we can always transform it into a bipartite graph while preserving the Holant\nvalue, as follows. For each edge in the graph, we replace it by a path of length two. (This operation\nis called the 2-stretch of the graph and yields the edge-vertex incidence graph.) Each new vertex\nis assigned the binary Equality signature (=2 ) = [1, 0, 1].\nFor an invertible 2-by-2 matrix T  GL2 (C) and a signature f of arity n, written as a column\nvector (contravariant tensor) f  C2n , we denote by T 1f = (T 1 )nf the transformed signature.\nFor a signature set F , dene T 1F = {T 1f | f  F } the set of transformed signatures. For\nsignatures written as row vectors (covariant tensors) we dene F T similarly. Whenever we write\n(cid:2) 1 1\n(cid:3), we also dene (cid:98)F = H2F . Note that\nT 1f or T 1F , we view the signatures as column vectors; similarly for f T or F T as row vectors.\nIn the special case of the Hadamard matrix H2 = 1\n1 1\n2\nH2 is orthogonal. Since constant factors are immaterial, for convenience we sometime drop the\nfactor 1\nwhen using H2 .\nLet T  GL2 (C). The holographic transformation dened by T is the following operation: given\n2\n(cid:48) = (H,  (cid:48) ) of Holant (cid:0)F T | T 1G (cid:1) by replacing each signature in F or G with the corresponding\na signature grid  = (H, ) of Holant (F | G ), for the same bipartite graph H , we get a new grid\nsignature in F T or T 1G .\nTheorem 2.1 (Valiants Holant Theorem [25]). For any T  GL2 (C),\nHolant(; F | G ) = Holant((cid:48) ; F T | T 1G ).\n\nTherefore, a holographic transformation does not change the complexity of the Holant problem\nin the bipartite setting. Clearly, this theorem holds for planar instances.\nDenition 2.2. We say a signature set F is C -transformable if there exists a T  GL2 (C) such\nthat (0, 1, 1, 0)T 2  C and T 1F  C .\nThis denition is important because if Pl-Holant(C ) is tractable, then Pl-Holant((cid:54)=2 | F ) is\ntractable for any C -transformable set F .\n\n2.4 Polynomial Interpolation\n\nLemma 2.3. Let f be a 4-ary signature with the signature matrix M (f ) =\n\n.\n\n(cid:21)\n(cid:20) 0 0 0 1\nPolynomial interpolation is a powerful technique to prove #P-hardness for counting problems. We\nintroduce this technique with the following lemmas.\n(cid:20) 0 0 0 1\n(cid:21)\n, where b (cid:54)= 0\n0 b 0 0\n0 0 b 0\n1 0 0 0\n0 1 0 0\n0 0 1 0\n1 0 0 0\n\nis not a root of unity. Let 1 be a 4-ary signature with the signature matrix M (1 ) =\nThen for any signature set F containing f , we have\nPl-Holant( (cid:54)=2 | F  {1}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).\n\n8\n\n\fM (f2s+1 ) = M (f )(N M (f ))2s =\n\nFigure 6: A chain of 2s + 1 many copies of f linked by double Disequality N\nProof. We construct a series of gadgets f2s+1 by a chain of 2s + 1 many copies of f linked by\n0\n .\ndouble Disequality N (See Figure 6). We know f2s+1 has the signature matrix\n0\n0 b2s+1\n0\n0\n1\n0\nThe matrix M (f2s+1 ) has a good form for polynomial interpolation. Suppose 1 appears m times\nin an instance  of Pl-Holant( (cid:54)=2 | F  {1}). We replace each appearance of 1 by a copy of the\ngadget f2s+1 to get an instance 2s+1 of Pl-Holant( (cid:54)=2 | F  {f2s+1}), which is also an instance of\nPl-Holant( (cid:54)=2 | F ). We divide 2s+1 into two parts. One part consists of m signatures f2s+1 and\nits signature is represented by (M (f2s+1 ))m . Here we rewrite (M (f2s+1 ))m as a column vector.\nThe other part is the rest of 2s+1 and its signature is represented by A which is a tensor expressed\nas a row vector. Then, the Holant value of 2s+1 is the dot product (cid:104)A, (M (f2s+1 ))m (cid:105), which is\na summation over 4m bits. That is, the value of the 4m edges connecting the two parts. We can\nstratify all 0, 1 assignments of these 4m bits having a nonzero evaluation of a term in Pl-Holant2s+1\ninto the following categories:\n There are i many copies of f2s+1 receiving inputs 0011 or 1100;\n There are j many copies of f2s+1 receiving inputs 0110 or 1001;\nwhere i + j = m.\nFor any assignment in the category with parameter (i, j ), the evaluation of (M (f2s+1 ))m is\nclearly b(2s+1)j . Let aij be the summation of values of the part A over all assignments in the\ncategory (i, j ). Note that aij is independent from the value of s since we view the gadget f2s+1 as\n(cid:88)\na block. Since i + j = m, we can denote aij by aj . Then, we rewrite the dot product summation\nand get\nPl-Holant2s+1 = (cid:104)A, (M (f2s+1 ))m (cid:105) =\n0(cid:54)j(cid:54)m\n(cid:88)\nUnder this stratication, the Holant value of Pl-Holant(, (cid:54)=2 | F  {1}) can be represented as\nPl-Holant = (cid:104)A, (M (1 ))m (cid:105) =\naj .\n0(cid:54)j(cid:54)m\n\n\nSince b (cid:54)= 0 is not a root of unity, the Vandermonde coecients matrix\n  \nbm\nb1\nb0\n  \n(b3 )m\n(b3 )1\n(b3 )0\n...\n...\n...\n...\n  \n(b2m+1 )m\n(b2m+1 )1\n(b2m+1 )0\n\n1\n0\n0\n0\nb2s+1 0\n0\n0\n\naj b(2s+1)j .\n\n9\n\n\fpolynomial time and obtain the value of p(x) = (cid:80)\nhas full rank. By oracle querying the values of Pl-Holant2s+1 , we can solve the coecients aj in\naj xj for any x. Let x = 1, we get Pl-Holant .\n(cid:20) 0 0 0 1\n(cid:21)\n0(cid:54)j(cid:54)m\nTherefore, we have Pl-Holant((cid:54)=2 | F  {1}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).\n0 b 0 0\n0 0 b 0\n1 0 0 0\n\nCorollary 2.4. Let f be a 4-ary signature with the signature matrix M (f ) =\n\n(cid:20) 0 0 0 1\n(cid:21)\n, where b (cid:54)=\n0 1 0 0\n0 0 1 0\n1 0 0 0\n\n.\n\nM (f2s+1 ) = M (f )(N M (f ))2s =\n\n0 is not a root of unity. Let 2 be a 4-ary signature with the signature matrix M (2 ) =\nThen for any signature set F containing f , we have\nPl-Holant( (cid:54)=2 | F  {2}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).\n .\n 0\nProof. We still construct a series of gadgets f2s+1 by a chain of odd copies of f linked by\ndouble Disequality N . We know f2s+1 has the signature matrix\n1\n0\n0\nb2s+1\n0\n0\n0\nb2s+1 0\n0\n0\n1\n0\n0\n0\nSuppose 2 appears m times in an instance  of Pl-Holant((cid:54)=2 | f  2 ). We replace each appearance\nof 2 by a copy of the gadget f2s+1 to get an instance 2s+1 of Pl-Holant((cid:54)=2 | F  {f2s+1}).\nSame as the proof of Lemma 2.3, we divide 2s+1 into two parts. One part is represented by\n(M (f2s+1 ))m and the other part is represented by A. Then, the Holant value of 2s+1 is the dot\nproduct (cid:104)A, (M (f2s+1 ))m (cid:105). We can stratify all 0, 1 assignments of these 4m bits having a nonzero\nevaluation of a term in Pl-Holant2s+1 into the following categories:\n There are i many copies of f2s+1 receiving inputs 0011;\n There are j many copies of f2s+1 receiving inputs 0110 or 1001;\n There are k many copies of f2s+1 receiving inputs 1100;\nwhere i + j + k = m.\nFor any assignment in those categories with parameters (i, j, k) where k  0(mod 2), the eval-\nuation of (M (f2s+1 ))m is clearly (1)k b(2s+1)j = b(2s+1)j . And for any assignment in those cat-\negories with parameters (i, j, k) where k  1(mod 2), the evaluation of (M (f2s+1 ))m is clearly\n(1)k b(2s+1)j = b(2s+1)j . Since i + j + k = m, the index i is determined by j and k . Let aj 0\nbe the summation of values of the part A over all assignments in those categories (i, j, k) where\nk  0(mod 2), and aj 1 be the summation of values of the part A over all assignments in those\ncategories (i, j, k) where k  1(mod 2). Note that aj 0 and aj 1 are independent from the value of\n(cid:88)\n(cid:88)\ns. Let aj = aj 0  aj 1 . Then, we rewrite the dot product summation and get\nPl-Holant2s+1 = (cid:104)A, (M (f2s+1 ))m (cid:105) =\n(aj 0 b(2s+1)j  aj 1 b(2s+1)j ) =\n0(cid:54)j(cid:54)m\n0(cid:54)j(cid:54)m\n(cid:88)\n(cid:88)\nUnder this stratication, the Holant value of Pl-Holant (; (cid:54)=2 | f  2 ) can be represented as\nPl-Holant = (cid:104)A, (M (2 ))m (cid:105) =\n(aj 0  aj 1 ) =\naj .\n0(cid:54)j(cid:54)m\n0(cid:54)j(cid:54)m\nsolve the coecients in polynomial time and obtain the value of p(x) = (cid:80)\nSince b (cid:54)= 0 is not a root of unity, the Vandermonde coecients matrix has full rank. Hence we can\naj xj for any x. Let\n0(cid:54)j(cid:54)m\nx = 1, we get Pl-Holant . Therefore, we have Pl-Holant( (cid:54)=2 | F  {2}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).\n\naj b(2s+1)j .\n\n10\n\n\fLemma 2.5. Let g = (0, 1, t, 0)T be a binary signature, where t (cid:54)= 0 is not a root of unity. Then\nPl-Holant (cid:0) (cid:54)=2 | F  {g (cid:48)}(cid:1) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .\nfor any binary signature g (cid:48) of the form (0, 1, t(cid:48) , 0)T and any signature set F containing g , we have\nInductively, for any nite signature set B consisting of binary signatures of the form (0, 1, t(cid:48) , 0)T\nand any signature set F containing g , we have\nPl-Holant ((cid:54)=2 | F  B) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .\nProof. Note that M (g) = [ 0 1\nt 0 ]. Connect the variable x2 of a copy of g with the variable x1\n(cid:20) 0\n(cid:21) (cid:20)0 1\n(cid:21) (cid:20)0 1\n(cid:20)0 1\n(cid:21)\n(cid:21)\nof another copy of g using ( (cid:54)=2 ). We get a signature g2 with the signature matrix\nM (g2 ) = Mx1 ,x2 (g)M ( (cid:54)=2 )Mx1 ,x2 (g) =\n1\nt2 0\nt 0\n1 0\nt 0\nThat is, g2 = (0, 1, t2 , 0)T . Recursively, we can construct gs = (0, 1, ts , 0)T for s  N. Here, g1\ndenotes g . Given an instance (cid:48) of Pl-Holant ((cid:54)=2 | F  {g (cid:48)}), same as the proof in Lemma 2.3, we\ncan replace each appearance of g (cid:48) by gs and get an instance s of Pl-Holant ((cid:54)=2 | F ). Similarly, the\n(cid:88)\nHolant value of s can be represented as\n0(cid:54)j(cid:54)m\n(cid:88)\nwhile the Holant value of (cid:48) can be represented as\n0(cid:54)j(cid:54)m\nSince t (cid:54)= 0 is not a root of unity, we know all ts are distinct, which means the Vandermonde\nthe value of p(x) = (cid:80)\ncoecients matrix has full rank. Hence, we can solve the coecients in polynomial time and obtain\naj xj for any x. Let x = t(cid:48) , we get Pl-Holant(cid:48) . Therefore, we have\n0(cid:54)j(cid:54)m\nPl-Holant( (cid:54)=2 | F  {g (cid:48)}) (cid:54)T Pl-Holant( (cid:54)=2 | F ). The second part of this lemma follows directly by\nthe rst part.\n\nPl-Holant(cid:48) =\n\nPl-Holants =\n\n=\n\n.\n\naj (ts )j ,\n\naj (t(cid:48) )j .\n\nRemark: Note that the reason why the interpolation can succeed is that we can construct poly-\nnomially many binary signatures gs of the form (0, 1, ts , 0)T , where all ts are distinct such that the\nVandermonde coecients matrix has full rank. According to this, we have the following corollary.\nCorollary 2.6. Given a signature set F , if we can use F to construct polynomial ly many distinct\nbinary signatures gs = (0, 1, ts , 0)T , then for any nite signature set B consisting of binary signatures\nof the form (0, 1, t(cid:48) , 0)T , we have\nPl-Holant ((cid:54)=2 | F  B) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .\nplane (cid:98)C = C  {}, the complex plane plus a point at innity, is a rational function of the form\nIn Lemma 6.4, we will show how to construct polynomially many distinct binary signatures gs =\n(0, 1, ts , 0)T using Mobius transformation[1]. A Mobius transformation of the extended complex\nz  az+b\ncz+d of complex variable z, where the coecients a, b, c, d are complex numbers satisfying\n\n11\n\n\fdet (cid:2) a b\n(cid:3) = ad  bc (cid:54)= 0. It is a bijective conformal map. In particular, a Mobius transformation\nc d\nmapping the unit circle S 1 = {z | |z | = 1} to itself is of the form (z) = ei (z+)\n1+ z denoted\nby M(, ei ), where || (cid:54)= 1. When || < 1, it maps the interior of S 1 to the interior, while\nwhen || > 1, it maps the interior of S 1 to the exterior. A Mobius transformation is completely\ndetermined by its values on any 3 distinct points.\nAn interpolation proof based on a lattice structure will be given in Lemma 6.1, where the\nfollowing lemma is used.\nLemma 2.7. [5] Suppose ,   C  {0}, and the lattice L = {(j, k)  Z2 | j  k = 1} has the form\n(cid:1), then\nj,k0, j+km (j  k )(cid:96)xj,k for (cid:96) = 1, 2, . . . (cid:0)m+2\ns t = 1. If we are given the values N(cid:96) = (cid:80)\nL = {(ns, nt) | n  Z}, where s, t  Z and (s, t) (cid:54)= (0, 0). Let  and  be any numbers satisfying\nwe can compute (cid:80)\n2\nj,k0, j+km j k xj,k in polynomial time.\n2.5 Tractable Signature Sets\n\nWe give some signatures that are known to be computable in polynomial time, called tractable.\nThere are three families: ane signatures, product-type signatures, and matchgate signatures [4].\n\nAne Signatures\n\nDenition 2.8. For a signature f of arity n, the support of f is\nsupp(f ) = {(x1 , x2 , . . . , xn )  Zn\n2 | f (x1 , x2 , . . . , xn ) (cid:54)= 0}.\n\nDenition 2.9. A signature f (x1 , . . . , xn ) of arity n is ane if it has the form\n  AX=0  iQ(X ) ,\nwhere   C, X = (x1 , x2 , . . . , xn , 1), A is a matrix over Z2 , Q(x1 , x2 , . . . , xn )  Z4 [x1 , x2 , . . . , xn ]\nis a quadratic (total degree at most 2) multilinear polynomial with the additional requirement that\n(cid:88)\nn(cid:88)\nthe coecients of al l cross terms are even, i.e., Q has the form\n1i<jn\nk=1\n\nQ(x1 , x2 , . . . , xn ) = a0 +\n\nak xk +\n\n2bij xixj ,\n\nand  is a 0-1 indicator function such that AX=0 is 1 i AX = 0. We use A to denote the set of\nal l ane signatures.\nFollows by the denition directly, one can check the following two lemmas.\nmatrix M (g) = a (cid:2) iq00 iq01\n(cid:3), where q00 , q01 , q10 , q11  N and q00 + q01 + q10 + q11  0(mod 2).\nLemma 2.10. Let g be a binary signature with support of size 4. Then, g  A i g has the signature\niq1 (cid:3), where q0 , q1  N.\nM (h) = a (cid:2)iq0\niq10 iq11\nLemma 2.11. Let h be a unary signature with support of size 2. Then, h  A i h is of the form\n\n12\n\n\fProduct-Type Signatures\n\nDenition 2.12. A signature on a set of variables X is of product type if it can be expressed as\na product of unary functions, binary equality functions ([1, 0, 1]), and binary disequality functions\n([0, 1, 0]), each on one or two variables of X . We use P to denote the set of product-type functions.\nTheorem 2.13. Let F be any set of complex-valued signatures in Boolean variables. If F  A or\nP , then Holant( (cid:54)=2 | F ) is tractable.\nProblems dened by A are tractable essentially by Gauss sums [2]. Problems dened by P are\ntractable by a propagation algorithm.\n\nMatchgate Signatures Matchgates were introduced by Valiant [23, 24] to give polynomial-time\nalgorithms for a collection of counting problems over planar graphs. As the name suggests, problems\nexpressible by matchgates can be reduced to computing a weighted sum of perfect matchings.\nThe latter problem is tractable over planar graphs by Kasteleyns algorithm [16], a.k.a. the FKT\nalgorithm [21, 15]. These counting problems are naturally expressed in the Holant framework using\nmatchgate signatures. We use M to denote the set of all matchgate signatures; thus Pl-Holant(M )\nis tractable.\nThe parity of a signature is even (resp. odd) if its support is on entries of even (resp. odd)\nHamming weight. We say a signature satises the even (resp. odd) Parity Condition if all entries\nof odd (resp. even) weight are zero. For signatures of arity no more than 4, the matchgate signatures\nare characterized by the following lemma.\nLemma 2.14. (cf. Lemma 2.3, Lemma 2.4 in [3]) If f has arity (cid:54) 3, then f  M i f satises\nthe Parity Condition.\n ,\nf0000\nIf f has arity 4 and f satises even Parity Condition, i.e.,\n0\n0\nf1100\n\n0\n0\nf0110 f0101\nf1010 f1001\n0\n0\n\nMx1 x2 ,x4 x3 (f ) =\n\nf0011\n0\n0\nf1111\n\nthen f  M i\ndet MOut (f ) = f0000f1111  f1100f0011 = f0110f1001  f1010f0101 = det MIn (f )\n\nHolographic transformations extend the reach of the FKT algorithm even further, as stated\nbelow.\nTheorem 2.15. Let F be any set of complex-valued signatures in Boolean variables.\nIf F is\n(cid:3). Follows by the denition directly,\n(cid:2) 1 1\nM -transformable, then Pl-Holant( (cid:54)=2 | F ) is tractable.\nRecall the signature class (cid:99)M = H2M , where H2 = 1\n1 1\n(cid:21)\n(cid:20) 0 0 0 a\n2\nOne can check the following lemmas:\nf  (cid:99)M .\n0 b c 0\nLemma 2.16. A signature f with the signature matrix M (f ) =\n0 z y 0\nx 0 0 0\n\nis M -transformable i\n\n13\n\n\fg10 g11 ] is in (cid:99)M i g00 = \u0001g11 and\nLemma 2.17. A signature g with the signature matrix M (g) = [ g00 g01\n(cid:20) 0 0 0 0\n(cid:21)\ng01 = \u0001g10 , where \u0001 = 1.\nis in (cid:99)M i b = \u0001y and\n0 b c 0\nLemma 2.18. A signature f with the signature matrix M (f ) =\n0 z y 0\n(cid:20) 0 0 0 a\n(cid:21)\nc = \u0001z , where \u0001 = 1.\n0 0 0 0\n, where abxy (cid:54)= 0, then f / (cid:99)M .\n0 b 0 0\n0 0 y 0\nx 0 0 0\n\nLemma 2.19. If f has the signature matrix M (f ) =\n\n2.6 Known Dichotomies and Hardness Results\nDenition 2.20. A 4-ary signature is non-singular redundant i in one of its four 4  4 signature\nf0000 f0010 f0011\n (cid:54)= 0.\nmatrices, the midd le two rows are identical and the midd le two columns are identical, and the\ndeterminant\nf0100 f0110 f0111\nf1100 f1110 f1111\nTheorem 2.21. [6] If f is a non-singular redundant signature, then Pl-Holant( (cid:54)=2 |f ) is #P-hard.\nTheorem 2.22. [17] Let G be a connected planar graph and E O(H ) be the set of al l Eulerian\n(cid:88)\nOrientation of the medial graph H = H (G). Then\nOE O(H )\n\n2 (O) = 2T (G; 3, 3),\n\ndet\n\nwhere  (O) is the number of sadd le vertices in orientation O, i.e., vertices in which the edges are\nRemark: Note that (cid:80)\noriented in, out, in, out in cyclic order.\n(cid:21)\n(cid:20) 0 0 0 1\nOE O(H ) 2 (O) can be expressed as Pl-Holant( (cid:54)=2 | f ), where f has the\n. Therefore, we have Pl-Holant( (cid:54)=2 | f ) is #P-hard.\n0 1 2 0\nsignature matrix M (f ) =\n0 2 1 0\n1 0 0 0\nPl-#CSP(F ) is #P-hard unless F  A , F  P , or F  (cid:99)M , in which case the problem is\nTheorem 2.23. [4] Let F be any set of complex-valued signatures in Boolean variables. Then\ncomputable in polynomial time. If F  A or F  P , then #CSP(F ) is computable in polynomial\n(cid:21)\n(cid:20) 0 0 0 a\ntime without planarity; otherwise #CSP(F ) is #P-hard.\n0 b c 0\n0 z y 0\nx 0 0 0\n\nTheorem 2.24. [5] Let f be a 4-ary signature with the signature matrix M (f ) =\nHolant ((cid:54)=2 | f ) is #P-hard except for the fol lowing cases:\n f  P ;\n f  A ;\n there is a zero in each pair (a, x), (b, y), (c, z );\nin which cases Holant ((cid:54)=2 | f ) is computable in polynomial time.\n\n, then\n\n14\n\n\f3 Main Theorem and Proof Outline\n\n(cid:20) 0 0 0 a\n(cid:21)\n, where a, b, c, x, y , z \n0 b c 0\nTheorem 3.1. Let f be a signature with the signature matrix M (f ) =\n0 z y 0\nC. Then Pl-Holant( (cid:54)=2 | f ) is polynomial time computable in the fol lowing cases, and #P-hard oth-\nx 0 0 0\nerwise:\n1. f  P or A ;\n3. f  M or (cid:99)M ;\n2. There is a zero in each pair (a, x), (b, y), (c, z );\n4. c = z = 0 and\n(i). (ax)2 = (by)2 , or\n\n\n, where ,  ,   N, and    (mod 2);\n\n\n(ii). x = ai , b = a\ni\ni\n, and y = a\nIf f satises condition 1 or 2, then Holant( (cid:54)=2 | f ) is computable in polynomial time without the\nplanarity restriction; otherwise (the non-planar) Holant( (cid:54)=2 | f ) is #P-hard.\nLet N be the number of zeros in {a, b, c, x, y , z}. If N (cid:62) 3, then either there is a zero pair, or\nwe have N = 3 and each pair has exactly one zero. We dene Case I as N = 3 with each pair\nhaving exactly one zero. We dene Case II as having a zero pair, which also includes some cases of\nN = 2. For the remaining cases of N = 2 the two zeros are in dierent pairs. In particular there\nis an outer pair that has a single zero. We dene Case III to be N = 2 and having no zero pair,\nor N = 1 and the zero is in an outer pair. In Case III an outer pair has exactly one zero, and the\nother two pairs together have at most one zero. Then we dene Case IV as having N = 1 and the\nzero is in an inner pair, or N = 0.\nCase I is tractable, even for non-planar Holant((cid:54)=2 | f ) (see [5]).\nIn Case II, depending on whether the zero pair is inner or outer we have two dierent connections\nto #CSP. A previously established connection to #CSP [5] can be adapted in the planar setting\nto the case with a zero outer pair. This connection is a local transformation, and we observe that\nit preserves planarity. A signicantly more involved non-local connection to #CSP is discovered in\nthis paper when the inner pair is zero (and no outer pair is zero). We show that by the support\nstructure of the signature we can dene a set of circuits, which forms a partition of the edge set.\nThere are exactly two valid congurations along such a circuit, corresponding to its two cyclic\norientations. These circuits may intersect in complicated ways, including self-intersections. But\nwe can dene a #CSP problem, where the variables are these circuits, and their edge functions\nexactly account for the intersections. We show that Pl-Holant( (cid:54)=2 | f ) is equivalent to these #CSP\nproblems, which are non-planar in general. However, crucially, because Pl-Holant( (cid:54)=2 | f ) is planar,\nevery two such circuits must intersect even times. Due to the planarity of Pl-Holant((cid:54)=2 | f ) we can\nexactly carve out a new class of tractable problems via this non-local #CSP connection.\nFor #P-hardness proofs in this paper, one particularly dicult case is in Lemma 6.4. In this\ncase, all constructable binary signatures correspond to points on the unit circle S 1 , and any iteration\nof the construction amounts to mapping this point by a Mobius transformation which preserves S 1 .\nThe 4 Cases are dened formally as follows:\nI. There is exactly one zero in each pair. In this case, Holant ((cid:54)=2 | f ) is tractable, proved in [5].\nII. There is a zero pair:\nif f  P , A , M or (cid:99)M , and is #P-hard otherwise.\n1. An outer pair (a, x) or (b, y) is a zero pair. We prove that Pl-Holant((cid:54)=2 | f ) is tractable\nIn this Case II.1, we can rotate the signature f such that the matrix MOut (f ) is the\n\n15\n\n\fIII.\n\nIV.\n\nzero matrix. We reduce Pl-#CSP(MIn (f )) to Pl-Holant ( (cid:54)=2 | f ) via a local replacement\n(Lemma 4.2). We apply the dichotomy of Pl-#CSP to get #P-hardness (Theorem 4.3).\nTractability of Pl-Holant ( (cid:54)=2 | f ) follows from known tractable signatures.\n2. The inner pair (c, z ) is a zero pair and no outer pair is a zero pair. We prove that\nPl-Holant( (cid:54)=2 | f ) is #P-hard unless f satises condition 4, in which it is tractable.\nThis is the non-local reduction described above. The tractable condition 4 is previously\nunknown. (Curiously, in Case II.2, condition 4 subsumes f  M .)\n1. There are exactly two zeros and they are in dierent pairs;\n2. There is exactly one zero and it is in an outer pair.\nWe prove that Pl-Holant( (cid:54)=2 | f ) is #P-hard unless f  M , in which case it is tractable.\n(cid:20) 0 0 0 0\n(cid:21)\nIn this case, there is a single zero in an outer pair. By connecting two copies of the signature\nf , we can construct a 4-ary signature f1 such that one outer pair is a zero pair. When f / M ,\n0 0 1 0\nwe can realize a signature M (g) =\nby interpolation using f1 (Lemma 5.1). This g\n0 1 0 0\n0 0 0 0\ncan help us extract MIn (f ). By connecting f and g , we can construct a signature that\nbelongs to Case II. We then prove #P-hardness using the result of Case II (Theorem 5.2).\n1. There is exactly one zero and it is in the inner pair;\n2. All values in {a, x, b, y , c, z} are nonzero.\nWe prove that Pl-Holant( (cid:54)=2 | f ) is #P-hard unless f  M , in which it is tractable.\nAssume f (cid:54) M . The main idea is to use Mobius transformations. However, there are some\nsettings where we cannot do so, either because we dont have the initial signature to start the\nprocess, or the matrix that would dene the Mobius transformation is singular. So we rst\ntreat the following two special cases.\n If a = \u0001x, b = \u0001y and c = \u0001z , where \u0001 = 1, by interpolation based on a lattice structure,\n If det (cid:2) b c\n(cid:3) = 0 or det [ a z\neither we can realize a non-singular redundant signature or reduce from the evaluation\nof the Tutte polynomial at (3, 3), both of which are #P-hard (Lemma 6.1).\nc x ] = 0, then either we can realize a non-singular redundant\nz y\nsignature or a signature that is #P-hard by Lemma 6.1 (Lemma 6.2).\nIf f does not belong to the above two cases, we want to realize binary signatures of the form\n(0, 1, t, 0)T , for arbitrary values of t. If this can be done, by carefully choosing the values of t,\nwe can construct a signature that belongs to Case III and it is #P-hard when f / M (Lemma\n6.3). We realize binary signatures by connecting f with ( (cid:54)=2 ). This corresponds naturally to\na Mobius transformation. By discussing the following dierent forms of binary signatures we\nget, we can either realize arbitrary (0, 1, t, 0)T or a signature belonging to Case II.2 that does\nnot satisfy condition 4, therefore is #P-hard (Theorem 6.8).\n If we can get a signature of the form g = (0, 1, t, 0)T where t (cid:54)= 0 is not a root of unity,\nthen by connecting a chain of g , we can get polynomially many distinct binary signatures\ngi = (0, 1, ti , 0)T . Then, by interpolation, we can realize arbitrary binary signatures of\nthe form (0, 1, t(cid:48) , 0)T .\n Suppose we can get a signature of the form (0, 1, t, 0)T , where t (cid:54)= 0 is an n-th primitive\nBut we can relate f to two Mobius transformations due to det (cid:2) b c\n(cid:3) (cid:54)= 0 and det [ a z\nroot of unity (n (cid:62) 5). Now, we only have n many distinct signatures gi = (0, 1, ti , 0)T .\nc x ] (cid:54)= 0.\nz y\nFor each Mobius transformation , we can realize the signatures g = (0, 1, (ti ), 0)T . If\non the extended complex plane (cid:98)C, it can map at most two points of S 1 to 0 or . Hence,\n|(ti )| (cid:54)= 0, 1 or  for some i, then this is treated above. Otherwise, since  is a bijection\n\n16\n\n\f|(ti )| = 1 for at least three ti . But a Mobius transformation is determined by any three\ndistinct points. This implies that  maps S 1 to itself. Such  have a known special\n. By exploiting its property we can construct a signature f (cid:48) such that\nform ei z + \n1 + z\nits corresponding Mobius transformation (cid:48) denes an innite group. This implies that\n(cid:48)k (t) are all distinct. Then, we can get polynomially many distinct binary signatures\n(0, 1, (cid:48)k (t), 0), and realize arbitrary binary signatures of the form (0, 1, t(cid:48) , 0)T (Lemma\n6.4).\n Suppose we can get a signature of the form (0, 1, t, 0)T where t (cid:54)= 0 is an n-th primitive\nroot of unity (n = 3, 4). Then we can either relate it to two Mobius transformations\nmapping the unit circle to itself, or realize a double pinning (0, 1, 0, 0)T = (1, 0)T  (0, 1)T\n(Corollary 6.5).\n Suppose we can get a signature of the form (0, 1, 0, 0)T . By connecting f with it, we can\nget new signatures of the form (0, 1, t, 0)T . Similarly, by analyzing the value of t, we can\neither realize arbitrary binary signatures of the form (0, 1, s, 0)T , or realize a signature\nthat belongs to Case II.2, which is #P-hard (Lemma 6.6).\n Suppose we can only get signatures of the form (0, 1, 1, 0). That implies a = \u0001x, b = \u0001y\nand c = \u0001z , where \u0001 = 1. This has been treated before.\nThese 4 Cases above cover all possibilities. If N  3, then there is a zero pair or there is exactly\none zero in each pair. This is either in Case I or Case II. If N = 2, it is either in Case II or Case\nIII.1. If N = 1, it is either in Case III.2 or Case IV.1. If N = 0, it is in Case IV.2.\n\n4 Case II: One Zero Pair\n\nIf an outer pair is a zero pair, by rotational symmetry, we may assume (a, x) is a zero pair.\n0 0 0 0\n ,\nDenition 4.1. Given a 4-ary signature f with the signature matrix\n0 b\nc 0\nM (f ) =\n(4.1)\n0 z y 0\nlet fIn denote the binary signature of the form M (fIn ) = MIn (f ) = (cid:2) b c\n(cid:3) . Given a set F consisting\n0 0 0 0\nz y\nof signatures of the form (4.1), let FIn denote the signature set {fIn | f  F }.\nLemma 4.2. Let F be a set consisting of signatures of the form (4.1). Then,\nPl-#CSP(FIn ) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .\n\nProof.[5] We prove this reduction in two steps. In each step, we begin with a signature grid\nand end with a new signature grid such that the Holant values of both signature grids are the same.\nFor step one, let G = (U, V , E ) be a planar bipartite graph representing an instance of Pl-#CSP(FIn ),\nwhere each u  U is a variable, and each v  V has degree two and is labeled by some fIn  FIn . We\ndene a cyclic order of the edges incident to each vertex u  U , and decompose u into k = deg(u)\nvertices. Then we connect the k edges originally incident to u to these k new vertices so that each\nvertex is incident to exactly one edge. We also connect these k new vertices in a cycle according to\n\n17\n\n\fu\n\nu(cid:48)\n\nu\n\nu(cid:48)\n\n(a)\n(b)\n(c)\nFigure 7: The reduction from #Pl-CSP(fIn ) to Pl-Holant( (cid:54)=2 | f ). The circle vertices are\nlabeled by (=d ), where d is the degree of the corresponding vertex, the diamond vertices are\nlabeled by fIn , the triangle vertices are labeled by the corresponding f , and the square vertices\nare labeled by ((cid:54)=2 ).\n\nthe cyclic order (see Figure 7b). Thus, in eect we have replaced u by a cycle of length k = deg(u).\n(If k = 1 there is a self-loop.) Each of k vertices has degree 3, and we label them by (=3 ). Clearly\nthis preserves planarity and does not change the value of the partition function. The resulting\ngraph has the following properties: (1) every vertex has either degree 2 or degree 3; (2) each degree\n2 vertex is connected to degree 3 vertices; (3) each degree 3 vertex is connected to exactly one\ndegree 2 vertex.\n\nu\n\nf\n\nx1 x4\nx2 x3\nu(cid:48)\n\nu\n\nf\n\nx1 x4\nx2 x3\nu(cid:48)\n\nu\n\nf\n\nx1 x4\nx2 x3\nu(cid:48)\n\nu\n\nf\n\nx1 x4\nx2 x3\nu(cid:48)\n\n(a) fIn00f0110\n\n(b) fIn01f0101\n\n(c) fIn10f1010\n\n(d) fIn11f1001\n\nFigure 8: Assign input variables of f : Suppose the binary signature fIn is applied to (the\nordered pair) (u, u(cid:48) ). The variables u and u(cid:48) have been replaced by cycles of length deg(u) and\ndeg(u(cid:48) ) respectively. For the cycle Cu representing a variable u, we associate the value u = 0\nwith a clockwise orientation, and u = 1 with a counter-clockwise orientation. Then by the\nsupport of f , (x1 , x4 ) can only take assignment (0, 1) or (1, 0), and similarly (x2 , x3 ) can only\ntake assignment (0, 1) or (1, 0).\nNow step two. For every v  V , v has degree 2 and is labeled by some fIn . We contract the\ntwo edges incident to v . The resulting graph G(cid:48) = (V (cid:48) , E (cid:48) ) is 4-regular and planar. We put a node\non every edge of G(cid:48) and labet it by ((cid:54)=2 ) (see Figure 7c). Next, we assign the corresponding f to\nevery v (cid:48)  V (cid:48) after this contraction. The input variables x1 , x2 , x3 , x4 are carefully assigned at each\ncopy of f (as illustrated in Figure 8) such that there are exactly two congurations to each original\ncycle, which correspond to cyclic orientations, due to the ( (cid:54)=2 ) on it and the support set of f . These\n\n18\n\n\fcyclic orientations correspond to the {0, 1} assignments at the original variable u  U . Under this\none-to-one correspondence, the value of fIn is perfectly mirrored by the value of f . Therefore, we\nhave Pl-#CSP(FIn ) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .\nis #P-hard unless F  P , F  A , or F  (cid:99)M , in which cases the problem is tractable.\nTheorem 4.3. Let F be a set consisting of signatures of the form (4.1). Then Pl-Holant( (cid:54)=2 | F )\nTractability follows by Theorems 2.13 and 2.15. For any f  F , notice that the\nProof.\nsupport of f is on x1 (cid:54)=x2 and x3 (cid:54)=x4 , where  is the indicator function. We have\nf (x1 , x2 , x3 , x4 ) = fIn (x1 , x4 )  x1 (cid:54)=x2  x3 (cid:54)=x4 .\nThus, FIn  P or A is equivalent to F  P or A . In addition, by Lemmas 2.17 and 2.18, FIn  (cid:99)M\nis equivalent to F  (cid:99)M . Therefore, if F (cid:42) P , A or (cid:99)M , then FIn (cid:42) P , A or (cid:99)M . By Theorem\n2.23, Pl-#CSP(FIn ) is #P-hard, and then by Lemma 4.2, Pl-Holant ( (cid:54)=2 | F ) is #P-hard.\nRemark: One may observe that if f  M , then Pl-Holant ( (cid:54)=2 | F ) is also tractable as F and (=2 )\nare both realized by matchgates. However, Theorem 4.3 already accounted for this case because\nfor F consisting of signatures of the form (4.1), F  M implies F  P .\nNow, we consider the case that the inner pair is a zero pair and no outer pair is a zero pair.\n ,\n0 0 0 a\nDenition 4.4. Given a 4-ary signature f with the signature matrix\n0 b 0 0\n0 0 y 0\nx 0 0 0\n(cid:20)ak1+(cid:96)1 yk2+(cid:96)2 xk3+(cid:96)3 bk4+(cid:96)4 ak2+(cid:96)4 yk3+(cid:96)1 xk4+(cid:96)2 bk1+(cid:96)3\n(cid:21)\nwhere (a, x) (cid:54)= (0, 0) and (b, y) (cid:54)= (0, 0), let Gf denote the set of al l binary signatures gf of the form\nM (gf ) =\nak4+(cid:96)2 yk1+(cid:96)3 xk2+(cid:96)4 bk3+(cid:96)1 ak3+(cid:96)3 yk4+(cid:96)4 xk1+(cid:96)1 bk2+(cid:96)2\nwhere k1 , k2 , k3 , k4 , (cid:96)1 , (cid:96)2 , (cid:96)3 , (cid:96)4  N, and k = k1 + k2 + k3 + k4 = (cid:96)1 + (cid:96)2 + (cid:96)3 + (cid:96)4 = (cid:96), and let Hf\nM (hf ) = (cid:2)am1 ym2 xm3 bm4 am3 ym4 xm1 bm2 (cid:3) ,\ndenote the set of al l unary signatures hf of the form\nwhere m1 , m2 , m3 , m4  N.\n(cid:104) a2 by\n(cid:105)\nLet k = k1 = (cid:96)1 = (cid:96) = 1, we get a specic signature in Gf denoted by g1f , where M (g1f ) =\n(cid:105)\n(cid:104) ax b2\n. Let k = k1 = (cid:96)3 = (cid:96) = 1, we get another specic signature in Gf denoted by g2f , where\nby x2\nM (g2f ) =\n.\ny2 ax\nRemark: For any i, j  {1, 2, 3, 4}, let k = ki = (cid:96)j = (cid:96) = 1, we can get 16 signatures in Gf that\nhave similar signature matrices to M (g1f ) and M (g2f ). In fact, Gf is the closure of the Hadamard\nproduct of these 16 basic signature matrices.\n\nM (f ) =\n\n(4.2)\n\n,\n\n19\n\n\f(4.3)\n\n(4.4)\n\nLemma 4.5. Let f be a signature of the form (4.2). Then,\nIf a2 = x2 (cid:54)= 0, b2 = y2 (cid:54)= 0 and (cid:0) b\n(cid:1)8 (cid:54)= 1, then\nPl-Holant( (cid:54)=2 | f ) (cid:54)T #CSP(Gf  Hf ),\na\n#CSP(g1f , g2f ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).\nProof. We divide the proof into two parts: We show the reduction (4.3) in Part I, and the\nreduction (4.4) in Part II.\nPart I: Since we are considering planar graph, we can view it in a plane. Given a vertex of arity\n4, list the four edges incident to it in counterclockwise order. We say two edges are not adjacent if\nthere is exactly one other edge between them. Given an instance  = (G, ) of Pl-Holant( (cid:54)=2 | f ),\ntwo edges in G are called 2-ary edge twins if they are incident to a vertex of degree 2, and 4-ary\nedge twins if they are incident to a vertex of degree 4 but they are not adjacent. Both 2-ary edge\ntwins and 4-ary edge twins are called edge twins.\nEach edge has an unique 2-ary edge twin at its endpoint of degree 2 and an unique 4-ary edge\ntwin at its endpoint of degree 4. That is, edge twins induce a binary relation and the transitive\nclosure of this relation on an edge forms a circuit. Therefore, graph G can be divided into some\ncircuits C1 , C2 ,    , Ck . Note that Ci may include repeated vertices called self-intersection vertices,\nbut no repeated edges. We pick an edge ei of Ci to be the leader edge of Ci . Given the leader edge\nei , the direction from its endpoint of degree 2 to its endpoint of degree 4 gives an orientation of\nthe circuit Ci . In edge twins, depending on the orientation, we can say one edge is the successor\nof the other edge. When we list the assignment of edges in a circuit, we start with its leader edge\nand follow with the successor of the leader edge, and so forth.\n(cid:89)\n(cid:88)\nFor any nonzero term in the sum\nvV\n :E (cid:55){0,1}\nthe assignment of all edges  : E (cid:55) {0, 1} can be uniquely extended by the assignment of all leader\nedges  (cid:48) : {e1 , e2 ,    ek } (cid:55) {0, 1}. This is because at each vertex v , fv ( |E(v) ) (cid:54)= 0 if and only\nif each pair of edge twins in E(v) is assigned value (0, 1) or (1, 0). More specically, all edges in\nCi take assignment (0, 1, 0, 1,    , 0, 1) when ei = 0 and (1, 0, 1, 0,    , 1, 0) when ei = 1. In other\n(cid:88)\n(cid:89)\nwords, all pairs of 4-ary edge twins in Ci take assignment (0, 1) when ei = 0 and (1, 0) when ei = 1.\nThen, we have\nfv ( (cid:48) |E(v) ).\nvV\n (cid:48) :{e1 , ,ek }(cid:55){0,1}\nLet Vi,j = Ci  Cj (i < j ) be the set of all intersection vertices in Ci and Cj . Since G is a planar\ngraph, |Vi,j |  0 ( mod 2). Let  (cid:48)\n(ei ,ej ) denote the restriction of  (cid:48) on edges ei and ej . Dene binary\n(cid:89)\n(ei ,ej ) : {ei , ej } (cid:55) {0, 1}, we have\nfunction gi,j on ei and ej : Given an assignment  (cid:48)\n(ei ,ej ) |E(v) ).\nfv ( (cid:48)\nvVi,j\n\nfv ( |E(v) ),\n\nPl-Holant =\n\nPl-Holant =\n\ngi,j (ei , ej ) =\n\nSince all edges incident to vertices in Vi,j are either in Ci or Cj , the assignment of these edges can\n(ei ,ej ) . Hence, gi,j is well-dened. We show gi,j  Gf by induction\nbe extended by the assignment  (cid:48)\non the number n of self-intersection vertices in Ci .\n\n20\n\n\f First, n = 0. That is, Ci is a simple cycle without self-intersection. By Jordan Curve\nTheorem, Ci divides the plane into an interior region and an exterior region. According to\nthe orientation of Ci , we denote the left side of Ci to be the interior region and the right side\nto be the exterior region. At a half of intersection vertices in Vi,j , Cj enters the interior of Ci ,\nand at the other half of intersection vertices, Cj exits. We call those vertices entry-vertices\nand exit-vertices separately (See Figure 9).\n\nFigure 9: Intersection vertices between Ci and Cj\n\nFor each vertex in Vi,j , consider the two pairs of edge twins incident to it. We label the\nedge twins in Ci by variables (x1 , x3 ) obeying the orientation of Ci . That is, x3 is always the\nsuccessor of x1 . Hence, all variables (x1 , x3 ) take the same assignment (0, 1) when ei = 0 and\n(1, 0) when ei = 1. Then, the labeling (x2 , x4 ) of edge twins in Cj is uniquely determined by\nthe counterclockwise order of (x1 , x2 , x3 , x4 ). Moreover, at any vertex in Vi,j , the variable x2\nis always in the exterior of Ci (See Figure 9), which means at entry-vertices, x4 is the successor\nof x2 , while at exit-vertices, x2 is the successor of x4 . Therefore, at entry-vertices, variables\n(x2 , x4 ) take assignment (0, 1) when ej = 0 and (1, 0) when ej = 1, while at exit-vertices,\nthey take assignment (1, 0) and (0, 1) correspondingly.\n\n(ei , ej )\n\n(0, 0)\n\n(0, 1)\n\n(1, 1)\n\n(1, 0)\n\nentry-vertices\n\n(x1 , x2 , x3 , x4 )\nf\nf\n2\n(0, 0, 1, 1)\n\na\n\ny\n\n(0, 1, 1, 0)\n\n(1, 1, 0, 0)\n\n(1, 0, 0, 1)\n\nb\n\nx\n\ny\n\na\n\nb\n\nx\n\nf \nx\n\ny\n\na\n\nb\n\n3\n2\n\nf\n\nb\n\nx\n\ny\n\na\n\nexit-vertices\n\n(x1 , x2 , x3 , x4 )\nf\nf\n2\n(0, 1, 1, 0)\n\na\n\nb\n\n(0, 0, 1, 1)\n\n(1, 0, 0, 1)\n\n(1, 1, 0, 0)\n\na\n\ny\n\nx\n\ny\n\nx\n\nb\n\nf \ny\n\nx\n\nb\n\na\n\n3\n2\n\nf\n\nx\n\nb\n\na\n\ny\n\nTable 1: The values of f at intersection vertices\n\nAccording to the dierent assignments of (ei , ej ) as listed in Column 1 of Table 1, Column\n2 and Column 7 (indexed by (x1 , x2 , x3 , x4 )) list the assignments of (x1 , x2 , x3 , x4 ) at entry-\n(cid:21)\n(cid:21)\n(cid:20) 0 0 0 b\n(cid:20) 0 0 0 x\n(cid:20) 0 0 0 y\n(cid:20) 0 0 0 a\n(cid:21)\n(cid:21)\nvertices and exit-vertices separately. Note that under this labeling, signature f has four\nrotation forms:\n0 y 0 0\n0 x 0 0\n0 b 0 0\n0 a 0 0\n0 0 a 0\n0 0 y 0\n0 0 x 0\n0 0 b 0\ny 0 0 0\na 0 0 0\nb 0 0 0\nx 0 0 0\n\n, M (f  ) =\n\nand M (f\n\nM (f ) =\n\n3\n2 ) =\n\n, M (f\n\n\n2 ) =\n\n.\n\n21\n\n\fColumns 3, 4, 5, 6 and Columns 8, 9, 10, 11 list the corresponding values of signature f in\n\n3\n2 , f  and f\nfour forms f , f\n2 separately.\n\nSuppose there are k1 many entry-vertices assigned f , k2 many entry-vertices assigned f\n2 , k3\n3\nmany entry-vertices assigned f  , and k4 many entry-vertices assigned f\n2 , while there are\n\n(cid:96)4 many exit-vertices assigned f , (cid:96)1 many exit-vertices assigned f\n2 , (cid:96)2 many exit-vertices\n3\nassigned f  , and (cid:96)3 many exit-vertices assigned f\n2 . Then, according to the assignments of\n(ei , ej ), the values of gi,j are listed in Table 2 :\n\n(0, 1)\n\n(1, 1)\n\n(1, 0)\n\n3\n2 )(cid:96)3 f (cid:96)4\n\n(ei , ej )\n(0, 0)\n\n\n3\n\ngi,j (ei , ej ) = f k1 (f\n2 )(cid:96)1 (f  )(cid:96)2 (f\n2 )k4 (f\n2 )k2 (f  )k3 (f\nak1 yk2 xk3 bk4 a(cid:96)1 y (cid:96)2 x(cid:96)3 b(cid:96)4\nbk1 ak2 yk3 xk4 y (cid:96)1 x(cid:96)2 b(cid:96)3 a(cid:96)4\nxk1 bk2 ak3 yk4 x(cid:96)1 b(cid:96)2 a(cid:96)3 y (cid:96)4\nyk1 xk2 bk3 ak4 b(cid:96)1 a(cid:96)2 y (cid:96)3 x(cid:96)4\nTable 2: The values of gi,j\n(cid:20)ak1+(cid:96)1 yk2+(cid:96)2 xk3+(cid:96)3 bk4+(cid:96)4 ak2+(cid:96)4 yk3+(cid:96)1 xk4+(cid:96)2 bk1+(cid:96)3\nak4+(cid:96)2 yk1+(cid:96)3 xk2+(cid:96)4 bk3+(cid:96)1 ak3+(cid:96)3 yk4+(cid:96)4 xk1+(cid:96)1 bk2+(cid:96)2\nSince the number of entry-vertices is equal to the number of exit-vertices, k1 + k2 + k3 + k4 =\n(cid:96)1 + (cid:96)2 + (cid:96)3 + (cid:96)4 . Hence, we have gi,j  Gf .\n Then, suppose gi,j  Gf holds for any circuit Ci with at most n many self-intersection vertices.\nCi can be decomposed into two edge-disjoint circuits, each of which has at most n many self-\nintersection vertices (See Figure 10).\n\nM (gi,j ) =\n\nThat is,\n\n(cid:21)\n\n.\n\nFigure 10: Decompose Ci into C 1\ni and C 2\ni .\ni  C 2\nNow, we have Ci = C 1\ni . Each C s\ni (s = 1, 2) is a circuit with at most n many self-\nintersection vertices. The orientation of Ci induces the orientations of C 1\ni and C 2\ni . Clearly,\nthe assignment of edges in C s\ni can be uniquely extended by the assignment of ei . We can still\ni  Cj be the set of all intersection\ni,j = C s\ni . Let V s\ni and C 2\nview ei as the leader edge of both C 1\ni,j  V 2\ni,j  V 2\ni,j = . Same as the denition of\nvertices in C s\ni,j , where V 1\ni and Cj . Then Vi,j = V 1\n(cid:89)\ngi,j , we dene binary function:\nvV s\ni,j\ni,j is well-dened and by induction hypothesis, it is in Gf . Also,\nAs we have showed above, g (s)\n\n(ei ,ej ) |E(v) ).\nfv ( (cid:48)\n\ng (s)\ni,j (ei , ej ) =\n\n22\n\n\fgi,j =\n\nwe have\n\n(ei ,ej ) |E(v) ) =\nfv ( (cid:48)\n\n(cid:89)\n(cid:89)\n(cid:89)\nvVi,j\nvV 2\nvV 1\ni,j\ni,j\nNote that Gf is closed under function multiplication. That is, gi,j  Gf .\nLet Vi be the set of all self-intersection vertices in Ci . Let  (cid:48)\n(ei ) denote the restriction of  (cid:48) on\n(ei ) : ei (cid:55) {0, 1}, we have\nei . Dene unary function hi on ei : Given an assignment  (cid:48)\n(cid:89)\n(ei ) |E(v) ).\nfv ( (cid:48)\nvVi\n\n(ei ,ej ) |E(v) ) = g (1)\nfv ( (cid:48)\ni,j g (2)\ni,j .\n\nhi (ei ) =\n\nThe assignment of those edges incident to vertices in Vi can be uniquely extended by the assignment\n(ei ) . Hence, hi is well-dened. We show hi  Hf .\n (cid:48)\nFor each vertex in Vi , since it is a self-intersection vertex, the two pairs of edge twins incident\nto it are both in Ci . We still label each pair of edge twins by a pair of variables (x1 , x3 ) obeying the\norientation of Ci . That is, x3 is always the successor of x1 . Now, at each vertex in Vi , the four edges\nincident to it are labeled by (x1 , x1 , x3 , x3 ) listed in counterclockwise order. We pick the proper\npair of variables (x1 , x3 ) and change it to (x2 , x4 ) such that the label of four edges is (x1 , x2 , x3 , x4 )\nin counterclockwise order. Clearly, (x2 , x4 ) and (x1 , x3 ) take the same assignment. That is, at each\nvertex in Vi , the assignment of (x1 , x2 , x3 , x4 ) is (0, 0, 1, 1) when ei = 0, and (1, 1, 0, 0) when ei = 1.\nUnder this labeling, signature f still has four rotation forms. The values of signature f in four\nforms are listed in Table 3.\n\nei\n0\n\n1\n\n(x1 , x2 , x3 , x4 )\n(0, 0, 1, 1)\n\n(1, 1, 0, 0)\n\nf\n\na\n\nx\n\n\n2\n\nf\n\ny\n\nb\n\nf \nx\n\na\n\n3\n2\n\nf\n\nb\n\ny\n\nTable 3: The values of f at self-intersection vertices\n\nSuppose there are m1 many vertices assigned f , m2 many vertices assigned f\n3\nassigned f  and m4 many vertices assigned f\n2 . Then, we have\n\n\n2 , m3 many vertices\n\nPl-Holant =\n\nM (hi ) = [am1 ym2 xm3 bm4 am3 ym4 xm1 bm2 ].\nThat is, hi  Hf .\n(cid:32) (cid:89)\n(cid:33)(cid:32) (cid:89)\nFor any vertex v  V , it is either in some Vi,j or some Vi . Thus,\n(cid:88)\nfv ( (cid:48) |E(v) )\n(cid:33)(cid:32) (cid:89)\n(cid:32) (cid:89)\nvVi\nvVi,j\n (cid:48) :{e1 , ,ek }(cid:55){0,1}\n(cid:88)\n1(cid:54)i(cid:54)k\n1(cid:54)i<j(cid:54)k\n (cid:48) :{e1 , ,ek }(cid:55){0,1}\n1(cid:54)i(cid:54)k\n1(cid:54)i<j(cid:54)k\nwhere gi,j  Gf and hi  Hf . Therefore, Pl-Holant((cid:54)=2 | f ) (cid:54)T #CSP(Gf  Hf ).\nHere, we give an example for the reduction (4.3).\n\ngi,j (ei , ej )\n\nhi (ei )\n\n=\n\nfv ( (cid:48) |E(v) )\n(cid:33)\n,\n\n(cid:33)\n\n23\n\n\fFigure 11: An example for the reduction (4.3)\n\nExample. Given an instance  = (G, ) of Pl-Holant ( (cid:54)=2 | f ), there are two circuits C1 (The\nSquare) and C2 (The Horizontal Eight) in G (See Figure 11). Each circuit Ci has a leader\nedge ei . Given the leader, the direction from its endpoint of degree 2 to the endpoint of degree\n4 gives a default orientation of the circuit. Given a nonzero term in the sum Pl-Holant , as a\nconsequence of the support of f , the assignment of edges in each circuit is uniquely determined by\nthe assignment of its leader. That is, any assignment of the leaders  (cid:48) : {e1 , e2} (cid:55) {0, 1} can be\nuniquely extended to an assignment of all edges  : E (cid:55) {0, 1} such that on each circuit, the {0, 1}\nassignments alternate.\nConsider the signatures f on the intersection vertices between C1 and C2 , (fv1 , fv2 , fv3 and fv4 ).\nDue to planarity, there are an even number of intersection vertices. Assume C1 does not have self-\nintersection (as is The Square). Otherwise, we will decompose C1 further and reason inductively.\nWithout self-intersection, C1 has an interior and exterior region (Jordan Curve Theorem) depending\non its default orientation. With respect to C1 , The circuit C2 enters and exits the interior of C1\nalternately. Thus, we can divide the intersection vertices into an equal number of entry-vertices\nand exit-vertices. In this example, fv1 and fv4 are on entry-vertices, while fv2 and fv3 are on\nexit-vertices. By analyzing the values of each f when e1 and e2 take assignment 0 or 1, we can\nview each f as a binary constraint on C1 and C2 . Depending on the 4 dierent rotation forms\nof f and whether f is on entry-vertices or exit-vertices, the resulting binary constraint has\n8 dierent forms (See Table 1). By multiplying these constraints, we get the binary constraint\ng1,2 . This can be viewed as a binary edge function on the circuits C1 and C2 . The property of\ng1,2 crucially depends on there are an even number of intersection vertices. Given a particular\n(e1 ,e2 ) : {e1 , e2} (cid:55) {0, 1}2 , we have\nassignment  (cid:48)\n(cid:89)\n1(cid:54)i(cid:54)4\n\n(e1 ,e2 ) |E(v) ).\nfvi ( (cid:48)\n\ng1,2 (e1 , e2 ) =\n\n\nIf the placement of fv1 were to be rotated clockwise \nv1 in the above\n2 , then fv1 will be changed to f\n2\n\nformula, where Mx1 x2 ,x4 x3 (f\nv1 ) = Mx2 x3 ,x1 x4 (fv1 ).\n2\nFor the self-intersection vertex fv5 , the notions of entry-vertex and exit-vertex do not apply.\nfv5 gives rise to an unary constraint h2 on e2 . Depending on the 4 dierent rotation forms of f , h2\n(e2 ) : e2 (cid:55) {0, 1},\nhas 4 dierent forms (see Table 3 in full version). Given a particular assignment  (cid:48)\nwe have\n(e2 ) |E(v) ).\nh2 (e2 ) = fv5 ( (cid:48)\n\n24\n\n\fTherefore, we have\n\nPl-Holant =\n\n=\n\n=\n\n(cid:89)\nvV\n\n(cid:88)\n(cid:88)\n :E (cid:55){0,1}\n(cid:88)\n (cid:48) :{e1 ,e2 }(cid:55){0,1}\n (cid:48) :{e1 ,e2 }(cid:55){0,1}\n\nfv ( |E(v) )\n(cid:32) (cid:89)\n1(cid:54)i(cid:54)4\n\n(cid:33)\nfv5 ( (cid:48) |E(v) )\n\nfvi ( (cid:48) |E(v) )\n\ng1,2 (e1 , e2 )h2 (e2 ).\n\nPart II: Given an instance I of #CSP(g1f , g2f ). Consider binary constraints on variables xi\nand xj (i < j ). Note that g1f is symmetric, that is, g1f (xi , xj ) = g1f (xj , xi ). We assume there\nare si,j many constraints g1f (xi , xj ), ti,j many constraints g2f (xi , xj ) and t(cid:48)\ni,j many constraints\ng2f (xj , xi ). These are all constraints between xi and xj . Let gi,j (xi , xj ) be the function product of\nthese constraints. That is,\n\nt(cid:48)\ni,j\n2f\n\n(xj , xi ).\n\nand 2 =\n\n#CSP(I ) =\n\nThen, we have\n\n. i is constructed as\n\ngi,j (xi , xj ) = gsi,j\n1f\n\n(xi , xj )g ti,j\n(xi , xj )g\n(cid:89)\n(cid:88)\n2f\ngi,j (xi , xj ).\n1(cid:54)i<j(cid:54)n\n (cid:48) :{x1 , ,xk }(cid:55){0,1}\n(cid:21)\n(cid:20) 0 0 0 1\n(cid:20) 0 0 0 1\n(cid:21)\nWe prove the reduction (4.4) in two steps. We rst reduce #CSP(I ) to an instance i (i = 1, 2)\nof Pl-Holant ( (cid:54)=2 | f , i ) repectively, where 1 =\n0 1 0 0\n0 1 0 0\n0 0 1 0\n0 0 1 0\n1 0 0 0\n1 0 0 0\nfollows:\n1. In a plane, draw cycle D1 . Then draw cycle D2 , and let D2 intersect with D1 at least\n2(s1,2 + t1,2 + t(cid:48)\n1,2 ) many times. This can be done since we can let D2 enter and exit the interior\nof D1 alternately and there will be an even number of intersection vertices. Successively draw\ncycles Dj until Dk , and let Dj intersect with Di at least 2(si,j + ti,j + t(cid:48)\ni,j ) many times\nfor 1 (cid:54) i < j (cid:54) k . This can be done since we can let Dj enter and exit the interiors of\nD1 ,    , Dj1 successively. Note that, when letting Dj intersect with other cycles, it may\nintersect with Di again. This is why we say Dj intersects with Di at least 2(si,j + ti,j + t(cid:48)\ni,j )\nmany times. We will deal with those extra intersection vertices later. Moreover, when drawing\nthese cycles, we can easily make them satisfy the following conditions:\na. There is no self-intersection.\nb. There is no more than two cycles that intersect with others at the same vertex. That is,\neach intersection vertex is of degree 4.\n2. Replace each edge by a path of length two. We nally get a planar bipartite graph G = (V , E ).\nOn one side, all vertices are of degree 2, and on the other side, all vertices are of degree 4.\nWe can still dene edge twins as dened in Part I. Moreover, we still divide the graph into\nsome circuits C1 ,    , Ck . In fact, Ci is just the cycle Di after the replacement of each edge\nby a path of length two.\nLet Vi,j = Ci  Cj (i < j ) be the intersection vertices in Ci and Cj . Clearly, |Vi,j | is even and\nno less than 2(si,j + ti,j + t(cid:48)\ni,j ). Since there is no self-intersection, each circuit is a simple cycle.\nWe can dene entry-vertices and exit-vertices as in Part I. Among Vi,j , a half of them\nare entry-vertices and the other half are exit-vertices. As we did in Part I, we pick an edge ei\n\n25\n\n\fas the leader edge of Ci and this gives an orientation of Ci . List the edges in Ci according to\nthe orientation of Ci . Then, all edges in Ci take assignment (0, 1, 0, 1,    , 0, 1) when ei = 0\nand (1, 0, 1, 0,    , 1, 0) when ei = 1.\n3. Label the vertex of degree 2 by ( (cid:54)=2 ). For any vertex in Vi,j , as we showed in Part I, we\ncan label the four edges incident to it by variables (x1 , x2 , x3 , x4 ) in a way such that when\n(ei , ej ) = (s, t), we have (x1 , x2 , x3 , x4 ) = (s, t, 1s, 1t) at entry-vertex, and (x1 , x2 , x3 , x4 ) =\n(s, 1  t, 1  s, t) at exit-vertex (See Table 1). Note that f has four rotation forms under this\n\nlabeling. Label si,j many entry-vertices by f and si,j many exit-vertices by f\n2 , ti,j many\n2 , and t(cid:48)\n3\ni,j many entry-vertices by f \nentry-vertices by f  and ti,j many exit-vertices by f\n2 . Let V (cid:48)\ni,j be the set of these 2(si,j + ti,j + t(cid:48)\n\nand si,j many exit-vertices by f\ni,j ) vertices. Refer\nto Table 2, we have(cid:89)\n(ei ,ej ) |E(v) ) = gsi,j\nfv ( (cid:48)\n1f\nvV (cid:48)\nij\n\nt(cid:48)\ni,j\n(ei , ej )g\n2f\n\n(ei , ej )g ti,j\n2f\n\n(ej , ei ) = gi,j (ei , ej ).\n(cid:20) 0 0 0 1\n(cid:21)\n0 1 0 0\n0 0 1 0\n1 0 0 0\n\n, then refer to\n\nFor any vertex in Vi,j \\V (cid:48)\ni,j , if label it by an auxiliary signature 1 =\nTable 2 (Here a = x = b = y = 1), we have(cid:89)\n(ei ,ej ) |E(v) )  1.\n1 ( (cid:48)\nvVi,j \\V (cid:48)\nij\n\n(cid:20) 0 0 0 1\n(cid:21)\nWe can also label the vertex in Vi,j \\V (cid:48)\n0 1 0 0\ni,j by auxiliary signature 2 =\n. Note that in\n(cid:21)\n(cid:20) 0 0 0 1\n0 0 1 0\n1 0 0 0\nVi,j \\V (cid:48)\ni,j , the number of entry-vertices are equal to the number of exit-vertices. We label all\n\n0 1 0 0\n0 0 1 0\n2 =\nentry-vertices by 2 and label all exit-vertices by its rotation form \n2\n(cid:89)\nTable 2 (Here a = b = y = 1, x = 1, and k = k1 = (cid:96)1 = (cid:96)), we have\n1 0 0 0\n(ei ,ej ) |E(v) )  1.\n2 ( (cid:48)\nvVi,j \\V (cid:48)\nij\nNow, we get an instance i (i = 1, 2) for each problem Pl-Holant ((cid:54)=2 | f , i ) respectively. Note\nthat i has the same support as f . As we have showed in Part I, for any nonzero term in the sum\nPl-Holanti , the assignment of all edges  : E (cid:55) {0, 1} can be uniquely extended by the assignment\n(cid:88)\n(cid:89)\nof all leader edges  (cid:48) : {e1 , e2 ,    , ek } (cid:55) {0, 1}. Therefore, we have\n(cid:33)(cid:32) (cid:89)\n(cid:32) (cid:89)\n(cid:88)\n1(cid:54)i<j(cid:54)n\n (cid:48) :{e1 , ,ek }(cid:55){0,1}\nfv ( (cid:48) |E(v) )\n (cid:48) :{e1 , ,ek }(cid:55){0,1}\nvV (cid:48)\nvVi,j \\V (cid:48)\ni,j\ni,j\n1(cid:54)i<j(cid:54)n\n1(cid:54)i<j(cid:54)n\n\ni v ( (cid:48) |E(v) )\n\n#CSP(I ) =\n\ngi,j (ei , ej )\n\n. Refer to\n\n(cid:33)\n\n=\n\n= Pl-Holanti\nThat is, #CSP(g1f , g2f ) (cid:54)T Pl-Holant ((cid:54)=2 | f , i ).\n\n26\n\n\fThen, we show\n\nPl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f )\nwhen a = \u0001x, b = \u0001y , where \u0001 = 1 and\nPl-Holant( (cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f )\nwhen a = \u0001x, b = \u0001y , where \u0001 = 1 by interpolation.\n(cid:20) 0 0 0 1\n(cid:21)\n If a = x and b = y , since they are all no zeros and ( b\na )8 (cid:54)= 1, by normalization, we may assume\n, where b (cid:54)= 0 and b8 (cid:54)= 1.\n0 b 0 0\nM (f ) =\n0 0 b 0\nIf b is not a root of unity, by Lemma 2.3, we have Pl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).\n1 0 0 0\nOtherwise, b is a root of unity. Construct gadget f(cid:2) as showed in Figure 12. Given an\n\nFigure 12: The Square gadget\nassignment (x1 , x2 , x3 , x4 ) of f(cid:2) , f(cid:2) (x1 , x2 , x3 , x4 ) (cid:54)= 0 if and only if (x1 , x2 , x3 , x4 ) is equal to\nthe assignment of fv5 . That is, f(cid:2) has support (0, 0, 1, 1), (1, 1, 0, 0), (0, 1, 1, 0) and (1, 0, 0, 1).\nIn fact, each Diagonal Line in this gadget is a part of some circuit, which means the edges\nin it can only take assignment (0, 1, 0, 1, 0, 1) or (1, 0, 1, 0, 1, 0), otherwise the term is zero.\nAlso, the Square cycle in this gadget is a circuit itself, which means the edges in it can\nonly take assignment (0, 1, 0, 1, 0, 1, 0, 1) or (1, 0, 1, 0, 1, 0, 1, 0). We simplify them by (0, 1)\nand (1, 0).\nFor the signature f , if one pair of its edge twins ips its assignment between (0, 1) and (1, 0),\nthen the value of f changes from 1 to b, or b to 1.\nIf two pairs of edge twins both ip\ntheir assignments, then the value of f does not change. According to this property, we give\nthe following Table 4. Here, we label vertices v1 , v2 , v3 , v4 and v5 in a way such that the\nvalues of f on these vertices are all 1 under the assignment (x1 , x2 , x3 , x4 ) = (0, 0, 1, 1) and\nSquare= (0, 1) (Row 2). When the assignment of Square ips from (0, 1) to (1, 0), one\npair of edge twins of each vertex except v5 ips its assignment. So the values of f on these\nvertices except v5 change from 1 to b (Row 3). When (x1 , x3 ) ips its assignment, one pair\nof edge twins of v1 , v3 and v5 ip their assignments. When (x2 , x4 ) ips its assignment, one\n(cid:35)\n(cid:34) 0\npair of edge twins of v2 , v4 and v5 ip their assignments. Using this fact, we get other rows\ncorrespondingly.\n0 1+b4\n0\n 0\n. Since |b| = 1 and b4 (cid:54)= 1, we have\n. Since b4 (cid:54)= 1, 1 + b4 (cid:54)= 0,\n2b3 0\nHence, f(cid:2) has the signature matrix M (f(cid:2) ) =\n0\n0\n0 2b3\n0\n0\n1+b4 0\n0\n0\n1\n0\n0\n0 2b3\n0\n0\n1+b4\n2b3\n1+b4 0\n0\n0\n0\n1\n\nby normalization, we have M (f(cid:2) ) =\n\n0\n0\n\n27\n\n\f(x1 , x2 , x3 , x4 ) Square fv1\n1\n(0, 1)\n(1, 0)\nb\n\n(0, 0, 1, 1)\n\nfv2\n1\nb\n\nfv3\n1\nb\n\nfv4\n1\nb\n\nfv5\n1\n1\n\n(1, 1, 0, 0)\n\n(0, 1, 1, 0)\n\n(1, 0, 0, 1)\n\n(0, 1)\n(1, 0)\n\n(0, 1)\n(1, 0)\n\n(0, 1)\n(1, 0)\n\nb\n1\n\n1\nb\n\nb\n1\n\nb\n1\n\nb\n1\n\n1\nb\n\nb\n1\n\n1\nb\n\nb\n1\n\nb\n1\n\nb\n1\n\n1\nb\n\n1\n1\n\nb\nb\n\nb\nb\n\nf(cid:2)\n\n1 + b4\n\n1 + b4\n\n2b3\n\n2b3\n\nTable 4: The values of gadget f(cid:2) when a = x = 1 and b = y\n\n1+b4 | > |b3 | = 1, which means 2b3\n|1 + b4 | < 2. Then | 2b3\n1+b4 is not a root of unity. By Lemma 2.3,\nwe have Pl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , f(cid:2) ). Since f(cid:2) is constructed by f , we have\n(cid:20) 0 0 0 a\n(cid:21)\nPl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).\n If a = x and b = y , then M (f ) =\n0 b 0 0\n0 0 b 0\n. Connect the variable x4 with x3 of f using\na 0 0 0\n((cid:54)=2 ), and we get a binary signature g (cid:48) , where\ng (cid:48) = Mx1 x2 ,x4 x3 (0, 1, 1, 0)T = (0, b, b, 0)T .\n(cid:20) 0 0 0 a\n(cid:21)\nSince b (cid:54)= 0, g (cid:48) can be normalized as (0, 1, 1, 0)T . Connect the variable x2 of g (cid:48) with the\nvariable x1 of f , and we get a signature f (cid:48) with the signature matrix M (f (cid:48) ) =\n0 b 0 0\n. As\n0 0 b 0\nwe have proved above, Pl-Holant((cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , f (cid:48) ). Since f (cid:48) is constructed\na 0 0 0\nby f , we have Pl-Holant((cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).\n(cid:20) 0 0 0 1\n(cid:21)\n If a = x, b = y or a = x, b = y , by normalization and rotational symmetry, we may\n, where b (cid:54)= 0 and b8 (cid:54)= 1.\n0 b 0 0\nassume M (f ) =\n0 0 b 0\n1 0 0 0\nIf b is not a root of unity, by Corollary 2.4, we have Pl-Holant((cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).\nOtherwise, b is a root of unity. Construct gadget f(cid:2) in the same way as showed above.\nSimilarly, given an assignment (x1 , x2 , x3 , x4 ) of f(cid:2) , f(cid:2) (x1 , x2 , x3 , x4 ) (cid:54)= 0 if and only if\n(x1 , x2 , x3 , x4 ) is equal to the assignment of fv5 . Also, the edges in Square can only take\nassignment (0, 1) or (1, 0).\nFor the signature f , if one pair of its edge twins ips its assignment between (0, 1) and (1, 0),\nthen the value of f changes from 1 to b, or b to 1. If two pairs of edge twins both ip\ntheir assignments, then the value of f does not change if the value is b, or change its sign if\nthe value is 1. According to this property, we have the following Table 5. Here, we label\nvertices v1 , v2 , v3 , v4 and v5 in a way such that the values of f on these vertices are all 1\nunder the assignment (x1 , x2 , x3 , x4 ) = (0, 0, 1, 1) and Square= (0, 1) (Row 2). When the\nassignment of Square ips from (0, 1) to (1, 0), one pair of edge twins of each vertex except\nv5 ips its assignment. So the values of f on these vertices except v5 change from 1 to b\n(Row 3). When (x1 , x3 ) ips its assignment, one pair of edge twins of v1 , v3 and v5 ip their\nassignments. When (x2 , x4 ) ips its assignment, one pair of edge twins of v2 , v4 and v5 ip\ntheir assignments. Using this fact, we get other rows correspondingly.\n\n28\n\n\f(0, 0, 1, 1)\n\n(1, 1, 0, 0)\n\n(0, 1, 1, 0)\n\nf(cid:2)\n\nfv2\n1\nb\n\nfv3\n1\nb\n\n(0, 1)\n(1, 0)\n\n(x1 , x2 , x3 , x4 ) Square fv1\n1\n(0, 1)\n(1, 0)\nb\n\nfv5\nfv4\n1\n1\n1 + b4\n1\nb\nb 1 (1 + b4 )\nb\nb\nb\n1 1 1 1 1\nb\n1\n1\nb\nb\nb 1\nb 1\nb\n1\n1\nb\nb\n(0, 1)\nb\n1\nb 1\nb\n(1, 0)\nb\nTable 5: The values of gadget f(cid:2) when a = x = 1 and b = y\n(cid:35)\n(cid:34)\n\n(0, 1)\n(1, 0)\n\n(1, 0, 0, 1)\n\n2b3\n\n2b3\n\n0 1+b4\n0\n0\n. Since |b| = 1 and b8 (cid:54)= 1, we have\n2b3 0\n0\n0\nHence, f(cid:2) has the signature matrix\n0 2b3\n0\n0\n(1+b4 ) 0\n0\n0\n1+b4 is not a root of unity. By Corollary 2.4, Pl-Holant((cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , f(cid:2) ),\n2b3\nand hence Pl-Holant((cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).\nIn summary, we have\n\n(cid:54)T\n\n#CSP(g1f , g2f )\n\nPl-Holant ((cid:54)=2 | f , 1 )\na = \u0001x, b = \u0001y (\u0001 = 1)\nPl-Holant ((cid:54)=2 | f )\n(cid:54)T\na = \u0001x, b = \u0001y (\u0001 = 1)\nPl-Holant ((cid:54)=2 | f , 2 )\nTherefore, we have #CSP(g1f , g2f ) (cid:54)T Pl-Holant ((cid:54)=2 | f ) when a2 = x2 (cid:54)= 0, b2 = y2 (cid:54)= 0 and\na )8 (cid:54)= 1.\n( b\nTheorem 4.6. Let f be a 4-ary signature of the form (4.2). Then Pl-Holant( (cid:54)=2 | f ) is #P-hard\nunless\n(i). (ax)2 = (by)2 , or\n\n\n, where ,  ,   N, and    (mod 2),\n\n(ii). x = ai , b = a\ni\ni\n, y = a\nin which cases, the problem is tractable.\n\n\n\nProof of Tractability:\n In case (i), if ax = by = 0, then f has support of size at most 2. So we have f  P ,\nand hence Pl-Holant( (cid:54)=2 | f ) is tractable by Theorem 2.13. Otherwise, (ax)2 = (by)2 (cid:54)= 0.\nFor any signature g in Gf , we have g00  g11 = (ax)k1+(cid:96)1+k3+(cid:96)3 (by)k2+(cid:96)2+k4+(cid:96)4 and g01  g10 =\n(ax)k2+(cid:96)2+k4+(cid:96)4 (by)k1+(cid:96)1+k3+(cid:96)3 . Since (k1+(cid:96)1+k3+(cid:96)3 )(k2+(cid:96)2+k4+(cid:96)4 )  k+(cid:96)  0 ( mod 2),\n(cid:18) ax\n(cid:19) (k1+(cid:96)1+k3+(cid:96)3 )(k2+(cid:96)2+k4+(cid:96)4 )\n(cid:19)(k1+(cid:96)1+k3+(cid:96)3 )(k2+(cid:96)2+k4+(cid:96)4 )\n(cid:18) (ax)2\nwe have\ng00  g11\n2\ng01  g10\n(by)2\nby\n\n= 1.\n\n=\n\n=\n\n29\n\n)\n)\n5\n5\n)\n)\n5\n5\n\fak+l\n\n= ak+(cid:96)\n\np01\n\np11\n\n,\n\np00 \np10 \n\nThat is, g  P . Since any signature h in Hf is unary, h  P . Hence, we have Gf  Hf  P .\nBy Theorem 2.23, #CSP(Gf  Hf ) is tractable. By reduction (4.3) of Lemma 4.5, we have\nPl-Holant( (cid:54)=2 | f ) is tractable.\n(cid:34)\n(cid:35)\n(cid:21)\n(cid:20)\n In case (ii), for any signature g in Gf , M (g) is of the form\n (k4+(cid:96)4 )+ (k2+(cid:96)2 )+2(k3+(cid:96)3 ) \n (k1+(cid:96)3 )+ (k3+(cid:96)1 )+2(k4+(cid:96)2 )\n\n (k3+(cid:96)1 )+ (k1+(cid:96)3 )+2(k2+(cid:96)4 ) \n\ni\ni\ni\ni\n (k2+(cid:96)2 )+ (k4+(cid:96)4 )+2(k1+(cid:96)1 )\ni\ni\ni\ni\n\ni in g correspondingly. Since  \nwhere p00 , p01 , p10 and p11 denote the exponents of\n (mod 2), if they are both even, then p00  p01  p10  p11  0 (mod 2); if they are both\nodd, then p00  p11  k2 + (cid:96)2 + k4 + (cid:96)4  k1 + (cid:96)1 + k3 + (cid:96)3  p01  p10 (mod 2). If these\n\ni. Hence, g is of the form a(cid:48) (iq00 , iq01 , iq10 , iq11 )T ,\n\nexponents are all odd, we can take out a\n2 or pij 1\ni, and for all i, j  {0, 1}, qij = pij\nwhere a(cid:48) = ak+l or ak+l\n. Thus,\n2\nq00 + q01 + q10 + q11  (p00 + p01 + p10 + p11 )/2 (mod 2).\nMoreover, since p00 + p01 + p10 + p11 = (k + (cid:96))( +  + 2)  0 ( mod 4), we have q00 + q01 +\nq10 + q11  0 (mod 2). Therefore, g  A by Lemma 2.10.\nm4+m2+2m1 (cid:105)\nam (cid:104)\nIn this case, for any signature h in Hf , M (h) is of the form\nm2+m4+2m3 \n.\ni\ni\nSince    (mod 2), we have m2 + m4  m4 + m2 (mod 2). Hence, h is of the form\n\ni. That is, h  A by Lemma 2.11. Hence, Gf  Hf  A .\na(cid:48)(cid:48) (iq0 , iq1 ), where a(cid:48)(cid:48) = am or am\nBy Theorem 2.23, #CSP(Gf  Hf ) is tractable. By reduction (4.3) of Lemma 4.5, we have\n(cid:20) 0 0 0 a\n(cid:21)\n(cid:20) 0 0 0 x\n(cid:21)\nPl-Holant( (cid:54)=2 | f ) is tractable.\n0 y 0 0\n0 b 0 0\nProof of Hardness: Note that Mx1 x2 ,x4 x3 (f ) =\nand Mx3 x4 ,x2 x1 (f ) =\n0 0 y 0\n0 0 b 0\nx 0 0 0\na 0 0 0\nConnect variables x4 , x3 of a copy of signature f with variables x1 , x2 of another copy of signature\n .\n 0\nf both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix\n0\n0\n0\nby\n0\nx2\n0\nSimilarly, connect x4 , x3 of a copy of signature f with x3 , x4 of another copy of signature f both\n .\n 0\nusing ( (cid:54)=2 ). We get a signature f2 with the signature matrix\n0\n0\nb2\n0\n0\nM (f2 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =\ny2\n0\n0\n(cid:34) 0 0 0 0\n(cid:34) 0 0 0 0\n(cid:35)\n(cid:35)\n(cid:105)\n(cid:104) ax b2\n(cid:105)\n(cid:104) a2 by\n0\n0\nax\n\n\n0 a2 by 0\n0 ax b2 0\nand M (g2f ) =\n, M (g1f ) =\n2 ) =\n1 ) =\n.\n, M (f\nNotice that M (f\n2\n2\ny2 ax\n0 y2 ax 0\nby x2\n0 by x2 0\n0 0 0 0\n0 0 0 0\ni In . Thus, we have fi (x1 , x2 , x3 , x4 ) = gi (x1 , x2 )  x2 (cid:54)=x3  x1 (cid:54)=x4 . Now, we analyze\n\nThat is, gif = f\n2\ng1f and g2f .\n\nM (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (f ) =\n\nax\n0\n0\n0\n\n0\nby\n0\n0\n\na2\n0\n0\n0\n\n.\n\n30\n\n\f.\n\n If {g1f , g2f }  P , then either (ax)2 = (by)2 due to degeneracy, or g1f and g2f are generalized\nEquality or generalized Disequality respectively. In the later case, since (a, x) (cid:54)= (0, 0)\nand (b, y) (cid:54)= (0, 0), it forces that ax = by = 0. So we still have (ax)2 = (by)2 . That is,\n{a, b, x, y} belongs to case (i).\n If {g1f , g2f }  A , there are two subcases.\n If both g1f and g2f have support of size at most 2, then we have ax = by = 0 due to\n(a, x) (cid:54)= (0, 0) and (b, y) (cid:54)= (0, 0). This belongs to case (i).\n Otherwise, both g1f and g2f have support of size 4, which means abxy (cid:54)= 0. Let x(cid:48) =\n(cid:20) 1\n(cid:21)\na and y (cid:48) = y\na , b(cid:48) = b\nx\na . By normalization, we have\nb(cid:48)y (cid:48)\nM (g1f ) = a2\nb(cid:48)y (cid:48) x(cid:48)2\nSince g1f  A , by Lemma 2.10, x(cid:48)2 and b(cid:48)y (cid:48) are both powers of i, and the sum of all\nexponents is even. It forces that x(cid:48)2 = i2 for some   N. Then, we can choose  such\n(cid:21)\n(cid:20) x(cid:48)\nx(cid:48) = i . Also, we have\nb(cid:48)2\nM (g2f ) = a2\ny (cid:48)2 x(cid:48)\n.\nSince g2f  A and x(cid:48) is already a power of i, y (cid:48)2 and b(cid:48)2 are both powers of i. That is,\n\n\n\n. Also, since g1f  A , b(cid:48)y (cid:48) =\nb(cid:48) =\nand y (cid:48) =\n\n\n+\nis a power of i, which means\ni\ni\ni\n If {g1f , g2f }  (cid:99)M , then by Lemma 2.17, we have a2 = x2 and b2 = y2 , denoted by case (iii).\n   (mod 2). That is, {a, b, x, y} belongs to case (ii).\nor (cid:99)M . By Theorem 2.23, we have Pl-#CSP(g1f , g2f ) is #P-hard. Recall g1f = f\nTherefore, if {a, b, x, y} does not belong to case (i), case (ii) or case (iii), then {g1f , g2f } (cid:42) P , A\n\n\n1 In and g2f = f\n2 In .\n2\n2\nBy Lemma 4.2, Pl-Holant( (cid:54)=2 | f\n2 ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\n\n\n1 , f\n2\n2\nOtherwise, {a, b, x, y} does not belong to case (i) or case (ii), but belongs to case (iii). Then\n\na2 = x2 (cid:54)= 0, b2 = y2 (cid:54)= 0 and b\na )8 (cid:54)= 1. By reduction (4.4)\ni, that is ( b\na is not a power of\nof lemma 4.6, we have #CSP(g1f , g2f ) (cid:54)T Pl-Holant( (cid:54)=2 | f ). Moreover, since {a, b, x, y} does not\nbelong to case (i) or case (ii), we have {g1f , g2f } (cid:42) P or A . By Theorem 2.23, #CSP(g1f , g2f ) is\n#P-hard. Therefore, we have Pl-Holant( (cid:54)=2 | f ) is #P-hard.\n\n5 Case III: Exactly Two Zeros and They Are in Dierent Pairs or\nExactly One Zero and It Is in an Outer Pair\n\nIf there are exactly two zeros and they are in dierent pairs, there must be a zero in an outer pair.\n(cid:21)\n(cid:20) 0 0 0 0\nBy rotational symmetry, we may assume a is zero and we prove this case in Theorem 5.2. We rst\ngive the following lemma.\n(cid:20) 0 0 0 0\n(cid:21)\n0 b c 0\n, where\nLemma 5.1. Let f be a 4-ary signature with the signature matrix M (f ) =\n0 z y 0\n0 0 0 0\ndet MIn (f ) = by  cz (cid:54)= 0. Let g be a 4-ary signature with the signature matrix M (g) =\n0 0 1 0\n0 1 0 0\nThen for any signature set F containing f , we have\n0 0 0 0\nPl-Holant( (cid:54)=2 | F  {g}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).\n\n.\n\n31\n\n\f= n\n1\n\n0\n\n0\n\n0\n\nP =\n\nand s =\n\n(cid:35)\n\n=\n\nand M (fn ) =\n\n(cid:34) 0 0\n0 0\n0 n\n1 0 0\n0 0 n\n1 0\n0 0\n0 0\n\nM (fs ) = M (f )(N M (f ))s1 = N (N M (f ))s = N\n\n(cid:34) 0 0\n(cid:35)\n(cid:21)\n(cid:20) 0 0 0 0\n1. If  = 0, and 2\nis a root of unity. Suppose ( 2\n1\n1\n0 0\n0 0 n\n1 0\n0 0 1 0\n0 n\n0 1 0 0\n1 0 0\n0 0 0 0\n0 0\n0 0\n\nProof. We construct a series of gadgets fs by a chain of s copies of f linked by double\n .\n 0\nDisequality N . fs has the signature matrix\n(cid:21)s\n(cid:20) z y\n0\n0\nc\nb\n0\n0\n0\nConsider the inner matrix NInMIn (f ) of N M (f ). Suppose NInMIn (f ) = Q1Q, where  = [ 1 \n]\nis the Jordan Canonical Form. Note that 12 = det  = det(NInMIn (f )) (cid:54)= 0. We have M (fs ) =\n0 2\n .\n 0\nN P 1sP , where\n 1 0 0\n\n(cid:21)s\n(cid:20) 1 \n0\n0 Q 0\n0\n0\n2\n0\n(cid:34) 0 0\n0 0 1\n0\n0\n0\n0 0\n0 n\n1 0 0\n)n = 1. Then n =\n0 0 n\n2 0\n0 0\n0 0\n(cid:34) 0 0 0 0\n(cid:35)\n. After normalization, we can realize the signature g .\n0 s\n1 0 0\n2. If  = 0, and 2\nhas a good form for\nis not a root of unity. The matrix s =\n0 0 s\n2 0\n1\ninterpolation. Suppose g appears m times in an instance  of Pl-Holant( (cid:54)=2 | F {g}). Replace\n0 0 0 0\neach appearance of g by a copy of gadget fs to get an instance s of Pl-Holant( (cid:54)=2 | F  {fs}),\nwhich is also an instance of Pl-Holant((cid:54)=2 | F ). We can treat each of the m appearances of fs\nas a new gadget composed of four functions in sequence N , P 1 , s and P , and denote this\ns into two parts. One part consists of m signatures m\ns . We divide (cid:48)\nnew instance by (cid:48)\n.\ns\nis expressed as a column vector. The other part is the rest of (cid:48)\nHere m\ns and its signature\ns\nis represented by A which is a tensor expressed as a row vector. Then the Holant value of\n(cid:105), which is a summation over 4m bits. That is, the value of\ns is the dot product (cid:104)A, m\n(cid:48)\ns\nthe 4m edges connecting the two parts. We can stratify all 0, 1 assignments of these 4m bits\nhaving a nonzero evaluation of a term in Pl-Holant(cid:48)\ns into the following categories:\n There are i many copies of s receiving inputs 0110;\n There are j many copies of s receiving inputs 1001;\n2 = sm (cid:16) 2\n(cid:17)sj\nwhere i + j = m.\nFor any assignment in the category with parameter (i, j ), the evaluation of m\ns\n1 sj\nsi\n. Let aij be the summation of values of the part A over all assignments in\n1\nthe category (i, j ). Note that aij is independent from the value of s since we view the gadget\ns as a block. Since i + j = m, we can denote aij by aj . Then we rewrite the dot product\n(cid:88)\nsummation and get\n0(cid:54)j(cid:54)m\n\ns = (cid:104)A, m\nPl-Holants = Pl-Holant(cid:48)\ns\n\n(cid:105) =\n\nsmaj (\n\n)sj .\n\n(cid:35)\n\n,\n\nis clearly\n\n2\n1\n\n32\n\n\f(m+1)m\n\naj .\n\nm\n\n2m\n\nm\n\n2m\n\nm\n\n2m\n\n(cid:19)m\n(cid:19)2m\n(cid:19)(m+1)m\n\n(cid:20) 0 0 0 0\n(cid:21)\nNote that M (g) = N P 1 (N M (g))P , where N M (g) =\n0 1 0 0\n. Similarly, divide  into two\n(cid:88)\n0 0 1 0\n0 0 0 0\nparts. Under this stratication, we have\nPl-Holant = (cid:104)A, (N M (g))m (cid:105) =\n0(cid:54)j(cid:54)m\n\n\n(cid:18) 2\n(cid:19)0\n(cid:18) 2\n(cid:19)1\n(cid:18) 2\nSince 2\nis not a root of unity, the Vandermonde coecients matrix\n1\n(cid:19)21\n(cid:19)20\n(cid:18) 2\n(cid:18) 2\n(cid:18) 2\n  \n1\n1\n1\n  \n(cid:18) 2\n(cid:19)(m+1)1\n(cid:18) 2\n(cid:19)(m+1)0\n(cid:18) 2\n1\n1\n1\n...\n...\n...\n...\n   (m+1)m\n(m+1)m\n1\n1\n1\n(cid:20) 0 0\n(cid:21)\nhas full rank. Hence, by oracle querying the values of Pl-Holants , we can solve coecients\naj and obtain the value of Pl-Holant in polynomial time.\n0\n0\n0 s ss1 0\n. We use this form to give\n3. If  = 1, and 1 = 2 denoted by . Then s =\ns\n0 0\n0\na polynomial interpolation. As in the case above, we can stratify the assignments of m\n0 0\n0\n0\ns\nof these 4m bits having a nonzero evaluation of a term in Pl-Holant(cid:48)\ns into the following\ncategories:\n There are i many copies of s receiving inputs 0110 or 1001;\n There are j many copies of s receiving inputs 0101;\nwhere i + j = m.\nFor any assignment in the category with parameter (i, j ), the evaluation of m\nis clearly\ns\nsi (ss1 )j = sm ( s\n )j . Let aij be the summation of values of the part A over all assignments\nin the category (i, j ). aij is independent from s. Since i + j = m, we can denote aij by aj .\n(cid:16) s\n(cid:17)j\n(cid:105) = sm (cid:88)\nThen, we rewrite the dot product summation and get\ns = (cid:104)A, m\nPl-Holants = Pl-Holant(cid:48)\ns\n\n0(cid:54)j(cid:54)m\nSimilarly, divide  into two parts. Under this stratication, we have\nPl-Holant = (cid:104)A, (N M (g))m (cid:105) = a0 .\n\n\n(cid:18) 1\n(cid:18) 1\n(cid:19)1\n(cid:19)0\nThe Vandermonde coecients matrix\n(cid:19)1\n(cid:18) 2\n(cid:19)0\n(cid:18) 2\n\n\n(cid:18) m + 1\n(cid:18) m + 1\n\n\n...\n...\n(m+1)m\n\n\nhas full rank. Hence, we can solve a0 in polynomial time and it is the value of Pl-Holant .\n\n  \n  \n...\n   (m+1)m\n\n(cid:18) 1\n(cid:19)m\n(cid:19)m\n(cid:18) 2\n\n(cid:18) m + 1\n\n...\n\n\nm\n\n2m\n\nm\n\n2m\n\nm\n\n2m\n\n(cid:19)m\n\naj\n\n.\n\n(cid:19)0\n\n(m+1)m\n\n(cid:19)1\n\n33\n\n\fwhere (cid:2) b1 c1\nz1 y1\n\nTherefore, we have Pl-Holant((cid:54)=2 | F  {g}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).\n0 0 0 0\n,\nTheorem 5.2. Let f be a 4-ary signature with the signature matrix\n0 b\nc 0\n0 z y 0\nx 0 0 0\nwhere x (cid:54)= 0 and there is at most one number in {b, c, y , z} that is 0. Then Pl-Holant( (cid:54)=2 | f ) is\n#P-hard unless f  M , in which case the problem is tractable.\nSuppose f / M . By Lemma 2.14, det MIn (f ) (cid:54)= det MOut (f ), that is det (cid:2) b c\n(cid:3) = by  cz (cid:54)= 0.\n(cid:20) 0 0 0 0\n(cid:20) 0 0 0 x\n(cid:20) 0 0 0 y\n(cid:21)\n(cid:21)\n(cid:21)\nProof. Tractability follows by Theorem 2.15.\nz y\n0 y c 0\n0 b c 0\n0 0 z 0\n.\n, and Mx2 x3 ,x1 x4 (f ) =\n, Mx3 x4 ,x2 x1 (f ) =\nNote that Mx1 x2 ,x4 x3 (f ) =\n0 z y 0\n0 c x 0\n0 z b 0\nb 0 0 0\nx 0 0 0\n0 0 0 0\nConnect variables x4 , x3 of a copy of signature f with variables x3 , x4 of another copy of signature\n0\n ,\nf both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix\n0\n0\n0\nc1 0\n0 b1\nM (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =\n0 z1 y1 0\n(cid:3) = (by  cz )2 (cid:54)= 0. By Lemma 5.1, we have\n(cid:3) . Here, det (cid:2) b1 c1\n(cid:3)  (cid:2) z b\n(cid:3) = (cid:2) b c\n0\n0\n0\n0\nz y\ny c\nz1 y1\n(cid:21)\n(cid:20) 0 0 0 0\nPl-Holant( (cid:54)=2 | f , g) (cid:54)T Pl-Holant( (cid:54)=2 | f , f1 ),\n0 0 1 0\nwhere g has the signature matrix M (g) =\n.\n0 1 0 0\n If bcyz (cid:54)= 0, connect variables x1 , x4 of signature f with variables x1 , x2 of signature g both\n0 0 0 0\n .\n0 0 0 0\nusing ( (cid:54)=2 ). We get a signature f2 with the signature matrix\n0 0 z 0\n0 c x 0\n0 0 0 0\n Otherwise, connect variables x4 , x3 of signature f with variables x1 , x2 of signature g both\n0 0 0 0\n ,\nusing ( (cid:54)=2 ). We get a signature f2 with the signature matrix\n0 b\nc 0\nM (f2 ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (g) =\n0 z y 0\n0 0 0 0\nIn both cases, the support of f2 has size 3, which means f2 / P , A or (cid:99)M . By Theorem 4.3,\nand there is exactly one in {b, c, y , z} that is zero.\nPl-Holant( (cid:54)=2 | f2 ) is #P-hard. Since\nPl-Holant( (cid:54)=2 | f2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , g) (cid:54)T Pl-Holant( (cid:54)=2 | f , f1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ),\nwe have Pl-Holant((cid:54)=2 | f ) is #P-hard.\n\nM (f2 ) = Mx2 x3 ,x1 x4 (f )N Mx1 x2 ,x4 x3 (g) =\n\nM (f ) =\n\n34\n\n\f6 Case IV: Exactly One Zero and It Is in the Inner Pair or All\nValues Are Nonzero\nBy rotational symmetry, if there is one zero in the inner pair, we may assume c = 0. We rst\nconsider the case that a = \u0001x, b = \u0001y and c = \u0001z , where \u0001 = 1.\n, \u0001 = 1 and abc (cid:54)= 0.\n 0\nLemma 6.1. Let f be a 4-ary signature with the signature matrix\na\n0\n0\nc\n0\nb\n0\n\u0001b 0\n\u0001c\n0\n0\n0\n0\n\u0001a\nThen Pl-Holant( (cid:54)=2 | f ) is #P-hard if f / M .\nIf \u0001 = 1. Connect the variable x4 with x3 of f using ((cid:54)=2 ), and we get a binary\nProof.\nsignature g1 , where\n\nM (f ) =\n\ng1 = Mx1 x2 ,x4 x3 (f )(0, 1, 1, 0)T = (0, b + c, (b + c), 0)T .\nAlso connect the variable x1 with x2 of f using ( (cid:54)=2 ), and we get a binary signature g2 , where\ng2 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 (f ))T = (0, b  c, (b  c), 0)T .\nSince bc (cid:54)= 0, b + c and b  c can not be both zero. Without loss of generality, assume b + c (cid:54)= 0. By\n(cid:20) 0 0 0 a\n(cid:21)\nnormalization, we have g1 = (0, 1, 1, 0)T . Then, connect the variable x2 of g1 with the variable x1\nof f using ( (cid:54)=2 ), and we get a signature with the signature matrix\n0 b c 0\n. Therefore, it suces\n0 c b 0\na 0 0 0\nto show #P-hardness for the case that \u0001 = 1.\nSince f / M , by Lemma 2.14, c2  b2 (cid:54)= a2 . We prove #P-hardness in three cases depending\non the values of a, b and c.\nCase 1: If c2  b2 (cid:54)= 0 and |c + b| (cid:54)= |c  b|, or c2  a2 (cid:54)= 0 and |c + a| (cid:54)= |c  a|. By rotational\n(cid:21)\n(cid:20) 0 0 0 1\nsymmetry, we may assume c2  b2 (cid:54)= 0 and |c + b| (cid:54)= |c  b|. Normalizing f by assuming a = 1, we\n, where c2  b2 (cid:54)= 0 or 1.\n0 b c 0\nhave M (f ) =\n0 c b 0\n1 0 0 0\nWe construct a series of gadgets fs by a chain of s copies of f linked by double Disequality\n .\n 1\nN . fs has the signature matrix\n(cid:21)s\n(cid:20) c\n0\n0\nM (fs ) = M (f )(N M (f ))s1 = N (N M (f ))s = N\nb\n0\n0\nb\nc\n(cid:3) (note that H 1 = H ), and get M (fs ) = N P sP , where\n(cid:2) 1 1\n(cid:3)s using H = 1\nWe diagonalize (cid:2) c b\n1\n0\n0\n .\n 1\n1 1\n ,\n 1 0 0\nb c\n2\n0\n0\n0\n0 (c + b)s\n0\n0\n(c  b)s 0\n0 H 0\n0\n0\n0 0 1\n1\n0\n0\n0\n\nand s =\n\nP =\n\n35\n\n\f(cid:35)\n(cid:34) 0 0 0 1\nThe the signature matrix s has a good form for polynomial interpolation. Suppose we have a\nproblem Pl-Holant((cid:54)=2 | f ), where M ( f ) =\nto be reduced to Pl-Holant( (cid:54)=2 | f ). Suppose\n0 b c 0\n0 c b 0\n1 0 0 0\nf appears m times in an instance  of Pl-Holant((cid:54)=2 | f ). We replace each appearance of f by\na copy of gadget fs to get an instance s of Pl-Holant( (cid:54)=2 | f , fs ), which is also an instance of\nPl-Holant( (cid:54)=2 | f ). We can treat each of the m appearances of fs as a new gadget composed of four\nfunctions in sequence N , P , s and P , and denote this new instance by (cid:48)\ns . We divide (cid:48)\ns into\ntwo parts. One part consists of m signatures m\n. Here, m\nis expressed as a column vector.\ns\ns\nThe other part is the rest of (cid:48)\ns and its signature is represented by A which is a tensor expressed\ns is the dot product (cid:104)A, m\n(cid:105), which is a summation\nas a row vector. Then the Holant value of (cid:48)\ns\nover 4m bits. That is, the value of the 4m edges connecting the two parts. We can stratify all\n0, 1 assignments of these 4m bits having a nonzero evaluation of a term in Pl-Holant(cid:48)\ns into the\nfollowing categories:\n There are i many copies of s receiving inputs 0000 or 1111;\n There are j many copies of s receiving inputs 0110;\n There are k many copies of s receiving inputs 1001;\nwhere i + j + k = m.\nFor any assignment in the category with parameter (i, j, k), the evaluation of m\nis clearly\ns\n(c + b)sj (c  b)sk . Let aij k be the summation of values of the part A over all assignments in the\ncategory (i, j, k). Note that aij k is independent on the value of s. Since i + j + k = m, we can\n(cid:88)\ndenote aij k by aj k . Then we rewrite the dot product summation and get\naj k (c + b)sj (c  b)sk .\n(cid:105) =\ns = (cid:104)A, m\nPl-Holants = Pl-Holant(cid:48)\ns\n0(cid:54)j+k(cid:54)m\n(cid:88)\nUnder this stratication, correspondingly we can dene (cid:48) and . Then we have\naj k (c + b)j (c  b)k .\nPl-Holant  = Pl-Holant (cid:48) = (cid:104)A, m (cid:105) =\nLet  = c + b and  = c  b. If we can obtain the value of p(, ) = (cid:80)\n0(cid:54)j+k(cid:54)m\n0(cid:54)j+k(cid:54)m\ntime, then we will have\nPl-Holant( (cid:54)=2 | f ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).\nLet  = c + b and  = c  b. Since c2  b2 (cid:54)= 0 or 1, we have  (cid:54)= 0,  (cid:54)= 0 and  (cid:54)= 1. Also,\nby assumption |c + b| (cid:54)= |c  b|, we have || (cid:54)= | |. Dene L = {(j, k)  Z2 | j  k = 1}. This is a\nsublattice of Z2 . Every lattice has a basis. There are 3 cases depending on the rank of L.\n L = {(0, 0)}. All j  k are distinct. It is an interpolation reduction in full power. That is,\n(cid:20) 0 0 0 1\n(cid:21)\nwe can interpolate p(, ) for any  and  in polynomial time. Let  = 4 and  = 0, that\nis b = 2 and c = 2, and hence M ( f ) =\n. That is, f is non-singular redundant. By\n0 2 2 0\n0 2 2 0\n1 0 0 0\nTheorem 2.21, Pl-Holant((cid:54)=2 | f ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\n L contains two independent vectors (j1 , k1 ) and (j2 , k2 ) over Q. Then the nonzero vectors\nj2 (j1 , k1 )  j1 (j2 , k2 ) = (0, j2k1  j1k2 ) and k2 (j1 , k1 )  k1 (j2 , k2 ) = (k2 j1  k1 j2 , 0) are in L.\nHence, both  and  are roots of unity. That is || = | | = 1. Contradiction.\n\naj k j k in polynomial\n\n36\n\n\f L = {(ns, nt) | n  Z}, where s, t  Z and (s, t) (cid:54)= (0, 0). Without loss of generality, we\nmay assume t (cid:62) 0, and s > 0 when t = 0. Also, we have s + t (cid:54)= 0, otherwise || = | |.\n . There are three cases depending on the values of s and t.\n 0\nContradiction. By Lemma 2.7, for any numbers  and  satisfying s t = 1, we can obtain\np(, ) in polynomial time. Since  = c + b and  = c  b, we have b = \nand c = +\n2 .\n2\n0\n0\n1\n0 \n+\n0\nThat is M ( f ) =\n2\n2\n\n0 +\n0\n2\n2\n0\n1\n0\n0\n If s (cid:62) 0 and s + t (cid:62) 2. Consider the function q(x) = (2  x)sxt  1. Since s (cid:62) 0 and\nt (cid:62) 0, it is a polynomial. Clearly, 1 is one of its roots and 0 is not its root. If q(x) has\nno other roots, then for some constant  (cid:54)= 0,\nq(x) = (2  x)sxt  1 = (x  1)s+t = (1)s+t((2  x)  1)s+t .\nNotice that xt |q(x) + 1, while xt (cid:45) (x  1)s+t + 1 for t (cid:62) 2. Also, notice that (2 \nx)s |q(x) + 1, while (2  x)s (cid:45) (1)s+t((2  x)  1)s+t for s (cid:62) 2. Hence, t = s = 1, which\n(cid:21)\n(cid:20) 0\n(cid:21)\n(cid:20) 0\nmeans  = 1. Contradiction.\nTherefore, q(x) has a root x0 , where x0 (cid:54)= 1 or 0. Let  = x0 and  = 2  x0 . Then\n0 0 1x0\n0\n1\n0\n0 1x0\ns t = 1 and M ( f ) =\n. Note that Mx2 x3 ,x1 x4 ( f ) =\n1\n0\n0\n1 1\n0\n1x0 0\n. Since\n1\n0\n0\n0\n1 1\n1x0 0 0\n0\n1\n0\n0\n0\n1  x0 (cid:54)= 0, f is non-singular redudant. By Theorem 2.21, Pl-Holant((cid:54)=2 | f ) is #P-hard\nand hence Pl-Holant ( (cid:54)=2 | f ) is #P-hard.\n If s < 0 and t > 0. Consider the function q(x) = xt  (2  x)s . Since t > 0 and s > 0,\nit is a polynomial. Clearly, 1 is one of its roots and 0 is not its root. Since t + s (cid:54)= 0,\nthe highest order term of q(x) is either xt or (x)s , which means the coecient of\nthe highest order term is 1. While the constant term of q(x) is 2s (cid:54)= 1. Hence,\nq(x) can not be of the form (x  1)max(t,s) for some constant  (cid:54)= 0. Moreover, since\nt + s (cid:54)= 0, max(t, s) (cid:62) 2, which means q(x) has a root x0 , where x0 (cid:54)= 1 or 0. Similarly,\nlet  = x0 and  = 2  x0 , and we have Pl-Holant ((cid:54)=2 | f ) is #P-hard.\n If s (cid:62) 0 and s + t = 1. In this case, we have s = 0, t = 1 or s = 1, t = 0 due to t (cid:62) 0.\n(cid:34) 0 0 0 1\n(cid:35)\n(cid:20) 0 0 0 1\n(cid:21)\n s = 1, t = 0. Let  = 1 and  = 1\n2 . Then we have 10 = 1 and M ( f ) =\n. Clearly, Pl-Holant( (cid:54)=2 | f (cid:48) ) (cid:54)T\n. Let M (f (cid:48) ) = 4Mx2 x3 ,x1 x4 ( f ) =\n0 1\n3\n4 0\n0 4 3 0\n4\n0 3 4 0\n1\n0 3\n4 0\n4\n1 0 0 0\nPl-Holant ((cid:54)=2 | f ). For M (f (cid:48) ), correspondingly we dene (cid:48) = 3 + 4 = 7 and  (cid:48) =\n1 0 0 0\n3  4 = 1. Obviously, (cid:48)\n(cid:54)= 0,  (cid:48)\n(cid:54)= 0, (cid:48) (cid:48)\n(cid:54)= 1, and |(cid:48) | (cid:54)= | (cid:48) |. Let L(cid:48) =\n{(j, k)  Z2 | (cid:48)j  (cid:48)k = 1}. Then we have L(cid:48) = {(ns(cid:48) , nt(cid:48) ) | n  Z}, where s(cid:48) = 0\n(cid:20) 0 0 0 1\n(cid:21)\nand t(cid:48) = 2. Therefore, s(cid:48) (cid:62) 0 and s(cid:48) + t(cid:48) (cid:62) 2. As we have showed above, we have\nPl-Holant ((cid:54)=2 | f (cid:48) ) is #P-hard, and hence Pl-Holant ( (cid:54)=2 | f ) is #P-hard.\n s = 0, t = 1. Let  = 3 and  = 1. Then we have 01 = 1 and M ( f ) =\n0 1 2 0\n.\n0 2 1 0\n1 0 0 0\nBy Theorem 2.22, Pl-Holant((cid:54)=2 | f ) is #P-hard, and hence Pl-Holant ((cid:54)=2 | f ) is\n#P-hard.\nCase 2: If c2  b2 (cid:54)= 0 and |c + b| = |c  b|, or c2  a2 (cid:54)= 0 and |c + a| = |c  a|. By rotational\n(cid:20) 0 0 0 a\n(cid:21)\nsymmetry, we may assume c2  b2 (cid:54)= 0 and |c + b| = |c  b|. Normalizing f by assuming c = 1, we\n, where 12  b2 (cid:54)= 0 and 12  b2 (cid:54)= a2 due to f / M . Since |1 + b| = |1  b|,\n0 b 1 0\nhave M (f ) =\n0 1 b 0\na 0 0 0\nb is a pure imaginary number.\n\n37\n\n\fM (f2 ) = Mx1 x2 ,x4 x3 (f1 )N Mx1 x2 ,x4 x3 (f1 ) =\n\nConnect variables x4 , x3 of a copy of signature f with variables x1 , x2 of another copy of\n 0\n .\nsignature f both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix\n0\n0\nb2 + 1\n0\n2b\nb2 + 1\n2b\n0\n(cid:35)\n(cid:34) 0\na2\n0\n0\n0\n1\n0\na. If c2  a2 = 0, that is a2 = 1, and then M (f1 ) =\nb2+1 0\n0 2b\n. Since b2 < 0, we have\n(cid:20) 0 0\n(cid:21)\n0 b2+1 2b 0\n1\n0\n0\n0\n(b2 + 1)2  (2b)2 = (b2  1)2 > 1 = (a2 )2 , which means f1 / M .\n0 1\n If b2 = 1, then M (f1 ) =\n. By Theorem 4.6, Pl-Holant((cid:54)=2 | f1 ) is #P-hard,\n0 2i 0 0\n(cid:34) 0\n(cid:35)\n0 0 2i 0\nand hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\n1 0\n0 0\n\n1\n0\n0\n If b2 = 2, then M (f1 ) =\n0 2\n2i 1\n\n0\n0 1 2\n2i 0\n0\n0\n0\n1\nsignature f2 with the signature matrix\n\n. Connect two copies of f1 , and we have a\n .\n0\n\n0\n1\n0\n7\n0 4\n\n0\n2i\n4\n7\n0\n2i 0\n0\n0\n1\n0\nIt is easy to check f2 / M . Then, f2 belongs to Case 1. Therefore, Pl-Holant((cid:54)=2 | f2 ) is\n#P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\n If b2 (cid:54)= 1 or 2, then b2 + 1 (cid:54)= 1, and hence 12  (b2 + 1)2 (cid:54)= 0 due to b (cid:54)= 0. Also,\n(cid:34) 0 0 0 a1\n(cid:35)\nsince b2 + 1 is a real number and b2 + 1 (cid:54)= 0, we have |(b2 + 1) + 1| (cid:54)= |(b2 + 1)  1|. Then,\nf1 / M has the signature matrix of form\n1  a2\n, where a1 b1 c1 (cid:54)= 0, c2\n1 (cid:54)= 0\n0 b1 c1 0\n0 c1 b1 0\na1 0 0 0\nand |c1 + a1 | (cid:54)= |c1  a1 |. That is, f1 belongs to Case 1. Therefore, Pl-Holant( (cid:54)=2 | f1 ) is\n#P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\nb. If c2  a2 (cid:54)= 0 and |c + a| = |c  a|, then a is also a pure imaginary number. Connect variables\n 0\n .\nx1 , x4 of a copy of signature f with variables x2 , x3 of another copy of signature f . We get\na signature f3 with the signature matrix\nb2\n0\n0\na2 + 1\n0\n2a\n0\na2 + 1\n0\n2a\n0\nb2\n0\n0\n0\nNote that f3  M implies (a2  1)2 = (b2 )2 . Since f / M , 1  a2 (cid:54)= b2 . Hence, f3  M\nimplies a2  1 = b2 . Similarly, f1  M implies b2  1 = a2 . Clearly, f1 and f3 can not be\nboth in M . Without loss of generality, we may assume f3 / M .\n If a2 (cid:54)= 1. There are two subcases.\n(cid:34) 0 0 0 a3\n(cid:35)\n (a2 + 1)2  (b2 )2 = 0. Since a is a pure imagianry number, |a2 + 1 + 2a| = |a + 1|2 =\n|a  1|2 = |a2 + 1  2a|. Then f3 has the signature matrix of form\n0 b3 c3 0\n0 c3 b3 0\na3 0 0 0\n\nM (f3 ) = Mx2 x3 ,x1 x4 (f )N Mx2 x3 ,x1 x4 (f ) =\n\nM (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (f ) =\n\na2\n0\n0\n0\n\n38\n\n,\n\n\f2i\n\n. Note that Mx2 x3 ,x1 x4 (f1 ) =\n\nwhere a3 b3 c3 (cid:54)= 0, c2\n3  b2\n3 (cid:54)= 0, |c3 + b3 | = |c3  b3 | and c2\n3  a2\n3 = 0. That is, f3 belongs\nto Case 2.a. Therefore, Pl-Holant((cid:54)=2 | f3 ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f )\nis #P-hard.\n (a2 + 1)2  (b2 )2 (cid:54)= 0. Since a2 + 1 and b2 are both nonzero real numbers due to a\n(cid:35)\n(cid:34) 0 0 0 a3\nand b are both pure imaginary numbers, we have |a2 + 1 + b2 | (cid:54)= |a2 + 1  b2 |. Then\n3  a2\n, where a3 b3 c3 (cid:54)= 0, c2\n3 (cid:54)= 0 and\n0 b3 c3 0\nf3 has the signature matrix of form\n0 c3 b3 0\na3 0 0 0\n|c3 + a3 | (cid:54)= |c3  a3 |. That is, f3 belongs to Case 1. Therefore, Pl-Holant( (cid:54)=2 | f3 ) is\n(cid:20) 0 0 0 b2\n(cid:21)\n#P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\n If a2 = 1 and b2 (cid:54)= 2, then M (f3 ) =\n, where |2a| = 2 (cid:54)= |b2 |. By Theorem\n0 2a 0 0\n(cid:34) 0\n(cid:35)\n0 0 2a 0\n4.6, Pl-Holant( (cid:54)=2 | f3 ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\nb2 0 0 0\n1\n\n0\n0\n(cid:34) 0\n(cid:35)\n If a2 = 1 and b2 = 2, then M (f1 ) =\n0 2\n2i 1\n\n0\n1 2\n0\n2i 0\n1\n\n0\n0\n0\n0 2\n0\n. We have f1 is non-singular redudant. Therefore, Pl-Holant((cid:54)=2 | f1 )\n1 1\n0\n0\n1 1\n\n0\n0\n2\nis #P-hard, and hence Pl-Holant( (cid:54)=2 | f ) is #P-hard.\n2i 0\n0\n0\nc. If c2  a2 (cid:54)= 0 and |c + a| (cid:54)= |c  a|. This is Case 1. Done.\nCase 3: c2  b2 = 0 and c2  a2 = 0. If c = b or c = a, then f is non-singular redudant,\n(cid:21)\n(cid:20) 0 0\n(cid:21)\n(cid:20) 0\nand hence Pl-Holant((cid:54)=2 | f ) is #P-hard. Otherwise, a = b = c. By normalization, we have\n0 1\n0\n0 1\n. Notice that 22  12 (cid:54)= 0 and |2+ 1| (cid:54)= |2 1|.\n0 1 1\n0 2 2 0\n0\n0 2 2 0\n1 1 0\n, and then M (f1 ) =\nM (f ) =\n0\n1 0\nThat is, f1 belongs to Case 1. Therefore, Pl-Holant( (cid:54)=2 | f1 ) is #P-hard, and hence Pl-Holant( (cid:54)=2 | f )\n0 0\n1 0\n0\n0\nis #P-hard.\n, abcxyz (cid:54)= 0.\n0 0 0 a\nLemma 6.2. Let f be a 4-ary signature with the signature matrix\n0 b\nc 0\n0 z y 0\nx 0 0 0\nIf by  cz = 0 or ax  cz = 0, then Pl-Holant( (cid:54)=2 | f ) is #P-hard.\n(cid:20) 0 0 0 a\n(cid:21)\nProof. By rotational symmetry, we assume by  cz = 0. By normalization, we assume b = 1,\n0 1 c 0\nand then y = cz . That is, Mx1 x2 ,x4 x3 (f ) =\n.\n0 z cz 0\nx 0 0 0\n If 1 + c (cid:54)= 0. Connect the variables x4 with x3 of f using ((cid:54)=2 ), and we get a binary signature\ng1 , where\n\nM (f ) =\n\ng1 = Mx1 x2 ,x4 x3 (f )(0, 1, 1, 0)T = (0, 1 + c, (1 + c)z , 0)T .\n(cid:21)\n(cid:20) 0\nNote that g1 (x1 , x2 ) can be normalized as (0, z1 , 1, 0)T . That is g(x2 , x1 ) = (0, 1, z1 , 0)T .\nConnect the variable x1 of g1 with the variable x1 of f , and we get a signature f1 with the\n0 0 a\n. Connect the variable x1 with x2 of f1 using ( (cid:54)=2 ), and we get a\n0\n1 c 0\n0\n1 c 0\nxz1 0 0 0\nbinary signature g2 , where\ng2 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 (f ))T = (0, 2, 2c, 0)T .\n\nsignature matrix\n\n39\n\n\fNote that g2 (x1 , x2 ) can be normalized as (0, c1 , 1, 0)T , That is g2 (x2 , x1 ) = (0, 1, c1 , 0)T .\n(cid:20) 0\n(cid:21)\nConnect the variable x1 of g2 with the variable x3 of f1 , and we get a signature f2 with\n0 0 ac1\n0\n0\n1 1\n. It is non-singular redudant. By Lemma 2.21, we have\nthe signature matrix\n0\n0\n1 1\nxz1 0 0\nPl-Holant( (cid:54)=2 | f2 ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.\n0\n If 1 + z (cid:54)= 0, then connect the variable x1 with x2 of f using ((cid:54)=2 ), and we get a binary\nsignature g (cid:48)\n1 , where\n\nand Mx3 x4 ,x2 x1 (f ) =\n\n,\n\nM (f3 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =\n\ng (cid:48)\n1 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 )T = (0, 1 + z , (1 + z )c, 0)T .\n1 (x1 , x2 ) can be normalized as (0, c1 , 1, 0)T . Same as the analysis of the case\nNote that g (cid:48)\n(cid:20) 0 0\n(cid:21)\n1 + c (cid:54)= 0, we still have Pl-Holant((cid:54)=2 | f ) is #P-hard.\n(cid:21)\n(cid:20) 0 0\n0 a\n Otherwise, 1 + c = 1 and 1 + z = 1, that is c = z = 1. Then Mx1 x2 ,x4 x3 (f ) =\n0 1 1 0\n0 1 1 0\nx 0\n0 0\n0 x\n0 1 1 0\n0 1 1 0\n. Connect variables x4 , x3 of a copy of signature f with\n ,\n 0\na 0\n0 0\nvariables x3 , x4 of another copy of signature f , and we get a signature f3 with the signature\nmatrix\nax\n0\n0\n0 2\n0\n2\n2 2\n0\n0\n0\nax\n0\n0\nClearly, ax (cid:54)= 0 and f3 / M . By Lemma 6.1, Pl-Holant ((cid:54)=2 | f3 ) is #P-hard and hence\nPl-Holant ((cid:54)=2 | f ) is #P-hard.\nIn the following Lemmas 6.3, 6.4, 6.6 and Corollaries 6.5, 6.7, let f be a 4-ary signature with\n,\n0 0 0 a\nthe signature matrix\n0 b\nc 0\nM (f ) =\n0 z y 0\nwhere abxyz (cid:54)= 0, det (cid:2) b c\n(cid:3) = by  cz (cid:54)= 0 and det [ a z\nx 0 0 0\nc x ] = ax  cz (cid:54)= 0. Moreover f / M , that is\nz y\ncz  by (cid:54)= ax.\nLemma 6.3. Let g = (0, 1, t, 0)T be a binary signature, where t (cid:54)= 0 is not a root of unity. Then\nPl-Holant( (cid:54)=2 | f , g) is #P-hard.\nProof. Let B = {g1 , g2 , g3} be a set of three binary signatures gi = (0, 1, ti , 0)T . By Lemma\n2.5, we have Pl-Holant ( (cid:54)=2 | {f }  B) (cid:54) Pl-Holant ((cid:54)=2 | f , g) . We will show Pl-Holant ((cid:54)=2 | {f }  B)\nis #P-hard and it follows Pl-Holant ( (cid:54)=2 | f , g) is #P-hard.\n(cid:20) 0\n(cid:21)\nConnect the variable x2 of gi (i = 1, 2) with the variable x1 of f using ((cid:54)=2 ) separately. We\n0\n0 a\n0\nb\nc 0\n. Note that det MIn (fti ) =\nget two signatures fti with the signature matrix M (fti ) =\n0 ti z ti y 0\nti x 0\n0 0\nti det MIn (f ) and det MOut (fti ) = ti det MOut (f ). Connect variables x4 , x3 of f with variables x1 ,\n\n40\n\n\fM (f1 ) =\n\nM (f2 ) =\n\n 0\n 0\n = M (f )N M (ft1 ) =\n .\nx2 of ft1 both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix\na2\n0\n0\na1\n0\n0\nt1 by + c2\n0\nb1\nc1\n0\n0\nt1 bz + bc\n0\nt1z 2 + yb\n0\nz1 y1\n0\n0\nt1yz + yc\n0\nt1x2\n0\n0\n0\nx1\n0\n0\n0\nWe rst show that there is a t1 (cid:54)= 0 such that b1y1 c1z1 (cid:54)= 0 and (b1z )(y1 c)  (c1 b)(z1y) (cid:54)= 0.\nConsider the quadratic function p(t) = (tbz + bc)(tyz + yc)cz  (tby + c2 )(tz 2 + yb)by . Then, we\nhave p(t1 ) = (b1z )(y1 c)  (c1 b)(z1y). Notice that the coecient of the quadratic term in p(t) is\nbyz 2 (cz  by). It is not equal to zero since byz 2 (cid:54)= 0 and cz  by (cid:54)= 0. That is, p(t) has degree 2, and\nhence it has at most two roots. Also, b1y1 = 0 implies t1 =  c\nz , c1 = 0 implies t1 =  c2\nby , and z1 = 0\nimplies t1 =  yb\nz ,  c2\nz2 . Therefore we can choose such a t1 that does not take these values 0,  c\nby\nand  yb\nz2 , and t1 is not a root of p(t). That is, t1 (cid:54)= 0, b1y1 c1z1 (cid:54)= 0 and (b1z )(y1 c)  (c1 b)(z1y) (cid:54)= 0.\nConnect variables x4 , x3 of f1 with variables x1 , x2 of ft2 both using ( (cid:54)=2 ). We get a signature\n\n 0\n = M (f1 )N M (ft2 ) =\n 0\nf2 with the signature matrix\n0\n0\na2\na1a\n0\n0\n0\n0\n0\nc2\nb2\n0\nt2 b1z + c1 b\nt2 b1y + c1 c\n0\n0\n0\nz2 y2\n0\nt2z1z + y1 b\nt2z1y + y1 c\n0\nx2\nt2x1x\n0\n0\n0\n0\n0\nSince b1z (cid:54)= 0 and c1 b (cid:54)= 0, we can let t2 =  c1 b\nb1 z and t2 (cid:54)= 0. Then b2 = t2 b1z + c1 b = 0. Since\n(b1z )(y1 c)  (c1 b)(z1y) (cid:54)= 0, we have y2 = t2z1y + y1 c (cid:54)= 0. Notice that\ndet MIn (f2 ) = det MIn (f1 )  (1)  det MIn (ft2 )\n= det MIn (f )  (1)  det MIn (ft1 )  (1)  det MIn (ft2 )\n= t1 t2 det MIn (f )3\n(cid:54)= 0.\n(cid:34) 0 0 0 a2\n(cid:35)\nWe have det MIn (f2 ) = b2y2  c2z2 = c2z2 (cid:54)= 0. Similarly, we have det MOut (f2 ) = a2x2 =\nt1 t2 det MOut (f )3 (cid:54)= 0. Therefore, M (f2 ) is of the form\n, where a2x2y2 c2z2 (cid:54)= 0. That\n0 0 c2 0\n0 z2 y2 0\nx2 0 0 0\nis, f2 is a signature in Case II. If f2 / M , then Pl-Holant ((cid:54)=2 | f2 ) is #P-hard by Theorem 5.2, and\nhence Pl-Holant ( (cid:54)=2 | {f }  B) is #P-hard.\ndet MIn (f )3\ndet MOut (f )3 = 1. Since f / M ,\nOtherwise, f2  M , which means\ndet MIn (f2 )\n= 1. That is\ndet MOut (f2 )\ndet MIn (f )7\ndet MOut (f )7 (cid:54)= 1. Similar to the construction of f1 , we construct f3 .\n(cid:54)= 1, and hence\ndet MIn (f )\n(cid:35)\n(cid:34) 0\ndet MOut (f )\nFirst, connect the variable x2 of g3 with the variable x1 of f1 using ( (cid:54)=2 ). We get a signature f1t3\n0 a1\n0\n0\nb1\nc1\n0\n. Note that det MIn (f1t3 ) = t3 det MIn (f1 )\nwith the signature matrix M (f1t3 ) =\nt3 z1 t3 y1 0\n0\nt3 x1\n0\n0\n0\nand det MOut (f1t3 ) = t3 det MOut (f1 ). Then connect variables x4 , x3 of f1 with variables x1 , x2 of\n\n41\n\n\fM (f3 ) =\n\n 0\n 0\n = M (f1 )N M (f1t3 ) =\n .\nf1t3 both using ( (cid:54)=2 ). We get a signature f3 with the signature matrix\na2\n0\n0\na3\n0\n0\nt3 b1y1 + c2\n0\nb3\nc3\n0\n0\nt3 b1z1 + b1 c1\n0\n1\nt3z 2\n0\nz3 y3\n0\n0\nt3y1z1 + y1 c1\n1 + y1 b1\n0\nt3x2\n0\n0\n0\nx3\n0\n0\n0\n1\nand t3 (cid:54)= 0. Then b3 = b1 (t3z1 + c1 ) = 0 and\nSince c1 (cid:54)= 0 and z1 (cid:54)= 0, we can dene t3 =  c1\nz1\ny3 = y1 (t3z1 + c1 ) = 0. Notice that\ndet MIn (f3 ) = det MIn (f1 )  (1)  det MIn (f1t3 )\n=  det MIn (f1 )  t3 det MIn (f1 )\n= t3 (det MIn (f )  (1)  det MIn (ft1 ))2\n= t3 t2\n1 det MIn (f )4\n(cid:54)= 0\n(cid:34) 0 0 0 a3\n(cid:35)\nWe have det MIn (f3 ) = c3z3 (cid:54)= 0 and similarly, det MOut (f3 ) = a3x3 = t3 t2\n1 det MOut (f )4 (cid:54)= 0.\nwhere a3x3 c3z3 (cid:54)= 0.\n0 0 c3 0\nThat is, M (f3 ) is of the form\n0 z3 0 0\nx3 0 0 0\nConnect variables x4 , x3 of f2 with variables x1 , x2 of f3 both using ( (cid:54)=2 ). We get a signature\n .\n 0\n 0\n = M (f2 )N M (f3 ) =\nf4 with the signature matrix\n0\n0\n0\n0\nx4\nx2x3\nClearly, f4 is a signature in Case II. Also, notice that\ndet MIn (f4 ) = det MIn (f2 )  (1)  det MIn (f3 )\n= t1 t2 det MIn (f )3  t3 t2\n1 det MIn (f )4\n= t3 t2 t3\n1 det MIn (f )7 .\n\n0\n0\nc2 c3\n0\nz2z3 y2 c3\n0\n0\n\na2a3\n0\n0\n0\n\nM (f4 ) =\n\n0\n0\nb4\nc4\nz4 y4\n0\n0\n\na4\n0\n0\n0\n\nand\n\ndet MOut (f4 ) = t3 t2 t3\n1 det MOut (f )7 .\n\nWe have\n\ndet MIn (f )7\ndet MOut (f )7 (cid:54)= 1,\ndet MIn (f4 )\n=\ndet MOut (f4 )\nwhich means f4 / M . By Theorem 5.2, Pl-Holant ( (cid:54)=2 | f4 ) is #P-hard, and hence Pl-Holant ((cid:54)=2 | {f }  B)\nis #P-hard.\n\nLemma 6.4. Let g = (0, 1, t, 0)T be a binary signature where t is an n-th primitive root of unity,\nand n  5. Then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.\n\n42\n\n\f=\n\n.\n\nProof. Note that Mx1 ,x2 (g) = [ 0 1\nt 0 ]. Connect the variable x2 of a copy of signature g with the\n(cid:21) (cid:20)0 1\n(cid:21) (cid:20)0 1\n(cid:20)0 1\n(cid:21)\n(cid:20) 0\n(cid:21)\nvariable x1 of another copy of signature g using ( (cid:54)=2 ). We get a signature g2 with the signature\nmatrix\n1\nMx1 ,x2 (g2 ) =\nt2 0\nt 0\n1 0\nt 0\nThat is, g2 = (0, 1, t2 , 0)T . Similarly, we can construct gi = (0, 1, ti , 0)T for 1 (cid:54) i (cid:54) 5. Here, g1\ndenotes g . Since the order n (cid:62) 5, gi are all distinct.\nConnect variables x4 , x3 of signature f with variables x1 , x2 of gi for 1 (cid:54) i (cid:54) 5 respectively.\n 0\n =\n 0\n\n .\n0 0 0 a\nWe get binary signatures hi , where\nb + cti\n1\n0 b\nc 0\nhi = Mx1 x2 ,x4 x3 (f )gi =\nti\nz + yti\n0 z y 0\n(cid:3) = by  cz (cid:54)= 0, (z) is a Mobius transformation of the extended\n. Since det (cid:2) b c\nx 0 0 0\n0\n0\nz + yz\ncomplex plane (cid:98)C. We rewrite hi in the form of (b + cti )(0, 1, (ti ), 0)T , with the understanding that\nLet (z) =\nz y\nb + cz\nif b + cti = 0, then (ti ) = , and we dene (b + cti )(0, 1, (ti ), 0)T to be (0, 1, z + yti , 0)T . If there\nis a ti such that (ti ) is not a root of unity, and (ti ) (cid:54)= 0 and (ti ) (cid:54)= , by Lemma 6.3, we have\n0,  or a root of unity for 1 (cid:54) i (cid:54) 5. Since (z) is a bijection of (cid:98)C, there is at most one ti such\nPl-Holant ((cid:54)=2 | f , hi ) is #P-hard, and hence Pl-Holant ( (cid:54)=2 | f , g1 ) is #P-hard. Otherwise, (ti ) is\nthat (ti ) = 0 and at most one ti such that (ti ) = . That means, there are at least three ti such\nthat |(ti )| = 1. Since a Mobius transformation is determined by any 3 distinct points, mapping\n3 distinct points from S 1 to S 1 implies that this (z) maps S 1 homeomorphically onto S 1 . Such a\nMobius transformation has a special form: M(, ei ) = ei (z + )\n, where || (cid:54)= 1.\n(cid:20) 0 0 0 y\n(cid:21)\n1 + z\nBy normalization in signature f , we may assume b = 1. Compare the coecients, we have\nc = , y = ei and z = ei . Here  (cid:54)= 0 due to z (cid:54)= 0. Also, since Mx2 x3 ,x1 x4 (f ) =\n0 a z 0\n0 c x 0\nb 0 0 0\nc x ] = ax  cz (cid:54)= 0, we have another Mobius transformation (z) =\nc + xz\ndet [ a z\n. Plug in c =  and\na + z z\nz = ei , we have\n\na + x\n\na z\n + xz\na + ei z\n1 + ei\na z\nBy the same proof for (z), we get Pl-Holant ( (cid:54)=2 | f , g) is #P-hard, unless (z) also maps S 1 to\nS 1 . Hence, we can assume (z) has the form M( , ei (cid:48)\n, where | | (cid:54)= 1. Compare the\n) = ei (cid:48) (z +  )\n\n1 +  z\ncoecients, we have\nei\n= \na\n= ei(cid:48)\n\na\n= ei(cid:48)\nx\na\n . Let  = \nei and x = \nSolving this equation, we get a = \n, and we have a =  ei and\n\n\nx =  , where | | (cid:54)= || since | | (cid:54)= 1 and  (cid:54)= 0 since x (cid:54)= 0. Then, we have signature matrices\n\n(z) =\n\nand\n\n=\n\n.\n\n.\n\n\n\n43\n\n\f(cid:35)\n\n(cid:35)\n\n(cid:35)\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\nM (f1 ) =\n\nMx1 x2 ,x4 x3 (f ) =\n\n, Mx3 x4 ,x2 x1 (f ) =\n\n(1 + 2 )ei\n( + )\n0\n\n0\n(1 + 2 )ei\n( + )\n\n(cid:34) 0\n(cid:34) 0\n(cid:34) 0\n0\n0 \n0  ei\nei\n0\n0\n0\n(cid:35)\n(cid:34) 0 0\nei  0\n0  ei ei 0\n0\n0\n\n1\n0\n, Mx2 x3 ,x1 x4 (f ) =\n0 ei 1 0\n0 ei ei\n0\n20\n0\n\n\n ei\n0\n\n0\n0\n0\n0 0\n1\n0\n0\n0\n1\n0\n0  ei 0\n. Connect variables x4 , x3 of a copy of signature f with variables\nand Mx4 x1 ,x3 x2 (f ) =\n0   ei 0\n .\n 0\nei 0\nx3 , x4 of another copy of signature f using ( (cid:54)=2 ). We get a signature f1 with the signature matrix\n0\n0\n  ei\n0\n0\n( + )ei\n1 + 2\n0\n0\nM (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =\n( + )ei\n(1 + 2 )ei2\n0\n0\n  ei\n0\n0\n0\n\n .\n If  +  (cid:54)= 0, normalizing Mx1 x2 ,x4 x3 (f1 ) by dividing by ( + )ei , we have\n \n( + )\n \n( + )\n(1 + 2 )ei\n(1 + 2 )ei\n(1 + 2 )ei\n, and then  =\nare conjugates. Let  =\nand\nNote that\n( + )\n( + )\n( + )\n(1 + 2 )ei\n(cid:3) . Notice\ndue to ||2 (cid:54)= 1. Consider the inner matrix of M (f1 ), we have MIn (f1 ) = (cid:2) 1 \n(1 + 2 )(1 + 2 )\n(cid:54)= 1 due to det MIn (f1 ) (cid:54)= 0, and  (cid:54)= 0\n. We have | |2 =   =\n(cid:12)(cid:12)(cid:12) 1| |\n(cid:12)(cid:12)(cid:12) (cid:54)= 1, which\n( + )2\n( + )\n 1\nthat the two eigenvalues of MIn (f1 ) are 1 + | | and 1  | |, and obviously\n1+| |\nmeans there is no integer n and complex number C such that M n\nIn (f1 ) = C I . Note that\nis a Mobius transformation of the form M(, 1) mapping S 1 to S 1 .\n + z\n1 + z\nConnect variables x4 , x3 of signature f1 with variables x1 , x2 of signatures gi . We get binary\n .\n 0\n = (1 +  ti )\n 0\n =\n 0\n0 0 0 \n\nsignatures g(i,1 ) , where\n0 1  0\n1 +  ti\n1\n1\nti\n + ti\n1 (ti )\n0  1 0\n 0 0 0\n0\n0\n0\nSince 1 is a Mobius transformation mapping S 1 to S 1 and |ti | = 1, we have |1 (ti )| = 1,\nwhich means 1 +  ti (cid:54)= 0. Hence, g(i,1 ) can be normalized as (0, 1, 1 (ti ), 1)T . Successively\n1 ) by connecting f1 with g(i,n1\n) . We have\nconstruct binary signatures g(i,n\n1\n(cid:0)1 + k\n1 (ti )(cid:1). We know C(i,n) (cid:54)= 0, because for any k , 1+ k1\nwhere C(i,n) = (cid:81)\n) = M n (f1 )gi = C(i,n) (0, 1, n\n1 (ti ), 1)T ,\n1 ) = M (f1 )g(i,n1\ng(i,n\n1\n1\n0(cid:54)k(cid:54)n1\n| + k1\n(ti )|\ndue to |k\n1 (ti )| =\n1\n1 ) can be normalized as (0, 1, n\n1 (ti ), 1)T .\n(ti )| = 1. Hence, g(i,n\n|1 + k1\n1\n\ng(i,1 ) = Mx1 x2 ,x4 x3 (f1 )gi =\n\n(ti ) (cid:54)= 0\n\n1 (z) =\n\n44\n\n\f.\n\n(cid:18) 1\n(cid:18) 1\n(cid:19)\n(cid:19)\nNotice that the nonzero entries (1, n\n1 (ti ))T of g(i,n\n1 ) are completely decided by the inner\nmatrix MIn (f1 ). That is\nM n\nIn (f1 )\n= C(i,n)\nti\n1 (ti )\nn\nIf for each i  {1, 2, 3}, there is some ni (cid:62) 1 such that (1, ni\n1 (ti ))T = (1, ti )T , then n0\n1 (ti ) = ti ,\nwhere n0 = n1n2n3 for 1 (cid:54) i (cid:54) 3, i.e., the Mobius transformation n0\n1 xes three distinct\ncomplex numbers t, t2 , t3 . So the Mobius transformation is the identity map, i.e., n0\n1 (z) = z\nfor all z  C. This implies that M n0\nIn (f1 ) = C [ 1 0\n0 1 ] for some constant C . This contradicts the\nfact that the ratio of the eigenvalues of MIn is not a root of unity. Therefore, there is an i such\n1 (ti ))T are all distinct for n  N. Then, we can realize polynomially many distinct\nthat (1, n\n1 (ti ), 1)T . By Lemma 2.6, we have Pl-Holant((cid:54)=2 | f , g) is\nbinary signatures of the form (0, 1, n\n#P-hard.\n Otherwise  +  = 0, which means  is a pure imaginary number. Suppose  = mi, where\nm  R and |m| (cid:54)= 0 or 1. Connect variables x1 , x4 of a copy of signature f with variables x4 ,\nx1 of another copy of signature f , we get a signature f2 with the signature matrix\n\n 0\n\n0 0 0 1\n\n0\nM (f2 ) = Mx2 x3 ,x1 x4 (f )N Mx4 x1 ,x3 x2 (f )\nei\n0\n0\n0\n ei miei\n0\n\n0 0 1 0\n0\n0\n0 mi\n0 mi\n 0\n .\n0 1 0 0\n0\n\nei\n1 0 0 0\n1\n0\n0\n0\n0\nei\n0\n0\n( 2  m2 )ei2\n( +  )miei\n0\n0\n( +  )miei\n 2  m2\n0\n0\nei\n0\n0\n0\n\n .\n If  +  (cid:54)= 0, normalizing M (f2 ) by dividing by ( +  )miei , we have\n( 2  m2 )ei\n( +  )mi\n1\n( 2  m2 )ei\n( +  )mi\n1\n( 2  m2 )ei\n( 2  m2 )ei\n( 2  m2 )ei\n( +  )mi\n( +  )mi\n( +  )mi\nNote that\n, and\nare conjugates. Let  =\nand\nthen | | (cid:54)= 1 due to det MIn (f2 ) (cid:54)= 0, and  (cid:54)= 0 due to | | (cid:54)= |m|. Same as the analysis\nof MIn (f1 ), the ratio of the two eigenvalues of MIn (f2 ) is also not equal to 1, which\nmeans there is no integer n and complex number C such that M n\nIn (f2 ) = C I . Notice\nis also a Mobius transformation of the form M( , 1) mapping S 1\n + w\n1 +  w\nto S 1 . Similarly, we can realize polynomially many distinct binary signatures, and hence\nPl-Holant( (cid:54)=2 | f , g) is #P-hard.\n Otherwise,  +  = 0, which means  is a real number. Suppose  = n, where n  R\nand |n| (cid:54)= 0 or |m|. Connect variables x4 , x3 of a copy of signature f with variables x1 ,\n\n0\n1\nmiei 0\n ei\n0\n0\n0\n\nthat 2 (w) =\n\n=\n\n=\n\nMIn (f2 ) =\n\n45\n\n\f=\n\n=\n\nM (f (cid:48) ) =\n\nnei\n0\n0\n0\n\n0\nmi\nei\n0\n\nx2 of another copy of signature f , we get a signature f (cid:48) with the signature matrix\n\n 0\n\n0 0 0 1\n\n 0\nM (f (cid:48) ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (f )\nnei\n0\n0\n0\nmi\n0 0 1 0\n0\n1\n0\n1\n0\n 0\n .\n0 miei\nei\n0 miei\n0 1 0 0\n0\n1 0 0 0\nn\n0\nn\n0\n0\n0\nn2ei2\n0\n0\nei  m2\n(ei  1)mi\n0\n0\n(ei2  ei )mi\nei  ei2m2\n0\n0\n(cid:20) 0 0 0 \n(cid:21)\nn2\n0\n0\n0\n If ei = 1, then M (f ) =\n 1 ] . Since || (cid:54)= 1, same as the\n, and MIn (f ) = [ 1 \n0 1  0\n0  1 0\n 0 0 0\nanalysis of MIn (f1 ), we can realize polynomially many binary signatures, and hence\nPl-Holant( (cid:54)=2 | f , g) is #P-hard.\n .\n\n Otherwise ei (cid:54)= 1, normalizing M (f (cid:48) ) by dividing by (ei  1)mi, we have\nn2\n(ei  1)mi\nei  m2\n1  eim2\nn2ei\nn2\n(ei  1)mi\n(ei  1)mi\n(ei  1)mi\n(ei  1)mi\nare conjugates, and\nNote that\n1  eim2\n(cid:34) 0\n(cid:35)\nn2ei\nare conjugates. Let (cid:48) =\n. Then M (f (cid:48) ) =\n(ei  1)mi\n(ei  1)mi\n0  (cid:48) ei\n0\n. Notice that M (f (cid:48) ) and M (f ) have the same forms. Similar to\n(cid:48)\n0\n0\n1\n0 (cid:48) ei ei\n0\n (cid:48)\n0\n0\n0\nthe construction of f2 , we can construct a signature f (cid:48)\n2 using f (cid:48) instead of f . Since\n (cid:48) +  (cid:48) =  n2ei\n=  n2\nn2\n(cid:54)= 0, by the analysis of f2 , we can\n(ei  1)mi\n(ei  1)mi\n+\nmi\nstill realize polynomially many binary signatures and hence Pl-Holant((cid:54)=2 | f , g) is\n#P-hard.\nRemark: The order n (cid:62) 5 promises that there are at least three points mapped to points on S 1 ,\nsince at most one point can be mapped to 0 and at most one can be mapped to . When the\norder n is 3 or 4, if no point is mapped to 0 or , then there are still at least three points mapped\nto points on S 1 . So, we have the following corollary.\n\n0\nei  m2\n(ei  1)mi\nei\n\n1\n1  eim2\n(ei  1)mi\n0\n\nn2ei\n(ei  1)mi\n0\n\nand  (cid:48) =\n\n0\n\n0\n\nand\n\n0\n\n ei\n\n0\n\n0\n\n0\n\nand\n\n ei\n\n0\n\nCorollary 6.5. Let g = (0, 1, t, 0)T be a binary signature where t is an n-th primitive root of unity,\nand n = 3 or 4. Let gm denote (0, 1, tm , 0)T . For any (i, j, k , (cid:96)) which is a cyclic permutation\n\n46\n\n\fof (1, 2, 3, 4), if there is no gm such that Mxi xj ,x(cid:96) xk (f )gm = d1 (0, 1, 0, 0)T or d2 (0, 0, 1, 0)T , where\nd1 , d2  C , then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.\nLemma 6.6. Let g = (0, 1, 0, 0)T be a binary signature. Then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.\n\nProof. Connect variables x4 , x3 of the signature f with variables x2 and x1 of g both using\n((cid:54)=2 ). We get a binary signature g1 , where\n\ng1 = Mx1 x2 ,x4 x3 (f )(0, 1, 0, 0)T = (0, 1, z , 0)T .\nNote that g1 (x1 , x2 ) can be normalized as (0, z1 , 1, 0)T since z (cid:54)= 0. That is, g1 (x2 , x1 ) =\n(cid:34) 0\n(cid:35)\n(cid:20) 0 0 0 a\n(cid:21)\n(0, 1, z1 , 0). Then connect the variable x1 of g1 with the variable x1 of f . We get a signature\na\n0\n0\n, where x1y1 (cid:54)= 0.\n0\n1\nc\n0\n0 1 c 0\n1 yz1 0\nf1 with the signature matrix M (f1 ) =\ndenoted by\n0 1 y1 0\n0\nxz1 0\nx1 0 0 0\n0\n0\n If c = 0, connect variables x4 , x3 of f1 with variables x1 , x2 of g both using ( (cid:54)=2 ). We get a\nbinary signature h1 , where\n\nh1 = Mx1 x2 ,x4 x3 (f1 )(0, 0, 1, 0)T = (0, 1, y1 , 0)T .\nAlso, connect the variable x4 with x3 of f1 using ( (cid:54)=2 ). We get a binary signature h2 , where\n\nh2 = Mx1 x2 ,x4 x3 (f1 )(0, 1, 1, 0)T = (0, 2, y1 , 0)T .\n2 , 0)T . Clearly, |y1 | (cid:54)= | y1\n2 |, so they can not both be\nNote that h2 can be normalized as (0, 1, y1\nroots of unity. By Lemma 6.3, Pl-Holant ( (cid:54)=2 | f , h1 , h2 ) is #P-hard, and hence Pl-Holant ((cid:54)=2 | f , g)\nis #P-hard.\n Otherwise c (cid:54)= 0. Connect variables x2 , x1 of g with variables x1 , x2 of f both using ((cid:54)=2 ).\nWe get a binary signature g2 , where\n\ng2 = ((0, 1, 0, 0)Mx1 x2 ,x4 x3 (f1 ))T = (0, 1, c, 0)T .\nNote that g2 (x1 , x2 ) can be normalized as (0, c1 , 1, 0)T since c (cid:54)= 0. That is, g2 (x2 , x1 ) =\n(cid:34) 0\n(cid:35)\n(cid:20) 0 0 0 a2\n(cid:21)\n(0, 1, c1 , 0)T . Then connect the variable x1 of g2 with the variable x3 of f . We get a signature\nac1\n0\n0\n(cid:20) 0 0 0 y2\n(cid:21)\n0\n0\n1\n1\n0 1 1 0\n1 yz1 c1\n0 1 y2 0\n0\n0\nxz1 0\nx2 0 0 0\n0\n0\na2x2y2 (cid:54)= 0. Notice that Mx2 x3 ,x1 x4 (f2 ) =\n0 a2 1 0\n. Connect variables x1 , x4 of signature\n0 1 x2 0\nf2 with variables x2 , x1 of g both using ( (cid:54)=2 ). We get a binary signature h3 , where\n1 0 0 0\n\nf2 with the signature matrix M (f2 ) =\n\ndenoted by\n\n, where\n\nh3 = Mx2 x3 ,x1 x4 (f2 )(0, 1, 0, 0)T = (0, a2 , 1, 0)T .\n\nh3 can be normalized as (0, 1, 1\n, 0)T . Also connect variables x1 , x4 of signature f2 with\nvariables x1 , x2 of g both using ( (cid:54)=2 ). We get a binary signature h4 , where\na2\n\nh4 = Mx2 x3 ,x1 x4 (f2 )(0, 0, 1, 0)T = (0, 1, x2 , 0)T .\nIf |a2 | (cid:54)= 1 or |x2 | (cid:54)= 1, then a2 or x2 is not a root of unity. By Lemma 6.3, Pl-Holant ((cid:54)=2 | f , h3 , h4 )\nis #P-hard, and hence Pl-Holant ( (cid:54)=2 | f , g) is #P-hard. Otherwise, |a2 | = |x2 | = 1. Same as\n\n47\n\n\f1 and h(cid:48)\nthe construction of h1 and h2 , construct binary signatures h(cid:48)\n2 using f2 instead of f1 .\nWe get\nh(cid:48)\n1 = Mx1 x2 ,x4 x3 (f2 )(0, 0, 1, 0)T = (0, 1, y2 , 0)T ,\n\nand\n\nh(cid:48)\n2 = Mx1 x2 ,x4 x3 (f2 )(0, 1, 1, 0)T = (0, 2, 1 + y2 , 0)T .\nNote that h(cid:48)\n2 can be normalized as (0, 1, 1+y2\n, 0)T .\n If y2 is not a root of unity, then by Lemma 6.3, Pl-Holant ( (cid:54)=2 | f , h(cid:48)\n2\n1 ) is #P-hard, and\nhence Pl-Holant ( (cid:54)=2 | f , g) is #P-hard.\n If y2 is an n-th primitive root of unity and n (cid:62) 5, then by Lemma 6.4, Pl-Holant ((cid:54)=2 | f , h(cid:48)\n1 )\nis #P-hard, and hence Pl-Holant ((cid:54)=2 | f , g) is #P-hard.\n\n), 2 or i, then 0 < | 1+y2\n| < 1, which means it is not zero\n If y2 = ( = 1+\n3i\nneither a root of unity. By Lemma 6.3, Pl-Holant ( (cid:54)=2 | f , h(cid:48)\n2\n2\n2 ) is #P-hard, and hence\nPl-Holant ((cid:54)=2 | f , g) is #P-hard.\n If y2 = 1, then f2 is non-singular redudant and hence Pl-Holant ( (cid:54)=2 | f , g) is #P-hard.\n 0\n .\n If y2 = 1. Connect two copies of f2 , we get a signature f3 with the signature matrix\n0 0 a2\n2\n0\n0\n2 0\n0\n0 2\n0\nx2\n2 0 0\n0\n2 | = 1 (cid:54)= 4. Therefore, {a2\nSince |a2 | = |x2 | = 1, |a2\n2 , 2} does not belong to\n2x2\n2 , 2, x2\ncase (i) or case (ii) in Theorem 4.6. Hence, Pl-Holant ((cid:54)=2 | f3 ) is #P-hard, and hence\nPl-Holant ((cid:54)=2 | f , g) is #P-hard.\nCombine Lemma 6.4, Corollary 6.5 and Lemma 6.6. We have the following corollary.\n\nM (f3 ) = Mx1 x2 ,x4 x3 (f2 )N Mx1 x2 ,x4 x3 (f2 ) =\n\nCorollary 6.7. Let g = (0, 1, t, 0)T be a binary signature where t is an n-th primitive root of unity,\nand n (cid:62) 3. Then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.\nNow, we are able to prove the following theorem for Case IV.\n0 0 0 a\n,\nTheorem 6.8. Let f be a 4-ary signature with the signature matrix\n0 b\nc 0\n0 z y 0\nx 0 0 0\nwhere abxyz (cid:54)= 0. Pl-Holant( (cid:54)=2 | f ) is #P-hard unless f  M , in which case, Pl-Holant( (cid:54)=2 | f ) is\ntractable.\n\nM (f ) =\n\nProof. Tractability follows by 2.15.\nNow suppose f / M . Connect the variable x4 with x3 of f using ((cid:54)=2 ), and we get a binary\nsignature g1 , where\n\ng1 = Mx1 x2 ,x4 x3 (0, 1, 1, 0)T = (0, b + c, z + y , 0)T .\nConnect the variable x1 with x2 of f using ( (cid:54)=2 ), and we get a binary signature g2 , where\ng2 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 )T = (0, b + z , c + y , 0)T .\n\n48\n\n\f If one of g1 and g2 is of the form (0, 0, 0, 0), then by = (c)(z ) = cz . That is by  cz = 0.\nHere c (cid:54)= 0 due to by (cid:54)= 0. By Lemma 6.2, Pl-Holant( (cid:54)=2 | f ) is #P-hard.\n If one of g1 and g2 can be normalized as (0, 1, 0, 0) or (0, 0, 1, 0). By Lemma 6.6, Pl-Holant( (cid:54)=2 |\nf ) is #P-hard.\n If one of g1 and g2 can be normalized as (0, 1, t, 0)T , where t (cid:54)= 0 is not a root of unity, then\nby Lemma 6.3, Pl-Holant( (cid:54)=2 | f ) is #P-hard.\n If one of g1 and g2 can be normalized as (0, 1, t, 0)T , where t is an n-th primitive root of unity\nand n (cid:62) 3, then by Corollary 6.7, Pl-Holant((cid:54)=2 | f ) is #P-hard.\n Otherwise, g1 and g2 do not belong to those cases above, which means both g1 and g2 both\ncan be normalized as (0, 1, \u00011 , 0) and (0, 1, \u00012 , 0), where \u00011 = 1 and \u00012 = 1. That is,\nb + c = \u00011 (z + y) (cid:54)= 0 and b + z = \u00012 (c + y) (cid:54)= 0.\n If b + c = z + y and b + z = c + y , then b = y and c = z . To be proved below.\n If b + c = (z + y) and b + z = c + y , then b + z = c + y = 0. Contradiction.\n If b + c = z + y and b + z = (c + y), then b + c = z + y = 0. Contradiction.\n If b + c = (z + y) and b + z = (c + y), we have g1 = (0, 1, 1, 0)T . Connect the\n(cid:21)\n(cid:20) 0\nvariable x2 of g1 with the variable x1 of f , and we get a signature f (cid:48) with the signature\n0 a\n0\n. Connect the variable x1 with x2 of f (cid:48) using ((cid:54)=2 ), and we\nmatrix M (f (cid:48) ) =\n0\nb\nc 0\n0 z y 0\nx 0\n0 0\nget a binary signature g (cid:48) = (0, b  z , c  y , 0)T . Same as the analysis of g1 and g2 above,\nwe have Pl-Holant((cid:54)=2 | f (cid:48) ) is #P-hard unless g (cid:48) can be normalized as (0, 1, \u00013 , 0), where\n\u00013 = 1. That is, b  z = \u00013 (c  y) (cid:54)= 0,\n If b  z = c  y , combine with b + c = (z + y). We have b = y and c = z . To\nbe proved below.\n If b  z = (c  y), combine with b + c = (z + y). We have b + c = z + y = 0.\nContradiction.\nTherefore, Pl-Holant( (cid:54)=2 | f (cid:48) ) is #P-hard and hence Pl-Holant((cid:54)=2 | f ) is #P-hard\nSo far, we have Pl-Holant( (cid:54)=2 | f ) is #P-hard unless b = \u0001y and c = \u0001z , where \u0001 = 1. Similarly,\nconnect the variable x2 with x3 of f using ((cid:54)=2 ), and we get a binary signature g3 = (0, a +\nc, z + x, 0)T . Connect the variable x1 with x4 of f using ( (cid:54)=2 ), and we get a binary signature\ng4 = (0, a + z , c + x, 0)T . Same as the analysis of g1 and g2 , we have Pl-Holant((cid:54)=2 | f ) is #P-hard\nunless a = \u0001(cid:48)x and c = \u0001(cid:48)z , where \u0001(cid:48) = 1. Therefore, Pl-Holant( (cid:54)=2 | f ) is #P-hard unless a = \u0001x,\nb = \u0001y and c = \u0001z , where \u0001 = 1. In this case, since z (cid:54)= 0, we have abc (cid:54)= 0. By Lemma 6.1,\nPl-Holant( (cid:54)=2 | f ) is #P-hard.\n\n49\n\n\fReferences\n\n[1] Lars Ahlfors. Complex Analysis, 3 ed, McGraw-Hill, 1979.\n[2] Jin-Yi Cai, Xi Chen, Richard J. Lipton, and Pinyan Lu. On tractable exponential sums, in\nproceedings of FAW 2010, 148-159.\n[3] Jin-Yi Cai and Zhiguo Fu. A collapse theorem for holographic algorithms with matchgates on\ndomain size at most 4, Inf. Comput. 239 (2014), 149-169.\n[4] Jin-Yi Cai and Zhiguo Fu. Holographic algorithm with matchgates is universal for planar\n#CSP over boolean domain, CoRR abs/1603.07046 (2016), to appear in STOC 2017.\n[5] Jin-Yi Cai, Zhiguo Fu and Mingji Xia. Complexity classication of the Six-Vertex Model,\nCoRR abs/1702.02863 (2017).\n[6] Jin-Yi Cai, Heng Guo and Tyson Williams. A complete dichotomy rises from the capture of\nvanishing signatures, SIAM J. Comput., 45(5) (2016), 1671-1728.\n[7] Jin-yi Cai, Pinyan Lu and Mingji Xia. Holant problems and counting CSP, in proceedings of\nSTOC 2009, 715-724.\n[8] Michael Freedman, Laszlo Lovasz and Alexander Schrijver. Reection positivity, rank con-\nnectivity, and homomorphism of graphs. Journal of the American Mathematical Society 20.1\n(2007), 37-51.\n[9] Jin-Yi Cai, Pinyan Lu and Mingji Xia. The complexity of complex weighted Boolean #CSP,\nJ. Comput. Syst. Sci. 80(1) (2014), 217-236.\n[10] Leslie Ann Goldberg, Mark Jerrum and Mike Paterson. The computational complexity of\ntwo-state spin systems, Random Struct. Algorithms 23(2) (2003), 133-154.\n[11] Leslie Ann Goldberg and Mark Jerrum. Approximating the partition function of the ferro-\nmagnetic potts model, J. ACM 59(5) (2012), 25.\n[12] Heng Guo and Tyson Williams. The complexity of planar boolean #CSP with complex\nweights, in proceedings of ICALP (1) 2013, 516-527.\n[13] Sangxia Huang and Pinyan Lu. A dichotomy for real weighted Holant problems, Computa-\ntional Complexity 25(1) (2016), 255-304.\n[14] Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the Ising\nModel, SIAM J. Comput. 22(5) (1993), 1087-1116.\n[15] Pieter W. Kasteleyn. The statistics of Dimers on a lattice, Physica 27 (1961), 1209-1225.\n[16] Pieter W. Kasteleyn. Graph theory and crystal physics, in Graph Theory and Theoretical\nPhysics, (F. Harary, ed.), Academic Press, London (1967), 43-110.\n[17] Michel Las Vergnas. On the evaluation at (3, 3) of the Tutte polynomial of a graph, J. Comb.\nTheory, Ser. B 45(3) (1988), 367-372.\n[18] Liang Li, Pinyan Lu and Yitong Yin. Correlation decay up to uniqueness in Spin systems, in\nproceedings of SODA 2013, 67-84.\n[19] Elliott H. Lieb. Residual entropy of square ice, Physical Review 162 (1) (1967), 162-172.\n[20] Linus C. Pauling. The structure and entropy of ice and of other crystals with some randomness\nof atomic arrangement, Journal of the American Chemical Society 57 (12) (1935), 2680-2684.\n[21] Harold N. V. Temperley and Michael E. Fisher. Dimer problem in statistical mechanics - an\nexact result, Philosophical Magazine 6 (1961), 1061- 1063.\n[22] Leslie G. Valiant. The Complexity of computing the permanent, Theor. Comput. Sci. 8 (1979),\n189-201.\n\n50\n\n\f[23] Leslie G. Valiant. Quantum circuits that can be simulated classically in polynomial time,\nSIAM J. Comput. 31(4) (2002), 1229-1254.\n[24] Leslie G. Valiant. Expressiveness of matchgates. Theor. Comput, Sci. 289(1) (2002), 457-471.\n[25] Leslie G. Valiant. Holographic algorithms, SIAM J. Comput. 37(5) (2008), 1565-1594.\n\n51\n\n\f", 
        "tag": "Computational Complexity", 
        "link": "https://arxiv.org/list/cs.CC/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\na\nM\n \n9\n2\n \n \n]\nh\np\n-\nd\ne\nm\n.\ns\nc\ni\ns\ny\nh\np\n[\n \n \n1\nv\n0\n6\n9\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nA coupled mitral valve  left ventricle model with\nuid-structure interaction\n\nHao Gao\n\nSchool of Mathematics and Statistics, University of Glasgow, UK\n\nLiuyang Feng\n\nSchool of Mathematics and Statistics, University of Glasgow, UK\n\nNan Qi\n\nSchool of Mathematics and Statistics, University of Glasgow, UK\n\nColin Berry\n\nInstitute of Cardiovascular and Medical Science, University of Glasgow, UK\n\nBoyce Grith\n\nDepartments of Mathematics and Biomedical Engineering and McAl lister Heart Institute,\nUniversity of North Carolina, Chapel Hil l, NC, USA\n\nSchool of Mathematics and Statistics, University of Glasgow, UK\n\nXiaoyu Luo\n\nAbstract\n\nUnderstanding the interaction between the valves and walls of the heart is im-\nportant in assessing and subsequently treating heart dysfunction. With ad-\nvancements in cardiac imaging, nonlinear mechanics and computational tech-\nniques, it is now possible to explore the mechanics of valve-heart interactions\nusing anatomically and physiologically realistic models. This study presents an\nintegrated model of the mitral valve (MV) coupled to the left ventricle (LV),\nwith the geometry derived from in vivo clinical magnetic resonance images.\nNumerical simulations using this coupled MV-LV model are developed using an\nimmersed boundary/nite element method. The model incorporates detailed\nvalvular features, left ventricular contraction, nonlinear soft tissue mechanics,\nand uid-mediated interactions between the MV and LV wall. We use the model\nto simulate the cardiac function from diastole to systole, and investigate how\nmyocardial active relaxation function aects the LV pump function. The results\nof the new model agree with in vivo measurements, and demonstrate that the\ndiastolic lling pressure increases signicantly with impaired myocardial active\nrelaxation to maintain the normal cardiac output. The coupled model has the\n\nPreprint submitted to Journal of Medical Engineering & Physics\n\nApril 7, 2017\n\n\fpotential to advance fundamental knowledge of mechanisms underlying MV-LV\ninteraction, and help in risk stratication and optimization of therapies for heart\ndiseases.\n\nKeywords: mitral valve, left ventricle, uid structure interaction, immersed\nboundary method, nite element method, soft tissue mechanics\n\n2\n\n\f1. Introduction\n\nThe mitral valve (MV) has a complex structure that includes two distinct\nasymmetric leaets, a mitral annulus, and chordal tendinae that connect the\nleaets to papillary muscles that attach to the wall of the left ventricle (LV).\nMV dysfunction remains a ma jor medical problem because of its close link to\ncardiac dysfunctions leading to morbidity and premature mortality [1].\nComputational modelling for understanding the MV mechanics promises\nmore eective MV repairs and replacement [2, 3, 4, 5]. Biomechanical MV\nmodels have been developed for several decades, starting from the simplied two-\ndimensional approximation to three-dimensional models, and to multi-physics/-\nscale models [6, 7, 8, 9, 10, 11, 12]. Most of previous studies were based on\nstructural and quasi-static analysis applicable to a closed valve [13]; however,\nMV function during the cardiac cycle cannot be fully assessed without modelling\nthe ventricular dynamics and the uid-structure interaction (FSI) between the\nMV, ventricles, and the blood ow [13, 14].\nBecause of the complex interactions among the MV, the sub-mitral ap-\nparatus, the heart walls, and the associated blood ow, few modelling stud-\nies have been carried out that integrate the MV and ventricles in a single\nmodel [15, 16, 17]. Kunzelman, Einstein, and co-workers rst simulated normal\nand pathological mitral function [18, 19, 20] with FSI using LS-DYNA (Liver-\nmore Software Technology Corporation, Livermore, CA, USA) by putting the\nMV into a straight tube. Using similar modelling approach, Lau et al. [21] com-\npared MV dynamics with and without FSI, and they found that valvular closure\nconguration is dierent when using the FSI MV model. Similar ndings are\nreported by Toma et al [22]. Over the last few years, there have also been a\nnumber of FSI valvular models using the immersed boundary (IB) method to\nstudy the ow across the MV [23, 24, 25]. In a series of studies, Toma [26, 22, 27]\ndeveloped a FSI MV model based on in vitro MV experimental system to study\nthe function of the chordal structure, and good agreement was found between\nthe computational model and in vitro experimental measurements. However,\nnone of the aforementioned MV models accounted for the MV interaction with\nthe LV dynamics. Indeed, Lau et al. [21] found that even with a xed U-shaped\nventricle, the ow pattern is substantially dierent from that estimated using\na tubular geometry. Despite the advancements in computational modelling of\nindividual MV [13, 12] and LV models [28, 29, 30], it remains challenging to de-\nvelop an integrated MV-LV model which includes the strong coupling between\nthe valvular deformation and the blood ow. Reasons for this include limited\ndata for model construction, dicult choices of boundary conditions, and large\ncomputational resources required by these simulations.\nWenk et al.\n[15] reported a structure-only MV-LV model using LS-DYNA\nthat included the LV, MV, and chordae tendineae. This model was later ex-\ntended to study MV stress distributions using a saddle shaped and asymmetric\nmitral annuloplasty ring [16]. A more complete whole-heart model was recently\ndeveloped using a human cardiac function simulator in the Dassault Systemess\nLiving Heart pro ject [17], which includes four ventricular chambers, cardiac\n\n3\n\n\fvalves, electrophysiology, and detailed myobre and collagen architecture. Us-\ning the same simulator, eects of dierent mitral annulus ring were studied by\nRausch et al. [31]. However, this simulator does not yet account for detailed\nFSI.\nThe earliest valve-heart coupling model that includes FSI is credited to Pe-\nskin and McQueens pioneering work in the 1970s [32, 33, 34] using the classical\nIB approach [35]. Using this same method, Yin et al. [36] investigated uid vor-\ntices associated with the LV motion as a prescribed moving boundary. Recently,\nChandran and Kim [37] reported a prototype FSI MV dynamics in a simplied\nLV chamber model during diastolic lling using an immersed interface-like ap-\nproach. One of the key limitations of these coupled models is the simplied\nrepresentation of the biomechanics of the LV wall. To date, there has been no\nwork reported a coupled MV-LV model which has full FSI and based on realistic\ngeometry and experimentally-based models of soft tissue mechanics.\nThis study reports an integrated MV-LV model with FSI derived from in vivo\nimages of a healthy volunteer. Although some simplications are made, this is\nthe rst three-dimensional FSI MV-LV model that includes MV dynamics, LV\ncontraction, and experimentally constrained descriptions of nonlinear soft tissue\nmechanics. This work is built on our previous models of the MV [24, 25] and\nLV [38, 29]. The model is implemented using a hybrid immersed boundary\nmethod with nite element elasticity (IB/FE) [39].\n\n2. Methodology\n\n2.1. IB/FE Framework\n\nThe coupled MV-LV model employs an Eulerian description for the blood,\nwhich is modelled as a viscous incompressible uid, along with a Lagrangian\ndescription for the structure immersed in the uid. The xed physical coordi-\nnates are x = (x1 , x2 , x3 )  , and the Lagrangian reference coordinate system\nis X = (X1 , X2 , X3 )  U . The exterior unit normal along U is N(X). Let\n(X, t) denote the physical position of any material point X at time t, so that\n(U, t) = s (t) is the physical region occupied by the immersed structure. The\n(cid:19)\n(cid:18) u\nIB/FE formulation of the FSI system reads\n(x, t) + u(x, t)  u(x, t)\n= p(x, t) + 2u(x, t) + f s (x, t),\n(cid:90)\n t\n  u(x, t) = 0,\n(cid:90)\nU\n\n(cid:90)\nU\n\n\nPs (X, t) N(X) (x  (X, t)) dA(X),\n(3)\n\n  Ps (X, t) (x  (X, t)) dX\n\nu(x, t) (x  (X, t)) dx,\n\n\n t\n\n(X, t) =\n\n(1)\n\n(2)\n\n(4)\n\n\n\nf s (x, t) =\n\n4\n\n\fwhere  is the uid density,  is the uid viscosity, u is the Eulerian velocity, p is\nthe Eulerian pressure, and f s is the Eulerian elastic force density. Dierent from\nthe classical IB approach [35], here the elastic force density f s is determined from\nthe rst Piola-Kircho stress tensor of the immersed structure Ps as in Eq. 3.\nThis allows the solid deformations to be described using nonlinear soft tissue\nconstitutive laws. Interactions between the Lagrangian and Eulerian elds are\nachieved by integral transforms with a Dirac delta function kernel (x) [35] in\nEqs. 3 4. For more details of the hybrid IB/FE framework, please refer to [39].\n\n2.2. MV-LV Model Construction\nA cardiac magnetic resonance (CMR) study was performed on a healthy\nvolunteer (male, age 28). The study was approved by the local NHS Research\nEthics Committee, and written informed consent was obtained before the CMR\nscan. Twelve imaging planes along the LV outow tract (LVOT) view were\nimaged to cover the whole MV region shown in Fig. 1(a). LV geometry and\nfunction was imaged with conventional short-axis and long-axis cine images.\nThe parameters for the LVOT MV cine images were: slice thickness: 3 mm with\n0 gap, in-plane pixel size: 0.70.7 mm2 , eld of view: 302  400 mm2 , frame\nrate: 25 per cardiac cycle. Short-axis cine images covered the LV region from\nthe basal plane to the apex, with slice thickness: 7 mm with 3 mm gap, in-plane\npixel size: 1.3  1.3 mm2 , and frame rate: 25 per cardiac cycle.\nThe MV geometry was reconstructed from LVOT MV cine images at early-\ndiastole, just after the MV opens. The leaet boundaries were manually delin-\neated from MR images, as shown in Fig. 1(a), in which the heads of papillary\nmuscle and the annulus ring were identied as shown in Fig. 1(b). The MV\ngeometry and its sub-valvular apparatus were reconstructed using SolidWorks\n(Dassault Systmes SolidWorks Corporation, Waltham, MA, USA). Because it\nis dicult to see the chordal structural in the CMR, we modelled the chordae\nstructure using sixteen evenly distributed chordae tendineae running through\nthe leaet free edges to the annulus ring, as shown in Fig. 1(c), following prior\nstudies [25, 24]. In a similar approach to the MV reconstruction, the LV geom-\netry was reconstructed from the same volunteer at early-diastole by using both\nthe short-axis and long-axis cine images [40, 29]. Fig. 1(d) shows the inow and\noutow tracts from one MR image. The LV wall was assembled from the short\nand long axis MR images (Fig. 1(e)) to form the three dimensional reconstruc-\ntion (Fig. 1(f )). The LV model was divided into four regions: the LV and the\nvalvular region and the inow and the outow tracts, as shown in Fig. 1(g).\nThe MV model was mounted into the inow tract of the LV model according\nto the relative positions derived from the MR images in Fig. 1(g). The left\natrium was not reconstructed but modelled as a tubular structure, the gap\nbetween the MV annulus ring and the LV model was lled using a housing\ndisc structure. A three-element Windkessel model was attached to the outow\ntract of the LV model to provide physiological pressure boundary conditions\nwhen the LV is in systolic ejection [40]. The chordae were not directly attached\nto the LV wall since the papillary muscles were not modelled, similar to [25].\nThe myocardium has a highly layered myobre architecture, which is usually\n\n5\n\n\fdescribed using a bre-sheet-normal (f , s, n) system. A rule-based method was\nused to construct the myobre orientation within the LV wall. The myobre\nangle was assumed to rotate from -60o to 60o from endocardium to epicardium,\nrepresented by the red arrows in Fig. 1(h). In a similar way, the collagen bres\nin the MV leaets were assumed to be circumferentially distributed, parallel\nalong the annulus ring, represented by the yellow arrows in Fig. 1(h).\n\n2.3. Soft Tissue Mechanics\n\n(cid:40)\nThe total Cauchy stress () in the coupled MV-LV system is\nfor x  s ,\notherwise,\n\n f (x, t) +  s (x, t)\n f (x, t)\n\n(x, t) =\n\nwhere  f is the uid-like stress tensor, dened as\n f (x, t) = pI + [u + (u)T ].\n\n(5)\n\n(6)\n\nPs =\n\n s is the solid stress tensor obtained from the nonlinear soft tissue consitutive\nlaws. The rst Piola-Kirchho stress tensor Ps in Eq. 3 is related to  s through\nPs = J  sFT ,\nin which F = /X is the deformation gradient and J = det(F).\nIn the MV-LV model, we assume the structure below the LV base is contrac-\ntile (Fig 1(g)), the regions above the LV basal plane, including the MV and its\n(cid:40)Pp + Pa below the basal plane,\napparatuses, are passive. Namely,\nPp\nabove the basal plane,\nwhere Pa and Pp are the active and passive Piola-Kirchho stress tensors, re-\nspectively. The MV leaets are modelled as an incompressible bre-reinforced\nmaterial with the strain energy function\nWMV = C1 (I1  3) +\n\nav\n2bv\nin which I1 = trace(C) is the rst invariant of the right Cauchy-Green deforma-\n0  (Cf c\ntion tensor C = FT F, I c\nf = f c\n0 ) is the squared stretch along the collagen\nbre direction, and f c\n0 denotes the collagen bre orientation in the reference\nconguration. The max() function ensures the embedded collagen network only\nbears the loads when stretched, but not in compression. C1 , av , and bv are\nmaterial parameters adopted from a prior study [25] and listed in Table 1. The\npassive stress tensor Pp in the MV leaets is\nF  C1FT + s log(I3 )FT ,\nWMV\n\nf , 1)  1)2 ]  1),\n(exp[bv (max(I c\n\nPp =\n\n(10)\n\n(7)\n\n(8)\n\n(9)\n\n6\n\n\fwhere I3 = det(C), and s is the bulk modulus for ensuring the incompressibility\nof immersed solid, so that the pressure-like term C1FT ensures the elastic stress\nresponse is zero when F = I.\nWe model the chordae tendineae as the Neo-Hookean material,\nWchordae = C (I1  3),\n\n(11)\n\nwhere C is the shear modulus. We further assume C is much larger in systole\nwhen the MV is closed than in diastole when the valve is opened. The much\nlarger value of C models the eects of papillary muscle contraction. Values of\nC are listed in Table 1. Pp for the chordae tendineae is similarly derived as in\nEq. 10.\nThe passive response of the LV myocardium is described using the Holzapfel-\n(cid:88)\nOgden model [41],\nexp[b(I1  3)] +\nai\n2bi\ni=f ,s\n{exp[bfs (I8fs )2 ]  1}\n\n{exp[bi (max(I4i , 1)  1)2 ]  1}\n\nWmyo =\n\n(12)\n\na\n2b\n\n+\n\nafs\n2bfs\n\nin which a, b, af , bf , as , bs , afs , bfs are the material parameters, I4f , I4s and I8fs\nare the strain invariants related to the the myobre orientations. Denoting the\nmyobre direction in the reference state is f 0 and the sheet direction is s0 , we\nhave\nI4f = f 0  (Cf 0 ), I4s = s0  (Cs0 ), and I8fs = f 0  (Cs0 ).\nThe myocardial active stress is dened as\nPa = J T F f 0  f 0\n\n(14)\n\n(13)\n\nwhere T is the active tension described by the myolament model of Niederer et\nal. [42], using a set of ordinary dierential equations involving the intracellular\ncalcium transient (Ca2+ ), sarcomere length and the active tension at the resting\nsarcomere length (T req ). In our simulations, we use the same parameters as in\nref. [42], except that T req is adjusted to yield realistic contraction as the imaged\nvolunteer.\nAll the constitutive parameters in Eqs.9, 11, 12 are summarized in Table 1.\n\n2.4. Boundary Conditions and Model Implementation\n\nBecause only the myocardium below the LV basal plane contracts, we x the\nLV basal plane along the circumferential and longitudinal displacements, but al-\nlow the radial expansion. The myocardium below the LV basal plane is left free\nto move. The valvular region is assumed to be much softer than the LV region.\nIn diastole, a maximum displacement of 6 mm is allowed in the valvular region\nusing a tethering force. In systole, the valve region is gradually pulled back to\nthe original position. The inow and outow tracts are xed. Because the MV\nannulus ring are attached to a housing structure which is xed, no additional\n\n7\n\n\fboundary conditions are applied to the MV annulus ring. Fluid boundary condi-\ntions are applied to the top planes of the inow and outow tracts. The function\nof the aortic valve is modelled simply: the aortic valve is either fully opened\nor fully closed, determined by the pressure dierence between the values inside\nthe LV chamber and the aorta. After end-diastole, the LV region will contract\nsimultaneously triggered by a spatially homogeneously prescribed intracellular\nCa2+ transient [29], as shown in Fig. 3. The ow boundary conditions in a\ncardiac cycle are summarized below.\n Diastolic lling: A linearly ramped pressure from 0 to a population-\nbased end-diastolic pressure (EDP=8 mmHg) is applied to the inow tract\nover 0.8 s, which is slightly longer than the actual diastolic duration of\nthe imaged volunteer (0.6 s).\nIn diastole about 80% of diastolic lling\nvolume is due to the sucking eect of the left ventricle in early-diastole [43].\nThis negative pressure eld inside the LV cavity is due to the myocardial\nrelaxation. We model this sucking eect using an additional pressure\nloading applied to the endocardial surface, denoted as Pendo , which is\nlinearly ramped from 0 to 12 mmHg over 0.4 s, and then linearly decreased\nto zero at end-diastole. The value of Pendo is chosen by matching the\nsimulated end-diastolic volume to the measured data from CMR images.\nBlood ow is not allowed to move out of the LV cavity through the inow\ntract in diastole. Zero ow boundary conditions are applied to the top\nplane of the outow tract.\n Iso-volumetric contraction: Along the top plane of the inow tract, the\nEDP loading is maintained, but we allow free uid ow in and out of the\ninow tract. Zero ow boundary conditions are retained for the outow\ntract. The duration of the iso-volumetric contraction is determined by\nthe myocardial contraction and ends when the aortic valve opens. The\naortic valve opens when the LV pressure is higher than the pressure in the\naorta, which is initially set to be the cu-measured diastolic pressure in\nthe brachial artery, 85 mmHg.\n Systolic ejection: When the aortic valve opens, a three-element Wind-\nkessl model is coupled to the top plane of the outow tract to provide\nafterload. The volumetric ow rates across the top plane of the outow\ntract is calculated from the three-dimensional MV-LV model, and fed into\nthe Windkessel model [44], which returns an updated pressure for the out-\now tract in the next time step. The systolic ejection phase ends when\nthe left ventricle cannot pump any ow through the outow tract, and the\nWindkessel model is detached.\n Iso-volumetric relaxation: Zero ow boundary conditions are applied\nto both the top planes of the outow and inow tracts until the total cycle\nends at 1.2 seconds.\nThe coupled MV-LV model is immersed in a 17cm  16cm  16cm uid\nbox. A basic time step size t0 = 1.22  104 s is used in the diastolic and\n\n8\n\n\frelaxation phases, a reduced time step size (0.25 t0 ) is used in the early systole\nwith a duration of 0.1 s, and an even smaller time step of 0.125 t0 is used in\nthe remainder of the systolic phase. Because explicit time stepping is used in\nthe numerical simulations [39], we need to use a time step size small enough to\navoid numerical instabilities, particularly during the systolic phase to resolve the\nhighly dynamic LV deformation. The MV-LV model is implemented using the\nopen-source IBAMR software framework (https://github.com/IBAMR/IBAMR),\nwhich provides an adaptive and distributed-memory parallel implementation of\nthe IB methods.\n\n3. Results\n\nFig. 2 shows the computed volumetric ow rates across the MV and the\nAV from beginning of diastole to end-systole. In diastole, the volumetric ow\nrate across the MV linearly increases with Pendo , with a maximum value of\n210 mL/s at 0.4 s. Diastolic lling is maintained by the increased pressure in\nthe inow tract, but with decreased ow rates until end of diastole at 0.8 s. The\nnegative ow rate in Fig. 2 indicates the ow is entering the LV chamber. After\nend-diastole, the myocardium starts to contract, and the central LV pressure\nincreases until it exceeds the aortic pressure (initially set to be 85 mmHg) at\n0.857 s. During iso-volumetric contraction, the MV closes with a total closure\nregurgitation ow of 7.2 mL, around 10% of the total lling volume, which is\ncomparable to the value reported by Laniado et al. [45]. There is only minor\nregurgitation across the MV during systolic ejection after the iso-volumetric\ncontraction phase. Blood is then ejected out of the ventricle through the AV,\nand the ow rate across the AV during systole reaches a peak value of 468 mL/s\n(Fig. 2). The total ejection duration is 243 ms with a stroke volume of 63.2 mL.\nThe total blood ejected out of the LV chamber, including the regurgitation\nacross the MV, is 72.1 mL, which corresponds to an ejection fraction of 51%.\nFig. 3 shows the proles of the normalized intracellular Ca2+ , LV cavity\nvolume, central LV pressure, and the average myocardial active tension from\ndiastole to systole. Until mid-diastole (0 s to 0.56 s), the central LV pressure\nis negative, and the associated diastolic lling volume is around 65 mL, which\nis 90% of the total diastolic lling volume.\nIn late-diastole, the LV pressure\nbecomes positive. There is a delay between the myocardial active tension and\nthe intracellular Ca2+ prole, but the central LV pressure follows the active\ntension closely throughout the cycle as shown in Fig 3.\nFig. 4 shows the deformed MV leaets along with the corresponding CMR\ncine images at early-diastole (the reference state), end-diastole, and mid-systole.\nIn general, the in vivo MV and LV dynamics from diastole to systole are qual-\nitatively captured well by the coupled MV-LV model. However, a discrepancy\nis observed during the diastolic lling, when the MV orice in the model is\nnot opened as widely as in the CMR cine image (Fig. 4(b)). In addition, the\nmodelled MV leaets have small gaps near the commissure areas even in the\nfully closure state. This is partially caused by the nite size of the regularized\n\n9\n\n\fdelta function at the interface and uncertainties in MV geometry reconstruction\nusing CMR images.\nFigs. 5(a, b, c, d) show the streamlines at early-diastolic lling, late-diastolic\nlling, when the MV is closing (iso-volumtric contraction), and mid-systolic\nejection when the left ventricle is ejecting. During the diastolic lling (Fig. 5(a)),\nthe blood ows directly through the MV into the LV chamber towards the LV\napex, in late-diastole in Fig. 5(b), the ow pattern becomes highly complex.\nWhen iso-volmeric contraction ends, the MV is pushed back towards the left\natrium. In mid-systole, the blood is pumped out of the LV chamber through\nthe aortic valve into the systemic circulation, forming a strong jet as shown in\nFig. 5 (d).\nThe LV systolic strain related to end-diastole is shown in Fig. 6 (a), which\nis negative throughout most of the region except near the basal plane, where\nthe LV motion is articially constrained in the model. The average myocardial\nstrain along myobre direction is -0.1620.05. Fig. 6(b) is the bre strain in the\nMV leaets at end-diastole, the leaets are mostly slightly stretched during the\ndiastolic lling. In systole, because of the much higher pressure in the LV, the\nleaets are pushed towards the left atrium side as shown in Fig. 6(c). Near the\nleaet tip and the commissiour areas, the leaets are highly compressed, while\nin the trigons near the annulus ring, the leaet is stretched.\nFrom Fig. 3, one can see that the applied endocardial pressure (Pendo ) cre-\nates a negative pressure inside the LV chamber, similar to the eects of the\nmyocardial active relaxation. We further investigate how Pendo aects the MV-\nLV dynamics by varying its value from 8 mmHg to 16 mmHg, and the eects\nwithout Pendo but with an increased EDP from 8 mmHg to 20 mmHg. We ob-\nserve that with an increased Pendo , the peak ow rate across the MV during the\nlling phase becomes higher with more ejected volume through the aortic valve.\nWe also have a longer ejection duration, shorter iso-volumetric contraction time,\nand higher ejection fraction as a result of increasing Pendo . On the other hand,\nif we dont apply Pendo , a much greater and nonphysiological EDP is needed for\nthe required ejection fraction. For example, with EDP=8 mmHg, the ejection\nfraction is only 29%. Only when EDP=20 mmHg, the pump function is compa-\nrable to the case with EDP=8 mmHg and Pendo = 16 mmHg. These results are\nsummarized in Table 2.\n\n4. Discussion\n\nThis study demonstrates the feasibility of integrating a MV model with a\nLV model from a healthy volunteer based on in vivo CMR images. This is the\nrst physiologically based MV-LV model with uid structure interaction that\nincludes nonlinear hyperelastic constitutive modelling of the soft tissue. The\ncoupled MV-LV model is used to simulate MV dynamics, LV wall deformation,\nmyocardial active contraction, as well as intraventricular ow. The modelling\nresults are in reasonable quantitative agreement with in vivo measurements\nand clinical observations. For example, the peak aortic ow rate is 468 mL/s,\nclose to the measured peak value (498 mL/s); the ejection duration is 243 ms,\n\n10\n\n\fand the measured value is around 300 ms; the peak LV pressure is 162 mmHg,\ncomparable to the cu-measured peak blood pressure 150 mmHg; the average\nLV systolic strain is around -0.16, which also lies in the normal range of healthy\nsub jects [46].\nDiastolic heart failure is usually associated with impaired myocardial relax-\nation and increased lling pressure [47, 48]. In this study, we model the eects of\nmyocardial relaxation by applying an endocardial surface pressure Pendo . Specif-\nically we can enhance or suppress the myocardial relaxation by adjusting Pendo .\nOur results in Table 2) show that, with an enhanced myocardial relaxation,\nsay, when Pendo  12 mmHg, there is more lling during diastole, compared to\nthe cases when Pendo < 12 mmHg under the same EDP. This in turn gives rise\nto higher ejection fraction and stroke volume. However, if myocardial relax-\nation is suppressed, diastolic lling is less ecient, with subsequently smaller\nejection fraction and stroke volume.\nIn the extreme case, when the myocar-\ndial relaxation is entirely absent, chamber volume increases by only 29.5 mL,\nand ejection fraction decreases to 29%. To maintain stroke volume obtained for\nPendo=12 mmHg, EDP needs to be as high as 20 mmHg. Indeed, increased EDP\ndue to an impaired myocardial relaxation has been reported in a clinical study\nby Zile et al. [48]. A higher EDP indicates the elevated lling pressure through-\nout the relling phase. Increased lling pressure can help to maintain a normal\nlling volume and ejection fraction, but runs the risks of ventricular dysfunction\nin the longer term, because pump failure will occur if no other compensation\nmechanism exists.\nDuring diastole, the MV-LV model seems to yield a smaller orice compared\nto the corresponding CMR images.\nIn our previous study [25], the MV was\nmounted in a rigid straight tube, the peak diastolic lling pressure is around\n10 mmHg, and the peak ow rate across the MV is comparable to the measured\nvalue (600 mL/s). While in this coupled MV-LV model, even though with ad-\nditional Pendo , the peak ow rate (200 mL/s) is much less than the measured\nvalue. One reason is because of the extra resistance from the LV wall, which\nis absent in the MV-tube model [25]. The diastolic phase can be divided into\nthree phasse [43]: the rapid lling, slow lling, and atrial contraction. During\nrapid lling, the transvalvular ow is resulted from myocardial relaxation (the\nsucking eect), which contributes to 80% of the total transvalvular ow volume.\nDuring slow lling and atrial contraction, the left atrium needs to generate a\nhigher pressure to provide additional lling.\nIn the coupled MV-LV model,\nthe ramped pressure in the top plane of the inow tract during late-diastole is\nrelated to the atrial contraction, and during this time, only 10% of the total\ntransvalvular ow occurs. However, the peak ow rate in rapid lling phase\nis much lower compared to the measured value, which suggests the myocardial\nrelaxation would be much stronger.\nIn a series of studies based on in vitro CT experiments, Toma [26, 27, 22]\nsuggested that MV models with simplied chordal structure would not compare\nwell with experimental data, and that a sub ject-specic 3D chordal structure\nis necessary. This may explain some of the discrepancies we observed here. A\nsimplied chordal structure is used in this study because we are unable to re-\n\n11\n\n\fconstruct the chordal structure from the CMR data. CT imaging may allow the\nchordae reconstruction but it comes with radiation risk. Patient-specic chordal\nstructure in the coupled MV-LV model would require further improvements of\nin vivo imaging techniques.\nSeveral other limitations in the model may also contribute to the discrepan-\ncies. These include the uncertainty of patient-specic parameter identication,\nuncertainties in MV geometry reconstruction from CMR images, the passive\nresponse assumption around the annulus ring and the valvular region of the LV\nmodel, and the lack of pre-strain eects. Studies addressing these issues are\nalready under way. We expect that further improvement in personalized mod-\nelling and more ecient high performance computing would make the modelling\nmore physiologically detailed yet fast enough for applications in risk stratica-\ntion and optimization of therapies in heart diseases.\n\n5. Conclusion\n\nWe have developed a rst fully coupled MV-LV model that includes uid-\nstructure interaction as well as experimentally constrained descriptions of the\nsoft tissue mechanics. The model geometry is derived from in vivo magnetic res-\nonance images of a healthy volunteer. It incorporates three-dimensional nite\nelement representations of the MV leaets, sub-valvular apparatus, and the LV\ngeometry. Fibre-reinforced hyperelastic constitutive laws are used to describe\nthe passive response of the soft tissues, and the myocardial active contraction is\nalso modelled. The developed MV-LV model is used to simulate MV dynamics,\nLV wall deformation, and ventricular ow throughout the cardiac cycle. Despite\nseveral modelling limitations, most of the results agree with in vivo measure-\nments. We nd that with impaired myocardial active relaxation, the diastolic\nlling pressure needs to increase signicantly in order to maintain a normal car-\ndiac output, consistent with clinical observations. The model thereby represents\na further step towards a whole-heart multiphysics modelling with a target for\nclinical applications.\n\nAcknowledgement\n\nWe are grateful for the funding from the UK EPSRC (EP/N014642/1, and\nEP/I029990/1) and the British Heart Foundation (PG/14/64/31043), and the\nNational Natural Science Foundation of China (No. 11471261).\nIn addition,\nFeng received the China Scholarship Council Studentship and the Fee Waiver\nProgramme at the University of Glasgow, Luo is funded by a Leverhulme Trust\nFellowship (RF-2015-510), and Grith is supported by the National Science\nFoundation (NSF award ACI 1450327) and the National Institutes of Health\n(NIH award HL117063)\n\nConict interests\n\nThe authors have no conicts of interest.\n\n12\n\n\fReferences\n\nReferences\n\n[1] A. S. Go, D. Mozaarian, V. L. Roger, E. J. Benjamin, J. D. Berry, M. J.\nBlaha, S. Dai, E. S. Ford, C. S. Fox, S. Franco, et al., Heart disease and\nstroke statistics-2014 update, Circulation 129 (3).\n\n[2] M. S. Sacks, W. David Merryman, D. E. Schmidt, On the biomechanics of\nheart valve function, Journal of biomechanics 42 (12) (2009) 18041824.\n\n[3] E. Votta, T. B. Le, M. Stevanella, L. Fusini, E. G. Caiani, A. Redaelli,\nF. Sotiropoulos, Toward patient-specic simulations of cardiac valves:\nState-of-the-art and future directions, Journal of biomechanics 46 (2)\n(2013) 217228.\n\n[4] W. Sun, C. Martin, T. Pham, Computational modeling of cardiac valve\nfunction and intervention, Annual review of biomedical engineering 16\n(2014) 5376.\n\n[5] A. Kheradvar, E. M. Groves, L. P. Dasi, S. H. Alavi, R. Tranquillo, K. J.\nGrande-Allen, C. A. Simmons, B. Grith, A. Falahatpisheh, C. J. Goergen,\net al., Emerging trends in heart valve engineering: Part i. solutions for\nfuture, Annals of biomedical engineering 43 (4) (2015) 833843.\n\n[6] K. S. Kunzelman, R. Cochran, Stress/strain characteristics of porcine mi-\ntral valve tissue: parallel versus perpendicular collagen orientation, Journal\nof cardiac surgery 7 (1) (1992) 7178.\n\n[7] S. K. Dahl, J. Vierendeels, J. Degroote, S. Annerel, L. R. Hellevik,\nB. Skallerud, Fsi simulation of asymmetric mitral valve dynamics during\ndiastolic lling, Computer methods in biomechanics and biomedical engi-\nneering 15 (2) (2012) 121130.\n\n[8] E. J. Weinberg, D. Shahmirzadi, M. R. Kaazempur Mofrad, On the multi-\nscale modeling of heart valve biomechanics in health and disease, Biome-\nchanics and modeling in mechanobiology 9 (4) (2010) 373387.\n\n[9] Q. Wang, W. Sun, Finite element modeling of mitral valve dynamic de-\nformation using patient-specic multi-slices computed tomography scans,\nAnnals of biomedical engineering 41 (1) (2013) 142153.\n\n[10] V. Prot, B. Skallerud, Nonlinear solid nite element analysis of mitral valves\nwith heterogeneous leaet layers, Computational Mechanics 43 (3) (2009)\n353368.\n\n[11] M. Stevanella, F. Maessanti, C. A. Conti, E. Votta, A. Arnoldi, M. Lom-\nbardi, O. Parodi, E. G. Caiani, A. Redaelli, Mitral valve patient-specic\nnite element modeling from cardiac mri: Application to an annuloplasty\nprocedure, Cardiovascular Engineering and Technology 2 (2) (2011) 6676.\n\n13\n\n\f[12] C.-H. Lee, C. A. Carruthers, S. Ayoub, R. C. Gorman, J. H. Gorman, M. S.\nSacks, Quantication and simulation of layer-specic mitral valve intersti-\ntial cells deformation under physiological loading, Journal of theoretical\nbiology 373 (2015) 2639.\n\n[13] D. R. Einstein, F. Del Pin, X. Jiao, A. P. Kuprat, J. P. Carson, K. S. Kun-\nzelman, R. P. Cochran, J. M. Guccione, M. B. Ratclie, Fluidstructure\ninteractions of the mitral valve and left heart: comprehensive strategies,\npast, present and future, International Journal for Numerical Methods in\nBiomedical Engineering 26 (3-4) (2010) 348380.\n\n[14] H. Gao, N. Qi, L. Feng, X. Ma, M. Danton, C. Berry, X. Luo, Modelling\nmitral valvular dynamicscurrent trend and future directions, International\nJournal for Numerical Methods in Biomedical Engineeringdoi:10.1002/\ncnm.2858.\n\n[15] J. F. Wenk, Z. Zhang, G. Cheng, D. Malhotra, G. Acevedo-Bolton,\nM. Burger, T. Suzuki, D. A. Saloner, A. W. Wallace, J. M. Guccione,\net al., First nite element model of the left ventricle with mitral valve:\ninsights into ischemic mitral regurgitation, The Annals of thoracic surgery\n89 (5) (2010) 15461553.\n\n[16] V. M. Wong, J. F. Wenk, Z. Zhang, G. Cheng, G. Acevedo-Bolton,\nM. Burger, D. A. Saloner, A. W. Wallace, J. M. Guccione, M. B. Rat-\nclie, et al., The eect of mitral annuloplasty shape in ischemic mitral\nregurgitation: a nite element simulation, The Annals of thoracic surgery\n93 (3) (2012) 776782.\n\n[17] B. Baillargeon, I. Costa, J. R. Leach, L. C. Lee, M. Genet, A. Toutain, J. F.\nWenk, M. K. Rausch, N. Rebelo, G. Acevedo-Bolton, et al., Human car-\ndiac function simulator for the optimal design of a novel annuloplasty ring\nwith a sub-valvular element for correction of ischemic mitral regurgitation,\nCardiovascular engineering and technology 6 (2) (2015) 105116.\n\n[18] D. R. Einstein, P. Reinhall, M. Nicosia, R. P. Cochran, K. Kunzelman, Dy-\nnamic nite element implementation of nonlinear, anisotropic hyperelastic\nbiological membranes, Computer Methods in Biomechanics and Biomedical\nEngineering 6 (1) (2003) 3344.\n\n[19] D. R. Einstein, K. S. Kunzelman, P. G. Reinhall, M. A. Nicosia, R. P.\nCochran, Non-linear uid-coupled computational model of the mitral valve,\nJournal of Heart Valve Disease 14 (3) (2005) 376385.\n\n[20] K. Kunzelman, D. R. Einstein, R. Cochran, Fluidstructure interaction\nmodels of the mitral valve:\nfunction in normal and pathological states,\nPhilosophical Transactions of the Royal Society B: Biological Sciences\n362 (1484) (2007) 13931406.\n\n14\n\n\f[21] K. Lau, V. Diaz, P. Scambler, G. Burriesci, Mitral valve dynamics in struc-\ntural and uidstructure interaction models, Medical engineering & physics\n32 (9) (2010) 10571064.\n\n[22] M. Toma, D. R. Einstein, C. H. Bloodworth, R. P. Cochran, A. P. Yo-\nganathan, K. S. Kunzelman, Fluidstructure interaction and structural\nanalyses using a comprehensive mitral valve model with 3d chordal struc-\nture, International Journal for Numerical Methods in Biomedical Engineer-\ningdoi:10.1002/cnm.2815.\n\n[23] P. N. Watton, X. Y. Luo, M. Yin, G. M. Bernacca, D. J. Wheatley, Eect\nof ventricle motion on the dynamic behaviour of chorded mitral valves,\nJournal of Fluids and Structures 24 (1) (2008) 5874.\n\n[24] X. Ma, H. Gao, B. E. Grith, C. Berry, X. Luo, Image-based uid\nstructure interaction model of the human mitral valve, Computers & Fluids\n71 (2013) 417425.\n\n[25] H. Gao, N. Ma, X.and Qi, C. Berry, B. E. Grith, X. Y. Luo, A nite strain\nnonlinear human mitral valve model with uid-structure interaction, Inter-\nnational journal for numerical methods in biomedical engineering 30 (12)\n(2014) 15971613.\n\n[26] M. Toma, M. . Jensen, D. R. Einstein, A. P. Yoganathan, R. P. Cochran,\nK. S. Kunzelman, Fluidstructure interaction analysis of papillary muscle\nforces using a comprehensive mitral valve model with 3d chordal structure,\nAnnals of biomedical engineering 44 (4) (2016) 942953.\n\n[27] M. Toma, C. H. Bloodworth, E. L. Pierce, D. R. Einstein, R. P. Cochran,\nA. P. Yoganathan, K. S. Kunzelman, Fluid-structure interaction analysis\nof ruptured mitral chordae tendineae, Annals of Biomedical Engineering\n(2016) 113doi:10.1007/s10439-016-1727-y.\n\n[28] M. P. Nash, P. J. Hunter, Computational mechanics of the heart, Journal\nof elasticity and the physical science of solids 61 (1-3) (2000) 113141.\n\n[29] W. W. Chen, H. Gao, X. Y. Luo, N. A. Hill, Study of cardiovascular func-\ntion using a coupled left ventricle and systemic circulation model, Journal\nof Biomechanics 49 (12) (2016) 24452454. doi:10.1016/j.jbiomech.\n2016.03.009.\n\n[30] A. Quarteroni, T. Lassila, S. Rossi, R. Ruiz-Baier, Integrated heart\ncoupling multiscale and multiphysics models for the simulation of the car-\ndiac function, Computer Methods in Applied Mechanics and Engineering\n314 (2016) 345407.\n\n[31] M. K. Rausch, A. M. Zollner, M. Genet, B. Baillargeon, W. Bothe, E. Kuhl,\nA virtual sizing tool for mitral valve annuloplasty, International Journal for\nNumerical Methods in Biomedical Engineeringdoi:10.1002/cnm.2788.\n\n15\n\n\f[32] C. S. Peskin, Flow patterns around heart valves: a numerical method,\nJournal of computational physics 10 (2) (1972) 252271.\n\n[33] D. M. McQueen, C. S. Peskin, E. L. Yellin, Fluid dynamics of the mitral\nvalve: physiological aspects of a mathematical model, American Journal of\nPhysiology-Heart and Circulatory Physiology 242 (6) (1982) H1095H1110.\n\n[34] C. S. Peskin, Numerical analysis of blood ow in the heart, Journal of\ncomputational physics 25 (3) (1977) 220252.\n\n[35] C. S. Peskin, The immersed boundary method, Acta Numerica 11 (2002)\n479517.\n\n[36] M. Yin, X. Y. Luo, T. J. Wang, P. N. Watton, Eects of ow vortex\non a chorded mitral valve in the left ventricle, International Journal for\nNumerical Methods in Biomedical Engineering 26 (3-4) (2010) 381404.\n\n[37] K. B. Chandran, H. Kim, Computational mitral valve evaluation and po-\ntential clinical applications, Annals of biomedical engineering 43 (6) (2015)\n13481362.\n\n[38] H. Gao, H. Wang, C. Berry, X. Y. Luo, B. E. Grith, Quasi-static image-\nbased immersed boundary-nite element model of left ventricle under di-\nastolic loading, International journal for numerical methods in biomedical\nengineeringdoi:10.1002/cnm.2652.\n\n[39] B. E. Grith, X. Y. Luo, Hybrid nite dierence/nite element\nversion of the immersed boundary method, eprint from arXiv (url:\nhttps://arxiv.org/abs/1612.05916).\n\n[40] H. Gao, C. Berry, X. Y. Luo, Image-derived human left ventricular mod-\nelling with uid-structure interaction, in: Functional Imaging and Modeling\nof the Heart, Springer, 2015, pp. 321329.\n\n[41] G. A. Holzapfel, R. W. Ogden, Constitutive modelling of passive my-\nocardium: a structurally based framework for material characterization,\nPhilosophical Transactions of the Royal Society of London A: Mathemati-\ncal, Physical and Engineering Sciences 367 (1902) (2009) 34453475.\n\n[42] S. Niederer, P. Hunter, N. Smith, A quantitative analysis of cardiac my-\nocyte relaxation: a simulation study, Biophysical journal 90 (5) (2006)\n16971722.\n\n[43] R. A. Nishimura, A. J. Ta jik, Evaluation of diastolic lling of left ventricle\nin health and disease: Doppler echocardiography is the clinician?s rosetta\nstone, Journal of the American College of Cardiology 30 (1) (1997) 818.\n\n[44] B. E. Grith, Immersed boundary model of aortic heart valve dynamics\nwith physiological driving and loading conditions, International Journal for\nNumerical Methods in Biomedical Engineering 28 (3) (2012) 317345.\n\n16\n\n\f[45] S. Laniado, E. Yellin, M. Kotler, L. Levy, J. Stadler, R. Terdiman, A study\nof the dynamic relations between the mitral valve echogram and phasic\nmitral ow, Circulation 51 (1) (1975) 104113.\n\n[46] K. Mangion, H. Gao, C. McComb, D. Carrick, G. Clerfond, X. Zhong,\nX. Luo, C. E. Haig, C. Berry, A novel method for estimating myocardial\nstrain: Assessment of deformation tracking against reference magnetic res-\nonance methods in healthy volunteers, Scientic Reports 6 (2016) 38774.\ndoi:10.1038/srep38774.\n\n[47] I. Hay, J. Rich, P. Ferber, D. Burkho, M. S. Maurer, Role of impaired\nmyocardial relaxation in the production of elevated left ventricular lling\npressure, American Journal of Physiology-Heart and Circulatory Physiol-\nogy 288 (3) (2005) H1203H1208.\n\n[48] M. R. Zile, C. F. Baicu, W. H. Gaasch, Diastolic heart failureabnormalities\nin active relaxation and passive stiness of the left ventricle, New England\nJournal of Medicine 350 (19) (2004) 19531959.\n\n17\n\n\fMV leaets\nAnterior\nPosterior\nChordae\nsystole\ndiastole\nMyocardium\npassive\n\nb\na (kPa)\n5.08\n0.24\nT req = 225 kPa\n\nactive\n\nav (kPa)\n31.3\n50.0\n\nbv (kPa)\n55.93\n63.48\n\nC1 (kPa)\n17.4\n10.2\nC (kPa)\n9000\n540\n\naf (kPa)\n1.46\n\nbf\n4.15\n\nas (kPa)\n0.87\n\nbs\n1.6\n\na8fs (kPa)\n0.3\n\nb8fs\n1.3\n\nTable 1: Material parameter values for MV leaftlets, chordae and the myocardium\n\n18\n\n\fCases (mmHg)\nEDP=8, Pendo=8\nEDP=8, Pendo=10\nEDP=8, Pendo=12\nEDP=8, Pendo=14\nEDP=8, Pendo=16\nEDP=8, Pendo=0\nEDP=12,Pendo=0\nEDP=14,Pendo=0\nEDP=16,Pendo=0\nEDP=18,Pendo=0\nEDP=20,Pendo=0\n\ntiso-con (ms)\n60\n58\n57\n55\n54\n75\n64\n61\n58\n56\n55\n\ntejection (ms)\n227\n237\n243\n251\n256\n174\n213\n226\n243\n251\n262\n\n(mL)\n\nV ejection\nLV\n52\n57.6\n63.2\n67.8\n72.3\n20.8\n41.0\n50.6\n61.8\n68.5\n75.7\n\nV lling\nMV (mL)\n60.6\n65.9\n72.1\n76.8\n81.3\n29.5\n50.9\n59.8\n71.9\n79.3\n86.2\n\nF peak\nMV (mL/s)\n412.93\n442.60\n468.41\n486.84\n503.76\n209.54\n343.47\n406.81\n459.64\n486.58\n511.16\n\nLVEF(%)\n47%\n49%\n51%\n53%\n54%\n29%\n42%\n47%\n51%\n54%\n55%\n\nTable 2: Eects of EDP and the endocardial pressure loading (Pendo ) on MV and LV dynamics.\n\n19\n\n\f(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n(f )\n\n(g)\n\n(h)\n\nFigure 1: The CMR-derived MV-LV model. (a) The MV leaets were segmented from a stack\nof MR images of a volunteer at early-diastole, (b) positions of the papillary muscle heads and\nthe annulus ring, (c) reconstructed MV geometry with chordae, (d) a MR image showing the\nLV and location of the outow tract (AV) and inow tract (MV), (e) the LV wall delineation\nfrom short and long axis MR images, (f ) the reconstructed LV model, in which the LV model\nis divided into four part: the LV region bellow the LV base, the valvular region, and the inow\nand outow tracts, (g) the rule-based bre orientations in the LV and the MV, and (h) the\ncoupled MV-LV model.\n\n20\n\n\fFigure 2: Flow rates across the AV and MV from diastole to systole. Diastolic phase: 0 s to\n0.8 s; Systolic phase: 0.8 s and onwards. Positive ow rate means the blood ows out of the\nLV chamber.\n\n21\n\n\fFigure 3: Normalized intracellular Ca2+, LV cavity volume, central LV pressure and average\nmyocardial active tension. All curves are normalized to their own maximum values, which are:\n1Mol for Ca2+, 145 mL for LV cavity volume, 162 mmHg for central LV pressure, 96.3 kPa\nfor average myocardial active tension.\n\n22\n\n\f(a)\n\n(b)\n\n(c)\n\nFigure 4: Comparisons between the MV and LV structures at (a) reference conguration, (b)\nend-diastole, and (c) end-systole, and the corresponding CMR cine images (left). Coloured\nby the displacement magnitude.\n\n23\n\n\f(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 5: Streamlines in the MV-LV model at early-diastolic lling (a), late-diastolic lling\n(b), when isovolumtric contraction ends (c), and at the mid-systole. Streamline are colored\nby velocity magnitude, the LV wall and MV are colored by the displacement magnitude. Red\n: high; blue: low\n\n24\n\n\f(a)\n\n(b)\n\n(c)\n\nFigure 6: Distributions of bre strain in the left ventricle at end-systole (a), in the MV at\nend-diastole (b) and end-systole (c).\n\n25\n\n\f", 
        "tag": "Computational Engineering, Finance, and Science", 
        "link": "https://arxiv.org/list/cs.CE/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nG\nC\n.\ns\nc\n[\n \n \n1\nv\n7\n8\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nComputational determination of the largest\nlattice polytope diameter\n\nNathan Chadder 1\n\nDepartment of Computing and Software\nMcMaster University\nHamilton, Canada\n\nAntoine Deza 2\n\nDepartment of Computing and Software\nMcMaster University\nHamilton, Canada\n\nAbstract\n\nA lattice (d, k)-polytope is the convex hull of a set of points in dimension d whose\ncoordinates are integers between 0 and k . Let (d, k) be the largest diameter over all\nlattice (d, k)-polytopes. We develop a computational framework to determine (d, k)\nfor small instances. We show that (3, 4) = 7 and (3, 5) = 9; that is, we verify for\n(d, k) = (3, 4) and (3, 5) the conjecture whereby (d, k) is at most (k + 1)d/2 and\nis achieved, up to translation, by a Minkowski sum of lattice vectors.\n\nKeywords: Lattice polytopes, edge-graph diameter, enumeration algorithm\n\n1 Email: chaddens@mcmaster.ca\n2 Email: deza@mcmaster.ca\n\n\f1 Introduction\n\nFinding a good bound on the maximal edge-diameter of a polytope in terms\nof its dimension and the number of its facets is not only a natural question of\ndiscrete geometry, but also historically closely connected with the theory of\nthe simplex method, as the diameter is a lower bound for the number of pivots\nrequired in the worst case. Considering bounded polytopes whose vertices are\nrational-valued, we investigate a similar question where the number of facets\nis replaced by the grid embedding size.\nThe convex hull of integer-valued points is called a lattice polytope and,\nif all the vertices are drawn from {0, 1, . . . , k}d , it is referred to as a lattice\n(d, k)-polytope. Let  (d, k) be the largest edge-diameter over all lattice (d, k)-\npolytopes. Naddef [7] showed in 1989 that  (d, 1) = d, Kleinschmidt and\nOnn [6] generalized this result in 1992 showing that  (d, k)  kd. In 2016,\nDel Pia and Michini [3] strengthened the upper bound to  (d, k)  kd  d/2\nfor k  2, and showed that  (d, 2) = 3d/2. Pursuing Del Pia and Michinis\napproach, Deza and Pournin [5] showed that  (d, k)  kd  2d/3  (k  3)\nfor k  3, and that  (4, 3) = 8. The determination of  (2, k) was investigated\nindependently in the early nineties by Thiele [8], Balog and Barany [2], and\nAcketa and Zunic [1]. Deza, Manoussakis, and Onn [4] showed that  (d, k) \n(k + 1)d/2 for all k  2d  1 and proposed Conjecture 1.1.\n\nConjecture 1.1  (d, k)  (k + 1)d/2, and  (d, k) is achieved, up to trans-\nlation, by a Minkowski sum of lattice vectors.\n\nIn Section 2, we propose a computational framework which drastically reduces\nthe search space for lattice (d, k)-polytopes achieving a large diameter. Ap-\nplying this framework to (d, k) = (3, 4) and (3, 5), we determine in Section 3\nthat  (3, 4) = 7 and  (3, 5) = 9.\n\nTheorem 1.2 Conjecture 1.1 holds for (d, k) = (3, 4) and (3, 5); that is,\n (3, 4) = 7 and  (3, 5) = 9, and both diameters are achieved, up to trans-\nlation, by a Minkowski sum of lattice vectors\n\nNote that Conjecture 1.1 holds for all known values of  (d, k) given in Ta-\nble 1, and hypothesizes, in particular, that  (d, 3) = 2d. The new entries\ncorresponding to (d, k) = (3, 4) and (3, 5) are entered in bold.\n\n\f6\n\n1\n\n6\n\n7\n\n1\n\n6\n\n8\n\n1\n\n7\n\n9\n\n1\n\n8\n\n10\n\n1\n\n8\n\n4\n\n1\n\n4\n\n7\n\n5\n\n1\n\n5\n\n9\n\nk\n\nd\n\n1\n\n2\n\n3\n\n4\n...\n\nd\n\n3\n\n1\n\n4\n\n6\n\n8\n\n1\n\n1\n\n2\n\n3\n\n4\n...\n\nd\n\n2\n\n1\n\n3\n\n4\n\n6\n...\n 3d\n2 \n\nTable 1\nThe largest possible diameter (d, k) of a lattice (d, k)-polytope\n\n2 Theoretical and Computational Framework\n\nSince  (2, k) and  (d, 2) are known, we consider in the remainder of the paper\nthat d  3 and k  3. While the number of lattice (d, k)-lattice polytopes\nis nite, a brute force search is typically intractable, even for small instances.\nTheorem 2.1, which recalls conditions established in [5], allows to drastically\nreduce the search space.\n\nTheorem 2.1 For d  3, let d(u, v ) denote the distance between two vertices\nu and v in the edge-graph of a lattice (d, k)-polytope P such that d(u, v ) =\ni , respectively F k\n (d, k). For i = 1, . . . , d, let F 0\ni , denote the intersection of P\nwith the facet of the cube [0, k ]d corresponding to xi = 0, respectively xi = k .\nThen, d(u, v )   (d  1, k) + k , and the fol lowing conditions are necessary for\nthe inequality to hold with equality:\n\n(1) u + v = (k , k , . . . , k),\n\n(2) any edge of P with u or v as vertex is {1, 0, 1}-valued,\ni , respectively F k\n(3) for i = 1, . . . , d, F 0\ni , is a (d  1)-dimensional face of P with\ni ) =  (d  1, k), respectively  (F k\ndiameter  (F 0\ni ) =  (d  1, k).\n\nThus, to show that  (d, k) <  (d  1, k) + k , it is enough to show that there is\nno lattice (d, k)-polytope admitting a pair of vertices (u, v ) such that d(u, v ) =\n (d, k) and the conditions (1), (2), and (3) are satised. The computational\nframework to determine, given (d, k), whether  (d, k) =  (d  1, k) + k is\noutlined below and illustrated for (d, k) = (3, 4) or (3, 5).\n\n\fAlgorithm to determine whether  (d, k) <  (d  1, k) + k\n\nStep 1: Initialization\nDetermine the set F of al l the lattice (d  1, k)-polytopes P such that  (P ) =\n (d  1, k). For example, for (d, k) = (3, 4), the determination of al l the 335\nlattice (2, 4)-polygons P such that  (P ) = 4 is straightforward.\n\nStep 2: Symmetries\nConsider, up to the symmetries of the cube [0, k ]d , the possible entries for a pair\nof vertices (u, v ) such that u + v = {k , k , . . . , k}. For example, for (d, k) =\n(3, 4), the fol lowing 6 vertices cover al l possibilities for u up to symmetry:\n(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 1), (0, 1, 2), and (0, 2, 2), where v = (4, 4, 4)  u.\n\nStep 3: Shelling\nFor each of the possible pairs (u, v ) determined during Step 2, consider al l pos-\nsible ways for 2d elements of the set F determined during Step 1 to form the 2d\nfacets of P lying on a facet of the cube [0, k ]d . For example, for (d, k) = (3, 4)\nand u = (0, 0, 0), we must nd 6 elements of F , 3 with (0, 0) as a vertex, and\n3 with (4, 4) as a vertex. In addition, if an edge of an element of F with u or\nv as vertex is not {1, 0, 1}-valued, this element is disregarded.\n\nNote that since the choice of an element of F denes the vertices of P be-\nlonging to a facet of the cube [0, k ]d , the choice for the next element of F to\nform a shel ling is signicantly restricted. In addition, if the set of vertices and\nedges belonging to the current elements of F considered for a shel ling includes\na path from u to v of length at most  (d  1, k) + k  1, a shortcut between u\nand v exists and the last added elements of F can be disregarded.\n\nStep 4. Inner points\nFor each choice of 2d elements of F forming a shel ling obtained during Step 3,\nconsider the {1, 2, . . . , k  1}-valued points not in the convex hul l of the vertices\nof the 2d elements of F forming a shel ling. Each such {1, 2, . . . , k  1}-valued\npoint is considered as a potential vertex of P in a binary tree. If the current\nset of edges includes a path from u to v of length at most  (d  1, k) + k  1, a\nshortcut between u and v exists and the corresponding node of the binary tree\ncan be disregarded, and the the binary tree is pruned at this node.\n\nA convex hul l and diameter computation are performed for each node of the\nobtained binary tree. If there is a node yielding a diameter of  (d  1, k) + k\n\n\fwe can conclude that  (d, k) =  (d  1, k) + k . Otherwise, we can conclude\nthat  (d, k) <  (d  1, k) + k . For example, for (d, k) = (3, 5), no choice of 6\nelements of F forming a shel ling such that d(u, v )  10 exist, and thus Step 4\nis not executed.\n\n3 Computational Results\n\nFor (d, k) = (3, 4), a shelling exists for which path lengths are not decidable\nby the algorithm without convex hull computations. However, this shelling\nonly achieves a diameter of 7. For (d, k) = (3, 5) the algorithm stops at Step\n3, as there is no combination of 6 elements of F which form a shelling such\nthat d(u, v )   (2, 5) + 5. Thus, no convex hull computations are required for\n(d, k) = (3, 5). A shortcut from u to v is typically found early on in the shelling,\nwhich leads to the algorithm terminating quickly. Run on a 2009 Intel R(cid:13)\nCoreTM2 Duo 2.20GHz CPU, the algorithm is able to terminate for (d, k) =\n(3, 4) and (3, 5) in under a minute. Consequently,  (3, 4) < 8 and  (3, 5) < 10.\nSince the Minkowski sum of (1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 1), (1, 0, 1), (1, 1, 0),\nand (1, 1, 1) forms a lattice (3, 4)-polytope with diameter 7, we conclude that\n (3, 4) = 7. Similarly, since the Minkowski sum of (1, 0, 0), (0, 1, 0), (0, 0, 1),\n(0, 1, 1), (1, 0, 1), (1, 1, 0), (0, 1, 1), (1, 0, 1), and (1, 1, 0) forms, up to trans-\nlation, a lattice (3, 5)-polytope with diameter 9, we conclude that  (3, 5) = 9.\nComputations for additional values of  (d, k) are currently underway. In par-\nticular, the same algorithm may determine whether  (d, k) =  (d  1, k) + k\nor  (d  1, k) + k  1 for (d, k) = (5, 3) and (4, 4) provided the set of all lat-\ntice (d  1, k)-polytopes achieving  (d  1, k) is determined for (d, k) = (5, 3)\nand (4, 4). Similarly, the algorithm could be adapted to determine whether\n (d, k) <  (d  1, k) + k  1 provided the set of all lattice (d  1, k)-polytopes\nachieving  (d  1, k) or  (d  1, k)  1 is determined. For example, the adapted\nalgorithm may determine whether  (3, 6) = 10.\n\nAcknowledgement\n\nThis work was partially supported by the Natural Sciences and Engineering\nResearch Council of Canada Discovery Grant program (RGPIN-2015-06163).\n\n\fReferences\n\n[1] Dragan Acketa and Jovisa Zunic, On the maximal number of edges of convex\ndigital polygons included into an m  m-grid, Journal of Combinatorial Theory\nA 69 (1995), 358368.\n\n[2] Antal Balog and Imre Barany, On the convex hul l of the integer points in a disc,\nProceedings of the Seventh Annual Symposium on Computational Geometry\n(1991), 162165.\n\n[3] Alberto Del Pia and Carla Michini, On the diameter of lattice polytopes, Discrete\nand Computational Geometry 55 (2016), 681687.\n\n[4] Antoine Deza, George Manoussakis, and Shmuel Onn, Primitive zonotopes,\nDiscrete and Computational Geometry (to appear).\n\n[5] Antoine Deza and Lionel Pournin, Improved bounds on the diameter of lattice\npolytopes, arXiv:1610.00341 (2016).\n\n[6] Peter Kleinschmidt and Shmuel Onn, On the diameter of convex polytopes,\nDiscrete Mathematics 102 (1992), 7577.\n\n[7] Dennis Naddef, The Hirsch conjecture is true for (0, 1)-polytopes, Mathematical\nProgramming 45 (1989), 109110.\n\n[8] Torsten Thiele, Extremalprobleme fur Punktmengen, Master thesis, Freie\nUniversitat, Berlin, 1991.\n\n\f", 
        "tag": "Computational Geometry", 
        "link": "https://arxiv.org/list/cs.CG/new"
    }, 
    {
        "text": "Statistical Estimation with Strategic Data Sources in Competitive\nSettings\nTyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry\n\n7\n1\n0\n2\n \nr\np\nA\n \n4\n \n \n]\nT\nG\n.\ns\nc\n[\n \n \n1\nv\n5\n9\n1\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstract In this paper, we introduce a preliminary model\nfor interactions in the data market. Recent research has shown\nways in which a data aggregator can design mechanisms for\nusers to ensure the quality of data, even in situations where\nthe users are effort-averse (i.e. prefer to submit lower-quality\nestimates) and the data aggregator cannot observe the effort\nexerted by the users (i.e. the contract suffers from the principal-\nagent problem). However, we have shown that these mechanisms\noften break down in more realistic models, where multiple\ndata aggregators are in competition. Under minor assumptions\non the properties of the statistical estimators in use by data\naggregators, we show that there is either no Nash equilibrium,\nor there is an innite number of Nash equilibrium. In the\nlatter case, there is a fundamental ambiguity in who bears\nthe burden of incentivizing different data sources. We are also\nable to calculate the price of anarchy, which measures how\nmuch social welfare is lost between the Nash equilibrium and\nthe social optimum, i.e. between non-cooperative strategic play\nand cooperation.\n\nI . IN TRODUC T ION\nThe proliferation of smart sensors in recent years has intro-\nduced the possibility of accurately detecting and estimating\na large new class of phenomena that affect society. These\nsensors, ranging from smart personal devices to more tradi-\ntional purpose-built sensors, may be owned by a multitude\nof sources, and can produce qualitatively different readings\nwhich can be combined to make inferences about an event\nof interest.\nIn turn,\nthis has led to the advent of crowd sensing,\nwherein a central data collector accrues the measurements\nmade by a multitude sources, using these data points to\ngenerate a single cohesive estimate for some phenomena of\ninterest to the data collector. However, the quality of this\ncentral estimate, and thus its value to the data collector,\ndepends fundamentally on the ability, and moreover the\nwillingness, of the data sources to produce accurate readings\nwhich are relevant\nto the phenomena the data collector\nwishes to study.\nUnfortunately, there may be instances where data sources\nhave some aversion to providing the data collector with the\nquality of estimates she desires. Take as an example, the case\nwhere the sensor must exert signicant resources to produce\nan accurate reading (e.g. time or network bandwidth), or\na situation where the source views the information she\n\nare with\nand S. S. Sastry\nT. Westenbroek, R. Dong,\nthe\nDepartment\nand Computer\nSciences,\nof Electrical Engineering\nof California, Berkeley, Berkeley, CA,\nUniversity\n94707, USA,\n{westenbroekt,roydong,sastry}@eecs.berkeley.edu.\nL. J. Ratliff is with the Department of Electrical Engineering, University\nof Washington, Seattle, WA, 98195, USA, ratliffl@uw.edu.\n\nis sharing as private, and has incentive to obfuscate the\ndata she shares [1], [2]. Consequently, in order to ensure\nshe consistently receives high quality measurements from\nthe data sources, the central data collector must design an\nincentive mechanism which:\n1) allows her to metricize the quality of the reading each\ndata source provides, and\n2) provides incentive for the data sources to produce\nreadings which are considered high quality under\nthis metric.\nGiven the wide range of applications and industries this\nproblem affects, many different compensation mechanisms\nhave been proposed to promote the production of high quality\nreadings from a collection of data sources. An overview of\nsuch mechanisms is given in [3].\nThe contribution of this section can be seen as an extension\nof [4],\nin which the authors design a general payment\nmechanism, by which a central data collector may induce\neach data source in the marketplace to exert precisely the\nlevel of effort in collecting data that the central data buyer\ndesires. The goal of the data buyer in this case is to obtain\na high quality estimator for some phenomena using the\nreadings from the data sources, while reducing the payments\nneeded to incentivize the necessary exertion of effort from\nthe sensors. Several other papers [5], [6] further investigate\nmechanisms of this sort, proposing several extensions.\nHowever, it has yet to be studied how such mechanisms\nperform in situations where more than one central data\nbuyer wishes to purchase readings from data sources in the\nmarketplace. A number of important questions arise when\nsuch data markets are considered. If the central data buyers\nare competing companies, will they permit data sources to\nalso sell information to their competitors? If the data buyers\ndo purchase readings from the same set of data sources, who\nwill foot the bill to incentivize the effort the data sources\nexert? Will the data buyers who provide larger payments to\nthe data sources be compensated with higher quality readings\nthan their competitors?\nMost signicantly, this section demonstrates that if all the\ndata buyers design compensation schemes as proposed in\n[4], each of the data buyers will receive the same quality\nof reading from a particular data source, regardless of how\nmuch each data buyer personally compensates the data\nsource for her effort. This leads to conicting objectives for\neach of the data buyers on several fronts. If a data buyer\nwishes to induce a data source to exert a high level of effort,\nshe must reconcile the fact that her competitors will also\nreceive a high quality reading from this data source. Even in\n\n\fthe personal\n\nthe case where the data buyers care little about the success\nof the other buyers in the marketplace, each data buyer still\nwants to incentivize the data sources to produce high quality\nreadings, but wants to force the other data buyers to offer\nthe lions share of the necessary compensation.\nIn this section, we analyze the competitive outcomes that\narise in such a marketplace by formulating a game between\nthe buyers wherein they\n1) compete by designing pricing mechanisms to affect the\nbehavior of the data sources, and\n2) design these mechanisms so as meet\nobjectives enumerated above.\nWe derive conditions for the existence of Nash Equilib-\nria in this game when a particular form is assumed, and\nanalyze the efciency and equity of these outcomes. We\ndemonstrate through both analytical and numerical exercises\nthat the outcomes of these games are often highly inefcient\nfrom a social standpoint, which motivates future work to\ndesign incentive mechanisms which more effectively handle\ncompetition between data buyers.\nThe rest of this paper proceeds as follows. In Section II, we\nlay out explicit mathematical structures for the data markets,\nstrategic data sources, strategic data buyers, and the class of\ncontracts we will consider between the sources and buyers.\nIn Section III, we analyze the game that forms between\nthe buyers in the data market, and demonstrate that the\noutcome of this game is in many cases socially inefcient,\nand often times. Section IV provides a numerical example\nwhich highlights the issues presented in Section III. And\nnally, Section V prescribes an agenda for future work, with\nthe aim of developing more rened incentive mechanism\nwhich do not suffer from the same shortcomings in the\ncompetitive setting.\n\nI I . MATH EMAT ICA L FORMU LAT ION\nIn this section we formulate our model for data markets.\nWe rst present our model for strategic data sources, and\nthen strategic buyers who issue incentives to strategic data\nsources. Based on recent research [4], we use incentives with\na particular payment structure. Then, we dene our overall\ngame, as well as a generalized Nash equilibrium for this\ngame.\n\nA. Data market\nAt a high level, a data market consists of a set S =\n{1, ..., N } of strategic data sources, and a set B = {1, ..., M }\nof strategic data buyers. Each data source i is equipped to\ngenerate an estimate of the function f : D  R at some data\npoint xi  D , and each data buyer j  B wishes to use these\nreadings to generate a personal estimator of f , which we will\ndenote f j . Each buyer bj is willing to form a contract with\neach data source i  S , which monetarily compensates i for\nthe readings she produces, and we assume it is under the\npurview of j to dene the structure of this contract.\nOne may think of D as a set of features or events the\ndata buyers are capable of observing,\nin order to make\na prediction about some phenomena. The value returned\n\nby the mapping f encapsulates the relationship between\nthe observable features and the outcome of interest. We\nfurther assume that each of the data sources and buyers acts\nstrategically; that is, each of these agents acts to maximize\nsome expected personal return from her transactions in this\nmarketplace. The following two subsections of the document\nprovide an explicit mathematical formulation describing the\nbehavior of the data sources and data buyers. The basis for\nthese denitions comes directly from [4].\n\nB. Strategic data sources\nIn this subsection, we dene our model for strategic data\nsources. Intuitively, data sources provide data samples (x, y)\nwhose variance depends on their effort. Thus,\nthe more\neffort exerted, the better the statistical estimation for any\ndata buyer who receives the data. Additionally, we assume\nthe data sources are effort-averse, i.e. all else equal, they\nprefer to exert minimal effort. Furthermore, the buyer has\nno direct way to verify the amount of effort exerted by the\ndata source. Thus, we have an issue commonly referred to\nas moral hazard.\nMore formally, all data sources share some function f :\nD  R, where f is the function which data buyers wish to\nestimate. One may think of D as a set of features or events\nthe data buyers are capable of observing, in order to make a\nprediction about some phenomena. The value returned by\nthe mapping f encapsulates the relationship between the\nobservable features and the outcome of interest.\nEach data source i has their own feature xi  D and their\ni : R  R+ . When data source\nown cost-of-effort function 2\ni exerts effort ei  R, they produce an estimate of the form:\n\u0001i (ei )  N (0, 2\nyi (ei ) = f (xi ) + \u0001i (ei )\ni (ei ))\nBoth xi and 2\ni are common knowledge, but the effort ei is\nprivate, as well as the the value yi (ei ) produced. We shall\ndesign contracts such that the data source i is incentivized\nto exert the correct amount of effort (to be dened), and\nreport yi truthfully.\nData source i will receive a payment from each buyer for\ntheir data. For buyer j , let this payment, potentially random,\nbe denoted pj\n  ei\n(cid:88)\ni . We assume that the data source has a utility\nfunction of the following form, should they opt-in:\njB\nIf they opt-out, they will receive utility 0.\nNote that this assumes that the data sources are risk-\nneutral, effort-averse, and must opt-in ex-ante. Additionally,\nwe assume the effort ei can be normalized to be comparable\nto the payments.\nThroughout the rest of this paper, we shall often omit the\nargument ei when context makes it evident.\n\n(1)\n\nE\n\npj\ni\n\nC. Strategic data buyers\nA strategic data buyer j  B is an agent who wishes\nto construct the best estimator f j for a function f . She\n\n\foptimizes a loss function across a class of estimators, which\nthe data buyer is free to select. In general, different buyers\nneed not t models of the same type; for example, one\ndata buyer may choose to generate her estimator via linear\nregression, while another data buyer constructs his estimator\nby tting the data to a polynomial model of higher degree.\nDifferences in the type of estimator data buyers use may be\nused to encapsulate competitive advantages one data buyer\nhas over another. For a more thorough review of the technical\nrequirements of these estimators, see [4].\nAdditionally, each data buyer j has a distribution Fj across\nD , which denotes how much they value an accurate estimate\nat various points in D .\nIn particular, let f j\ndenote the estimator that buyer j\n((cid:126)x, (cid:126)yj )\nconstructs, based on the location of the data sources, (cid:126)x, and\nthe reports she receives from the data sources, (cid:126)y j . (Here,\n(cid:126)x = (x1 , . . . , xN ) and similarly (cid:126)y j is the vector of y values\nreported to buyer j .)\nBeyond any intrinsic utility buyer j experiences from\nincreasing the quality of her estimator, j also wishes to\nconstruct an estimator that\nis better than the estimator\nconstructed by her competitors, the other members of B .\nEach data buyer j commits to a payment function pj\ni to\neach data source i  S , where pj\ni : DN  RN  R may\ndepend not only on the reading reported by data source i,\nbut also the readings reported by the other members of S ,\nwith consideration given to the location of the data sources.\n(cid:34) (cid:16) f j\nIn particular, buyer j constructs her various contracts with\n(cid:17)2 \nthe data sources so as to minimize:\n(x )  f (x )\nJ j ( (cid:126)pj , (cid:126)pj ) = E\n(cid:17)2\n(cid:16) f k\n+  j (cid:88)\n(cid:88)\n(cid:126)x, (cid:126)yj\n(cid:126)x, (cid:126)yk (x )  f (x )\niS\nkj\nThe expectation in (2) is taken across x  Fj as well as\nthe randomness in the reported data (cid:126)yk for k  B . (Recall\nthat Fj weighs the importance data buyer j places on an\nan accurate estimator about different points x  D .) Here,\nas per typical game theory notation, we will let i denote\nS \\ {i} and j denote B \\ {j }, and when i or j is used as\na subscript, this denotes everyone elses variables, e.g. (cid:126)pj\ndenotes the vector of payment plans of all the data buyers\nthat are not j .\nk  [0, 1] parameterizes the level of competition\nHere,  j\nbetween buyers j and k , and we assume this competition is\nsymmetric so  j\nj . When  j\nk = 0, j is indifferent to the\nk = k\nsuccess of k , and competes with k only insofar as trying\nto determine who will pay to incentive the data sources.\nMeanwhile,  j\nk = 1 denotes a situation akin to a zero-sum\ngame between data buyers j and k .\nThe parameter  j > 0 denotes a conversion between dollar\namounts allocated by the payment functions and the utility\ngenerated by the quality of the various estimators that are\nconstructed.\nIn order for the objective expressed in (2) to be well\ndened, we assume that buyer j chooses to construct an\n\n(cid:35) (2)\ni ((cid:126)x, (cid:126)y j )\npj\n\n j\nk\n\nE\n\n(4)\n\n(3)\n\n(cid:20)(cid:16) f j\n(cid:17)2(cid:21)\nestimator for which there exists a function gj such that, for\nall distributions F j over D , (cid:126)x, and (cid:126)2  RN :\n(x )  f (x )\ngj ((cid:126)x, Fj , (cid:126)2 ) = E\n(cid:126)x, (cid:126)yj\nHere the (cid:126)y j have variance (cid:126)2 .\nFinally, we assume that buyer j has knowledge of what\nclass of estimator each of the other data buyers plans to use.1\nThe data buyers are interested in offering payment con-\ntracts to data sources. These contracts must be designed such\nthat, for each data source i, when i selects her effort ei to\n(cid:88)\n  ei  0\nmaximize to (1), given the payment contracts from all of the\nother data buyers:\ni ( (cid:126)y j (ei ))\npj\n(cid:105)  0\nE (cid:104)\njB\ni ( (cid:126)y j (ei ))\npj\nNote that (4) is an ex-ante constraint for data source i that i\nreceives non-negative payoff in expectation. This depends on\nthe payments of the other data buyers. The second is an ex-\nante constraint that data source i never opts into any contract\nwith negative payments.\nWe model\nthe resulting competition between the data\nbuyers, subject to these coupled constraints, as a generalized\nNash equilibrium problem (GNEP) [7].\nDenition 1. Each player j from a nite set of players B\naims to solve an optimization problem given by:\n{J j (pj , pj )|pj  Mj (pj )}\nBR(pj ) = arg min\npj\nMj (pj ) is called the feasible set\nfor player j , which\ndepends on the actions taken by the other players j . A\nvector p = (p1 , p2 , . . . pM ) is called a (generalized) Nash\nequilibrium (GNE) if pj = BR(pj ) for all j  B , i.e. the\npj are simultaneously solutions to each players optimization\n(6).\nHaving laid out the general formulation for this problem,\nin the nal portion of this paper we lay out the form of the\npayment contracts that we consider between the buyers and\nsellers.\n\n(5)\n\n(6)\n\nD. Structure of payment contracts\nIn [4] the particular case where |B | = 1 is analyzed, and\nno competition between buyers of data must be considered.\n(cid:17)2\n(cid:16)\nTheir work considers payment plans from the single buyer\nto each data source i of the form:\nyi  f((cid:126)x, (cid:126)yj )i\npi ((cid:126)x, (cid:126)y) = ci  di\n(xi )\nwhere f((cid:126)x,(cid:126)y)i (xi ), is the optimal estimate for f (xi ) that the\ndata buyer can construct from the readings reported by the\n\n(7)\n\n,\n\n1 This is a heavy-handed assumption, given that competing data buyers\nare unlikely to inform their competitors how they intend to process the\ndata supplied by the sources. However, this is keeping with the goal of the\npaper, as we shall demonstrate that even when there is complete information\nbetween the buyers, inefciencies still arise in the data market.\n\n\fdata sources other than source i, and ci  0, di  0 are\nscalars to be chosen strategically by the buyer. The authors\nof [4] demonstrate an algorithm for selecting ci and di which\nallows the buyer to:\n1) precisely incentive data source i to exert any level of\neffort ei that the buyer desires (the authors can make\nei a dominant strategy for data source i), and\n2) precisely compensate data source i for her effort\n(Epi (yi (ei ), (cid:126)yi ((cid:126)ei )) = ei , making the contract\ntightly satisfy individual rationality constraints).\nOur goal is to study how pricing schemes of this form\nperform in the more general case where |B | > 1, and\ncompetition between multiple data buyers becomes a critical\nconsideration. In particular, we assume the following form\nfor each of the incentive mechanisms offered in the data\nmarket.\nAssumption 1. Consider a data buyer j and data source i.\n(cid:17)2\n(cid:16)\nIt is assumed that j offers i a payment function of the form\nyi  f j\ni  dj\ni ((cid:126)x, (cid:126)y) = cj\npj\n(8)\n,\n((cid:126)x,(cid:126)y)i\ni\ni  0 are\nin exchange for knowledge of yi ,where cj\ni , dj\nparameters that the buyer j is free to choose.\n\n(xi )\n\nNote that these payments do not directly depend on the\nlevel of effort that any of the data sources exert, since the data\nbuyers do not have a means to directly observe these values.\nThe payments only depend on the data reported to them,\nand can be calculated by data buyers. Having dened the\nnecessary structures for the data markets we wish to study,\nwe are now ready to study the competitive equilibria that\narise in these marketplaces.\nFirst, we note that for any data source i, due to the form\nof the payment contract, they will report the same value to\nall data buyers.\nProposition 1. Fix any data source i. Pick any vector of\nvariances (cid:126)2 (one variance for each data buyer), and let e =\nmax {e : 2\ni (e) = ( (cid:126)2 )j }, i.e. e is the minimum amount of\neffort for data source i to generate measurements of variance\n(cid:126)2 . Then, data source i has higher payoff, dened by (1), by\nchoosing variances 2\ni (e) for all j , than the payoff earned\nfrom providing each buyer j with data of variance ( (cid:126)2 )j .\nIn other words, since the payment contract from each data\nbuyer j is increasing (in expectation) with respect to effort,\ndata source i will never have incentive to add noise to a\nmeasurement once the effort has been exerted. Thus, for the\nrest of this paper, we shall write (cid:126)y to denote the measurement\nreported to all data sources j .\n\nI I I . R E SU LT S\nIn this section, we analyze the behavior we can expect\nfrom each of the agents in the market place, by considering\nthe game that forms between the members of B as they select\nthe parameters in the contracts they offer to the data sources.\nAdopting standard game-theoretic short-hand notion, we\ndenote the set of pricing parameters buyer k selects by\n\nE\n\ndj\nq\n\n=\n\ndj\nq\n\n(9)\n\n(ck , dk ) , and we denote the choice of the pricing parameters\nof the other members of B by (ck , dk ). From now on, we\nuse the index k to single out a specic buyer, the index q to\nsingle out a data source, the index j to sum over a collection\nof buyers, and the index i (and sometimes l) to sum over a\ncollection of sources.\nWe begin our analysis by determining under what condi-\ntions the data sources will accept the collection of contracts\noffered to them by the data buyers. Recall that data source q\nwill accept all of the contracts offered by the data sources if\nand only if the ex-ante total payments are non-negative (4)\nand each data buyers payment is non-negative ex-ante (5).\nLet x denote the probability measure that puts mass 1 at\n(cid:88)\n =\npoint x. Then, we may simplify (4) for a xed q by noting\n(cid:17)2\n(cid:16)\n(cid:88)\nqE (cid:88)\nthat:\nyq  f j\ncj\npj\n(cid:16)\n(cid:17)\n(xq )\nq ((cid:126)x, (cid:126)y)\n(cid:88)\nq  (cid:88)\n(cid:126)xq ,(cid:126)yq\njB\njB\njB\nq (eq ) + gj ( xq , xq , (cid:126)2q )\ncj\ndj\n2\nq\njB\njB\n(cid:16)\n(cid:17)  eq\n(cid:88)\nq  (cid:88)\nThen, (4) holds if and only if:\nq (eq ) + gj ( xq , xq , (cid:126)2q )\ncj\n2\njB\njB\n(cid:16)\n(cid:17)\nSimilarly, (5) holds if and only if:\nq  dj\nq (eq ) + gj ( xq , xq , (cid:126)2q )\ncj\n2\nq\nAs our goal is to nd situations where the buyers receive\ndata from each of the data sources, we shall include equations\n(9) and (10) as constraints in the game between data buyers.\nIndeed, given a choice of (ck , dk ), the objective of buyer\nk is to optimize the following problem:\nE (cid:104)(cid:80)\n(cid:105)  e\nJ k ((ck , dk ), (ck , dk ))\nE (cid:104)(cid:80)\n(cid:105)  ei\ni  0\ni ((cid:126)x, (cid:126)y( (cid:126)e ))\njB pj\ni ((cid:126)x, (cid:126)y((cid:126)e))(cid:3)  0\nE (cid:2)pk\ne\ni ((cid:126)x, (cid:126)y( (cid:126)e ))\njB pj\n(13)\ni = arg maxei\n(14)\ni  0, dk\ni  0\n(15)\nck\nEach constraint holds for all i  S . Recall that J k was\ndened in (2). Note that [4] showed that the payments induce\ndominant strategies, so (13) is an optimization that does not\ndepend on ei .\nIn general, this may be a computationally difcult problem\nfor bk to solve. For illustrative purposes, for the rest of this\npaper, we will assume specic forms for the estimators the\nbuyers employ and the  functions which dene the data\nsources. We rst assume:\nAssumption 2. For each data source i, i (ei ) is character-\nized by the the constant i > 0 and of the form:\ni (ei ) = exp(i ei )\nNote that this implies that  is convex, strictly decreasing\nand always positive, which are all desirable properties in\n\nmin\nck ,dk\ns.t.\n\n(10)\n\n(11)\n\n(12)\n\n(16)\n\n\f2\n\nour context. Furthermore, note that this is the form of the\nstandard deviation, not the variance.\nWe next determine the level of effort data sources will\nexert given the pricing parameters set by the data buyers.\nFix a data source q and taking the derivative of (1) with\n i (eq )\n(cid:88)\nrespect to eq , we obtain:\nq (eq )  1 =\nd\ndj\nq\ndeq\njB\n(cid:88)\n q exp(2q eq )  1\njB\n(cid:17)\n(cid:16)(cid:80)\n(cid:16)\n(cid:17)\nSetting this derivative equal to 0 yields:\njB dj\nq\n2q\n\ne\nq =\n\n(17)\n\ndj\nq\n\nq\n\nln\n\n2\n\n2\n\n2\n\n2\n\nq\n\n(18)\n\ne\nq =\n\n\n dj\nq\n\nq (e\n2\nq ) =\n\nhk (xi , (cid:126)x, F )2\ni\n\ngk ((cid:126)x, F , (cid:126)2 ) =\n\nThis is the optimum effort selection for data source q . We\ncan also compute how this optimal point varies with dj\ni :\n(cid:17)\n(cid:16)(cid:80)\n1\njB dj\nq\nAlso we can easily calculate the optimum variance:\n(cid:17)\n(cid:16)(cid:80)\n1\njB dj\nq\nq\nAssumption 3. (Separable estimators) For each buyer k \nB , the estimator for f that buyer k employs, f k , is separable.\n(cid:88)\nIn other words, there exists a function hk such that:\niS\nFurthermore, we assume that h  0.\nNote that\nlinear regression, polynomial regression and\nnite-kernel regression all produce separable estimators. Ap-\nplying Assumption 3 for the estimators, we may rewrite the\n(cid:88)\nloss function for buyer k as:\nJ k ((ck , dk ), (ck , dk )) =\n(cid:88)\n(cid:88)\niS\n(cid:34)\n(cid:32)\nk\nk (cid:88)\n(cid:88)\nj\niS\njk\ni  dk\ni (e\n2\nck\ni ) +\ni\niS\nli\nRecall that each xi is xed and common knowledge; thus,\nwe can replace each of the above evaluations of the h\nfunctions with constants. Dene  j\ni = hj (xi , (cid:126)x, Fj ),  j\ni,l =\nhj (xl , (cid:126)xi , xi ) for i (cid:54)= l and  j\ni,i = 1. Note that   0.\n\ni ) \ni (e\nhk (xi , (cid:126)x, Fk )2\ni (e\n(cid:35)(cid:33)\nhj (xi , (cid:126)x, Fj )2\ni ) +\nl (e\nhk (xl , (cid:126)xi , xi )2\nl )\n\n(cid:88)\n(cid:88)\niS\niS\n\n2\n\n2\n\n=\n\n+\n\n=\n\n k\ni\njB dj\ni\n\nj  j\nk\n(cid:88)\ni\nli\n\nk (cid:88)\niS\n\nThen, this becomes:\ni ) \nJ k ((ck , dk ), (ck , dk )) =\ni (e\n(cid:88)\ni 2\n k\ni (e\n(cid:32)\n(cid:34)\n(cid:35)(cid:33)\n j\nk\ni 2\ni ) +\n(cid:88)\nk (cid:88)\nj\njk\ni  dk\ni (e\nl (e\n 2\n k\n2\nck\ni,l2\n k\ni ) +\nl )\ni  (cid:88)\n(cid:88)\ni\niS\nli\ni (e\n(cid:34)\n(cid:35)(cid:33)\n(cid:32)\ni ) +\nk (cid:88)\niS\njk\ni  dk\ni (e\nl (e\n2\nck\ni,l2\n k\ni ) +\nl )\ni  (cid:80)\ni\niS\nj  j\nIn efforts towards succinctness, let  k\ni .\njk k\ni =  k\ni (e\ni ) in (18),\nWe will now plug in the expression for 2\n(cid:88)\nyielding:\ni (e\nJ k ((ck , dk ), (ck , dk )) =\n(cid:35)(cid:33)\n(cid:32)\n(cid:34)\ni 2\n k\ni ) +\nk (cid:88)\n(cid:88)\niS\ni  dk\ni (e\nl (e\ni,l2\n k\n2\nck\n(cid:88)\ni ) +\nl )\ni\n(cid:16)(cid:80)\n(cid:17)\niS\nli\n(cid:32)\n(cid:34)\niS\ni\n(cid:17)\n(cid:16)(cid:80)\ni  dk\n1\n(cid:35)(cid:33)\nck\n(cid:88)\ni\njB dj\ni\n2\n(cid:16)(cid:80)\n(cid:17)\ni\n k\ni,l\n=\n(cid:88)\njB dj\nli\n2\nl\n(cid:16)(cid:80)\n(cid:17)\nl\n k\n\n(cid:88)\nck\ni\n+\nk (cid:88)\njB dj\niS\n2\ni\ni\n(cid:16)(cid:80)\n(cid:17)\n k\ni  dk\ni,l\ni\njB dj\niS\nlS\nl\n(Note here we joyfully take advantage of our convention that\ni,i = 1.)\n k\nFinally, similar reasoning lets us write for any data source\n(cid:16)\n(cid:17)\nq ((cid:126)x, (cid:126)y)(cid:3) = ck\nE (cid:2)pk\nq and data buyer k :\nq  dk\n2\n =\nq (eq ) + gk ( xq , xq , (cid:126)2q )\n2\n(cid:88)\nq\nq  dk\nck\nhk (xi , (cid:126)xq , xi )2\ni (ei )\nq (eq ) +\n(cid:33)\n(cid:32)(cid:88)\nq\niq\nq  dk\nq ,i2\n k\nck\ni (ei )\nq\niS\n(cid:32)(cid:88)\nq ((cid:126)x, (cid:126)y)(cid:3) = ck\nE (cid:2)pk\nAt optimum effort levels, this becomes:\nq  dk\nq\niS\n\ni (e\nq ,i2\n k\ni )\n\n(cid:33)\n\nl\n\n+\n\n=\n\n=\n\n\fi\n\n(22)\n\n(21)\n\njB\n\n 0\n\n 0 (20)\n\nq  dk\nck\nq\n\nk\ni,l\n(cid:17)\n2dtotal\nl\nl\n\nlS\n(cid:16)\n2dtotal\ni\n2i\n\n(cid:88)\n\n(cid:16)(cid:80)\n(cid:17)\n k\nq ,i\njB dj\niS\n2\ni\ni\nAlso using the expression for e\ni given in (17), buyer k\n(cid:35)(cid:33)\n(cid:32)\n(cid:34)(cid:80)\nhas the following optimization problem:\n+ k (cid:80)\n(cid:80)\ni  dk\nk\n(cid:32)(cid:80)\n(cid:34)\n(cid:33)(cid:35)\nck\n(19)\ni\nmin\niS\niS\ns.t. (cid:80)\ni\n2dtotal\ni\nck ,dk\ni\nj\ni  dj\n ln\n\n(cid:32)(cid:80)\n(cid:33)\ncj\ni,l\nlS\n2dtotal\ni\nl\nl\nk\ni  dk\n= (cid:80)\ni,l\nck\nlS\ni\n2dtotal\nl\nl\njB dj\ndtotal\ni\ni\ni  0, dk\ni  0\nck\n(23)\nEvery constraint above holds for all i  S . Here, (22)\nis a denitional, rather than binding, constraint. Also, note\nthat without loss of generality, we can take k = 1, by\n(cid:19)\n(cid:18)(cid:80)\ni accordingly. Additionally, we can remove\nnormalizing the  k\ni  0, as it is redundant in light of the\nthe constraint ck\n 0, since   0 and\ni  dk\nk\nconstraint ck\ni,l\nlS\ni\n2dtotal\nd  0.\nl\nThis leads to the following result.\nTheorem 1. Consider the game where each buyers objective\ni  0 for\nis to solve the optimization in (19), and assume  j\nall i  S , j  B . Then there are either an innite number of\ngeneralized Nash equilibria, or there is no generalized Nash\nequilibrium.\nFurthermore,\nin the case where there are an innite\nnumber of generalized Nash equilibria, there is a unique\ncollection of d parameters, in the sense that if ((cid:126)c, (cid:126)d) and\n((cid:126)c(cid:48) , (cid:126)d(cid:48) ) are both generalized Nash equilibria, then (cid:126)d = (cid:126)d(cid:48) .\n(cid:33)\n(cid:32)(cid:88)\nAdditionally, the c parameters lie in the convex polytope\nln (cid:0)2dtotal\n(cid:1)\n(cid:88)\n(cid:88)\ndened by the following constraints:\n j\ni,l\n(cid:33)\n(cid:32)(cid:88)\ni\n2dtotal\n2i\njB\njB\nlS\nl\n k\ni  dk\ni,l\nck\ni\n2dtotal\nlS\nl\nThe effort exerted by each data source is the same in each\ngeneralized Nash equilibrium.\n\ncj\ni =\n\ndj\ni\n\ni\n\nl\n\nl\n\n+\n\nl\n\nBefore proving this theorem, we discuss the assumption\ni  0. This implies that, for each data buyer, the\nthat  j\npenalty for other data buyers successful estimation does\nnot outweigh the benet of having a good estimator. This\nassumption means that no data buyer will have incentive to\ndrive the variance of one data source up towards innity.\nWe prove the following useful lemma, and then prove our\ntheorem.\nLemma 1. Suppose ((cid:126)c, (cid:126)d) is a GNE for the game dened by\n(cid:33)\n(cid:32)(cid:88)\nln (cid:0)2dtotal\n(cid:1)\n (cid:88)\n(cid:88)\n(19). The following equality holds for all i and k:\n j\ni,l\ni\n2dtotal\n2i\njB\njk\nlS\nl\nIn other words, (20) is always tight in equilibrium.\n\nck\ni =\n\ndj\ni\n\ncj\ni\n\ni\n\nl\n\n+\n\nl\n\nl\n\ni\n\ncj\ni\n\n+\n\nl\n\ndj\ni\n\n= 0\n\n(cid:1)\n\nck\ni >\n\ni  dk\nck\ni\n\n k\ni,l\n2dtotal\nl\n\n(cid:88)\njB\n\n j\ni,l\n2dtotal\nl\n\nProof. To prove this, note that, by the cost function of buyer\nk , ck\ni will always be chosen such that at least one of (20)\n(cid:32)(cid:88)\n(cid:33)\nand (21) is tight. Suppose (21) is exclusively active, i.e.\n k\ni  dk\ni,l\nck\n= 0\n(cid:33)\n(cid:32)(cid:88)\nln (cid:0)2dtotal\ni\n2dtotal\nl\nlS\nl\ni\n2i\nlS\n\n (cid:88)\njk\n(24)\nNote that (24) is the same constraint for every data buyer.\nIn other words, if it is loose for k , it is loose for all other\nj . Thus, some other buyer j can reduce their cj\ni and lower\ntheir cost, and thus ((cid:126)c, (cid:126)d) cannot be an equilibrium.\nThis argument does fall apart in one situation, however.\nNo buyer can reduce their cost just by modifying c if (21)\n(cid:33)\n(cid:32)(cid:88)\nis tight for all buyers k , i.e. for all k :\nlS\nIn this case, (20), which we assumed held loosely, becomes\n(cid:18)(cid:80)\n(cid:19)\ni such that 2dtotal\ni < 1. Let buyer k increase dk\n2dtotal\ni =\ni\ni\n1, and then choose a new ck such that (21) holds tightly, i.e.\nk\n. Note that this decreases their\nck = dk\n(cid:88)\n(cid:88)\ni,l\nlS\ni\n2dtotal\nl\ncost:\niS\niS\n(This uses the fact that, since (21) holds for all buyers j , the\nsecond term disappears.) Additionally, all the constraints of\nthe original optimization are still satised, so (ck\ni ) was\ni , dk\nnot an optimizer for buyer k .\nThis concludes our proof.\nProof.\n(Theorem 1) We invoke Lemma 1 and substitute this\n(cid:32)\n(cid:16)\n(cid:17)\n(cid:88)\ninto the objective function, (19), for buyer k . Let:\n k\n(cid:126)c, (cid:126)d\ni\nJ\n+\n(cid:32)(cid:88)\n(cid:32)\n(cid:33)\n(cid:33)\nln (cid:0)2dtotal\n2dtotal\ni\niS\ni\ni\n2i\nlS\n\n cj\n+\ni\n(cid:16)\n(cid:17)\n(cid:18)(cid:80)\n(cid:126)c, (cid:126)d\nJ\n= (cid:80)\ni  dk\nk\nck\ni,l\nlS\ni\n2dtotal\nl\nl\njB dj\ndtotal\ni\ni\ni  0\ndk\n(cid:32)\nWe quickly manipulate the cost function a little to a more\ndesirable form: (cid:88)\niS\n\n(cid:88)\njk\n\n j\ni,l\n2dtotal\nl\n\n k\ni\n2dtotal\ni\n\n k\ni\n2dtotal\ni\n\nThis yields:\n\nsubject to\n\nmin\nck ,dk\n\n(cid:33)\n\n(cid:19)\n\n 0\n\n(cid:1)\n\n k\ni\n\n+\n\ni\n\n>\n\ni\n\n=\n\nl\n\ndj\ni\n\ni\n\n\f(cid:32)\ndj\ni\n\n(cid:88)\njk\n\n(cid:33)\n\n(cid:1)\n\ni\n\n=\n\n+\n\n+\n\n+\n\nl\n\ni\n\n(cid:33)\n\n j\ni,l\n2dtotal\nl\n\n(cid:33)\nln (cid:0)2dtotal\n cj\ni\n+\n(cid:33)\nln (cid:0)2dtotal\n(cid:1)\ni\n2i\ni\ni\n2i\n(cid:88)\n (cid:88)\ni  j\ndj\ni,l\ncj\ni =\n(cid:33)\nln (cid:0)2dtotal\n(cid:1)\n2dtotal\nl\niS\njk\nl\ni\ni\n2i\n(cid:88)\n (cid:88)\niS\njk\n\n(cid:32)(cid:88)\n(cid:32)\n(cid:88)\nlS\n k\ni\n2dtotal\n(cid:88)\n(cid:88)\n(cid:88)\niS\ni\n(cid:32)\n(cid:88)\niS\njk\nlS\n k\ni\n2dtotal\n(cid:88)\n(cid:88)\n(cid:88)\niS\ni\nl  j\ndj\nl,i\n(cid:32)\n2dtotal\ni\n(cid:88)\niS\nlS\njk\ni\n(cid:32)(cid:88)\niS\nlS\n\n k\ni\n(cid:33)\n2dtotal\ni\ni\n\ndj\nl  j\nl,i\n2dtotal\ni\ni\n\n cj\ni\n\ncj\ni =\n\ni\n\n+\n\n+\nln (cid:0)2dtotal\ni\n2i\n\n(cid:33)\n\n(cid:1)\n\ni\n\n+\n\n(cid:88)\njk\n\ni\n\n(cid:1)\n\ncj\ni +\n\nmin\nck ,dk\n\nsubject to\n\n k\ni\nln (cid:0)2dtotal\n(cid:1)\n+\n2dtotal\ni\ni\ni\ni\n=\nln (cid:0)2dtotal\n2i\ni\n2i\n\nNote the index swap on the  terms in the second equality.\nThen dene:\ni , ck , dk ) =\n(cid:32)(cid:88)\n(cid:33)\ni (dk\nJ k\n(cid:88)\ndj\nl  j\n cj\nl,i\n+\ni + (cid:80)\n(cid:80)\ni\n2dtotal\ni\n (cid:88)\nlS\njk\ni\nlS dj\nl  j\n k\njk\nl,i\n2dtotal\ni\njk\ni\n(cid:80)\nThus, the overall optimization can again be re-written:\n(cid:19)\n(cid:18)(cid:80)\ni , ck , dk )\niS J k\ni (dk\n= (cid:80)\n 0\ni  dk\nk\nck\ni,l\nlS\ni\n2dtotal\nl\nl\njB dj\ndtotal\ni\ni\ni  0\ndk\n(cid:88)\nWe differentiate the cost with respect to dk\nq :\ni , ck , dk ) =\nq , ck , dk ) =\nJ k\ni (dk\nq (dk\nJ k\nq + (cid:80)\n(cid:80)\niS\nlS dj\nl  j\n  k\njk\nl,q\nq  (cid:80)\n(cid:80)\n2(dtotal\n)2q\nq\n k\nlS dj\nl  j\nl,q + dtotal\njk\nq\nq  (cid:80)\n(cid:80)\n2(dtotal\n)2q\nq\n k\nl  j\nlq dj\nl,q + dk\njk\nq\n)2q\n2(dtotal\nq\n\n1\n2dtotal\nq\n\n\n dk\nq\n\n\n dk\nq\n\nq\n\n=\n\n=\n\n+\n\nk )\n\nk , d\n\n\n dk\nq\n\nq (dk\nJ k\nq , c\n\nq =  k\ndk\nq +\n\nNote that we use the fact that  j\nq ,q = 1 for all j . It is easy\n< 0 if 0  dk\nq + (cid:80)\n(cid:80)\nto see that:\nq + (cid:80)\n(cid:80)\nlq dj\nl j\nq <  k\nq + (cid:80)\n(cid:80)\njk\nl,q\nlq dj\nl j\n= 0 if dk\nq =  k\njk\nl,q\nl j\nlq dj\n> 0 if dk\nq >  k\njk\nl,q\n(cid:88)\n(cid:88)\nThus, the maximizing dk\nq is given by:\nl  j\ndj\nl,q\njk\nlq\nPerforming this analysis for all combinations of q  S and\nk  B yields a system of M  N equations with M  N\nunknowns, of the form (25).\nAs we have before, let (cid:126)d denote a column vector with\ni for each i  S and j  B . Similarly, let (cid:126) denote\nentries dj\na column vector containing all the terms of the form  j\ni .\nThen, we may represent this system of equations with the\nfollowing matrix equation:\n(cid:126)d = A (cid:126)d + (cid:126)\n\n(25)\n\n(26)\n\nHere, A is a non-negative matrix whose entries are the values\nof the various  parameters at the appropriate places, such\nthat (26) expresses the set of equality constraints dened by\n(25) for all q  S and k  B . To nd an GNE of this game,\ni  0 for all\nit sufces to nd a solution to (26) such that dj\ni and j .\nSystems of equations of this form are well studied in the\neconomics literature, as they are of the form specied by the\ncelebrated Leontief input-output model. It has been shown\nthat such systems of equations have a non-negative solution\nif and only if (A) < 1, where (A) is the spectral radius of\nA [8]. Moreover, if such a solution exists, it must be unique.\nThus, if (A) < 1, inversion of this A matrix yields the\n(cid:33)\n(cid:32)(cid:88)\nequilibrium (cid:126)d, and, by Lemma 1, we can pick any (cid:126)c that\nln (cid:0)2dtotal\n(cid:1)\nsatises:(cid:88)\n(cid:88)\n j\ni,l\ni\n(cid:33)\n(cid:32)(cid:88)\n+\n2dtotal\n2i\njB\njB\nlS\nl\n k\ni  dk\ni,l\nck\ni\n2dtotal\nl\nlS\nl\nIf (A)  1, there will not exist a non-negative solution\nand there is no point ((cid:126)c, (cid:126)d) that simultaneously optimizes\n(19) for all k . It follows that there is either a unique (cid:126)d that\nwill constitute a Nash solution for the game, which produces\na convex polytope of potential GNE, or there this no solution\nto the game, as desired.\n\ncj\ni =\n\ndj\ni\n\ni\n\nl\n\nIt is interesting to note that the existence of GNE depends\nsolely on the value of the  parameters; it does not depend\non the magnitude of the  parameters. This implies that the\nexistence or non-existence of GNE in this game is simply\nan artifact of the incentive mechanisms we have chosen to\nanalyze, and does not depend on whether or not there are\nsolutions that are benecial to all parties involved. Note\nthat we chose this incentive mechanism based on several\ndesirable properties in the single-buyer case; whether or not\nthere exist mechanisms that extend to multi-buyer games in\n\n\fa fashion that provides good efciency properties is an open\nproblem that we are currently investigating. In Section IV,\nwe calculate the  parameters for a specic example, and see\nhow equilibrium solutions in these marketplaces collapse as\nthe characteristics are varied.\nAdditionally, note that,\nin the case where there is a\ncontinuum of GNE, the effort exerted by data sources and (cid:126)d\nparameters are the same across all equilibria. The ambiguity\narises in the (cid:126)c parameters. In other words, the ambiguity\narises in determining which data buyers will pay to ensure\nthat each data sources total compensation covers the cost\nof their effort. In the extreme case, it is possible for one\non expectation. That is, for some k  B , (cid:80)\nrm to pay for the entirety of the expected compensation\n(cid:80)\ni , and for all j (cid:54)= k , (cid:80)\noffered to the data sources, while the the rms pay nothing\niS pk\ni ((cid:126)x, (cid:126)y) =\niS e\niS pj\ni ((cid:126)x, (cid:126)y) = 0. In Section V\nwe discuss possible mechanisms to alleviate the disparity that\nmay arise in these situations.\nWe next turn to analyzing the total utility experienced in\nthe marketplace for a given outcome of the game. We begin\nwith the following denition.\nDenition 2. (Ex-ante social loss of the data market) Sup-\npose that  j = 1 for all buyers j . Let (cid:126)e be the vector denoting\nthe level of effort the data sources exert. Then, we dene the\nex-ante social loss the marketplace to be the sum of the utility\n(cid:34) (cid:16) f j\n(cid:32)\n(cid:17)2 \n(cid:88)\nfunctions of all the data buyers and data sources:\n(x )  f (x )\nL((cid:126)e) =\nE\n(cid:17)2 (cid:35)(cid:33)\n(cid:16) f k\n(cid:126)x, (cid:126)yj\n(cid:88)\n(cid:88)\njB\n(cid:126)x, (cid:126)yk (x )  f (x )\niS\nkj\nNote that this sum does not include any of the payments\nmade in the marketplace, as they are simply lossless transfers\nof wealth. We require the additionally assumption that  j = 1\nfor all buyers j to ensure that these transfers of wealth are\nlossless from a utility perspective, i.e. the buyers and sources\nvalue the payment equally. This assumption allows us to\nisolate the social loss due to the mechanism, and ignore any\nlosses due to differential preferences in payment currency.\nTheorem 2. Suppose that Assumptions 2 and 3 hold. Fur-\ni > 0, for all i  S and j  B , and that\nther, assume that  j\ni,l > 0 for some i, l  S , j  B . Finally, suppose GNE\n j\nsolutions exist for the game, and let (cid:126)e denote the unique\nexists (cid:126)e  RN such that L (cid:16)(cid:126)e\n(cid:17)\n< L (cid:0) (cid:126)e (cid:1). Furthermore, the\nlevel of effort exerted by the data sources across each of\nthese GNE solutions, as stipulated by Theorem 1. Then, there\nsocially optimal levels of effort, (cid:126)e, are always less than the\ninduced levels of effort at equilibrium (cid:126)e .\nwhich minimizes the value of L (cid:16)(cid:126)e\n(cid:17)\nProof. We begin by calculating solving for the value of (cid:126)e\n. Invoking Assumption 3\nL (cid:16)(cid:126)e\n(cid:17)\n(cid:88)\n(cid:88)\n(cid:88)\nand our denition of  , we may write:\n j\ni 2\ni ( ei ) +\niS\njB\niS\n\n(27)\n\n j\nk\n\nei\n\n=\n\n+\n\nei\n\n j\nq\n\n2q\n\nTaking the derivative with respect to eq and repeating\nour analysis with Assumption 2, and setting the resulting\n(cid:88)\n exp(2q eq ) + 1 = 0\nequation to zero we obtain:\njB\n(cid:80)\nWe can re-arrange this to yield:\njB  j\nq )\nln(2q\neq =\n2q\nproduce the unique minimizer of L (cid:16)(cid:126)e\n(cid:17)\nNote, that L is strictly convex with respect to eq , so\nchoosing the entries of (cid:126)e according to equation (28) must\n.\nNext we compare this to the level of effort the sources\nproduce in the GNE of the game between the buyers. By\n(cid:88)\n(cid:88)\n(25), we obtain that in the GNE for all i  S and k  B :\nl,i   k\nl  j\ndj\ni > 0\njk\nli\n\ni =  k\ndk\ni +\n\n(28)\n\ndj\nq >\n\nFurthermore, since there exists at least one  > 0, we know\nq for some data source q and buyer k . It follows\nthat dk\nq >  k\nthat, for this particular q :(cid:88)\n(cid:88)\njB\njB\n\n j\nq\n(cid:80)\n(cid:80)\nThus, by (17), we see that\njB  j\njB dj\nq )\nln(2q\nq )\nln(2q\ne\n(29)\n= eq .\n>\nq =\n2q\n2q\nL (cid:16)(cid:126)e\n(cid:17)\n< L (cid:0) (cid:126)e (cid:1) since we chose (cid:126)e to be the unique minimizer\nThus the theorem is proved, since it must be the case that\nof L.\n\nTheorem 2 shows that there is always some social loss, ex-\nante, from a Nash solution compared to the social optimum.\nFurthermore, the proof provides a way to identify where\nthis loss is incurred, and how to calculate how much is\nlost. Note that the social welfare is always lost because\nthe effort induced in equilibrium is higher than is socially\noptimal. This captures the intuition that each data buyer has\na negative externality: they wish to improve their estimates\nwithout considering how their improved estimates hurt other\ndata buyers.\nThe proof itself also provides strong intuition on the\n parameters between buyers. Loosely speaking,\nthese \nparameters can be thought of as a measure of each buyers\nmarket power, in the sense that it quanties how much one\nbuyer can inuence the payment contracts of other buyers\nin the data market to his advantage. As an extremal case,\ni,l = 0 for all i, l  S and j  B , there is no\nwhen  j\ncoupling between the payments the buyers make, and the\nsocial optimum coincides with the Nash solution.\n\n\fIV. EXAM PL E : B E TW E EN TWO FIRM S\nIn this section, we present an example which demonstrates\nhow a data market may collapse as the parameters of the\nsystem are varied. This example will also demonstrate how\nthe efciency of the data market, in terms of the ex-ante\nsocial loss function L, changes as the market approaches\nthis collapse. In particular, we consider the case where there\nare two data sources (s1 and s2 ) and two rms acting as data\nbuyers (b1 and b2 ). Each of the data sources is capable of\nestimating the function\nf : [1, 1]  R.\n(30)\nLet x1 , x2  [1, 1] denote the locations where s1 and s2\nsample f , respectively. Assume that each of the data sources\nare as dened in Assumption 2, with the characteristic\nparameters 1 = 2 = 1.\nNext, we assume that each of the data buyers is performing\n(cid:34) (cid:20)x\nlinear regression on f , using the samples reported by the data\n(cid:21)T\nsource. In this case [4]:\n1\n\ngj ((cid:126)x, Fj , 2 ((cid:126)e)) = ExFj\n\n2 (e2 ))  X (X T X )1\n1 (e1 ), 2\ndiag(2\n\n(X T X )1X T \n(cid:21) (cid:35)\n(cid:20)x\n1\n\n=\n\ni =  2\n 1\ni =\n\n2,1 = g(x1 , x2 , 2\n2,1 =  2\n 1\n1 (e1 )) =\n\n1,2 = g(x2 , x1 , 2\n1,2 =  2\n 1\n2 (e2 )) =\n\n1 (e1 ) +  j\n j\n2 2\n1 2\n2 (e2 )\nIn this example, we assume F1 = F2 as the uniform\ndistribution on the domain of f , [1, 1]. Thus, for i  {1, 2}:\n(x1  x2 )2/3 + (x2\ni  x1x2 )2\n2  2x1x2 )2\n(x2\n1 + x2\nNote that, by these assumptions, g1 = g2 , and furthermore:\n(x1x2 + 1)2\n(x2\n2 + 1)2\n(x1x2 + 1)2\n(x2\n1 + 1)2\nFor illustrative purposes, we x x2 = 1, and see what\nhappens to the data market as we vary x1 along the interval\n[1, 1].\nNote that, when x1 = x2 = 1, it is no longer possible to\nconstruct a linear estimator of f because there is insufcient\ndata. Thus, the example shows how the game between buyers\nbehaves as it becomes increasingly difcult to construct good\nestimators. The (cid:126)d parameters of any Nash solution can be\nd1\n\n\n 1\n =\n 1\nfound by solving:\n1\n1\nd2\n 2\n0\n1\n1\n(cid:124)\n(cid:123)(cid:122)\n(cid:125)\nd1\n 1\n0\n 1\n2\n2\nd2\n 2\n2\n2\n1,2\nB\nNote that this B matrix is equal to I  A as dened in the\nproof. We numerically solve this system of equations for\nvarying values of x1  [1, 1], and the results are shown in\nFigures 1 through 4.\n\n 2\n2,1\n0\n0\n1\n\n0\n1\n 2\n1,2\n0\n\n0\n 1\n2,1\n1\n0\n\n(31)\n\nFigures 1 and 2, demonstrate how the  and  parameters\nof the game change as a function of x1 . Figure 3 demon-\nstrates the d parameters that the buyers will offer the data\nsources as x1 varies. And nally, Figure demonstrates the\nL (cid:0) (cid:126)e (cid:1)\nprice of anarchy in the data market, as a function of x1\nL (cid:16)(cid:126)e\n(cid:17)\nwhich is given by:\nHere, (cid:126)e is the induced effort of the sensors in the Nash\nsolution of the game between data buyers, and (cid:126)e is the\nsocially optimal effort for data sources to exert. Further\ncomments in the captions of Figures 3 and 4 demonstrate\nthe inefciencies that arise in this example.\n\nFig. 1.\nThis gure depicts how the various  parameters of the system\nvary as a function of x1 . Note that as x1  1,  diverges to innity, which\nreects the fact that as x1 and x2 become increasingly close it becomes\nmore difcult to generate a linear estimator from samples at these data\npoints.\n\nFig. 2. This gure depicts how the various  parameters vary as a function\nof x1 . As all of the  parameters converge to a value of 1 as x1  1, the\nmatrix B in equation (31) becomes singular, causing the breakdown of\nsolutions for the d parameters, as is depicted in Figure 3.\n\nV. C LO S ING REMARK S\nWeve analyzed the game that forms between a set of\ndata buyers when they wish to communally incentivize a\ncollection of strategic data sources, using a mechanism\nthat has been proposed in the literature. We derived, for a\nparticular form of the game, conditions for the existence of\nGNE, and demonstrated that these solutions are frequently\nsocially inefcient. This motivates future work to develop a\n\n1.00.50.00.51.0x110-1100101102103104105ji Parameters vs. x111,2112,221.00.50.00.51.0x10.00.20.40.60.81.01.21.41.6ji,l parameters vs. x111,2,21,212,1,22,1\f[6] F. Farokhi, I. Shames, and M. Cantoni, Budget-constrained contract\ndesign for effort-averse sensors in averaging based estimation, arXiv\npreprint arXiv:1509.08193, 2015.\n[7] D. Dorsch, H. T. Jongen, and V. Shikhman, On structure and compu-\ntation of generalized nash equilibria, SIAM J. Optimization, vol. 23,\nno. 1, pp. 452474, 2013.\n[8] S. Sta nczak, M. Wiczanowski, and H. Boche, Chapter 2: On the Positive\nSolution to a Linear System with Nonnegative Coefcients. Berlin,\nHeidelberg: Springer Berlin Heidelberg, 2006, pp. 5168.\n\nThis gure depicts the Nash equilibrium (cid:126)d parameters for the\nFig. 3.\ngame between the buyers as a function of x1 . Note that, as x1  1,\nthe (cid:126)d parameters go off to innity, and the Nash equilibria between the\nbuyers breaks down. Comparing these results to Figure 1, we see that the\n(cid:126)d parameters diverge much more quickly than the  parameters, meaning\nthat in the Nash equilibria to the game between the two buyers becomes\nincreasingly inefcient as x1  1.\n\nFig. 4.\nThis gure depicts the price of anarchy for the marketplace as\na function of x1 . When x1 = 1, the payments in the marketplace are\ndecoupled and the  parameters are all zero; in this instance the price of\nanarchy is 1, and the market is perfectly efcient. However, as x1  1,\nthe price of anarchy diverges asymptotically to innity, and the marketplace\nbecomes increasingly inefcient as it becomes more difcult for the buyers\nto construct the estimators they desire.\n\nricher class of incentive mechanisms which alleviate these\nissues. Possible solutions include more complex pricing\nmechanisms, or perhaps the addition of a trusted third party\nmarket-maker to mediate socially benecial transactions in\nthese data markets.\n\nR E FER ENC E S\n\n[1] D. E. Bakken, R. Rarameswaran, D. M. Blough, A. A. Franz, and T. J.\nPalmer, Data obfuscation: anonymity and desensitization of usable data\nsets, IEEE Security Privacy, vol. 2, no. 6, pp. 3441, Nov 2004.\n[2] C. Dwork and A. Roth, The Algorithmic Foundations of Differential\nPrivacy.\nFoundations and Trends in Theoretical Computer Science,\n2014.\n[3] H. Gao, C. H. Liu, W. Wang, J. Zhao, Z. Song, X. Su, J. Crowcroft,\nand K. K. Leung, A survey of incentive mechanisms for participatory\nsensing, IEEE Communications Surveys & Tutorials, vol. 17, no. 2,\npp. 918943, 2015.\n[4] Y. Cai, C. Daskalakis, and C. Papdimitriou, Optimum statistical\nestimation with strategic data sources, in JMLR: Workship and Conf.\nProc., vol. 40, 2015, pp. 117.\n[5] D. G. Dobakhshari, N. Li, and V. Gupta, An incentive-based approach\nto distributed estimation with strategic sensors, in Decision and Control\n(CDC), 2016 IEEE 55th Conference on.\nIEEE, 2016, pp. 61416146.\n\n1.00.50.00.51.0x110-110110310510710910111013101510171019102110231025djid parameters vs. x1d11,d21d12,d221.00.50.00.51.0x105101520253035L(~e)L(~e)Price of Anarchy vs. x1\f", 
        "tag": "Computer Science and Game Theory", 
        "link": "https://arxiv.org/list/cs.GT/new"
    }, 
    {
        "text": "Generate To Adapt: Aligning Domains using Generative Adversarial Networks\n\nSwami Sankaranarayanan \n\nYogesh Balaji \nRama Chellappa\nUMIACS, University of Maryland, College Park\n\nCarlos D. Castillo\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nV\nC\n.\ns\nc\n[\n \n \n1\nv\n5\n0\n7\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nVisual Domain adaptation is an actively researched\nproblem in Computer Vision.\nIn this work, we propose\nan approach that leverages unsupervised data to bring the\nsource and target distributions closer in a learned joint fea-\nture space. We accomplish this by inducing a symbiotic re-\nlationship between the learned embedding and a generative\nadversarial framework. This is in contrast to methods which\nuse an adversarial framework for realistic data generation\nand retraining deep models with such data. We show the\nstrength and generality of our method by performing ex-\nperiments on three different tasks: (1) Digit classication\n(MNIST, SVHN and USPS datasets) (2) Object recognition\nusing OFFICE dataset and (3) Face recognition using the\nCelebrity Frontal Prole (CFP) dataset.\n\n1. Introduction\nThe development of powerful learning algorithms such\nas Convolutional Neural Networks (CNNs) have provided\na classic pipeline for solving many classication problems\n[28]. The abundance of labeled data has resulted in re-\nmarkable improvements for tasks such as the Imagenet chal-\nlenge: beginning with the CNN framework of Krizhevsky et\nal [13] and more recently Resnet [9] and its variants. An-\nother example is the steady improvements in performance\non the LFW dataset [26]. The common theme across all\nthese approaches is the dependence on lot of labeled data.\nWhile labeled data is available and getting labeled data has\nbeen easier over the years, the lack of uniformity of label\ndistributions across different domains results in suboptimal\nperformance of even the powerful CNN-based algorithms\non realistic unseen test data. This is abundantly clear in the\nexample of faces where most available labeled data tends to\nbe frontal. Hence the learning algorithm, in general, does\nnot perform equally well across viewpoints. The use of un-\nlabeled target data to mitigate the shift between source and\ntarget distributions is the most promising direction domain\n\n First two authors contributed equally\n\nadaptation. Hence this paper focuses on the topic of unsu-\npervised domain adaptation.\n\nFigure 1. The top half provided an illustration of our image gen-\neration pipeline and a visual demonstration of how the proposed\napproach is able to reconstruct source and target images. The tar-\nget labels were not used during training. In the bottom half, we\nshow a t-SNE visualization of the unadapted and adapted encoder\nrepresentations demonstrating how our approach is able to adapt\nthe source and target distributions. Blue dots represent source data\nand Red dots represent the target data.\n\nIn this work, we propose a way to learn an embedding\nthat is robust to the shift between source and target distri-\nbutions. We intend to achieve this by using unsupervised\ndata sampled from the target distribution to guide the su-\npervised learning procedure that uses data sampled from\nthe source distribution. The main contribution of this work\nis that we propose an adversarial image generation frame-\nwork to directly learn the shared feature embedding using\nlabeled data from source and unlabeled data from the tar-\nget.\nIt should be noted that while several methods have\n\n1\n\nRandom noiseEmbeddingSource: MNISTTarget: USPSOriginal ImagesReconstructed ImagesImage generation pipelineAdaptedLearning to adapt between source and targetNon-adapted\fused an adversarial framework for solving the domain adap-\ntation problem, the novelty of the proposed approach is in\nusing a joint generative discriminative method where the\ngeneration is performed using a variant of Generative Ad-\nversarial Network (GAN) [7]. During training, the source\nimages are passed through the encoder to obtain an em-\nbedding which is then used by the classier for predict-\ning the source label and also used by the generator to gen-\nerate a realistic source image. The realistic nature of the\nimages from the generator is controlled by the discrimina-\ntor. The embedding is updated based on the discrimina-\ntive gradients from the classier and generative gradients\nfrom the adversarial framework. Given unlabeled target im-\nages, the embedding is updated using only gradients from\nthe adversarial part, since the labels are unavailable. Thus,\nthe embedding learns to discriminate better even in the tar-\nget domain using the knowledge imparted by the generator-\ndiscriminator pair. We would like to point out that although\nthe proposed approach uses an image generation procedure\nfor learning the domain shift, the quality of the image re-\nconstruction is not our focus. By using the discriminator\nas a multi-class classier, we ensure that the gradient sig-\nnals backpropagated by the discriminator for the unlabeled\ntarget images belong to the feature space of the respective\nclasses. By sampling from the distribution of the generator\nafter training, we show that the network has indeed learnt\nto bring the source and target distributions closer. The bot-\ntom half of gure 1 shows a t-SNE [19] visualization of the\nembeddings for the MNISTUSPS setting for two cases:\n(1) Non-adapted: Encoder trained with images from source\nonly (2) Adapted: Encoder trained with the proposed ap-\nproach. It can be observed that the proposed approach re-\nsults in a closer match between the source and target distri-\nbutions. We show examples of such reconstructions in Sec-\ntion 4. To summarize, the major contribution of this work\nis to provide an adversarial image generation framework for\nunsupervised domain adaptation that directly learns a joint\nfeature space in which the distance between source and tar-\nget distributions is minimized. Our experiments show that\nthe proposed framework yields superior results compared to\nsimilar approaches which update the embedding based on\nauto-encoders [4] or disentangling the domain information\nfrom the embedding by learning a separate domain classi-\ner [3]. This paper is organized as follows: Section 2 de-\nscribes contemporary approaches for the unsupervised do-\nmain adaptation problem and places our work among them.\nIn section 3, we describe in detail the formulation of our ap-\nproach and the iterative training procedure. In section 4, we\ndescribe the experimental setups and discuss the results us-\ning both quantitative and qualitative experiments. We con-\nclude this paper in section 5.\n\n2. Related Work\nDomain adaptation is an actively researched topic in\nmany areas including Machine Learning, Natural Language\nProcessing and Computer Vision. In this section, we focus\non visual domain adaptation since it is more relevant to our\nwork. Earlier approaches to domain adaptation focussed on\nbuilding feature representations that are invariant across do-\nmains. This was accomplished either by feature reweighting\nand selection mechanisms[10] [2], or by learning an explicit\nfeature transformation that aligns source distribution to the\ntarget[8] [22] [6].\nRecently, Deep Neural Networks have been shown to be\nsuccessful in learning complex feature representations that\nenable them to achieve state-of-the-art performance in most\nmachine learning tasks [13] [9]. This ability to learn power-\nful representations has been harnessed to perform unsuper-\nvised domain adaptation in [3][31][16] [18][30]. The un-\nderlying idea behind such methods is to minimize a suitable\nloss function that captures domain discrepancy, in addition\nto the task being solved.\nDeep learning methods for visual domain adaptation can\nbe broadly grouped into few major categories. One line of\nwork uses Maximum Mean Discrepancy(MMD) as a metric\nto measure the shift across domains. Deep Domain Con-\nfusion (DDC) [31] jointly minimizes the classication loss\nand MMD loss of the last fully connected layer. Deep Adap-\ntation Networks (DAN) [16] extends this idea by embed-\nding all task specic layers in a reproducing kernel Hilbert\nspace and minimizing the MMD in the projected space. In\naddition to MMD, Residual Transfer Networks (RTN) [18]\nuses a gated residual layer for classier adaptation.\nAnother class of methods uses adversarial losses to per-\nform domain adaptation. Revgrad [3] poses the domain\nadaptation problem as a minimax game between a domain\nclassier and a feature extractor. The goal of the feature\nextractor is to produce embeddings that fool the domain\nclassier, while at the same time minimize the classica-\ntion loss. Adversarial Discriminative Domain Adaptation\n(ADDA) [30] on the other hand learns separate feature ex-\ntraction networks for source and target domains, the target\nCNN is trained so that a domain classier cannot distinguish\nthe embeddings produced by the source or target CNN.\nAdversarial networks have been successfully applied for\nimage generation tasks where the generator network G\nand the discriminator network D compete in a 2-player\ngame [7]. G models the data distribution while D dis-\ntinguishes the distribution produced by G from the true\ndata distribution. Following the success of GAN, several\nmethods have tried to use GAN based approaches for un-\nsupervised domain adaptation. Taigman et al. [29] train a\ncross-domain generative model that maps samples from the\nsource domain to the target domain without utilizing any\nsource-target correspondence. Domain adaptation is then\n\n\fperformed by learning a classier on the transferred images.\nCoupled GAN (CoGAN) [15] on the other hand trains a\ncoupled generative model that learns the joint data distribu-\ntion across the two domains. A domain invariant classier\nis learnt by sharing weights with the discriminator of the\nCoGAN network.\nUnlike the methods discussed above, we use image gen-\neration as a sub-task for domain adaptation. Related works\nthat use a similar approach are Deep Reconstruction Classi-\ncation Networks(DRCN) [4] and Domain Separation Net-\nworks(DSN) [1]. DRCN uses feature embedding to mul-\ntitask source label prediction and target image reconstruc-\ntion. DSN explicitly models the private and shared compo-\nnents of source and target feature representations, and learns\nsuch representations using a combination of feature similar-\nity loss and image reconstruction loss. Unlike these meth-\nods, we enforce the domain alignment constraint strongly\nby training a generative model for the source data, and forc-\ning the encoder to produce the embeddings for the target\ndata, which when fed to the generative model produces\ngood source-like images.\n\n3. Method\nIn this section, we provide a formal treatment of the pro-\nposed approach and explain in detail our iterative optimiza-\ntion procedure. Let X = {xi}N\ni=1 be an input space of im-\nages and Y = {yi }N\ni=1 be the label space. We assume there\nexists a source distribution, S (x, y) and target distribution\nT (x, y) over the samples in X.\nIn unsupervised domain\nadaptation, we have access to the source distribution using\nlabeled data from X and the target distribution via unlabeled\ndictor that is optimal in the joint distribution space S (cid:78) T\ndata sampled from X. Operationally, the problem of unsu-\npervised domain adaptation can be stated as learning a pre-\nby using labeled source data and unlabeled target data sam-\npled from X. We consider problems where the data from X\nhave discrete labels from the set L = {1, 2, 3, ...Nc }, where\nNc is the total number of classes. Our objective is to learn\nan embedding map F : X (cid:55) Rd that is used by a prediction\nfunction C : Rd (cid:55) L. The predictor has access only to the\nlabels for the data sampled from the source distribution and\nnot from the target distribution. By extracting information\nfrom the target data during training, F implicitly learns the\ndomain shift between S and T . In the rest of this section,\nwe use the terms source (target) distribution and source (tar-\nget) domain interchangeably.\nAs explained in section 2, several approaches including\nlearning entropy-based metrics, learning a domain classier\nbased on a embedding network or denoising autoencoders\nhave been used to transfer information between the source\nand target distributions. In this work, we use a GAN to help\nthe embedding bridge the gap between source and target\ndomains, since this enables a rich information transfer by\n\nIn\nusing both a generative and a discriminative process.\na traditional GAN, two competing mappings are learned:\nthe discriminator D and the generator G, both of which are\nmodeled as deep neural networks. G and D play a minimax\ngame where D tries to classify the generated samples as\nfake and G tries to fool D by producing examples that are\nas realistic as possible. More formally, to train a GAN, the\nfollowing optimization problem is solved in an alternative\nmanner:\n\n(1)\n\nmin\nG\n\nmax\nD\n\nExpdata (log(D(x))\n+ Ezpnoise log(1  D  G(z ))\nAs an extension to traditional GANs, conditional GANs\n[20] enable conditioning the generator and discriminator\nmappings on additional data such as a class label or an em-\nbedding. They have been shown to generate images of digits\nand faces conditioned on the class label or the embedding\nrespectively [29]. Training a conditional GAN involves op-\ntimizing the following minimax objective:\n\n(2)\n\nmin\nG\n\nmax\nD\n\nExpdata (log(D(x))\n+ E{zpnoise ,y} log(1  D  G(y , z ))\nIn this work, we employ a conditional GAN by condi-\ntioning the generator using the embedding. Specically,\nthe input to the generator is a concatenated version of the\nembedding and a random noise vector :\n[F (x), z ] where\nz  N (0, 1); F is the encoder mapping and [...] repre-\nsents vector concatenation. The dimensionality of z is a\nhyperparameter of our method; however for all our exper-\niments we have achieved consistent performance by set-\nting it equal to the dimensionality of F . The intuition be-\nhind using a random noise vector is to give the generator\nsome extra degrees of freedom to model external variations\nthat are absent in the source data. The discriminator map-\nping D is a (Nc+1)-way classier taking labels from the set\nLD = {L  {Nc + 1}}, with Nc real classes and an extra\nclass being the fake class. We denote the set {1, 2, . . . , Nc }\nas real labels and Nc + 1 as the fake label. The inputs to\nD can be real images from the source domain or generated\nfake images from the source or target domains. To jointly\nlearn the embedding and the generator-discriminator pair,\nwe employ an alternating optimization procedure:\n\n1. Given labeled source images as input, D classies the\nreal images into one of the real classes and classies\nthe generated fake images into the fake class.\n\n2. Using the gradients from D , G is updated to produce\nrealistic class consistent source images.\n\n3. F and C are updated based on the source images and\nreal labels in a traditional supervised manner.\n\n\fFigure 2. The data ow during forward pass (solid lines) and gra-\ndients during backward pass (dashed lines) are shown. A dashed\nboundary for a block implies that block is being updated and solid\nboundary implies it is held xed.\n\n4. In the nal step, given the target images, we update\nF by minimizing the probability of D to classify the\ngenerated target images as fake.\n\nFor ease of implementation, steps 2 and 3 are combined\ninto a single update. Figure 2 shows the direction of data\nand gradient ow through our setup in each optimization\nstep.\n\n(3)\n\nlog(1  D  G([F (xi ), zi ]))\n\nUse of target data: The main strength of our approach is\nhow the target images are used to update the embedding.\nGiven a batch of target images [xi ]N\ni=1 as input, we update\nthe embedding F by reversing the gradients of the following\nN(cid:88)\nloss function:\ni=1\nwhere zi a random noise vector sampled from N (0, 1).\nThe loss in (3) encourages D to classify the target images as\nfake. Our objective during the target update step is that: For\nthe target domain, the embedding should be learnt so that\nG, conditioned on the embedding, produces source-like im-\nages that fool D .\nIf this is achieved optimally, then one\ncan infer that the embedding has fully learnt to map the tar-\nget distribution to the source distribution. To enable this\nbehavior, the loss function in (3) is used to update the em-\nbedding by reversing the gradient of the discriminator cor-\nresponding to the fake label. This update will enable F to\npreserve the class information for the target domain due to\nthe following reason: During the source update step, both\nF and D are learned in a class consistent manner using la-\nbels from the source domain. As training progresses, the\nreversed gradients that are used to update the embedding\nbecome well conditioned on the actual class of the target\nimage even though target label information is never pro-\nvided. This symbiotic relationship between the embedding\n\nFigure 3. Visualization of digit datasets\n\nand the adversarial framework contributes to the success of\nthe proposed approach. It should be noted that the gradient\nsignals from (3) are used to update F only and reversal of\nthe gradient is performed as shown in gure 2.\nOur iterative optimization procedure can be summarized\nas follows:\n For source images, we update the mappings D , G, C and\nF using the gradient of the loss function:\n\nLsrc = Ladv + Lcls where,\nExS log(D(x) +\nLadv = min\nmin\nmax\nF\nD\nG\nE{zpnoise ,xS } log(1  D  G([F (x), z ]))\nExS log(C  F (x)\nmin\nLcls = min\nF\nC\n\n(8)\n is the coefcient that trades off between the classica-\ntion loss and the adversarial loss.\n For the target images, the loss function involves updating\nonly the embedding F and does not involve the classier,\nsince no target labels are observed during training.\nExT ,zpnoise  log(1  D  G([F (x), z ]))\n(9)\n\nLtgt = max\nF\n\nThe pseudocode for this iterative procedure is given in\nAlgorithm 1. We nd that our approach is not overly sen-\nsitive to the cost coefcient . However, the value of the\nparameter is dependent on the application and size of the\ndataset. Such specications are mentioned in section 4.\n\n4. Experiments and Results\nThis section reports the experimental validation of our\nmethod. To demonstrate the versatility of our approach,\nwe perform experiments on three different tasks that span\nacross multiple domains - digit recognition, object recog-\nnition and face recognition.\nIn the process, we also test\nthe sensitivity of our approach to the size of dataset. Each\ndataset we experimented on, varies greatly in size, with\nsome containing just a few hundred images to others hav-\ning tens of thousands of images.\n\n    G   D   FSource Input    G   D   FSource InputStep 1: Update DStep 2: Update G, F, C      G   D   FTarget InputStep 3:  Update F C C CReverse gradient\f7:\n\n8:\n\nmax\nD\n\nAlgorithm 1 Iterative training procedure of our approach\n1: training iterations = N\n2: for t in 1:N do\nSample k images from source domain S : {si , yi }k\n3:\ni=1\nLet fi = F (si ) be the embeddings computed for the source images.\n4:\ni=1  N (0, 1),where dim(zi ) = dim(F (si ))\nSample k random noise samples {zi }k\n5:\nUpdate discriminator to classify real/fake samples using the adversarial loss.\nk(cid:88)\n6:\n(cid:2) log(D(si )) + log(1  D  G([fi : zi ]))(cid:3)\ni=1\nUpdate the generator through the discriminator gradients computed using real labels.\nk(cid:88)\n(cid:2) log(1  D  G([fi : zi ]))(cid:3)\n1\nmin\nk\nG\ni=1\nUpdate the embedding using a linear combination of the adversarial loss and classication loss.\nk(cid:88)\n(cid:2) log(C (fi )) +  log(1  D  G([fi : zi ]))(cid:3)\n1\nmin\nk\nF\ni=1\nSample k images from target domain T : {ti }k\ni=1\nLet fi = F (ti ) be the embeddings computed for the target images.\nSample k random noise samples {zi}k\ni=1  N (0, 1), where dim(zi ) = dim(F (ti ))\nUpdate the embedding by minimizing the likelihood (or maximizing the negative log likelihood) of the target images\nbeing classied as fake by the discriminator.\nk(cid:88)\n(cid:2)  log(1  D  G([fi : zi ]))(cid:3)\ni=1\n\n9:\n10:\n11:\n12:\n\nmax\nF\n\n1\nk\n\n(7)\n\n1\nk\n\n(4)\n\n(5)\n\n(6)\n\n13: end for\nTable 1. Accuracy (mean  std%) values for cross-domain recognition tasks over ve independent runs on the digits based datasets. The\nbest numbers are indicated in bold and the second best are underlined.  denotes unreported results. MN: MNIST, US: USPS, SV: SVHN\nSV  MN MN  SV\nUS  MN\nMN  US\nMethod\n26.0  1.2\n60.3  1.5\n75.2  1.6\n57.1  1.7\nSource only\n77.1  1.8\n73.0  2.0\nRevGrad [3]\n73.9\n-\n82.0  0.16\n73.7  0.04\n91.8  0.09\n40.1  0.07\nDRCN [4]\n89.1  0.8\n91.2  0.8\n-\nCoGAN [15]\n-\n76.0  1.8\n89.4  0.2\n90.1  0.8\nADDA [30]\n-\n36.4  1.2\n92.5  0.7\n90.8  1.3\n84.7  0.9\nOurs\n DRCN approach uses more convolutional lters and cross-validates the number of neurons in fully connected layers\n\n4.1. Digit Experiments\nIn the rst set of experiments, we validate our method\nby performing domain adaptation on three standard digit\ndatasets - MNIST [14], USPS [11] and SVHN [21]. Each\ndataset contains digits belonging to 10 classes (0-9), each\ncaptured under different conditions. We test\nthe four\ncommon domain adaptation settings: SVHN  MNIST,\n\nMNIST  SVHN, MNIST  USPS and USPS  MNIST.\nIn each setting, we use the label information only from the\nsource domain, thus following the unsupervised protocol.\nMNIST and USPS are large datasets of handwritten dig-\nits captured under constrained conditions. Both these do-\nmains are visually very similar and this makes adaptation\nrelatively easy. SVHN dataset, on the other hand was ob-\n\n\fFigure 4. In each set of images, the top row shows the original images and the bottom row shows the reconstructed images from the\nrespective dataset. The top half shows the reconstructions for SVHN  MNIST task and the bottom half for MNIST  USPS task.\n\ntained by cropping house numbers in Google Street View\nimages and hence captures much more diversity. As can\nbe seen from Figure 3, in the SVHN dataset, there is sig-\nnicantly more domain shift with respect to the other two\ndatasets which makes adaptation hard.\nArchitecture and Preprocessing For all digit experi-\nments, following other recent works [3][30], we use a mod-\nied version of Lenet architecture as our encoder. The en-\ncoder pipeline has three 5  5 convolution layers containing\n64, 64 and 128 lters respectively, followed by ReLU and\npooling. This maps a 32  32 image to a 128 dimensional\nembedding. The label predictor containing two FC layers\n(128  128  10) maps this 128 dimensional embed-\nding to a 10 dimensional vector. The generator architecture\nwas adopted from DCGAN [23] - it contains 4 full convo-\nlution layers with 512, 256, 128 and 1 lters respectively,\neach followed by ReLU and batch normalization except the\nlast layer. The output of the last layer is the generated im-\nage. The discriminator architecture contains three convolu-\ntional layers with 64, 128 and 256 lters, followed by two\nfully connected layers of size 128 and 11. We used Adam\nsolver [12] with base learning rate of 0.0002 to train our\nmodels. The cost coefcient  is set to 0.1. We resize all\ninput images to 32  32 and scale their values to the range\n[0, 1]. No data augmentation was performed.\n(a) MNIST  USPS\nWe start with the easy case of adaptation involving MNIST\nand USPS. The MNIST dataset is split into 60000 train-\ning and 10000 test images, while the USPS contains 7291\ntraining and 2007 test images. In our experiments, we fol-\n\nlow the protocol established in [17], sampling 2000 images\nfrom MNIST and 1800 images from USPS. Since random\nsampling is prone to high variance in performance, we con-\nduct ve independent runs, sampling data randomly in each\nrun and report the average performance. We observe that\nour method performs well in both directions, MNIST \nSVHN and SVHN  MNIST. Specically, we achieve the\nbest performance of 92.5% in MNIST  USPS and 90.8%\nin USPS  MNIST among the compared methods.\n(b) SVHN  MNIST\nCompared to the previous experiment, SVHN  MNIST\npresents a harder case of domain adaptation owing to larger\ndomain gap. Following other works [3]\n[30], we use the\nentire training set (73257 SVHN images and 60000 MNIST\nimages) to train our model, and evaluate on the test set of\ntarget domain. Adaptation is much harder in MNIST \nSVHN direction compared to SVHN  MNIST, as SVHN\nis more diverse than the constrained MNIST dataset. In fact,\nmost methods fail to adapt [3], or did not report results in\nthis setting. However, our method achieves 36.4% accuracy,\nwhich is 10% higher than the baseline. We would like to\npoint out that the best performing method DRCN uses data\naugmentation and denoising to improve their performance,\nnone of which we perform. In addition, they use more lters\nin the convolutional layers and select their model architec-\nture by cross-validating the number of neurons in the fully\nconnected layers. On the other hand, we use a xed standard\narchitecture for all our experiments. In SVHN  MNIST,\nour method performs considerably better than other meth-\nods achieving state-of-the-art accuracy of 84.7%. A sam-\n\nSVHN-sourceMNIST-targetSVHN to MNISTMNIST to USPSMNIST-sourceUSPS-target\fFigure 5. Visualization of ofce datasets\n\nple of reconstructions obtained from our approach for two\ntasks, SVHN  MNIST and MNIST  USPS are visual-\nized in Figure 4.\n4.2. OFFICE dataset\nThe next set of experiments involve the OFFICE dataset,\nwhich is a small scale dataset containing images belong-\ning to 31 classes from three domains - Amazon, Webcam\nand DSLR, each containing 2817, 795 and 498 images re-\nspectively. The small dataset size poses a challenge to our\napproach since we rely on GAN which demands more data\nfor better image generation. Nevertheless, we perform ex-\nperiments on OFFICE dataset since our interest is not in\ngenerating good images, rather in utilizing the generative\nprocess to obtain domain invariant feature representations.\nArchitecture and Preprocessing Training deep net-\nworks from scratch on small datasets give poor perfor-\nmance. So, an effective technique used in practice is to\nne-tune networks trained on a related task having large\ndata [32]. Following other domain adaptation works [3] [4]\n[18], we use a pre-trained Alexnet model trained on Ima-\ngenet as our encoder. We plug in an FC layer to the encoder\nto produce a 256 dimensional vector output, which we use\nas our feature embedding. The generator contains 5 full\nconvolution layers with 1024, 512, 256, 128 and 1 lters re-\nspectively, each followed by ReLU and batch normalization\nexcept the last layer. Each layer upsamples its input to twice\nthe size. The output of the last layer is the generated image.\nThe discriminator architecture contains four convolutional\nlayers with 128, 128 and 128 lters each of size 55 ex-\ncept the last layer whose lters are 55. This is followed\nby three fully connected layers of size 500, 500 and 32. We\ninitialize all newly added layers using the method of Glorot\net al. [5] and learn them from scratch. It should be noted\nthat even though the inputs are 224  224, the generator is\nmade to reconstruct a downsampled version of size 64  64.\nWe use Adam solver for optimization with a base learning\nrate of 0.0002 and momentum 0.8. The dimension of the\nrandom noise vector is set to the dimension as the encoder\nrepresentation, which in this case is 256. The cost coef-\ncient  is set as 0.1. We rst resize all images to 256  256\nand then randomly select 224  224 crops as input images.\nIn addition to random cropping, we also perform random\n\nFigure 6. Visualization of CFP dataset\n\nmirroring. We would like to point out that these are the\nstandard augmentation techniques performed in recent do-\nmain adaptation works [3][18][31]\nIn all our experiments, we follow the standard unsuper-\nvised protocol - using the entire labeled data in the source\ndomain and unlabeled data in the target domain. Table 2 re-\nports the performance of our method in comparison to other\nmethods. From these results, we can make the following\nobservation - Our method performs the best when we have\nmore data in the source domain. In particular, we observe\ngood performance improvement with Amazon as source:\n0.8% and 3.3% in A  W and A  D respectively. Our\nmethod performs on par with the best performing method\non the other settings too.\n\n4.3. CFP dataset\nIn this experiment, we evaluate our approach on the do-\nmain of faces. The Celebrities in Frontal-Prole (CFP)\ndataset [27] was curated to evaluate the strength of face ver-\nication approaches across pose, more specically, between\nfrontal pose (yaw < 10  ) and prole poses (yaw > 60\n ). They argue that commonly used CNN-based approaches\nthat perform well on the frontal pose setting perform poorly\non prole pose setting. The dataset contains 500 individu-\nals in total with 10 frontal images and 4 prole images per\nindividual. The CFP dataset denes a verication protocol\nby providing frontal-frontal and frontal-prole pairs with a\n1 or 0 label indicating same/different pair. This protocol\ndoes not provide access to class labels during the training\nphase. Since the focus of the current work is improving\nclassication performance for domain adaptation, we create\na modied version of the CFP protocol for face recognition\nsimilar to the OFFICE protocol: We treat all frontal images\nas source and all prole images as target. We then train the\ncompared approaches using labeled source images and un-\nlabeled target images and test them on the target images.\nWe do not consider the adaptation in the opposite direction\nsince it is not a realistic situation. We will make the codes\nand models publicly available.\nArchitecture Training deep networks for face recogni-\ntion from scratch using a small dataset leads to severe over-\ntting. Based on our preliminary experiment done by ne-\ntuning Alexnet [13] on this dataset, we achieved very low\nRank-1 accuracy on the target images. Hence, we test\n\nSample Frontal ImagesSample Profile Images\fTable 2. Accuracy (mean  std%) values on the OFFICE dataset for the standard protocol for unsupervised domain adaptation [6]. Results\nare reported as an average over 5 independent runs. The best numbers are indicated in bold and the second best are underlined.  denotes\nunreported results. A: Amazon, W: Webcam, D: DSLR\nW  A\nA  W\nMethod\n61.1  0.5\n48.6  0.4\nAlexnet - Source only\n61.0  0.5\n49.4  0.6\nDDC [31]\n49.8  0.3\n68.5  0.3\nDAN [16]\n73.0  0.6\nRevGrad [3]\n-\n73.3  0.3\n51.1  0.5\nRTN [18]\n54.9  0.5\n68.7  0.3\nDRCN [4]\n53.5  0.8\n74.1  0.5\nOurs\n\nD  W Average\n95.6  0.3\n69.1\n95.0  0.3\n69.3\n96.0  0.1\n71.7\n96.4  0.4\n-\n96.8  0.2\n73.7\n96.4  0.3\n73.6\n96.6  0.2\n74.7\n\nA  D\n64.4  0.3\n64.9  0.4\n66.8  0.2\n-\n71.0  0.2\n66.8  0.5\n74.3  0.6\n\nD  A\n46.1  0.6\n47.2  0.5\n50.0  0.4\n-\n50.5  0.3\n56.0  0.5\n50.6  0.7\n\nW  D\n99.1  0.2\n98.5  0.3\n99.0  0.1\n99.2  0.3\n99.6  0.1\n99.0  0.2\n99.3  0.3\n\nthe domain adaptation approaches by initializing our en-\ncoder with the weights of a pretrained face recognition net-\nwork [25]. We add a fully connected layer with 256 neurons\nwhich is used as our feature embedding. The same architec-\ntures as the OFFICE experiment are used for the classier\nand discriminator networks. We nd that the generator is\nunable to reconstruct the full input image of size 224x224,\nhence it is made to reconstruct only a 32x32 downsampled\nversion of the input. For the generator, we use four full con-\nvolution layers. We initialize all newly added layers using\nthe method of Glorot et al. [5] and learn them from scratch.\nWe use Adam solver for optimization with a base learning\nrate of 0.0002 and momentum 0.8. The dimension of the\nrandom noise vector is set to the same dimension as the fea-\nture embedding. The cost coefcient  is set as 0.05.\nPreprocessing: We align all the face images from the\nCFP dataset using a similarity transformation as required by\nthe pretrained model [25]. The landmarks used for align-\nment were obtained using the HyperFace approach [24].\nThe aligned images were then resized to a 256x256 frame.\nDuring training, we applied data augmentation in the form\nof random cropping and mirroring as in the previous exper-\niments. Please note that the same preprocessing and data\naugmentation techniques were applied to all the approaches\ncompared in Table 3. The inputs to the encoder are sub-\ntracted using the mean provided with [25] and hence the\ndiscriminator inputs are scaled to be in the range [1, 1].\n\nRank-5\nRank-1\nMethod\n78.1  0.4\n57.8  0.5\nSource-only\n78.7  0.2\n58.5  0.3\nRevGrad [3]\n79.1  0.3\n60.1  0.4\nRevGrad++\n83.2  0.2\n62.3  0.5\nOurs\nTable 3. Domain adaptation performance on CFP dataset. Rank-1\nand Rank-5 accuracies (mean  std%) are reported for all the ap-\nproaches as an average over 5 independent runs. The CFP dataset\nhas 2000 prole images in total which is used as the target data.\n\nAs this is a new dataset for general domain adaptation\npractitioners, we compare our method with two baselines:\n\n(1) Source only, where we directly ne-tune the encoder and\nclassier layers in a supervised manner to predict 1 of 500\nidentities using only source data (2) RevGrad, which corre-\nsponds to the original architecture from the gradient reversal\nwork [3] (3) RevGrad++, where we make the domain classi-\ner stronger by adding more neurons to the fully connected\nlayers. The original RevGrad approach has a (10241024)\narchitecture for the domain classier while the stronger ar-\nchitecture we use has a (30722048) architecture. We used\nthis stronger architecture for gradient reversal in order to\nprovide a fair comparison. Except the source only baseline,\nother methods are trained using labeled source data and un-\nlabeled target data. The results of the CFP experiment is\nshown in Table 3. The reported numbers are Rank-1 and\nRank-5 accuracies of the compared methods. Note that a\nstrong pretrained model used as our baseline network, still\nyields only with 57.8% Rank-1 accuracy even after netun-\ning on this dataset using labeled source data. The RevGrad\nand RevGrad++ approaches yield only moderate improve-\nments over the source only baseline.\nIn comparison, our\napproach yields a signicant improvement (4.5%) over\nthe source-only baseline on Rank-1 accuracy and outper-\nforms the stronger RevGrad++ baseline by 2.2%. We\nshow a signicant improvement over the compared methods\non Rank-5 accuracy by outperforming the closest method\nby 4.1%. This shows that our method is able to leverage\ntarget data even in a difcult case such as frontal to prole\ncomparisons.\n5. Conclusion and Future Work\nIn this paper, we have addressed the problem of unsuper-\nvised domain adaptation. We proposed a joint adversarial-\ndiscriminative approach that transfers the information of\nthe target distribution to the learned embedding using a\ngenerator-discriminator pair. We have shown the superi-\nority of our approach over existing methods that address\nthis problem using experiments on three different tasks, thus\nmaking our approach more generally applicable and versa-\ntile. Some avenues for future work include using stronger\nencoder architectures and applications of our approach to\nmore domain adaptation problems such as RGB-D object\n\n\frecognition and medical imaging.\n\nAcknowledgement\nThis research is based upon work supported by the Of-\nce of the Director of National Intelligence (ODNI), In-\ntelligence Advanced Research Projects Activity (IARPA),\nvia IARPA R&D Contract No. 2014-14071600012. The\nviews and conclusions contained herein are those of the au-\nthors and should not be interpreted as necessarily represent-\ning the ofcial policies or endorsements, either expressed\nor implied, of the ODNI, IARPA, or the U.S. Government.\nThe U.S. Government is authorized to reproduce and dis-\ntribute reprints for Governmental purposes notwithstanding\nany copyright annotation thereon.\n\nReferences\n[1] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and\nD. Erhan. Domain separation networks. In Advances in Neu-\nral Information Processing Systems 29: Annual Conference\non Neural Information Processing Systems 2016,, 2016. 3\n[2] H. Daume III. Frustratingly easy domain adaptation. In Pro-\nceedings of the 45th Annual Meeting of the Association of\nComputational Linguistics, June 2007. 2\n[3] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation\nby backpropagation. arXiv preprint arXiv:1409.7495, 2014.\n2, 5, 6, 7, 8\n[4] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and\nW. Li. Deep reconstruction-classication networks for un-\nsupervised domain adaptation. In European Conference on\nComputer Vision. Springer, 2016. 2, 3, 5, 7, 8\n[5] X. Glorot and Y. Bengio. Understanding the difculty of\ntraining deep feedforward neural networks. In Aistats, vol-\nume 9, 2010. 7, 8\n[6] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ow ker-\nnel for unsupervised domain adaptation. In 2012 IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n20662073, 2012. 2, 8\n[7] I. Goodfellow,\nJ. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-\nerative adversarial nets. In NIPS, 2014. 2\n[8] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation\nfor object recognition: An unsupervised approach. In Pro-\nceedings of the 2011 International Conference on Computer\nVision, ICCV 11, 2011. 2\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 2016. 1,\n2\n[10] J. Huang, A. J. Smola, A. Gretton, K. M. Borgwardt, and\nB. Scholkopf. Correcting sample selection bias by unlabeled\ndata. In Proceedings of the 19th International Conference on\nNeural Information Processing Systems, NIPS06, 2006. 2\n[11] J. J. Hull. A database for handwritten text recognition re-\nsearch. IEEE Transactions on pattern analysis and machine\nintelligence, 16(5):550554, 1994. 5\n\n[12] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. CoRR, abs/1412.6980, 2014. 6\nImagenet\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nclassication with deep convolutional neural networks.\nIn\nAdvances in neural information processing systems, 2012.\n1, 2, 7\n[14] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11), 1998. 5\n[15] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-\nworks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,\nand R. Garnett, editors, Advances in Neural Information Pro-\ncessing Systems 29, pages 469477. 2016. 3, 5\n[16] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning\ntransferable features with deep adaptation networks. In Pro-\nceedings of the 32nd International Conference on Machine\nLearning, pages 97105, 2015. 2, 8\n[17] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer\nfeature learning with joint distribution adaptation. In IEEE\nInternational Conference on Computer Vision, ICCV 2013.\nIEEE Computer Society, 2013. 6\n[18] M. Long, J. Wang, and M. I. Jordan. Unsupervised do-\nmain adaptation with residual transfer networks. CoRR,\nabs/1602.04433, 2016. 2, 7, 8\n[19] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.\nJournal of Machine Learning Research, 9, 2008. 2\n[20] M. Mirza and S. Osindero. Conditional generative adversar-\nial nets. arXiv preprint arXiv:1411.1784, 2014. 3\n[21] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.\nNg. Reading digits in natural images with unsupervised fea-\nture learning. In NIPS workshop on deep learning and unsu-\npervised feature learning, volume 2011, page 5, 2011. 5\n[22] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain\nadaptation via transfer component analysis. In Proceedings\nof the 21st International Jont Conference on Artical Intelli-\ngence, IJCAI09, 2009. 2\n[23] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-\nsentation learning with deep convolutional generative adver-\nsarial networks. arXiv preprint arXiv:1511.06434, 2015. 6\n[24] R. Ranjan, V. M. Patel, and R. Chellappa. Hyperface: A deep\nmulti-task learning framework for face detection, landmark\nlocalization, pose estimation, and gender recognition. arXiv\npreprint arXiv:1603.01249, 2016. 8\n[25] S. Sankaranarayanan, A. Alavi, C. D. Castillo, and R. Chel-\nlappa. Triplet probabilistic embedding for face verication\nand clustering. In Biometrics Theory, Applications and Sys-\ntems (BTAS), 2016 IEEE 8th International Conference on,\npages 18. IEEE, 2016. 8\n[26] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-\ned embedding for face recognition and clustering. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2015. 1\n[27] S. Sengupta, J. C. Chen, C. Castillo, V. M. Patel, R. Chel-\nlappa, and D. W. Jacobs. Frontal to prole face verication\nin the wild. In 2016 IEEE Winter Conference on Applica-\ntions of Computer Vision (WACV), pages 19, March 2016.\n7\n\n\f[28] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls-\nson. Cnn features off-the-shelf: an astounding baseline for\nrecognition. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition Workshops, 2014. 1\n[29] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-\ndomain image generation. CoRR, abs/1611.02200, 2016. 2,\n3\n[30] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial\ndiscriminative domain adaptation. CoRR, abs/1702.05464,\n2017. 2, 5, 6\n[31] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.\nDeep domain confusion: Maximizing for domain invariance.\nCoRR, abs/1412.3474, 2014. 2, 7, 8\n[32] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-\nferable are features in deep neural networks? In Advances in\nNeural Information Processing Systems 27: Annual Confer-\nence on Neural Information Processing Systems 2014, 2014.\n7\n\n\f", 
        "tag": "Computer Vision and Pattern Recognition", 
        "link": "https://arxiv.org/list/cs.CV/new"
    }, 
    {
        "text": "Contextual Data Collection for Smart Cities\n\nHenrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and\nDeborah L. McGuinness1\n\n1 Rensselaer Polytechnic Institute, Troy, NY, U.S.A.\n2 Universidade de Fortaleza, Fortaleza, CE, Brazil\n3 Fundacao de Ciencia, Tecnologia e Inovacao de Fortaleza, Fortaleza, CE, Brazil\n\nAbstract. As part of Smart Cities initiatives, national, regional and lo-\ncal governments all over the globe are under the mandate of being more\nopen regarding how they share their data. Under this mandate, many of\nthese governments are publishing data under the umbrella of open gov-\nernment data, which includes measurement data from city-wide sensor\nnetworks. Furthermore, many of these data are published in so-called\ndata portals as documents that may be spreadsheets, comma-separated\nvalue (CSV) data les, or plain documents in PDF or Word documents.\nThe sharing of these documents may be a convenient way for the data\nprovider to convey and publish data but it is not the ideal way for data\nconsumers to reuse the data. For example, the problems of reusing the\ndata may range from diculty opening a document that is provided in\nany format that is not plain text, to the actual problem of understand-\ning the meaning of each piece of knowledge inside of the document. Our\nproposal tackles those challenges by identifying metadata that has been\nregarded to be relevant for measurement data and providing a schema\nfor this metadata. We further leverage the Human-Aware Sensor Net-\nwork Ontology (HASNetO) to build an architecture for data collected in\nurban environments. We discuss the use of HASNetO and the supporting\ninfrastructure to manage both data and metadata in support of the City\nof Fortaleza, a large metropolitan area in Brazil.\n\nKeywords: smart cities; sensor network; data quality\n\n1\n\nIntroduction\n\nSmart Cities should be sustainable, safe, inclusive, walkable, creative and in-\nnovative. In one way or another, the availability of each one of these features\ncharacterize a desirable place to live. Obtaining any of them, however, requires\ntwo key components: access to and understanding of city data. Consequently, a\ncitys ability to produce and share relevant data that can be understood is crit-\nical and can be viewed as key indicator of a Smart City. Further a citys ability\nto derive knowledge from this data and further, to use it to power innovation is\nan even better indicator of a smart city.\nTwo complementary trends are particularly relevant in this context: the Inter-\nnet of Things (IoT) and the Open Government Data approach (OGD). The IoT\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nI\nA\n.\ns\nc\n[\n \n \n1\nv\n2\n0\n8\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\n\f2\n\nHenrique Santos et al.\n\nis already a reality in many large metropolitan areas around the world. A myr-\niad of sensors operating at dierent levels of autonomy are deployed throughout\ncities collecting data from every aspect of these cities rich urban environments.\nGovernments are increasingly sharing their data, often with the goal of promot-\ning innovation via societal participation with the use of the data. In the context\nof data sharing, dierent categories of stakeholders may be identied: designers\nand software developers may use data to produce public services through the\nuse of web and mobile applications; scientists may produce elaborate analyses\nand studies about the cities; public ocers may use the data to improve city ad-\nministration through the use of eective data-based decision-making techniques;\njournalists may use open data to produce more reliable, factually-based and at-\ntractive news. Briey, IoT and OGD are enabling knowledge production that is\nfundamental for enhancing city smartness. While foundational elements exist\nwith IoT and OGD, many challenges remain to truly realize the potential of\nwidespread knowledge generation from the raw data sources increasingly avail-\nable in many cities.\n\nOne key challenge that we address in this paper is the need for cities to\nprovide metadata that enables data consumers to understand publicly shared\ndata, including the context within which data was collected. The lack of proper\nmetadata signicantly impacts the extraction of knowledge from data for any of\nstakeholders identied above. The reliability of data collected by extensive sensor\nnetworks, for example, relies heavily on knowledge about sensor deployment,\nsensor calibration, measurement units, measurement accuracy, and many other\ntypes of knowledge that is often lost at measurement time.\n\nOpen government data (which includes monitored data from city sensors) is\ntypically published in data portals that provide access to the contents of datasets.\nIt is true that datasets are a convenient way to convey and publish data but,\nwhile that holds true on a technological perspective, they are not always the best\nsuited for nal users to use as the boundaries of a dataset may be more driven\nby technical considerations of sensor collection rather than boundaries more\nnatural for users. Further the context for the boundaries is often not captured\nor communicated. The datasets are merely enclosures for data, which may be\nserialized inside them in a number of formats using dierent domain vocabularies.\nThe datasets hopefully come with metadata that attempt to explain the formats\nused to serialize the data and also to give meaning to the domain vocabularies\nused. Accurately interpreting the contents of the data in one data portal even\nwith a metadata le is often a challenge and greater challenges exist when users\nattempt to integrate data from multiple portals.\n\nOur work addresses the lack of a vocabulary to describe the knowledge about\nsensor network measurements in the context of Smart Cities. We introduce the\nHuman-Aware Sensor Network Ontology for Smart Cities (HASNetO-SC) used\nto describe knowledge associated with the collection of city empirical data, focus-\ning in particular on the collection of city empirical data coming from city-wide\nsensor networks. The rest of the paper is organized as it follows: In Section\n2, we discuss work related to the handling of urban stream data and to the\n\n\fContextual Data Collection for Smart Cities\n\n3\n\ndevelopment of ontologies describing empirical data collection. In Section 3, we\npresent and discuss our research choices regarding HASNetO-SC and the process\nof collecting urban data. In Section 4, we describe the use of a HASNetO-SC-\nbased tool when applied to a network of sensors deployed throughout a large\nmetropolitan area. In Section 5, we discuss our ndings and future work.\n\n2 Related work\n\n2.1 Open Data Portals\n\nOpen government data initiatives around the world are often very large pro jects\nin terms of complexity and data volume [9] [17]. Goals behind these initiatives\nrange from government transparency to fostering societal engagement in govern-\nment decisions. Typically open government data eorts for disseminating data\nare centered around the development and publication of online data portals. For\nexample, many of these portals are built on top of CKAN4 , which operates as a\ncomprehensive dataset repository with data coming from a plethora of distinct\ngovernment agencies. Many times, CKAN-hosted datasets convey data gathered\nempirically by city sensors. The data often are consolidated into datasets be-\ncause of technology restrictions. This approach, although convenient for data\nproducers, is often a hindrance for data consumers.\nMuch work exists on the integration and publication of urban data collec-\ntions. QuerioCity [11] describes a Linked Data platform to publish, search and\nlink city data from static datasets or stream data coming from deployed sensors.\nThis platform is able to create a semantic catalog that describes the content of\nthe datasets that reuses standard vocabularies to improve interoperability and\ndiscoverability. AECIS [6] describes an automated discovery and integration sys-\ntem for urban data streams. Both works, although capable of dealing with urban\ndata streams, are not concerned with data quality, in terms of comprehensive\nmetadata and the sensor network and context where the data was collected.\n\n2.2 Semantic sensor network and Observational data\n\nThe concept of Observational data is treated in the literature [15] [18] [14] as\ndata that is obtained while sensing some property of an entity from the real\nworld. The result of an observation is a value for that property [20]. City-wide\nsensors are also performing observations, making content annotation crucial.\nAnnotations enable interoperability and discoverability, making data easier to\nbe understood and thus (re)used. To leverage the potential benets of data reuse,\nseveral approaches exist to model the infrastructure that generates the data and\nto describe data content and context.\nSemantic Sensor Networks make use of the description of instruments and\ndetectors (many times referred to as sensors in the literature) to leverage and\n\n4 http://ckan.org\n\n\f4\n\nHenrique Santos et al.\n\nmaintain complex networks of sensors, while providing integration of the col-\nlected data. In [2], twelve dierent sensor network ontologies are studied and\ncompared. This work preceded the W3Cs Semantic Sensor Network Ontology\n(SSN) [1]. SSN is an ontology that aims to describe sensors, observations and\nits related concepts, like sensor capabilities, measurement processes and deploy-\nments. SSN is able to annotate data in a manner that makes it possible to to tell\nif it is coming from a certain sensor, using some measurement process to mea-\nsure a certain property of an entity of interest. BOnSAI [19] and the SESAME\nMeter Data Ontology (SMDO) [4] are other sensor network ontologies that are\nfocused on smart buildings. Although they are all able to describe the sensor\nnetwork behind the collected data, SSN does not rely on standard provenance\napproaches, like the W3Cs PROV-O. Besides that, SSN is an ontology, and is\nnot concerned about how the data is conveyed from its collection in terms of\nformat and datasets. BOnSAI and SMDO are not scientic-centric ontologies,\nnot containing key concepts like scientic activities.\nO&M [3] is an XML implementation from the Open Geospatial Consor-\ntium(OGC) that denes a schema for modeling observations and their results.\nIn [10], an observation and measurement ontology is proposed that makes use of\nO&Ms denitions. OBOE (The Extensible Observation Ontology) [12] is an on-\ntology for ecological observational data. It provides a data model that captures\nmeasurement semantics and that can be used to streamline data integration. To\nachieve this goal, the OBOE ontology contains concepts and relationships for\ndescribing observational datasets.\nThe Human-Aware Sensor Network Ontology (HASNetO) [13] is an ontology\nfor describing the scientic activities involved with the data collection of observa-\ntional data, i.e., data that is collected while sensing the environment. HASNetO\nreuses three ontologies to achieve this task: W3Cs PROV-O, the Virtual Solar-\nTerrestrial Observatory - Instrument module (VSTOI) [5] and OBOE. By using\nPROV-O, HASNetO is capable of asserting the provenance of the measured data\nusing PROV terms: activities, entities and agents. HASNetO links VSTOI and\nOBOE concepts to PROV concepts: (i) vstoi:Deployment and oboe:Observation\nare subclasses of prov:Activity; (ii) vstoi:Dataset is a prov:Entity and (iii) vs-\ntoi:Instrument becomes a prov:Agent. By doing this, HASNetO enables prove-\nnance tracking of data collection activities using W3Cs recommended standard.\nCollected data can be encoded in many distinct formats including CSV, XML,\nand NetCDF [16]. In many cases, CSV is a format of choice because of its ease\nof use by either computers or people. People often manually enter collected data\nin a spreadsheet application (like MS Excel or LibreOce Calc). Spreadsheets\nare also capable of exporting content in CSV format. Basically, the CSV format\ncan be seen as a minimalist enabling approach for data interoperability.\nRegardless of the format, no single encoding provides eective mechanisms\nfor annotating observational data in a way that supports observation as a contex-\ntualized measurement collection. For instance, CSV lacks features for expressing\nthe semantics associated with the data contained in it, so it is challenging to\nknow, in an automated and interoperable way, the meaning of the data enclosed\n\n\fContextual Data Collection for Smart Cities\n\n5\n\ninside a CSV le. For example, it can be dicult to determine if two entries are\nobservationally equivalent (measured under the same conditions, using the same\nunits, in the same area, etc.). Nonetheless, dierent agents may generate data\nin dierent formats and standards, making CSV even more dicult to process\nautomatically.\nAlthough there are existing approaches for accessing CSV metadata and also\nfor providing a metadata vocabulary for CSV data, they are typically more con-\ncerned with content restrictions, rather than the context in which the CSV data\nwas collected. W3Cs drafts from the CSV on the Web Working Group5 elabo-\nrate on techniques for enabling the access of CSV metadata by describing the\ncontent metadata in a separate JSON le that makes use of RDF vocabulary. To\nbridge this gap, we propose Contextualized CSV (CCSV)6 as a format that deals\nwith both content and context restrictions of the observational data enclosed in\nit. The CCSV dataset is basically a regular CSV le with a Turtle preamble\non top of it. The Turtle is used to assert that a certain dataset comes from a\nparticular instrument during an specic deployment, as context metadata. It is\nalso used to assert content, by stating the property being measured and which\ncolumn contains the measured value.\n\n3 A streamlined process to collect and publish data for\nSmart Cities\n\nOur approach relies on the HASNetO ontology. HASNetO was primarily con-\nceived to be used in scientic environments by scientists who conduct observation\nand data collection activities on entities of interest, such as a physical, chemical,\nbiological, social or cultural feature. While comprehensive enough to provide\nwhat scientists need to keep track of their activities, HASNetO faces challenges\nwhen dealing with data collected in large complex settings with broad user bases,\nsuch as urban environments. One of those challenges is that the collected data\nmust be used not only by the city administration or people involved with their\nassociated measurements (people who have some knowledge about the data at\nsome level), but, most importantly, by citizens who have, in general, little to\nno knowledge about the collection environment and the collected data. For in-\nstance, the accuracy or resolution of an instrument used to collect data may not\nbe as important for a regular citizen as it is for a scientist. To address these\nchallenges, we propose a modied version of HASNetO called the Human-Aware\nData Collection ontology for Smart Cities (HASNetO-SC). Our main goal with\nHASNetO-SC is to provide smart city stakeholders with a streamlined way of\ncollecting, preserving and disseminating urban data with an appropriate level of\ncontextual metadata for data understanding. The following subsections discuss\nthe aspects of this process.\n\n5 http://www.w3.org/2013/csvw/wiki/Main Page\n6 http://tw.rpi.edu/web/pro ject/JeersonPro jectAtLakeGeorge/download/ccsv\n\n\f6\n\nHenrique Santos et al.\n\n3.1 HASNetO-SC\n\nIn 2007, a report on the ranking of the smartest medium-sized European cities\nwas published [7]. This report aimed to, among other goals, dene the general\nconcept of a Smart City by specifying a set of factors and indicators that a\ncity should be pursuing in order to increase its level of smartness. The work\npresented six key areas (characteristics) that were gathered from a literature\nsearch on previous denitions of the term Smart City: smart economy, smart\npeople, smart governance, smart mobility, smart environment and smart living.\nInside each characteristic, a number of factors were also compiled to support it\nand, in turn, each factor consisted of a set of indicators that can identify well a\ncity is doing with respect to that factor. We developed an extension to HASNetO,\ncalled HASNetO-SC, to describe concepts that exist in urban environments.\nHASNetO-SC has been motivated by the smart cities characteristics identied\nin the report (not to be mistaken with characteristics of physical entities), while\nfocusing on the four that had higher potential to produce data that can be\ncollected empirically and managed using a streamlined process:\n\n Smart people\n Smart mobility\n Smart environment\n Smart living\n\nThe extension basically renes VSTOI concepts that are integrated into HAS-\nNetO to better suit urban data collections. The following subsections further\ndetail our modeling and discuss how we are using HASNetO concepts to cope\nwith smart cities.\n\nPeople People are a central component of cities. Cities are made for the citizens,\nas the government is intended to foster its society as a whole. Citizens, use city\nfacilities and participate in the government discussions. In this scenario, people\nare capable of providing valuable information to the city. In previous work, [8]\nhas studied the view of a citizen as a sensor. Our approach is compatible with\nthis perspective and enables people to either deploy instruments or conduct data\ncollection activities. By the means of prov:Person, a person is a prov:Agent , as\nasserted in the PROV ontology, and the person can have an associated list of\nprov:Activity. In HASNetO, prov:Activity is extended to both vstoi:Deployment\nand hasneto:DataCollection.\n\nMobility Mobility is one of the most discussed and explored aspects of smart\ncities, due to its need of near-real time raw data (i.e., data from sensors) and\nderived data (i.e., results of data analysis). Mobility is central to many key city\ngoals including accessibility, safe and sustainable transportation etc. To achieve\nthose goals, many city governments are deploying sensors to monitor a number of\nurban mobility indicators. HASNetO enables provenance capture by describing\n\n\fContextual Data Collection for Smart Cities\n\n7\n\nhow sensors are deployed, in particular how agents perform instrument deploy-\nments at platforms. In VSTOI, a Platform is a surface where Instruments and\nDetectors may be attached to measure, in the context above, characteristics of\nurban mobility. As a mobility example, we can identify some platforms and in-\nstruments that are known to be related to urban mobility. We have extended both\nvstoi:Platform and vstoi:Instrument to describe these more specialized mobility-\nrelated classes, as seen in Fig. 1. The gure shows that vstoi:Platform includes\nthree classes: subclasses hasneto-sc:Bus, hasneto-sc:RoadSegment, and hasneto-\nsc:LampPost. In a city, we dened buses, roads and lamp posts as being capable\nof hosting mobility-related instruments. Furthermore, vstoi:Instrument has been\nextended to a series of generic instruments that we propose should be used as a\nbasis from which to derive specialized instruments, as each city will have distinct\nmanufacturers, suppliers and vendors of instruments and platforms. The generic\nmodeling approach described above makes it possible to further connect data and\nmetadata, making them meaningful to data consumers (e.g., citizens) who are\nnot necessarily involved with the specication of specialized instruments, even\nthough they may know consumer instruments including cameras and GPSs.\n\nFig. 1. HASNetO-SC classes.\n\nEnvironment Another well-discussed smart city aspect is environment. Ex-\namples of smart environment entities are air, water, and their associated\nmonitoring and governance eorts such as sustainable resource management,\npollution control, and environmental protection. Forward-thinking cities are\noften concerned with citizen well being and thus are critically interested in envi-\n\n\f8\n\nHenrique Santos et al.\n\nronmental aspects. Signals of ecient use of water, electricity, and green space\nare some examples of indicators that a city that is doing well in the environment\ncharacteristic. To evaluate those indicators, many instruments are already in\nuse in big cities that enable the collection of data about air quality, noise levels,\nwater conditions and so forth. Fig. 1 shows more specialized platforms and in-\nstruments. Again, those concepts are generic concepts that should be specialized\nto fully describe the actual city sensor network.\n\nLiving Smart living includes the notions of public health, public safety, edu-\ncation and cultural facilities. Smart buildings are becoming a reality with more\nsensors being deployed to control access and gather the people ow information.\nPublic services addressing public health and safety considerations often are plac-\ning sensors including cameras to help monitor potentially dangerous situations.\nFig. 1 also depicts some of the most common platforms and instruments relevant\nto smart city living.\n\n3.2 Contextualized Comma Separated Values\n\nAs mentioned before, we proposed CCSV as an extension to CSV to address\nboth content and context restrictions when dealing with observational data. In\nour approach, both content and context metadata are needed in order to provide\nthe connection between data and metadata. From the Smart City perspective,\nwe needed to align the Turtle preamble of a CCSV dataset with the ontology\nconcepts introduced by HASNetO-SC. Further in this subsection, the displayed\ncode is an example of a CCSV preamble of a city dataset.\nThe preamble contains the following descriptions:\n\n Knowledge base: In order to provide the possibility of multi-contextual data\ncollections and also to make the solution more scalable, we make it possible\nfor the dataset to state which knowledge base should be used for validation\nand persistence.\n Deployment: Deployment information makes it possible to link the data the\nCCSV dataset conveys to metadata information: (i) instrument and detec-\ntor used; (ii) platform and (iii) all attached information to those, including\naccuracy, precision, platform location etc.\n Data collection: Data collection is a prov:Activity as asserted in HASNetO.\nThe use of Data collection information in the CCSV preamble enables the\narchitecture to have knowledge about this dataset: it is able to know if\ndistinct datasets were produced under the same context. In other words, the\narchitecture is able to provide the user enough context information for him\nto decide if the data within dierent datasets can be comparable, joined,\nanalyzed together, based on his needs.\n Dataset: As previously discussed, datasets are not scientic boundaries for\ndata, but data collection activities are. In this description, we link datasets\nto their corresponding data collections.\n\n\fContextual Data Collection for Smart Cities\n\n9\n\n Measurement(s): Here all measured characteristics are described. For each\nmeasurement type we link its description to a unit and a measured charac-\nteristic. Well discuss the need for a domain ontology in the next section.\n\nExample of a CCSV Preamble\n< c i t y kb>\na c c s v : K n ow l e d g eB a s e ;\nc c s v : h a sC o n n e c t i o nUR L [ s o m e u r l ]   x s d : anyURI\n\n.\n\n<d e p l o ym e n t>\na v s t o i : D e p l o ym e n t ;\np r o v : s t a r t e d A t T i m e [ s om e t i m e s t am p ]   x s d : d a t eT im e ;\nh a s n e t o : h a s D a t a C o l l e c t i o n <d a t a c o l l e c t i o n 001> .\n<d a t a c o l l e c t i o n >\na h a s n e t o : D a t a C o l l e c t i o n ;\na t i m e : I n t e r v a l ;\np r o v : s t a r t e d A t T i m e [ s om e t i m e s t am p ]   x s d : d a t eT im e .\n\n<d a t a s e t >\na v s t o i : D a t a s e t ;\np r o v : w a sG e n e r a t e dB y <d a t a c o l l e c t i o n > ;\nh a s n e t o : h a sM e a s u r em e n tT y p e <mt0> .\n\n<mt0>\na o b o e : M e a s u r em e n t ;\na t i m e : I n s t a n t ;\nt i m e : i nD a t eT im e < t s 0 > ;\nc c s v : a tC o lumn 1 ;\n[ s o m e c h a r a c t e r i s t i c ] ;\no b o e : o f C h a r a c t e r i s t i c\n.\no b o e : u s e s S t a n d a r d [ s o m e s t a n d a r d ]\n\n< t s 0 >\na t i m e : I n s t a n t ;\nc c s v : a tC o lumn 0 .\n\n3.3 Domain ontology\n\nAn Observation from the scientic point of view is dened as the act of observ-\ning some entitys property and obtaining a measured value. Measurements are\nexpressed using a specic standard (or unit). To have measurements properly\nannotated, we need a hierarchy of entities of interest with relevant properties,\na hierarchy of entity characteristics with relevant properties, and a hierarchy\nof standards to be used in in measurements. The OBOE ontology already pro-\nvides a hierarchy of a number of entities, characteristics and measurement units,\n\n\f10\n\nHenrique Santos et al.\n\nalthough it is focused on ecological studies. A domain ontology enables deep\nlinking of the collected data to its metadata. For cities, Section 4 discusses our\ninitial approach to this ontology in the context of a public transportation system.\n\n4 Use case: Fortaleza urban transportation system\n\nOur primary use cases focus on the city of Fortaleza, Brazil. The city of Fortaleza\nis the capital of the Ceara state in the Northeast region of Brazil with approx-\nimately 2.5 million inhabitants. Recently, the City Hall launched its smart city\nprogram called Fortaleza Inteligente. This pro ject, coordinated by the Foun-\ndation for Science, Technology and Innovation of Fortaleza (CITINOVA), has as\none of its main components the city open data portal, which contains a plethora\nof dierent datasets from a variety of public segments. We are collaborating with\nCITINOVA to use our approach in the city open data portal, in particular to\nrepresent data about the segment of urban mobility.\nThe rst initiative was to model public transportation data. Fortaleza has\ndeployed, on each of its running buses, GPS instruments capable of transmitting\nthe current geographical position of the bus. GPS data is transmitted to a central\nserver where they are stored in datasets that follow an ad hoc logic. The les\nshould refer to a GPS position of a bus, for a given bus line and during a certain\nday. In other words GPS data is represented by a triplet bus-id, bus-line, date.\nHowever, this structure is not always followed. There are situations where the\nle becomes too large, and is then divided into two datasets. In addition, when\na bus happens to be used in more than one line, then the same GPS receiver is\nused to collect more than one bus lines position. This can all be represented in\nthe same dataset, which again breaks the informal rule (bus-id, bus-line, date).\nOther variations exist, but the decisive factor is that none of these informal\nrules used for the construction of datasets were specied or clearly documented.\nThey were discovered in several rounds of interviews with providers of data only\nwhen other users decided to explore the data. This makes the process not easily\nreusable or scalable.\n\n4.1 Building the Semantic Sensor Network\n\nIn order to develop a proof of concept, we gathered three datasets from the For-\ntaleza Dados Abertos7 portal that related to the public transportation system:\n\n Bus checkpoints: A list with all the bus checkpoints around the city. Each\ncheckpoint is composed of its code, a name, a lat/long position and a radius\naround that lat/long to indicate the area the checkpoint that it covers.\n Bus companies: A list with all the bus companies on contract with City Hall\nto provide that public service. Each company has a company id, a name and\nother information.\n\n7 http://dados.fortaleza.gov.br\n\n\fContextual Data Collection for Smart Cities\n\n11\n\n Bus eet: A list with all the buses running in the city. Each bus has an\nid, a model, a maker, which company it belongs to and other information\nincluding plates, serial numbers and so forth.\n GPS bus information for 2015/02: A report including when a specic bus\nentered and left a bus checkpoint.\n\nWe note that rst three datasets can be viewed as metadata that plays a\nsupport role for the fourth dataset. This last dataset is the actual observed data\nthat was collected during a data collection activity. We also note that information\nsuch as bus company and bus checkpoint provide contextual metadata. We have\nmapped the checkpoints and eet datasets using the HASNetO-SC ontology:\n\n Checkpoints: By analysing the GPS dataset, we can infer that all the mea-\nsurements contained inside it are coming from the checkpoints themselves.\nTo reect this, we then mapped all checkpoints to individual instances of\nthe vstoi:Instrument as they are able to measure when a particular bus is\nentering or leaving its coverage area.\n Checkpoints location: We have also inferred that the actual instruments iden-\ntied in the previous item were deployed at the place described in it. For the\npurpose of this proof of concept, we dened that the place of deployment is\nan individual instance of hasneto-sc:RoadSegment, as all checkpoints lat/-\nlong locations are on roads.\n\nThe following is part of the Turtle describing our sensor network using\nHASNetO-SC.\n\nSerialized RDF Model of the Sensor Network\n< c h e c k p o i n t 1>\na v s t o i : I n s t r u m e n t\n;\nr d f s : l a b e l  D a l l a s / S o b r a d i n h o /T F P a i v a  .\n< c h e c k p o i n t 2>\na v s t o i : I n s t r u m e n t\n;\nr d f s : l a b e l  P e d r o P e r e i r a / I m p e r a d o r /T G o n c a l v e s  .\n. . .\n< c h e c k p o i n t p l a t f o r m 1>\na h a s n e t o s c : RoadS egm en t\n;\nr d f s : l a b e l  D a l l a s / S o b r a d i n h o /T F P a i v a  .\ng e o : l a t  3 . 7 9 4 8 6 6 0 0 ;\ng e o : l o n g  3 8 . 6 1 6 2 5 7 0 0 .\n< c h e c k p o i n t p l a t f o r m 2>\na h a s n e t o s c : RoadS egm en t\n;\nr d f s : l a b e l  P e d r o P e r e i r a / I m p e r a d o r /T G o n c a l v e s  .\ng e o : l a t  3 . 7 2 7 9 1 2 0 0 ;\ng e o : l o n g  3 8 . 5 3 4 0 5 2 0 0 .\n\n. . .\n\n\f12\n\nHenrique Santos et al.\n\n4.2 Annotating the CSV dataset\n\nOnce the semantic sensor network was ready, the next step was to annotate the\nCSV dataset containing the GPS bus information. The former dataset was a\ncombined collection of measurements from dierent checkpoint instruments. To\nfully integrate this dataset with our process, we split it so that each resulting\ndataset contains information collected by a single instrument. To annotate each\ndataset, as mentioned before, we need a domain ontology that includes a hi-\nerarchy of entities, characteristics and units. For the purpose of the use case,\nwe have developed a small ontology capable of describing the identied con-\ncepts, i.e., to state that we are measuring the occurrence (unit) of an arrival\nor departure (characteristic) of a bus (entity). As shown in Fig. 2, the ontology\ndenes two classes under the namespace pmf (acronym for Prefeitura Municipal\nde Fortaleza - Fortaleza City Hall): pmf:BusArrivalDeparture that specializes\noboe:Characteristic and pmf:Binary that specializes oboe:BaseUnit. Also, note\nthat pmf:ArrivalDeparture characteristic has an associated entity pmf:Bus.\nGiven this, we annotated each dataset to build the following CCSV preamble.\n\n.\n\nGPS bus dataset CCSV preamble\n<pmfkb>\na c c s v : K n ow l e d g eB a s e ;\nc c s v : h a sC o n n e c t i o nUR L  h t t p . . .    x s d : anyURI\n<d e p l o ym e n tc h e c k p o i n t 1>\na v s t o i : D e p l o ym e n t ;\np r o v : s t a r t e d A t T i m e 2015 0201T00 : 0 0 : 0 0 Z    x s d : d a t eT im e ;\nh a s n e t o : h a s D a t a C o l l e c t i o n < d a t a c o l l e c t i o n c h e c k p o i n t 1> .\n< d a t a c o l l e c t i o n c h e c k p o i n t 1>\na h a s n e t o : D a t a C o l l e c t i o n ; a t i m e : I n t e r v a l ;\np r o v : s t a r t e d A t T i m e 2015 0201T00 : 0 0 : 0 0 Z    x s d : d a t eT im e .\n<g p sb u si n f o r m a t i o n c h e c k p o i n t 1>\na v s t o i : D a t a s e t ;\np r o v : w a sG e n e r a t e dB y < d a t a c o l l e c t i o n c h e c k p o i n t 1> ;\nh a s n e t o : h a sM e a s u r em e n tT y p e <mt0> .\n\n<mt0>\na o b o e : M e a s u r em e n t ; a t i m e : I n s t a n t ;\nt i m e : i nD a t eT im e < t s 0 > ;\nc c s v : a tC o lumn 1 ;\no b o e : o f C h a r a c t e r i s t i c pmf : A r r i v a l D e p a r t u r e ;\no b o e : u s e s S t a n d a r d pmf : B i n a r y .\n\n< t s 0 >\na t i m e : I n s t a n t ; c c s v : a tC o lumn 0 .\n\n\fContextual Data Collection for Smart Cities\n\n13\n\nFig. 2. Domain ontology for the bus use case.\n\nThe dataset was annotated to contain deployment information for that single\ninstrument and also to specify that a new data collection activity was being\ninitiated. It is also shown that this dataset has a measurement of characteristic\npmf:ArrivalDepartureEvent in the pmf:Binary unit located on column number\n1 that was taken on the timestamp specied at column 0.\n\n4.3 CCSV parsing and indexing\n\nIn order to complete the connection between data and metadata described pre-\nviously including the full integration with the semantic sensor network, we have\ndeployed an instance of Apache SOLR8 to index and store both data and meta-\ndata. SOLR is a NoSQL database capable of indexing documents in a number of\ndierent formats including CSV, XML and JSON. SOLR document collections\nshould include eld denitions, including data types, for content that is to be\nindexed and stored. Inside SOLR, we have created two collections: (i) a meta-\ndata collection for storing the semantic sensor network, the domain ontology and\nthe deployment metadata, and (ii) a measurement collection for storing the data\nitself.\nThe metadata collection is basically a triple store where the sensor network\nTurtle was loaded along with the domain ontology described. The measurement\ncollection is a regular SOLR collection with the following elds being indexed\nand stored:\n\n Entity, characteristic and unit\n Location and timestamp\n Measured value\n Instrument\n Data collection to which the measurement belongs\n\nTo automate the process of storing this information, we have developed an\napplication called the CCSV-Loader9 . This loader performs, in order, the follow-\ning tasks:\n\n1. Extract the Turtle preamble of the CCSV dataset\n2. Get instrument information based on deployment\n\n8 http://lucene.apache.org/solr/\n9 http://tw.rpi.edu/web/pro ject/JeersonPro jectAtLakeGeorge/download/ccsv\n\n\f14\n\nHenrique Santos et al.\n\n3. Get platform information based on deployment\n4. Generate a normalized CSV le with the extra columns containing metadata\ninformation\n5. Index the normalized CSV le in the measurement collection\n\nFig. 3. Faceted-browsing of Fortaleza city transportation data.\n\nOnce the data is indexed, we make use of SOLR faceted-search capabilities\nto present a navigation UI to the user. Fig. 3 shows an example of that UI.\nOn the right side, measurements are shown, based on the user current search\nparameters. On the left side, we have dened some elds to be faceted by SOLR\nto provide easy access to commonly requested content. For this use case, we have\ndened checkpoints as a eld to be faceted so a user can select measurements\ncoming from a specic checkpoint. Also, we added buses and companies to the\nfacet eld list, giving the ability for users to rene the search results by a bus or\na company.\n\n\fContextual Data Collection for Smart Cities\n\n15\n\n5 Discussion\n\nThis paper presented a streamlined process to collect, store and disseminate mon-\nitored data in an urban environment. The work was motivated by the challenges\nrelated to understanding, using, and integrating data from dataset portals based\non current monitoring and publishing approaches. One issue we introduced is the\nboundary of a dataset as a natural context boundary vs. a technologically con-\nvenient boundary. Our Fortaleza public transportation system use case showed\nthat our approach enables users to have access not only to the sensor network\nmetadata but also to metadata driven navigation of the data. This combination\nwe believe helps circumvent many of the challenges with understanding, using,\nand integrating data, even in settings where datasets have varied and potentially\narbitrary boundaries and also in settings where metadata is limited.\nIn future work, we will articulate and implement additional use cases. We\nare also working on tighter integration with the real time sensor network content\nin order to augment it using our metadata scheme thus further evaluating our\nHASNetO-SC ontology. Additionally, we plan to apply our approach in other\nSmart City initiatives. Our vision is to deploy our approach alongside open data\nportals, providing end users with a better understanding of complex monitored\nurban environment data.\n\nAcknowledgements. The rst author is supported by CNPq - Brazil.\n\nReferences\n\n1. Compton, M., Barnaghi, P., Bermudez, L., Garca-Castro, R., Corcho, O., Cox,\nS., Graybeal, J., Hauswirth, M., Henson, C., Herzog, A., Huang, V., Janowicz,\nK., Kelsey, W.D., Le Phuoc, D., Lefort, L., Leggieri, M., Neuhaus, H., Nikolov,\nA., Page, K., Passant, A., Sheth, A., Taylor, K.: The SSN ontology of the W3c\nsemantic sensor network incubator group. Web Semantics: Science, Services and\nAgents on the World Wide Web 17, 2532 (Dec 2012)\n2. Compton, M., Henson, C., Lefort, L., Neuhaus, H., Sheth, A.: A Survey of the\nSemantic Specication of Sensors. CEUR Workshop Proceedings pp. 1732 (Oct\n2009)\n3. Cox, S.: Observations and Measurements - XML Implementation (Mar 2011)\n4. Fensel, A., Tomic, S., Kumar, V., Stefanovic, M., Aleshin, S.V., Novikov, D.O.:\nSESAME-S: Semantic Smart Home System for Energy Eciency. Informatik-\nSpektrum 36(1), 4657 (Dec 2012)\n5. Fox, P., McGuinness, D.L., Cinquini, L., West, P., Garcia, J., Benedict, J.L.,\nMiddleton, D.: Ontology-supported scientic data frameworks: The Virtual Solar-\nTerrestrial Observatory experience. Computers & Geosciences 35(4), 724738 (Apr\n2009)\n6. Gao, F., Ali, M.I., Mileo, A.: Semantic Discovery and Integration of Urban Data\nStreams. In: Proceedings of the Fifth Workshop on Semantics for Smarter Cities.\npp. 1530. Riva del Garda, Italy (Oct 2014)\n\n\f16\n\nHenrique Santos et al.\n\n7. Ginger, R., Fertner, C., Kramar, H., Kalasek, R., Pichler-Milanovic, N., Mei-\njers, E.: Smart cities-Ranking of European medium-sized cities. Tech. rep., Vienna\nUniversity of Technology (2007)\n8. Goodchild, M.F.: Citizens as sensors: the world of volunteered geography. Geo-\nJournal 69(4), 211221 (Nov 2007)\n9. Hendler, J., Holm, J., Musialek, C., Thomas, G.: US Government Linked Open\nData: Semantic.data.gov. IEEE Intelligent Systems 27(3), 2531 (May 2012)\n10. Kuhn, W.: A Functional Ontology of Observation and Measurement. In: Janowicz,\nK., Raubal, M., Levashkin, S. (eds.) GeoSpatial Semantics, pp. 2643. No. 5892 in\nLecture Notes in Computer Science, Springer Berlin Heidelberg (2009)\n11. Lopez, V., Kotoulas, S., Sbodio, M.L., Stephenson, M., Gkoulalas-Divanis, A.,\nAonghusa, P.M.: QuerioCity: A Linked Data Platform for Urban Information Man-\nagement. In: Cudre-Mauroux, P., Hein, J., Sirin, E., Tudorache, T., Euzenat, J.,\nHauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., Blomqvist,\nE. (eds.) The Semantic Web  ISWC 2012, pp. 148163. No. 7650 in Lecture Notes\nin Computer Science, Springer Berlin Heidelberg (Jan 2012)\n12. Madin, J., Bowers, S., Schildhauer, M., Krivov, S., Pennington, D., Villa, F.: An\nontology for describing and synthesizing ecological observation data. Ecological\nInformatics 2(3), 279296 (Oct 2007)\n13. Pinheiro, P., McGuinness, D.L., Santos, H.: Human-Aware Sensor Network Ontol-\nogy: Semantic Support for Empirical Data Collection. In: Proceedings of the 5th\nWorkshop on Linked Science. Bethlehem, PA, USA (2015)\n14. Probst, F.: Ontological Analysis of Observations and Measurements. In: Raubal,\nM., Miller, H.J., Frank, A.U., Goodchild, M.F. (eds.) Geographic Information Sci-\nence, pp. 304320. No. 4197 in Lecture Notes in Computer Science, Springer Berlin\nHeidelberg (2006)\n15. Quine, W.V.O.: From Stimulus to Science. Harvard University Press (1995)\n16. Rew, R., Davis, G.: NetCDF: an interface for scientic data access. IEEE Computer\nGraphics and Applications 10(4), 7682 (Jul 1990)\n17. Shadbolt, N., OHara, K., Berners-Lee, T., Gibbins, N., Glaser, H., Hall, W.,\nschraefel, m.: Linked Open Government Data: Lessons from Data.gov.uk. IEEE\nIntelligent Systems 27(3), 1624 (May 2012)\n18. Stasch, C., Janowicz, K., Broring, A., Reis, I., Kuhn, W.: A Stimulus-Centric\nAlgebraic Approach to Sensors and Observations. In: Trigoni, N., Markham, A.,\nNawaz, S. (eds.) GeoSensor Networks, pp. 169179. No. 5659 in Lecture Notes in\nComputer Science, Springer Berlin Heidelberg (2009)\n19. Stavropoulos, T.G., Vrakas, D., Vlachava, D., Bassiliades, N.: BOnSAI: A Smart\nBuilding Ontology for Ambient Intelligence. In: Proceedings of the 2Nd Inter-\nnational Conference on Web Intelligence, Mining and Semantics. pp. 30:130:12.\nWIMS 12, ACM, New York, NY, USA (2012)\n20. Usbeck, R.: Combining Linked Data and Statistical Information Retrieval. In: Pre-\nsutti, V., dAmato, C., Gandon, F., dAquin, M., Staab, S., Tordai, A. (eds.) The\nSemantic Web: Trends and Challenges, pp. 845854. No. 8465 in Lecture Notes in\nComputer Science, Springer International Publishing (Jan 2014)\n\n\f", 
        "tag": "Computers and Society", 
        "link": "https://arxiv.org/list/cs.CY/new"
    }, 
    {
        "text": "Adequacy of the Gradient-Descent Method for Classier Evasion Attacks\nYi Han, Benjamin I. P. Rubinstein\nSchool of Computing and Information Systems, University of Melbourne\n{yi.han, benjamin.rubinstein}@unimelb.edu.au\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nR\nC\n.\ns\nc\n[\n \n \n1\nv\n4\n0\n7\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstract\nDespite the wide use of machine learning in ad-\nversarial settings including computer security, re-\ncent studies have demonstrated vulnerabilities to\nevasion attackscarefully crafted adversarial sam-\nples that closely resemble legitimate instances, but\ncause misclassication.\nIn this paper, (1) we\nanalyse the effectiveness of the gradient-descent\nmethodthe leading approach for generating ad-\nversarial samplesagainst non-linear support vec-\ntor machines, and conclude that carefully reduced\nkernel smoothness can signicantly increase ro-\nbustness to the attack; (2) we propose a quantity\nsimilar to margin that can efciently predict poten-\ntial susceptibility to gradient-descent attack, before\nthe attack is launched; and (3) we design a new ad-\nversarial sample construction algorithm based on\noptimising the multiplicative ratio of class decision\nfunctions. Our results demonstrate that the new\nmethod not only increases the attacks success rate,\nbut also achieves success with less perturbation.\n\n1 Introduction\nRecent years have witnessed several demonstrations of ma-\nchine learning vulnerabilities in adversarial settings [Dalvi\net al., 2004; Lowd and Meek, 2005; Barreno et al., 2006;\nRubinstein et al., 2009; Br uckner and Scheffer, 2011; Big-\ngio et al., 2012; Goodfellow et al., 2014; Alfeld et al., 2016;\nLi et al., 2016]. Due to its approximation of best-response\nand its effective simplicity, gradient descent [Biggio et al.,\n2013] has emerged as a leading approach to evasion attacks.\nWe refer to the carefully crafted inputs that resemble le-\ngitimate instances but cause misclassication, as adversar-\nial samples, and the malicious behaviours that generate them\nas evasion attacks [Russu et al., 2016]. Figure 1 illus-\ntrates the attacks effect in the previously-explored vision\ndomain [Szegedy et al., 2013; Goodfellow et al., 2014;\nPapernot et al., 2016c]: Figures 1a and 1b present original im-\nages from [Samaria and Harter, 1994] for Adam and Lu-\ncas, who are correctly identied by a face recogniser. How-\never, after indiscernible changes are applied to Figure 1a the\nmodel mistakenly identies Figure 1c as Lucas.\nCan these attacks be thwarted, are there effective attack al-\nternatives? This paper addresses these questions, with a case\nstudy on the support vector machine with radial basis func-\ntion (RBF) kernel. Our main contributions include:\n An analysis of kernel precision parameters impact on\nthe success rate of evasion attacks, concluding that larger\nprecision (less smooth kernels) achieves robustness to\ngradient-descent attack;\n\n1\n\n(a) Adam\n\n(b) Lucas\n\n(c) Adam or Lucas?\n\nFigure 1: An example evasion attack against a learning\nmodel: Image (c) is misclassied as Lucas.\n\n An novel geometric classier parameter related to mar-\ngin that strongly correlates with model vulnerability,\nproviding an avenue to predict (unseen) attack vulner-\nability; and\n A new approach for generating adversarial samples in\nmulticlass scenarios, with results demonstrating signi-\ncantly higher effectiveness in manipulating test data and\nfooling the target model.\nThe remainder of this paper is organised as follows: Sec-\ntion 2 overviews previous work on evasion attacks; Section 3\npresents our research problem; we present a detailed exam-\nple of how gradient-descent can fail in Section 4; Section 5\npresents the gradient-quotient approach for constructing ad-\nversarial samples; experimental results are presented in Sec-\ntions 6 and 7; and Section 8 concludes the paper.\n\n2 Related Work\nBarreno et al. [2006] categorise how an adversary can tamper\nwith a classier based on whether they have (partial) control\nover the training data: in causative attacks, the adversary can\nmodify the training data to manipulate the learned model; in\nexploratory attacks, the attacker does not poison training, but\ncarefully alters target test instances to ip classications. See\nalso [Barreno et al., 2010; Huang et al., 2011]. This paper fo-\ncuses on the targeted exploratory case, also known as evasion\nattacks [Biggio et al., 2013].\nGeneralising results on efcient evasion of linear classi-\ners via reverse engineering [Lowd and Meek, 2005], Nelson\net al. [2012] consider families of convex-inducing classiers,\nand propose query algorithms that require polynomially-\nmany queries and achieve near-optimal modication cost.\nSzegedy et al. [2013] demonstrate changes imperceptible\nto humans that cause DNNs to misclassify images. Addi-\ntionally, they offer a linear explanation of adversarial sam-\nples and design a fast gradient sign method for generat-\ning such samples [Goodfellow et al., 2014].\nIn a similar\nvein, Nguyen et al. [2015] propose an approach for producing\n\n\fDNN-adversarial samples unrecognisable as such to humans.\nPapernot et al. published a series of further works in this\narea: (1) introducing an algorithm that searches for mini-\nmal regions of inputs to perturb [Papernot et al., 2016c];\n(2) demonstrating effectiveness of attacking target models via\nsurrogateswith over 80% of adversarial samples launched\nfooling the victim in one instance [Papernot et al., 2016b];\n(3) improved approaches for tting surrogates, with further\ninvestigation of intra- and cross-technique transferability be-\ntween DNNs, logistic regression, SVMs, decision trees and\nk-nearest neighbours [Papernot et al., 2016a].\nMoosavi-Dezfooli et al. [2016b] propose algorithm D E E P -\nFOO L for generating adversarial samples against DNNs,\nwhich leads samples along trajectories orthogonal to the de-\ncision boundary. A similar approach against linear SVM is\nproposed in [Papernot et al., 2016a]. Based on D E E PFOOL,\nMoosavi-Dezfooli et al. [2016a] design a method for comput-\ning universal perturbations that fool multiple DNNs.\nto this paper is the work by Russu et\nMost relevant\nal. [2016], which analyses the robustness of SVMs against\nevasion attacks, including the selection of the regularisation\nterm, kernel function, classication costs and kernel parame-\nters. Our work delivers a much more detailed analysis of ex-\nactly how the kernel parameters impact vulnerability of RBF\nSVM, and explanations of why.\n3 Preliminaries & Problem Statement\nThis section recalls evasion attacks,\nthe gradient-descent\nmethod, the RBF SVM, and summarises the research prob-\nlem addressed by this paper.\nEvasion Attacks. For target classier f : Rd  {1, 1},\nthe purpose of an evasion attack is to apply minimum change\n to a target input x, so that the perturbed point is misclassi-\ned, i.e., f (x) (cid:54)= f (x+). The magnitude of adversarial per-\nturbation  is commonly quantied in terms of L1 distance.\nFormally, evasion attacks are framed as optimisation:\n(cid:107)(cid:107)1\nf (x) (cid:54)= f (x + ) .\ns.t.\narg min\nRd\nNote that we permit attackers that can modify all fea-\ntures of the input, arbitrarily, but that aim to minimise the\nmagnitude of changes. Both binary and multiclass scenarios\nfall into the evasion problem as described; we consider both\nlearning tasks in this paper. In multiclass settings, attacks in-\ntending to cause specic misclassication of the test sample\nare known as mimicry attacks.\n\nGradient-Descent Method. The gradient descent method1\nhas been widely used for generating adversarial samples for\nevasion attacks [Biggio et al., 2013; Goodfellow et al., 2014;\nMoosavi-Dezfooli et al., 2016b; Papernot et al., 2016c], when\nf outputs condence scores in R and classications are ob-\ntained by thresholding at  = 0. The approach applies gradi-\nent descent to f directly, initialised at the target instance. For-\nmally given target instance x0  Rd evaluating f (x0 ) >  ,\nxt+1 = xt  t  x f (xt ) ,\n1Not to be confused with gradient descent for local optimisation.\n\nwhere t follows an appropriately-selected step size schedule,\nand the iteration is terminated when f (xt ) <  .\n\nThe Support Vector Machine. Recall the dual program of\nthe soft-margin SVM classier learner, with hinge-loss\n\n1(cid:48)  1\n(cid:48)G s.t. (cid:48)y = 0, 0 (cid:22)  (cid:22) C 1 (1)\narg max\nRn\n2\nwhere {(xi , yi ), i = 1, . . . , n} is the training data with\nxi  Rn and yi  {1, 1}n ,  are Lagrange multipliers, 0,\n1 the all zeros, ones vectors, C > 0 the regularisation penalty\nparameter on misclassied samples, and G the n  n Gram\n > 0 that controls kernel width (cid:112)2/ . By the Representer\nmatrix with entries Gij = yi yj k(xi , xj ). The RBF kernel\nk(xi , xj ) = exp( (cid:107)xi  xj (cid:107)2\n2 ) has precision parameter\nn(cid:88)\nTheorem, the learned classier\ni=1\n\ni yik(xi , x) + b .\n\nf (x) =\n\n(2)\n\nAdequacy & Improvements to Gradient-Descent Method.\nWhile the gradient-descent method has been effective against\na number of machine learning models, e.g., DNNs, linear\nSVMs, logistic regression [Biggio et al., 2013; Goodfellow\net al., 2014; Moosavi-Dezfooli et al., 2016b; Nguyen et al.,\n2015; Papernot et al., 2016b], there is no general guaran-\ntee that gradient descent converges to a global minimum of\nf () or even converges to local optima quicklyrelevant to\ncomputational complexity (a measure of hardness) of eva-\nsion. Under linear modelsa major focus of past work\ngradient descent quickly nds global optima. While DNNs\nhave been argued to exhibit local linearity. The existing body\nof evidence is insufcient to properly assess the effectiveness\nof the attack approach. As we argue in the next section, the\napproach is in fact unlikely to be successful against certain\nSVMs with RBF kernels, when kernel parameter  is chosen\nappropriately.\nProblem 1 What limitations of the gradient-descent method\nare to be expected when applied to evasion attacks?\nProblem 2 Are there simple defences to the gradient-descent\nmethod for popular learners?\nProblem 3 Are there more effective alternative approaches\nto generating adversarial samples?\n\nWe address each of these problems in this paper, with spe-\ncial focus on the RBF SVM as a case study and important\nexample of where the most popular approach to evasion at-\ntack generation can predictably fail, and be improved upon.\n\n4 Gradient-Descent Method Failure Modes\nIn this section, we explore how the gradient-descent method\ncan fail against RBF SVMs with small kernel widths.\n\n\f(a)  = 102 model: two black curves display attack paths under gradi-\nent descent, of two target points reaching the decision boundary.\n\nFigure 3: Gradient-descent method failure modes.\n\n.\n\n =\n\nsign of k can be ipped by various choices of  . Suppose\n2 > x1 , then solving for 1 = 0, we obtain the point at\nx1\n1 , x1\nwhich this phase transition occurs as:\n1  x1 )  log 2 (x1\n2  x1 )\nlog 1 (x1\n2  (cid:107)x  x2(cid:107)2\n(cid:107)x  x1(cid:107)2\n2\nThe failure modes hold true in multiclass scenarios. For\ntest sample x the classier evaluates an fi (x) per class i and\nselects the maximiser (a one-vs-all reduction). Suppose that\nf1 (x) and f2 (x) are the highest class scores. If  is chosen\nappropriately as above, then gradient descent reduces both\nf1 (x) and f2 (x) without ever reranking the two classes.\nFigure 3 presents a geometric explanation, where distance\nbetween support vectors of opposite classes exceeding kernel\nwidth results in gradient-descent method iterates becoming\ntrapped in the gap between. This section partially addresses\nProblem 1 through the discussed limitations, while setting \ncan provide a level of defence per Problem 2.\n\n5 The Gradient-Quotient Method\nThe previous section motivates Problem 3s search for effec-\ntive alternatives to decreasing current class i fi (x) while in-\ncreasing desired class j fj (x). Rather than moving in the\ndirection fi (x) as the gradient-descent method does (not-\ning this is in the subgradient for the one-vs-all reduction), we\npropose following the gradient of the quotient fi (x)/fj (x):\nxt+1 = xt  t  (fi (x)/fj (x)) .\nRemark 1 Employing fi (x)fj (x) in place of fi (x)/fj (x)\ndoes not achieve the desired result by the same aws suffered\nby the gradient-descent method: fi (x), fj (x) and fi (x) \nfj (x) are decreased simultaneously, while fi (x) can remain\nlarger than fj (x), with no misclassication occurring.\nNote that while in the above, i is taken as the current (max-\nimising) class index, taking j as the next highest-scoring class\ncorresponds to evasion attacks while taking j as any xed\ntarget class corresponds to a mimicry attack. The results of\nSection 7 establish that this method can be more effective for\nmanipulating test data in multiclass settings. However, it is\nnot appropriate to binary-class cases as f1 (x)/f2 (x) = 1.\n\nStep Size. The step size t is important to select carefully:\ntoo small and convergence slows; too large and the attack\nincurs excessive L1 change, potentially exposing the attack.\n\n(b)  = 104 model: two magenta curves display attack paths under\ngradient descent, of the two target points now move away from the\nboundary or take signicantly more steps.\n\nFigure 2: Heatmaps visualising decision boundaries of RBF\nSVMs trained on a sample dataset.\n\nIllustrative Example. To demonstrate our key observation,\nwe trained two RBF SVMs on a toy two-class dataset com-\nprising two features [Chang and Lin, 2011; Chang and Lin,\n2016], using two distinct values for  : 102 and 104 . Figure 2\ndisplays the heatmaps of the two models decision functions.\nAs can be seen, for the larger  case, additional regions result\nwith at, approximately-zero, decision values. Since the gra-\ndients in these regions are vanishingly small, it is signicantly\nmore likely that an iterate in the gradient-descent methods at-\ntack trajectory will become trapped, or even move towards a\ndirection away from the decision boundary altogether. No-\ntably, both models achieve test accuracies of 100%.\nIn Figure 2(a), the two black curves marked with crosses\ndemonstrate how two initial target points (0.55, 0.1) and\n(0.75, 0.5), move towards the decision boundary follow-\ning the gradient-descent method. However, the two magenta\ncurves marked with squares in Figure 2(b) demonstrate how\nthe same two points either move away from the boundary or\ntake signicantly more steps to reach it, following the same\nalgorithm but under a different model with a much larger  .\nThis example illustrates that although the gradient-descent\nmethod makes the test sample less similar to the original\nclass, it does not necessarily become similar to the other class.\n\nDiscussion. We employ Figure 3 to further explain pos-\nsible failure modes of the gradient-descent method: points\nmay get stuck or even move in the wrong direction. In this\n(cid:80)2\ngiven 2D case, x1 belongs to Class 1, x2 to Class 2. At in-\nstance x, the k th component of the gradient is k f (x) =\ni  xk )  exp( (cid:107)x  xi (cid:107)2\n2 ). Clearly, the\ni=1 2i yi (xk\n\nFeature 1-1-0.500.5Feature 2-1-0.500.5-0.02-0.0100.010.02Feature 1-1-0.500.5Feature 2-1-0.500.510-3-2-1012x1x2Class 1 (+)Class 2 ()tSmaller widthLarger gap+width = (2/) gap\fAlgorithm 1: Gradient-quotient step size.\n: Iterate xt ; Current quotient gradient   Rd ;\nInput\nParameter  > 0\nOutput: Step size t\n1 Select i  arg maxj[d] |j |.\n2 Select t > 0 such that t  |i |  \n\nt[5 , 10 ].\n\nIn our experiments, we limit the largest change made to a\nsingle feature per iteration, and determine the step size ac-\ncordingly as described in Algorithm 1. Here  is a domain-\nspecic value corresponding to a unit change in a feature, e.g.,\nfor a grayscale image  = 1 corresponds to a unit change\nintensity level. The select rules [5 , 10 ] is motivated by\nround-off practicalities in steps: if the largest gradient com-\nponent were smaller than 5 , it is likely that most other com-\nponents would be 0, making convergence extremely slow.\nSince [5 , 10 ] is a relatively conservative start, we increase\nit gradually; as explained next, the maximum step number in\nour experiments is 30. This increasing step size corresponds\nto a variant of guess-then-double.\n\n6 Experiments: Gradient-Descent Method\nSection 4 demonstrates that RBF SVM is less vulnerable to\nevasion attacks when  is set appropriately. In this section,\nwe present a more detailed analysis of the impact of  on the\nattacks success rate, further addressing Problems 1 and 2.\n6.1 Dataset\nMNIST is a dataset of handwritten digits [LeCun et al.,\n1998a; LeCun et al., 1998b]. We choose this dataset as it\nfacilitates comparison of our results with past work. MNIST\ncontains a training set of 6104 samples (only the rst 5104\nare used in our experiments), and a test set of 104 samples.\nFollowing the approach of [Papernot et al., 2016a], we divide\nthe training set into 5 subsets of 104 samples, D1  D5 . Each\nsubset is used to train models separately. Specically, in the\nexperiments that study binary-class settings, only the data in\nD1 (or the test dataset) that belong to the two classes are used\nfor training (or testing respectively).\n6.2\nImpact of  on Vulnerability (Binary Class)\nWe begin with the binary scenario and investigate how  im-\npacts the success rate of causing SVMs to misclassify three\npairs of digits1 & 2, 3 & 4, 5 & 7. Note that all models\ndiscussed in this subsection are trained on D1 . An attack is\nconsidered successful if the perturbed test sample is misclas-\nsied within 30 steps. The reason why we choose 30 is that\nalthough larger values will increase the attacks success rate,\nthe changes made to the original samples are so obvious they\nwould be easily detected by manual audit. Tables 1a1c il-\nlustrate a phase transition in each of the three cases: a small\ndecrease of  causes a signicant jump in success rate.2\n\n2Testing for effect of C revealed much less impact on success\nrate, especially when  = 0.5 in these cases, a smaller C = 10 is\nchosen.\n\nFigure 4: Minimum distance between each support vector of\nclass 3 and all support vectors of class 4.\n\nFigure 5: Minimum distance between each support vector of\nclass 4 and all support vectors of class 3.\n\nInter-Class-SV Distance. Since  controls how quickly the\nRBF kernel vanishes, we are motivated to compare minimum\n(Euclidean) distance between each support vector of one class\n(sv1i ) and all support vectors of the opposite class (sv2j ),\ni.e., M inDist(sv1i ) = arg minj Distance(sv1i , sv2j ). Our\nintuition is that a larger  suggests (1) a quicker drop of values\nfor both the kernel function and the gradient; (2) a wider gap\nbetween the two classes. Both observations contribute to the\nlower success rate of the evasion attack.\nFigures 4 and 5 present the minimum distance between\nsupport vectors of classes 3 and 4 (due to space limi-\ntations, we omit the similar two sets of results). Observe that\nwhen  rst decreases from 0.5, the support vectors of the\nopposite class move further awayhere the corresponding\nmodel is still less vulnerable to evasion attack. As  contin-\nues to decrease the trend reverses, i.e., the support vectors of\nopposite class move closer to each other. A smaller  already\nmeans the RBF kernel vanishes more slowly, and the closer\ndistance between the two classes makes it even easier for a\ntest sample to cross the decision boundary. Consequently, the\ncorresponding model becomes much more vulnerable.\nThis prompts the question: Given a model with a  , is there\na way to determine whether the model is robust? The results\n\nin Tables 1a1c witness a strong correlation between success\n  minimum distance\nrate and the percentage of 2/\nthe lower the percentage the less vulnerable the model.\n\nMargin Explanation. We have observed a positive corre-\nlation between margin per support vector and this minimum\ndistance. These ndings suggest that separated inter-class\nsupport vectors leads to more secure models, lending experi-\nmental support to the geometric argument (Section 4).\n\nMinimum distance between support vectors56789101112Cumulative percentage(%)0255075100 = 0.01 = 0.025 = 0.035 = 0.05 = 0.125 = 0.5Minimum distance between support vectors56789101112Cumulative percentage(%)0255075100 = 0.01 = 0.025 = 0.035 = 0.05 = 0.125 = 0.5\fTable 1: Success rate and average L1 change for gradient-descent method evasion attack (binary class).\n(a) RBF SVM: digits 1 and 2.\n0.05\n0.025\n0.02\n5  104\n5  104\n5  104\n99.5\n99.6\n99.5\n100\n100\n100\n100\n100\n100\n17.0\n54.8\n68.9\n100\n100\n59.3\n\n\nC\nAccuracy(%)\nSucc rate (%)\n\n  M inDist) (%)\nSucc rate (%)\n\n  M inDist) (%)\n\n0.01\n5  104\n99.4\n100\n100\n94.1\n100\n\n0.1\n5  104\n99.8\n90.1\n68.4\n11.3\n7.5\n\n0.11\n5  104\n99.7\n68.9\n55.0\n13.3\n4.8\n\n0.125\n5  104\n99.5\n36.3\n35.0\n15.2\n2.8\n\nP (2/\n\nP (2/\n\n1  2\n\n2  1\n\n0.5\n10\n98.6\n1.5\n0.1\n12.9\n0.1\n\nP (2/\n\nP (2/\n\n(b) RBF SVM: digits 3 and 4.\n0.035\n0.025\n0.01\n104\n104\n104\n99.6\n99.7\n99.5\n100\n94.5\n76.0\n96.7\n100\n100\n98.1\n100\n100\n100\n100\n99.2\n\n\nC\nAccuracy(%)\nSucc rate (%)\n\n  M inDist) (%)\nSucc rate (%)\n\n  M inDist) (%)\n(c) RBF SVM: digits 5 and 7.\n0.035\n0.025\n0.01\n104\n104\n104\n99.6\n99.5\n99.3\n100\n97.0\n87.4\n98.7\n100\n100\n99.8\n100\n100\n100\n100\n100\n\n\nC\nAccuracy(%)\nSucc rate (%)\n\n  M inDist) (%)\nP (2/\nSucc rate (%)\n\n  M inDist) (%)\nP (2/\n(d) Linear SVM.\n\n3  4\n\n4  3\n\n5  7\n\n7  5\n\n0.05\n104\n99.5\n53.4\n61.7\n82.5\n79.7\n\n0.05\n104\n99.6\n71.1\n75.6\n91.1\n87.2\n\n0.125\n104\n99.5\n14.8\n0.29\n11.8\n0.4\n\n0.1\n104\n99.1\n41.8\n0.2\n15.5\n0.2\n\n0.5\n10\n99.8\n5.3\n0.1\n1.6\n0.1\n\n0.5\n10\n99.7\n6.2\n0.1\n2.3\n0.1\n\nC = 1000\nSucc rate (%) Ave L1 change Accuracy (%)\n5812\n100\n100\n9575\n10085\n100\n8372\n100\n7748\n100\n100\n7408\n\n99.3\n\n99.0\n\n99.7\n\nC = 5000\nSucc rate (%) Ave L1 change Accuracy (%)\n4685\n100\n100\n8883\n9520\n100\n6593\n100\n7169\n100\n100\n6024\n\n99.6\n\n99.2\n\n99.0\n\n1  2\n2  1\n3  4\n4  3\n5  7\n7  5\n\nLinear SVM. The experiments were performed for linear\nSVMs, with results serving as baseline. Table 1d demon-\nstrates that success rates under linear models are 100%, as\nexpected. However a larger C requires smaller changes to the\ntarget sample, as larger C leads to smaller margin.\n6.3\nImpact of  on Vulnerability (Multiclass)\nThis section further investigates the impact of  on success\nrate of evasion attacks, in multiclass scenarios. Two RBF\nSVMs with  as 0.05 and 0.5, are trained on both D1 and\nD2 , respectively. For comparison, four linear SVMs are also\ntrained on the two datasets with different values of C . An\nattack is considered successful if the perturbed test sample is\nmisclassied within 30 steps. As can be seen from Table 2:\n(1) for RBF SVMs, the success rates under the models with\nlarger  are much lower; (2) for linear SVMs the success rates\nare always 100%, but the average L1 change is smaller as C\nincreasesobservations consistent with previous results.\n\nTable 2: Success rate and average L1 change of gradient-\ndescent method evasion attack, in multiclass scenarios.a\nAveL1\nAccuracy\nSucc rate\nchange\n(%)\n(%)\nRBF ( =\n91.8\n87.2\nModel 1\n10037b\n0.05, C = 103 )\n92.8\n87.7\nModel 2\n10323b\n94.8\n24.4\nModel 1\nRBF ( =\n4509b\n0.5, C = 10)\n23.7\n94.8\nModel 2\n4637b\nLinear\n7259\n100\n89.0\nModel 1\n(C = 103 )\n6837\n100\n89.2\nModel 2\nLinear\n91.4\n100\n4766\nModel 1\n(C = 2  104 )\n4604\n100\n91.7\nModel 2\na Since it takes more than an order of magnitude longer to run experi-\nments using RBF SVM than linear kernel, each result regarding\nRBF SVM is based on 1000 test samples, while each linear SVM\nresult is based on 5000 test samples.\nb Only the successful cases are counted.\n\n\f7 Experiments: Gradient-Quotient Method\nIn this section we present experimental results establishing\nthe effectiveness of our proposed method for generating ad-\nversarial samples.\n7.1 Attacking the Model Directly\nRecall that based on our new approach, a test sample x is up-\ndated as xt+1 = xt  t  (f1 (x)/f2 (x)), where f1 (x) and\nf2 (x) are the scores for the top two scoring classes for x. In\norder to test whether this method is more effective than the\npopular gradient-descent method, we run similar experiments\nto Section 6.3, where one RBF SVM ( = 0.5, C = 10) and\none linear SVM (C = 1000) are trained on D1 , . . . , D5 , re-\nspectively. Comparing the results in Table 2 and Table 3, we\nobserve that: (1) for the RBF SVMs with  = 0.5, the suc-\ncess rates increase from around 24% to a resounding 100%.\nMoreover we have tested a wide range of values for  from\n0.01 through 10, with resulting success rates always 100%\nunder our new approach; (2) the required L1 perturbation\nalso decreases. These two observations establish that the new\napproach is more effective in crafting adversarial samples.\nTable 3: Success rate and average L1 change of the evasion\nattack (the gradient quotient method, multiclass scenarios).a\nAveL1\nAccuracy\nSucc rate\nchange\n(%)\n(%)\n4532\n100\n94.8\nModel 1\n4571\n100\n94.8\nModel 2\n4429\n100\n95.0\nModel 3\n95.2\n100\n4475\nModel 4\n4610\n100\n95.0\nModel 5\n4927\n100\n89.0\nModel 1\n5251\n100\n89.2\nModel 2\n89.1\n100\n5317\nModel 3\n5311\n100\n89.2\nModel 4\n88.9\n100\n5066\nModel 5\na Each result regarding RBF SVM is based on 800 test samples,\nwhile each result on linear SVM is based on 5000 test samples.\n7.2 Attacking via Surrogate\nUp until now, we have implicitly assumed that the attacker\npossesses complete knowledge of the target classier, which\nmay be unrealistic in practice. Hence, we next examine at-\ntacks carried out via a surrogate. For example, in order to\nmislead a RBF SVM, the attacker rst trains their own RBF\nSVM on a similar dataset, builds the attack path of how a test\nsample should be modied, then applies it to the target SVM.\nPreviously, modication is terminated upon misclassica-\ntion or once 30 steps are taken. However, since there is no\nguarantee that the surrogate and target classiers misclassify\nthe test sample simultaneously, all test samples are modied\n30 times by the surrogate in this experiment; an attack is con-\nsidered as successful if the target classier also misclassi-\nxt  t  (f1 (xt )/f2 (xt ))\nes the adversarial sample within 30 steps. We modify the\nmethod as\nxt + t  (f1 (xt )/f2 (xt ))\n\nprior to surrogate\nmisclassication\notherwise\n\nxt+1 =\n\nRBF\n( = 0.5,\nC = 10)\n\nLinear\n(C = 103 )\n\nIn other words, before the test sample is misclassied by\nthe surrogate, it travels downhill, but after crossing the de-\ncision boundary it travels uphill. Otherwise the test case\ncontinues oscillating back and forth around the boundary.\nWe reuse the RBF SVMs trained in the last section, each\nof which serving as both surrogate (Si ) and target (Ti ) clas-\nsiers. As can be seen from Table 4, the success rates are\nall over 65%. Specically, those values inside the bracket\nare the success rates when the target classier misclassies\nbefore the surrogate, while the values outside are the overall\nsuccess rates. For comparison, the same experiments have\nbeen performed for linear SVMs, producing similar success\nrates (at around 60%), notably higher than previous ndings\n(around 40%) reported by Papernot et al. [2016a].\n\nTable 4: Success rate of evasion attacks via surrogate (RBF\nSVM).a\n\nT3\nT2\nT1\n69.8 (9.5)\n66.1 (9.2)\n100\nS1\n67.2 (8.4)\n100\n68.0 (8.2)\nS2\n100\n66.1 (9.4)\n65.0 (8.4)\nS3\n67.0 (10.6)\n67.2 (10.8)\n65.3 (10.0)\nS4\n69.4 (9.0)\n67.8 (8.9)\n67.0 (9.2)\nS5\na Each result is based on 800 test samples.\n\nT4\n68.3 (9.8)\n72.4 (10.4)\n67.3 (11.1)\n100\n69.4 (9.1)\n\nT5\n66.4 (7.1)\n67.9 (7.4)\n65.9 (8.5)\n65.3 (8.8)\n100\n\n7.3 Mimicry Attacks\nWe tested the gradient-quotient method for mimicry attacks.\nOur results show that in most cases this approach can suc-\ncessfully make the original digit misclassied as any of the\nother nine digits. Due to space limitations, those results are\nomitted.\n\n8 Conclusions and Future Work\nRecent studies have shown that it is relatively easy to fool\nmachine learning models via adversarial samples. In this pa-\nper, we demonstrate that the gradient-descent methodthe\nleading approach to generate adversarial sampleshas limi-\ntations against RBF SVMs, when the precision parameter \ncontrolling kernel smoothness is chosen properly. We nd\npredictable phase transitions of attack success occur at thresh-\nolds that are functions of geometric margin-like quantities\nmeasuring inter-class support vector distances. Our charac-\nterisation can be used to make RBF SVM more robust against\ncommon evasion and mimicry attacks.\nWe propose a new method for manipulating target samples\ninto adversarial instances, with experimental results showing\nthat this new method not only increases attack success rate,\nbut decreases the required changes made to input points.\nFor\nfuture work,\n(1)\nregarding the gradient-descent\nmethod, we intend to replicate and expand ndings for  and\nsmoothness in general, in other settings and for other classi-\ners. (2) We will explore suitability of our new generation ap-\nproach when the target is not an SVM, with direct attacks or\nSVM surrogates. (3) Further investigation into light-weight\nyet efcient countermeasures also serves as an important di-\nrection for future work.\n\n\fReferences\n[Alfeld et al., 2016] Scott Alfeld, Xiaojin Zhu, and Paul Bar-\nford. Data poisoning attacks against autoregressive mod-\nels. In AAAI, pages 14521458, 2016.\n[Barreno et al., 2006] Marco Barreno, Blaine Nelson, Rus-\nsell Sears, Anthony D. Joseph, and J. D. Tygar. Can ma-\nchine learning be secure? In AsiaCCS, pages 1625, 2006.\n[Barreno et al., 2010] Marco Barreno, Blaine Nelson, An-\nthony D. Joseph, and J. D. Tygar. The security of machine\nlearning. Machine Learning, 81(2):121148, 2010.\n[Biggio et al., 2012] Battista Biggio, Blaine Nelson, and\nPavel Laskov. Poisoning attacks against support vector\nmachines. In ICML, pages 18071814, 2012.\n[Biggio et al., 2013] Battista Biggio, Igino Corona, Davide\nMaiorca, Blaine Nelson, Nedim S rndi c, Pavel Laskov,\nGiorgio Giacinto, and Fabio Roli. Evasion attacks against\nIn ECML PKDD, pages\nmachine learning at test time.\n387402, 2013.\n[Br uckner and Scheffer, 2011] Michael Br uckner and Tobias\nScheffer. Stackelberg games for adversarial prediction\nproblems. In KDD, pages 547555, 2011.\n[Chang and Lin, 2011] Chih-Chung Chang and Chih-Jen\nLin. LIBSVM: A library for support vector machines.\nACM Transactions on Intelligent Systems and Technology,\n2(3):127, 2011.\n[Chang and Lin, 2016] Chih-Chung Chang and Chih-Jen\nLin.\nLIBSVM data: Classication (binary class).\nhttps://www.csie.ntu.edu.tw/cjlin/\nlibsvmtools/datasets/binary.html#\nfourclass, 2016.\n[Dalvi et al., 2004] Nilesh Dalvi,\nPedro Domingos,\nMausam, Sumit Sanghai, and Deepak Verma.\nAd-\nversarial classication. In KDD, pages 99108, 2004.\n[Goodfellow et al., 2014] Ian\nJ. Goodfellow,\nJonathon\nShlens, and Christian Szegedy. Explaining and harnessing\nadversarial examples. eprint arXiv:1412.6572, 2014.\n[Huang et al., 2011] Ling Huang, Anthony D.\nJoseph,\nBlaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar.\nAdversarial machine learning. In ACM AISec Workshop,\npages 4357, 2011.\n[LeCun et al., 1998a] Yann LeCun, Leon Bottou, Yoshua\nBengio, and Patrick Haffner. Gradient-based learning ap-\nplied to document recognition. Proc. IEEE, 86(11):2278\n2324, 1998.\n[LeCun et al., 1998b] Yann LeCun, Corinna Cortes, and\nChristopher J.C. Burges. The MNIST database of hand-\nwritten digits.\nhttp://yann.lecun.com/exdb/\nmnist/, 1998.\n[Li et al., 2016] Bo Li, Yining Wang, Aarti Singh, and\nYevgeniy Vorobeychik.\nData poisoning attacks on\nfactorization-based collaborative ltering. In NIPS, pages\n18851893, 2016.\n\n[Lowd and Meek, 2005] Daniel Lowd\nand Christopher\nIn KDD, pages 641647,\nMeek. Adversarial learning.\n2005.\n[Moosavi-Dezfooli et al., 2016a] Seyed-Mohsen Moosavi-\nDezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal\neprint\nFrossard. Universal adversarial perturbations.\narXiv:1610.08401, 2016.\n[Moosavi-Dezfooli et al., 2016b] Seyed-Mohsen Moosavi-\nDezfooli, Alhussein Fawzi, and Pascal Frossard. Deep-\nFool: A simple and accurate method to fool deep neural\nnetworks. In CVPR, pages 25742582, 2016.\n[Nelson et al., 2012] Blaine Nelson, Benjamin I. P. Rubin-\nstein, Ling Huang, Anthony D. Joseph, Steven J. Lee,\nSatish Rao, and J. D. Tygar. Query strategies for evad-\ning convex-inducing classiers. J. Machine Learning Re-\nsearch, 13(1):12931332, 2012.\n[Nguyen et al., 2015] Anh Nguyen, Jason Yosinski, and Jeff\nClune. Deep neural networks are easily fooled: High con-\ndence predictions for unrecognizable images. In CVPR,\npages 427436, 2015.\n[Papernot et al., 2016a] Nicolas Papernot,\nPatrick Mc-\nDaniel, and Ian Goodfellow. Transferability in machine\nlearning:\nfrom phenomena to black-box attacks using\nadversarial samples. eprint arXiv:1605.07277, 2016.\n[Papernot et al., 2016b] Nicolas Papernot,\nPatrick Mc-\nDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik,\nand Ananthram Swami.\nPractical black-box attacks\nagainst deep learning systems using adversarial examples.\neprint arXiv:1602.02697, 2016.\n[Papernot et al., 2016c] Nicolas Papernot,\nPatrick Mc-\nDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik,\nand Ananthram Swami. The limitations of deep learning\nIn EuroS&P, pages 372387,\nin adversarial settings.\n2016.\n[Rubinstein et al., 2009] Benjamin I. P. Rubinstein, Blaine\nNelson, Ling Huang, Anthony D. Joseph, Shing-hon Lau,\nSatish Rao, Nina Taft, and J. D. Tygar. ANTIDOTE: un-\nderstanding and defending against poisoning of anomaly\ndetectors. In IMC, pages 114, 2009.\n[Russu et al., 2016] Paolo Russu, Ambra Demontis, Battista\nBiggio, Giorgio Fumera, and Fabio Roli. Secure kernel\nIn ACM AISec Work-\nmachines against evasion attacks.\nshop, pages 5969, 2016.\n[Samaria and Harter, 1994] Ferdinando Samaria and Andy\nHarter. Parameterisation of a stochastic model for human\nface identication. In IEEE Workshop on Applications of\nComputer Vision, pages 138142, 1994.\n[Szegedy et al., 2013] Christian\nWojciech\nSzegedy,\nZaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus.\nIntriguing properties of\nneural networks. eprint arXiv:1312.6199, 2013.\n\n\f", 
        "tag": "Cryptography and Security", 
        "link": "https://arxiv.org/list/cs.CR/new"
    }, 
    {
        "text": "Streaming Pattern Matching with d Wildcards\n\nShay Golan\nBar Ilan University\ngolansh1@cs.biu.ac.il\n\nTsvi Kopelowitz\nUniversity of Waterloo\nkopelot@gmail.com\n\nEly Porat\nBar Ilan University\nporately@cs.biu.ac.il\n\nApril 7, 2017\n\nAbstract\n\nIn the pattern matching with d wildcards problem one is given a text T of length n and a\npattern P of length m that contains d wildcard characters, each denoted by a special symbol\n(cid:48) ?(cid:48) . A wildcard character matches any other character. The goal is to establish for each\nm-length substring of T whether it matches P . In the streaming model variant of the pattern\nmatching with d wildcards problem the text T arrives one character at a time and the goal\nis to report, before the next character arrives, if the last m characters match P while using\nonly o(m) words of space.\nIn this paper we introduce two new algorithms for the d wildcard pattern matching\nproblem in the streaming model. The rst is a randomized Monte Carlo algorithm that\nis parameterized by a constant 0    1. This algorithm uses O(d1 ) amortized time\nper character and O(d1+ ) words of space. The second algorithm, which is used as a black\nbox in the rst algorithm, is a randomized Monte Carlo algorithm which uses O(d + log m)\nworst-case time per character and O(d log m) words of space.\n\n7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nS\nD\n.\ns\nc\n[\n \n \n1\nv\n6\n4\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\n.\n\nPart of this work took place while the second author was at University of Michigan. This work is supported\nin part by the Canada Research Chair for Algorithm Design, NSF grants CCF-1217338, CNS-1318294, and\nCCF-1514383, by ISF grant 1278/16, and by the BIU Center for Research in Applied Cryptography and Cyber\nSecurity in conjunction with the Israel National Cyber Bureau in the Prime Ministers Oce .\n\n\f1\n\nIntroduction\n\nWe investigate the pattern matching with d wildcards problem (PMDW) in the streaming model.\nLet  be an alphabet and let (cid:48)?(cid:48) /  be a special character called the wildcard character which\nmatches any character in . The PMDW problem is dened as follows. Given a text string\nT = t0 t1 . . . tn1 over  and a pattern string P = p0p1 . . . pm1 over alphabet   {?} such that\nP contains exactly d wildcard characters, report all of the occurrences of P in T . This denition\nof a match is one of the most well studied problems in pattern matching [22, 35, 26, 28, 19, 10].\n\nThe streaming model. The advances in technology over the last decade and the massive\namount of data passing through the internet has intrigued and challenged computer scientists,\nas the old models of computation used before this era are now less relevant or too slow. To this\nend, new computational models have been suggested to allow computer scientists to tackle these\ntechnological advances. One prime example of such a model is the streaming model [1, 25, 34, 29].\nPattern matching problems in the streaming model are allowed to preprocess P into a data\nstructure that uses space that is sublinear in m (notice that space usage during the preprocessing\nphase itself is not restricted). Then, the text T is given online, one character at a time, and the\ngoal is to report, for every integer   m  1, whether tm+1 . . . t matches P . This reporting\nmust take place before t+1 arrives. Throughout this paper we let  denote the index of the last\ntext character that has arrived.\nFollowing the breakthrough result of Porat and Porat [36], recently there has been a rising\ninterest in solving pattern matching problems in the streaming model [7, 20, 33, 8, 27, 14, 15].\nHowever, this is the rst paper to directly consider the important wildcard variant.\n\nRelated work. Notice that one way for solving PMDW (not necessarily in the streaming\nmodel), is to treat (cid:48)?(cid:48) as a regular character, and then run an algorithm that nds all occurrences\nof P (that does not contain any wildcards) in T with up to k = d mismatches. This is known\nas the k-mismatch problem [32, 37, 2, 13, 12, 17, 15]. The most recent result by Cliord et\n\nal. [15] for the k-mismatch problem in the streaming model implies a solution for PMDW in the\nstreaming model that uses O(d2 polylog m) words1 of space and O(\nd log d + polylog m) time\nper character. Notice that Cliord et al. [15] focused on solving the more general k-mismatch\nproblem.\nWe mention that while our work is in the streaming model, in the closely related online model\n(see [18, 16]), which is the same as the streaming model without the constraint of using sublinear\nspace, Cliord et al. [11] presented an algorithm, known as the black box algorithm, which solves\nseveral pattern matching problems. When applied to PMDW, the black box algorithm uses\nO(m) words of space and O(log2 m) time per arriving text character. In the oine model the\nmost ecient algorithms for PMDW take O(n log m) time and were introduced by Cole and\nHariharan [19] and by Cliord and Cliord [10].\n\n1.1 New results\n\nWe improve upon the work of Cliord et al. [15], for the special case that applies to PMDW, by\nintroducing the following algorithms (the O notation hides logarithmic factors). Notice that\nTheorem 2 improves upon the results of Cliord et al. [15] whenever  > 1/2. We also emphasize\nthat our proof of Theorem 2 makes use of Theorem 1.\n\n1We assume the RAM model where each word has size of O(log n) bits.\n\n1\n\n\fTheorem 1. There exists a randomized Monte Carlo algorithm for the PMDW problem in the\nstreaming model that succeeds with probability 1  1/poly(n), uses O(d log m) words of space and\nspends O(d + log m) time per arriving text character.\nTheorem 2. For any constant 0    1 there exists a randomized Monte Carlo algorithm for\nthe PMDW problem in the streaming model that succeeds with probability 1  1/poly(n), uses\nO(d1+ ) words of space and spends O(d1 ) amortized time per arriving text character.\n\n1.2 Algorithmic Overview\n\nOur algorithms make use of the notion of a candidate, which is a location in the last m indices of\nthe current text that is currently considered as a possible occurrence of P . As more characters\narrive, it becomes clear if this candidate is an actual occurrence or not. In general, an index\ncontinues to be a candidate until the algorithm encounters proof that the candidate is not a\nvalid occurrence (or until it is reported as a match). The algorithm of Theorem 1 works by\nobtaining such proofs eciently.\n\nOverview of algorithm for Theorem 1. For the streaming pattern matching problem\nwithout wildcards, the algorithms of Porat and Porat [36] and Breslauer and Galil [7] have\nthree ma jor components2 . The rst component is a partitioning of the interval [0, m  1] into\npattern intervals of exponentially increasing lengths. Each pattern interval [i, j ] corresponds to\na text interval [  j + 1,   i + 1], where  is the index of the last text character that arrived3 .\nNotice that when a new text character arrives, the text intervals are shifted by one location. The\nsecond component maintains all of the candidates in a given text interval. This implementation\nleverages periodicity properties of strings in order to guarantee that the candidates in a given\ntext interval form an arithmetic progression, and thus can be maintained with constant space.\nThe third component is a ngerprint mechanism for testing if a candidate is still valid. Whenever\nthe border of a text interval passes through a candidate, that candidate is tested.\nThe main challenge in applying the above framework for patterns with wildcards comes\nfrom the lack of a good notion of periodicity which can guarantee that the candidates in a text\ninterval form an arithmetic progression. To tackle this challenge, we design a new method for\npartitioning the pattern into intervals, which, combined with new fundamental combinatorial\nproperties, leads to an ecient way for maintaining the candidates in small space. In particular,\nwe prove that with our new partitioning there are at most O(d log m) candidates that are not\npart of any arithmetic progression for any text interval. Remarkably, the proof bounding the\nnumber of such candidates uses a more global perspective of the pattern, as opposed to the\ntechniques used in non-wildcard results.\n\nOverview of algorithm for Theorem 2. The algorithm of Theorem 2 uses the algorithm\nof Theorem 1 (with a minor adaptation) combined with a new combinatorial perspective on\nperiodicity that applies to strings with wildcards. The notion of periodicity in strings (without\nwildcards) and its usefulness are well studied [21, 31, 36, 7, 24, 23]. However, extending the\nusefulness of periodicity to strings with wildcards runs into diculties, since the notions are\n\n2The algorithms of Porat and Porat [36] and Breslauer and Galil [7] are not presented in this way. However, we\nnd that this new way of presenting our algorithm (and theirs) does a better job of explaining what is going on.\n3The rst pattern interval starts at 0, and so the last text interval ends at location  + 1, which is a location\nof a text character that has yet to arrive. To understand why this convention is appropriate, notice that initially\nevery text location should be considered as a candidate, but in order to save space we only address such candidates\na moment before their corresponding character arrives since this is the rst time the algorithm can obtain proof\nthat the candidate is not a match.\n\n2\n\n\feither too inclusive or too exclusive (see [5, 4, 6, 9, 38]). Thus, we introduce a new denition of\nperiodicity, called the wildcard-period length that captures, for a given pattern with wildcards,\nthe smallest possible average distance between occurrences of the pattern in any text. See\nDenition 6. For a string S with wildcards, we denote the wildcard-period length of S by S .\nLet P  be the longest prex of P such that P   d . The algorithm of Theorem 2 has two\nmain components, depending on whether P  = P or not. In the case where P  = P , the algorithm\ntakes advantage of the wildcard-period length of P being small, which, together with techniques\nfrom number theory and new combinatorial properties of strings with wildcards, allows to spend\nonly O(1) time per character and uses O(d1+ ) words of space. This is summarized in Theorem 17.\nOf particular interest is Lemma 16 which combines number theory with combinatorial string\nproperties in a new way. We expect these ideas to be useful in other applications.\nIf P  (cid:54)= P , then we use the algorithm of Theorem 17 to locate occurrences of P  , and by\nmaximality of P  , occurrences of prexes of P that are longer than P  must appear far apart\n(on average). These occurrences are given as input to a minor adaptation of the algorithm of\nTheorem 1 in the form of candidates. Utilizing the large average distance between candidates,\nwe obtain an O(d1 ) amortized time cost per character.\n\n2 Preliminaries\n\n2.1 Periods\nWe assume without loss of generality that the alphabet is  = {1, 2, . . . , n}. For a string\nS = s0s1 . . . s(cid:96)1 over  and integer 1  k  (cid:96) , the substring s0s1 . . . sk1 is called a prex of S\nand s(cid:96)k . . . s(cid:96)1 is called a sux of S .\nA prex of S of length i  1 is a period of S if and only if sj = sj+i for every 0  j  (cid:96)  i  1.\nThe shortest period of S is called the principle period of S , and its length is denoted by S . If\nS  |S |\n2 we say that S is periodic.\nThe following lemma is due to Breslauer and Galil [7].\n\nLemma 3 ([7, Lemma 3.1]). Let u and v be strings such that u contains at least three occurrences\nof v . Let t1 < t2 <    < th be the locations of al l occurrences of v in u. Assume that h  3 and\nthat for i = 1, . . . , h  2, we have ti+2  ti  |v |. Then, the sequence (t1 , t2 , . . . , th ) forms an\narithmetic progression with dierence v .\n\nThe following lemmas follow from Lemma 3.\n\nLemma 4. Let v be a string of length (cid:96) and let u be a string of length at most 2(cid:96). If u contains\nat least three occurrences of v then the distance between any two occurrences of v in u is a\nmultiple of v and v is a periodic string.\nProof. Let 0  c1 < c2 < c3  |u|  1 be three occurrences of v in u. Thus, c3  (|u|  1)  (|v | \n1)  2(cid:96)  (cid:96) = (cid:96), and so c3  c1  (cid:96). Therefore, by Lemma 3, all the occurrences of v in u form an\narithmetic progression with common dierence v . In particular, the distance between any two\noccurrences of v in u is a multiple of v . Hence, v + v  (c3  c2 ) + (c2  c1 ) = c3  c1  (cid:96) = |v |\nand v  |v |\n2 . Thus, by denition, v is a periodic string.\n\nLemma 5. Let u be a periodic string over  with principle period length u . If v is a substring\nof u of length at least 2u then u = v .\n\nProof. Since v is a substring of u, we have by denition that u is a period length of v , and thus\nv  u by the minimality of v .\n\n3\n\n\fIt only remains to prove that u  v , which we do by showing that v is a period length of\nu. We denote u = u0u1 . . . u|u|1 .\nLet 0  i < |u|  v be an index in u, we have to prove that ui = ui+v .\n(cid:109)\n(cid:108) ai\nLet a be an index such that v occurs in u in position a, thus uaua+1 . . . ua+2u1 is a substring\nof both u and v . Since u is a period length of u, ui = ui+z u for any z  Z if 0  i + z  u < |u|.\n(cid:109)\n(cid:108) ai\n. Let b = i + z  u . Notice that\nIn particular, for z =\nwe have that ui = u\nu\nu\ni+\na  b < a + u and a  b + v < a + 2u . Therefore, b and b + v are both indices of characters\nu\nin v , and thus ub = ub+v . Hence, we have that ui = ui+z u = ui+z u+v = ui+v , where the\nlast equality is based again on the fact that u is a period length of u.\n\nPeriods and wildcards. For a string u with no wildcards, there is an inverse relationship\nbetween the maximum number of occurrences of u in a text of a given length and the principle\nperiod length of u. Next, we dene the wildcard-period length of a string over   {?} which\ncaptures a similar type of relationship for strings with wildcards. The usefulness of this denition\nfor our needs is discussed in more detail in Section 6. Let occ(S (cid:48) , S ) be the number of occurrences\nof a string S in a string S (cid:48) .\n(cid:26)(cid:24)\n(cid:25)(cid:27)\nDenition 6. For a string S over   {?}, the wildcard-period length of S is\n|S |\nS = min\nocc(S (cid:48) , S )\nS (cid:48)2|S |1\nFor the following let u, v  (cid:83)n\n2.2 Fingerprints\nBreslauer and Galil [7] proved the existence of a sliding ngerprint function  : (cid:83)n\ni=0 i be two strings of size at most n. Porat and Porat [36] and\ni=0 i  [nc ],\nfor some constant c > 0, which is a function where:\n1. If |u| = |v | and u (cid:54)= v then (u) (cid:54)= (v) with high probability (at least 1  1\nnc1 ).\n2. The sliding property: Let w=uv be the concatenation of u and v . If |w|  n then given\nthe length and the ngerprints of any two strings from u,v and w, one can compute the\nngerprint of the third string in constant time.\n\n.\n\n3 A Generic Algorithm\n\nWe start with a generic algorithm (pseudo-code is given in Figure 1) for solving pattern matching\nproblems in the streaming model. With proper implementations of the algorithms components,\nthe algorithm solves the PMDW problem. The generic algorithm makes use of the notion of a\ncandidate. Initially every text index c is considered as a candidate for a pattern occurrence from\nthe moment tc1 arrives. An index continues to be a candidate until the algorithm encounters\nproof that the candidate is not a valid occurrence (or until it is reported as a match). A candidate\nis alive until such proof is given.\nThe generic algorithm is composed of three conceptual parts that aect the complexities of\nthe algorithm. An example of an execution of the generic algorithm appears in Figures 2 and 3:\n Pattern and text intervals. The rst part is an ordered list I = (I0 , . . . , Ik ) of intervals.\nThe disjoint union of the intervals of I is exactly [0, m  1] and the intervals are ordered\nsuch that I = [i, j ] precedes I (cid:48) = [i(cid:48) , j (cid:48) ] if and only if j < i(cid:48) . Each interval I  I is called\n\n4\n\n\fInit()\n1 Q0 .Enqueue(0)\n\nProcess-Character(t )\nfor h = 0 to k\n1\nc = Qh .Dequeue()\n2\n3\nif c exists and c is valid\nif h = k\n4\n5\nreport c as a match\n6\nelse Qh+1 .Enqueue(c)\n7 Q0 .Enqueue( + 1)\n\nFigure 1: Generic Algorithm. The purpose of the initialization is to consider location 0 as a\ncandidate before any candidate has arrived.\n\na pattern interval. For each pattern interval I = [i, j ]  I we dene a corresponding\ntext interval, text interval(I , ) = [  j + 1,   i + 1]. When character t arrives,\na text location c  text interval(I , ) is a candidate if and only if tc    tc+i1 matches\np0    pi1 . The candidate set C (I , ) is the set of text positions in text interval(I , )\nwhich are candidates right after the arrival of t .\n Candidate queues. The second conceptual part of the generic algorithm is an imple-\nmentation of a candidate-queue data structure. For any interval I  I , the algorithm\nmaintains a candidate queue QI . At any time , which is the time right after t arrives,\nbut before t+1 arrives, QI stores a (possibly implicit) representation of C (I , ). Thus,\nthe operations of the data structure are time-dependent. Candidate-queues support the\nfollowing operations.\nDenition 7. A candidate-queue for an interval [i, j ] = I  I supports the fol lowing\noperations at time, where t is the last text character that arrived.\n1. Enqueue(): add c =   i + 1 to the candidate-queue.\n2. Dequeue(): remove and return a candidate c =   j , if such a candidate exists.\n\nSince there is a bijection between pattern intervals and text intervals we say that a\ncandidate-queue that is associated with pattern interval I is also associated with the\ncorresponding text interval text interval(I , ).\n Assassinating candidates. The third conceptual part addresses the following. When\na new text character arrives, all the text intervals move one position ahead, and some\ncandidates leave some text intervals and their corresponding candidate sets. The third\nconceptual part is a mechanism for testing if a candidate is valid after that candidate\nleaves a candidate set. This mechanism is used in order to determine if the candidate\nshould enter the candidate-queue of the next text interval, or be reported as a match if\nthere are no more text intervals.\n\nThe implementation of each of the three components controls the complexities of the algorithm.\nMinimizing the number of intervals reduces the number of candidates leaving text intervals\n\n5\n\n\fat a given time. Ecient implementations of the candidate-queue operations and testing if a\ncandidate is valid control both the space usage and the amount of time spent on each candidate\nthat leaves an interval. Notice that the implementations of these components may depend on\neach other, which is also the case in our solution.\n\nFigure 2: Example of a pattern and its arbitrarily chosen pattern intervals. The pattern length\nis 10 and the pattern intervals are [0, 3], [4, 7] and [8, 9].\n\nFigure 3: Example of an execution of the generic algorithm with the pattern of Figure 2. In\neach row a new text character arrives. The bold borders illustrate the text intervals. Each blue\ncell is a position of a candidate and the green cell corresponds to a match.\nWhen t52 arrives, the candidate c1 = 45 is tested, since it exits a text interval. The candidate c1\nremains alive because abababaa is a prex of the pattern. Notice that at this time the candidate\nc2 = 47 in not a valid occurrence of the pattern, but, the algorithm does not remove c2 until c2\nreaches the end of the text interval.\nWhen t54 arrives, the candidates c1 = 45 and c2 = 47 are tested, as they have reached the end of\ntheir text intervals. At this time, c2 is removed since the text ababaaab is not a prex of the\npattern. The candidate c1 remains alive and is reported as a match, since c1 reached the end of\nthe last text interval.\n\n6\n\nabababaaab\"#$%&()*+[0\t\t\t\t\t\t\t\t\t\t,\t\t\t3][4\t\t\t\t\t\t\t\t\t\t,\t\t\t7][8\t\t\t,\t\t\t9]babacabaabab\"#\"$\"%\"&\"\"\"\"(\")\"*\"+#$[43\t,\t44][45\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t48][49\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t52]ababacabaabab\"#\"$\"%\"&\"\"\"\"(\")\"*\"+#$%[44\t,\t45][46\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t49][50\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t53]aababacabaabab\"#\"$\"%\"&\"\"\"\"(\")\"*\"+#$%&[45\t,\t46][47\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t50][51\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t54]baababacabaabab\"#\"$\"%\"&\"\"\"\"(\")\"*\"+#$%&\"[46\t,\t47][48\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t51][52\t\t\t\t\t\t\t,\t\t\t\t\t\t\t\t55]\fA nave implementation. The following nave implementation of the generic algorithm is\nhelpful for gaining intuition as to how the algorithm works. Let Inave = ([0, 0], [1, 1], . . . , [m \n1, m  1]). The implementation of candidate queue QI explicitly stores the set C (I , ) at time .\nNotice that C (I , ) contains at most one candidate. The task of verifying that a candidate c\nis valid in between text intervals is a straightforward comparison of pc with t . Each such\ncomparison costs O(1) time. The runtime of the algorithm is (m) time per character in the\nworst-case, and the space usage is also (m).4 We refer to this algorithm as the nave algorithm.\n\nUsing ngerprints.\nIf there are no wildcards in P , then one can use the following nger-\nprint based algorithm that veries the validity of a candidate c only once all the characters\ntc , tc+1 , . . . , tc+m1 have arrived. This algorithm is closely related to the Karp and Rabin [30]\nalgorithm. The algorithm uses a partitioning of [0, m  1] into only one interval containing all of\n[0, m  1].\nThe algorithm maintains the text ngerprint which is the ngerprint of the text from its\nbeginning up to the last arriving character. For each text index c, just before tc arrives the\nalgorithm creates a candidate for the index c and stores the text ngerprint (t0 t1 . . . tc1 ) as\nsatellite information of the candidate c. Then, c (together with its satellite information) is added\nto the candidate-queue via the Enqueue() operation. When the character tc+m1 arrives, the\ntext ngerprint is (t0 . . . tc+m1 ). At this time, the algorithm uses the Dequeue() operation to\nextract c together with (t0 t1 . . . tc1 ) from the candidate-queue. Then, the algorithm tests if c\nis valid by computing (tc . . . tc+m1 ) from the current text ngerprint (t0 t1 . . . tc+m1 ) and\nthe ngerprint (t0 t1 . . . tc1 ) (using the sliding property of the ngerprint function), and then\ntesting if (tc . . . tc+m1 ) equals (p0 . . . pm1 ). The ngerprint algorithm spends only constant\ntime per text character, but, like the nave algorithm, uses (m) words of space to store the\ncandidate-queue.\n\n3.1 Fingerprints with Wildcards\n\nUsing ngerprints together with wildcards seems to be a dicult task, since for any string S with\nx wildcards there are ||x dierent strings over  that match the string S . Each one of these\ndierent strings may have a dierent ngerprint and therefore there are (||x ) ngerprints to\nstore, which is not feasible. In order to still use ngerprints for solving PMDW we use a special\npartitioning of [0, m  1], which is described in Section 4. The partitioning in Section 4 is based\non the following preliminary partitioning.\n\nThe preliminary partitioning. We use a representation of P as P = P0?P1? . . .?Pd where\neach subpattern Pi contains only characters from  (and may also be an empty string). Let\nW = (w1 , w2 , . . . , wd ) be the indices of wildcards in P such that for all 1  i < d we have\nwi < wi+1 . The interval [0, m  1] is partitioned into pattern intervals as follows:\nJ = ([0, w1  1], [w1 , w1 ], [w1 + 1, w2  1], . . . , [wd , wd ], [wd + 1, m  1]).\n\nSince some of the pattern intervals in this partitioning could be empty, we discard such intervals.\nThe pattern intervals of the form [wi , wi ] are called wildcard interval s and the other pattern\nintervals are called regular intervals. Notice that for a text index c, the substring tc . . . tc+m1\nmatches P if and only if for each regular interval [i, j ], tc+i . . . tc+j = pi . . . pj .\n\n4For example, if the pattern is aa . . . a = am and the text is an , then each candidate c is alive as long as the\ncharacters tc , . . . tc+m1 arrive. Therefore, after the arrival of the rst m  1 characters, any additional arriving\ncharacter is compared with m pattern characters.\n\n7\n\n\fA preliminary algorithm. Given the preliminary partition J , one could use the following\nalgorithm for testing the validity of a candidate c whenever it leaves a text interval. During the\ninitialization of the algorithm we precompute and store the ngerprints for all of the subpatterns\ncorresponding to regular intervals. Each time a candidate c is added to a candidate-queue for\ninterval [i, j ]  J via the Enqueue() operation, the algorithm stores the current text ngerprint\n(t0 . . . tc+i1 ) together with the candidate c. When the character tc+j arrives, the text ngerprint\nis (t0 . . . tc+j ). At this time, the algorithm uses the Dequeue() operation to extract c together\nwith (t0 t1 . . . tc+i1 ) from the candidate-queue of interval [i, j ]. If [i, j ] is a regular interval,\nthen the algorithm tests if c is valid, and removes (assassinates) c if it is not. This validity test is\nexecuted by applying the sliding property of the ngerprint function to compute (tc+i . . . tc+j )\nfrom the current text ngerprint (t0 t1 . . . tc+j ) and the ngerprint (t0 t1 . . . tc+i1 ), and then\ntesting if (tc+i . . . tc+j ) is the same as (pi . . . pj ). If [i, j ] is a wildcard interval then c stays\nalive without any testing.\nA nave implementation of the candidate queues provides an algorithm that costs O(d) time\nper character, but uses (m) words of space. To overcome this space usage we employ a more\ncomplicated partitioning, which, together with a modication of the requirements from the\ncandidate-queues, allows us to design a data structure that uses much less space. However, this\nspace eciency comes at the expense of a slight increase in the time per character.\n\n4 The Partitioning\n\nThe key idea of the new partitioning is to use the partitioning of Section 3.1 as a preliminary\npartitioning, and then perform a secondary partitioning of the regular pattern intervals, thereby\ncreating even more regular intervals. As mentioned, the intervals are partitioned in a special\nway which allows us to implement candidate-queues in a compact manner (see Section 5).\nThe following denition is useful in the next lemma.\nDenition 8. For an ordered set of intervals I = (I0 , I1 , . . . Ik ) and for any integer 0  x  k ,\nlet I (x) = max0yx {|Iy |} be the length of the longest interval in the sequence I0 , . . . Ix . When\nI is clear from context we simply write (x) = I (x)\n\nThe following lemma provides a partitioning which is used to improve the preliminary\npartitioning algorithm. The properties of the partitioning that are described in the statement\nof the lemma are essential for our new algorithm. The most essential property is property 3,\nsince it guarantees that for each pattern interval I = [i, j ], there exists a substring of P prior to\npi and with no wildcards whose length is |I |. If this substring is not periodic, then for any ,\nC (I , ) does not contain more than two candidates. If this substring is periodic, then we show\nhow to utilize the periodicity of the string in order to eciently maintain all the candidates in\nC (I , ) for any  (see Section 5). In the proof of the lemma we introduce a specic partitioning\nwhich has all of the stated properties.\n\nLemma 9. Given a pattern P of length m with d wildcards, there exists a partitioning of the\ninterval [0, m  1] into subintervals I = (I0 , I1 . . . , Ik ) which has the fol lowing properties:\n\n1. If I = [i, j ] is a pattern interval then pi . . . pj either corresponds to exactly one wildcard\nfrom P (and so j = i) or it is a substring that does not contain any wildcards.\n\n2. k = O(d + log m).\n3. For each regular pattern interval I = [i, j ] with |I | > 1, the length i prex of P contains a\nconsecutive sequence of |I | non-wildcard characters.\n\n8\n\n\fFigure 4: The general case: for each Jh  J we rst create two intervals of length h and then\nwe iteratively create pattern intervals where the length of each pattern interval is double the\nlength of the previous pattern interval.\n\n4. |{I (0), I (1) . . . I (k)}| = O(log m).\n\nProof. We introduce a secondary partitioning of the preliminary partitioning described in\nSection 3.1, and prove that the secondary partitioning has all the required properties; see\nFigures 4, 5 and 6. Recall that we use a representation of P as P = P0?P1? . . .?Pd . Let Jh be\nthe preliminary pattern interval corresponding to Ph . The secondary partitioning is executed\non the pattern intervals J = (J0 , J1 , . . . , Jd ), where the partitioning of Jh is dependent on the\npartitioning of J0 , . . . , Jh1 . Thus, for h > 0, the secondary partitioning of Jh takes place only\nafter the secondary partitioning of Jh1 .\nWhen partitioning pattern interval Jh = [i, j ], let gh be the number of pattern intervals in\nthe secondary partitioning of [0, i  1], and let h be the length of the longest pattern interval in\nthe secondary partitioning of [0, i  1]. For the rst pattern interval let 0 = 1. If j  i + h  1\nthen the only pattern interval is all of Jh . If j  i + 2  h  1 then we create the pattern intervals\n[i, i + h  1] and [i + h , j ]. Otherwise, we rst create the pattern intervals [i, i + h  1] and\n[i + h , i + 2  h  1]5 , and for as long as there is enough room in the remaining preliminary\npattern interval Jh (between the position right after the end of the last secondary pattern interval\nthat was just created and j ) we iteratively create pattern intervals where the length of each\npattern interval is double the length of the previous pattern interval. Once there is no more\nroom left in Jh , let (cid:96) be the length of the last pattern interval we created. If the remaining part\nof the preliminary pattern interval is of length at most (cid:96), then we create one pattern interval for\nall the remaining preliminary pattern interval. Otherwise we create two pattern intervals, the\nrst pattern interval of length (cid:96) and the second pattern interval using the remaining part of Jh .\nThe secondary partitioning implies all of the desired properties:\n\nProperty 1. Being that the secondary partitioning is a sub partitioning of the preliminary\npartitioning and the preliminary partitioning already had this property, then the secondary\npartitioning has this property as well.\n\nProperty 2. For a subpattern Ph , the length of every pattern interval created from Jh during\nthe secondary partitioning, except for the rst two pattern intervals and possibly also the last\ntwo pattern intervals, is at least twice the length of the longest pattern interval preceding it. So\nthe total number of such pattern intervals is O(log m). The number of other regular pattern\nintervals is at most 4(d + 1). Additionally, there are d wildcard pattern intervals. So the total\nnumber of pattern intervals is at most 4(d + 1) + d + O(log m) = O(d + log m).\nProperty 3. If there is a regular pattern interval I (cid:48) = [i(cid:48) , j (cid:48) ] such that j (cid:48) < i and |I (cid:48) |  |I |,\nthen the subpattern associated with I (cid:48) meets the requirement.\n5The choice of having the rst two intervals to be of the same length h is in order to guarantee the third\nproperty in the lemma, as shown below.\n\n9\n\n1124%2%?%=%1=max.:0%134567894%,%7878<478<6%\fFigure 5: Once there is no more room left in Jh , if the remaining interval is of length at most\n(cid:96) (the top case), then we create one pattern interval for all the remaining interval. Otherwise\n(the bottom case) we create two pattern intervals, the rst pattern interval of length (cid:96) and the\nsecond pattern interval using the remaining part of Jh .\n\nIf there is no such pattern interval, it must be the case that the length of I is twice the\nlength of the pattern interval preceding I , and I is contained in a preliminary pattern interval\nJh for some h. Let the length of the rst pattern interval created in Jh be denoted by h . Let\nIh,1 , Ih,2 , . . . Ih,r be the rst r pattern intervals created in Jh such that Ih,r = I . The length of\nany pattern interval Ih,r (cid:48) for 1 < r (cid:48)  r is 2r (cid:48)2 h (since |Ih,1 | = |Ih,2 | = h , and for 2 < r (cid:48)  r\nwe have |Ih,r (cid:48) | = 2|Ih,r1 |), and in particular the length of I is 2r2 h . Recall that I = [i, j ]. The\nIh,r (cid:48) for r (cid:48) < r . These lengths sum up to (1 + (cid:80)r1\nlength of the prex of Ph up to the index i is the sum of the lengths of all the pattern intervals\nr (cid:48)=2 2r (cid:48)2 )h = 2r2 h = |I |. So the prex of Ph\nfullls the requirement.\nProperty 4. We prove a stronger claim: for each 0  h  k , I (h) is a power of 2.\nThis is true by induction. The rst pattern interval is of length 1, and therefore I (0) = 1 = 20 .\nIf |Ih |  I (h  1) then I (h) = I (h  1) which is a power of 2 by the induction hypothesis.\nOtherwise, if |Ih | > I (h  1) then by the secondary partitioning algorithm |Ih | = 2I (h  1),\nand I (h) = 2I (h  1). Hence I (h) is also a power of 2.\nThe largest pattern interval is at most of length m, and therefore there are at most (cid:100)log m(cid:101)\ndierent values in {I (0), I (1), . . . , I (k)}.\n\n5 The Candidate-ngerprint-queue\n\nThe algorithm of Theorem 1 is obtained via an implementation of the candidate-queues that uses\nO(d log m) words of space, at the expense of having O(d + log m) intervals in the partitioning.\nSuch space usage implies that we do not store all candidates explicitly. This is obtained by\nutilizing properties of periodicity in strings. Since candidates are not stored explicitly, we cannot\nstore explicit information per candidate, and in particular we cannot explicitly store ngerprints.\nOn the other hand, we are still interested in using ngerprints in order to perform assassinations.\nTo tackle this, we strengthen our requirements from the candidate-queue data structure to\nreturn not just the candidate but also the ngerprint information that is needed to perform the\ntest of whether the candidate is still valid. For our purposes, this data structure cannot explicitly\nmaintain all the ngerprints information. Thus, we extend the denition of a candidate-queue\nto a candidate-ngerprint-queue as follows.\n\n10\n\n\fFigure 6: Example of patterns and their intervals in the secondary partitioning. Each bold\nrectangle corresponds to an interval in the partition.\n\nDenition 10. A candidate-ngerprint-queue for an interval [i, j ] = I  I supports the fol lowing\noperations, where t is the last text character that arrived.\n1. Enqueue((t0 . . . ti )): add c =   i + 1 to the candidate-queue.\n2. Dequeue(): remove and return a candidate c =   j , if such a candidate exists, together\nwith (t0 . . . tc1 ) and (t0 . . . tc+i1 ).\n\nIn order to reduce clutter of presentation, in the rest of this section we refer to the candidate-\nngerprint-queue simply as the queue.\n\n5.1 Implementation\n\nOur implementation of the queue assumes that we use a partitioning that has the properties\nstated in Lemma 9. Let I = [i, j ] be a pattern interval in the partitioning and let c be a candidate\nfrom C (I , ). The entrance prex of c is the substring tc . . . tc+i1 , and the entrance ngerprint\nis (tc . . . tc+i1 ). By denition, since c  C (I , ), the entrance prex of c matches p0 . . . pi1\n(which may contain wildcards). Recall that a candidate c is inserted into QI together with\n(t0 . . . tc1 ), which we call the candidate ngerprint of c.\n\nSatellite information. The implementation associates each candidate c with satel lite in-\nformation (SI), which includes the candidate ngerprint and the entrance ngerprint of the\ncandidate. The SI of a candidate combined with the sliding property of ngerprints are crucial\nfor the implementation of the queue. When c is added to QI , for some I = [i, j ], we compute\nthe entrance ngerprint of c from the candidate ngerprint and from (t0 . . . tc+i1 ) which is\nthe text ngerprint at that time. When c is removed from QI , we compute (t0 . . . tc+i1 ) in\nconstant time from the SI of c. See Figure 7.\n\nArithmetic progressions and entrance prexes.\nIn order to implement the queue using\na small amount of space, we distinguish between two types of candidates for each interval\nI = [i, j ]  I . The rst type are candidates that share a specic entrance prex, uI , which is\ndened solely by p0 . . . pi1 and is chosen such that if there are more than two candidates in\nC (I , ) with the same entrance prex then this entrance prex must be uI (see Lemma 11). In\nLemma 12 we prove that all the candidates in C (I , ) that have entrance prex uI , form an\narithmetic progression. This leads to Lemma 13 where we show that all of theses candidates\n\n11\n\n\fFigure 7: The satellite information of a candidate c in a text interval text interval(I , )\nfor I = [i, j ] includes the candidate ngerprint (t0 . . . tc  1) and the entrance ngerprint\n(tc . . . tc+i1 ). The ngerprint of (t0 . . . tc+i1 ) can be computed in constant time, using the\nsliding ngerprint property.\n\nand their SI information can be stored implicitly using O(1) words of space. The second type of\ncandidates are the rest of the candidates, and these candidates are stored explicitly together\nwith their SI information. We prove in Lemma 14 that the total number of such candidates is\nO(d log m), thereby obtaining our claimed space usage.\nLemma 11. Suppose I is a partitioning that satises the properties of Lemma 9. For a pattern\ninterval I = [i, j ]  I , there exists a string uI such that for any text T (cid:48) and time   0 the set\nC (I , ) does not contain three candidates with the same entrance prex u (cid:54)= uI .\nProof. Let c1 < c2 < c3 be three dierent candidates in C (I , ) with the same entrance prex\nu. By property 3 of Lemma 9 there is a string v of length |I | = j  i + 1 containing only\nnon-wildcard characters that is a substring of the length i prex of P .\nLet r be an arbitrary location of v in p0 . . . pi1 (since v could appear several times in the\nprex). The three candidates imply that after a shift of r characters from the candidates\nlocations, there are three occurrences of v in the text. These occurrences are within a substring\nof the text of length at most 2|v |, since all three candidates are in C (I , ) and so the distance\nbetween the rst and last occurrence is at most |I |  1 = |v |  1 (the 2 factor accommodates the\nfull occurrence of the third v). Thus, by Lemma 4, v must be periodic, and |v |  2v .\nSince c1 , c2 , and c3 are all occurrences of u then c3  c2 and c2  c1 are period lengths of\n2  (i+1)(j+1)\n|I |\n|v |\n ji\nu. Thus, u  min{c2  c1 , c3  c2}  c3c1\n2 . Therefore, by\n2 =\n2 <\nLemma 5, u = v . Similarly, let (cid:48) >  and suppose there are three candidates c4 , c5 , c6 in\n2\nC (I , (cid:48) ). Notice that it is possible that c1 , c2 and c3 are not in C (I , (cid:48) ) since it is possible that\nenough time has passed for them to leave. Suppose c4 , c5 and c6 share the same entrance prex\nu(cid:48) . Then u(cid:48) = v = u .\nAssume by contradiction that u(cid:48) (cid:54)= u. Notice that the only possible locations of mismatches\nbetween u and u(cid:48) are the positions of wildcards in the i length prex of P , since both u and\nu(cid:48) match this prex. In particular, v occurs in the rth location of both u and u(cid:48) . Let k be an\nindex of a mismatch between u and u(cid:48) . In particular, let the k th character of u be x, and the\nk th character of u(cid:48) be x(cid:48) (cid:54)= x. Let  be an integer (possibly negative) such that the k +   v\nlocation in u is within the occurrence of v in u (and so also within the occurrence of v in u(cid:48) ).\nNotice that such a  must exist since |v |  2v . Since u(cid:48) = v = u , the character at location\nk +   v in u must be x, while the character at location k +   v in u(cid:48) must be x(cid:48) . But u and\nu(cid:48) match at all of the locations corresponding to v . Thus we have obtained a contradiction, and\nso u = u(cid:48) is unique, as required.\n\n12\n\n\"#T$\t$&candidate fingerprint\"$*#entrance fingerprint$$&*#\"$&*#\t$&*#$*#\fLemma 12. Suppose I is a partitioning that satises the properties by Lemma 9. For a pattern\ninterval I = [i, j ]  I and time   0 if there are h  3 candidates c1 < c2 <    < ch in\nC (I , ) that have uI as their entrance prex, then the sequence c1 , c2 , . . . , ch forms an arithmetic\nprogression whose dierence is uI .\nProof. The distance between any two candidates in C (I , ) is at most |I |, and |I |  i by\nProperty 3 of Lemma 9. Hence, by Lemma 3, all of the occurrences of uI in T that begin\nin text interval(I , ) form an arithmetic progression with dierence uI . Each of these\noccurrences matches the i length prex of P , and therefore is a candidate in C (I , ). Hence, all\nthe candidates of C (I , ) with uI as their entrance prex form an arithmetic progression with\ndierence of uI .\n\nImplementation details. For any pattern interval I = [i, j ] and time  we split the set of\ncandidates C (I , ) into two disjoint sets. The set Cap (I , ) = {c  C (I , ) | tc . . . tc+i1 = uI }\ncontains all the candidates whose entrance prex is uI , and the set Cap (I , ) = C (I , ) \\ Cap (I , )\ncontains all the other candidates of C (I , ). We use a linked list LQI to store all of the candidates\nof Cap (I , ) together with their SI. Adding and removing a candidate that belongs in LQI\ntogether with its SI is straightforward. The candidates of Cap (I , ) are maintained using a\nseparate data structure that leverages Lemmas 11 and 12. Thus, during a Dequeue() operation,\nthe queue veries if the candidate to be returned is in LQI or in the separate data structure\nfor the Cap (I , ) candidates. Finally, for each pattern interval I the data structure stores the\nngerprint of the the principle period of uI .\n\nLemma 13. There exists an implementation of candidate-ngerprint-queues such that the queue\nQI at time  > 0 maintains al l the candidates of Cap (I , ) and their SI using O(1) words of\nspace.\nProof. If |Cap (I , )|  2 then QI stores the candidates of Cap (I , ) explicitly in O(1) words of\nspace. Otherwise, by Lemma 12, all the candidates of Cap (I , ) form an arithmetic progression.\nAn arithmetic progression of arbitrary length can be represented using O(1) words of space.\nHowever, QI also needs access to the SI for the candidates in this progression. To do this, QI\nexplicitly stores the rst candidate (min Cap (I , )) together with its SI, the common dierence of\nthe progression (uI ), the length of the current progression, and the ngerprint of the principle\nperiod of uI . When a new candidate c with entrance ngerprint (uI ) enters QI , c becomes the\nlargest element in Cap (I , ), and so we rst increment the length of the arithmetic progression,\nand if c is currently the only candidate in the arithmetic progression, then QI stores c and\nits SI (since then c is the rst candidate in the progression). When a Dequeue() operation\nneeds to remove the rst candidate c in the progression, then QI removes c, which is stored\nexplicitly together with its SI, decrements the length of the progression, and if there are remaining\ncandidates in the progression then QI computes the information for the new rst remaining\ncandidate in order to store its information explicitly. To do this, QI rst computes the location\nof the new rst candidate from uI and the location of c. The SI of the new rst candidate is\ncomputed in constant time (via the sliding property) from the ngerprint of the principle period\nof uI and the candidate ngerprint of c.\n\nSpace usage. The space usage of all of the queues has three components. The rst component\nis the lists LQI , which maintains the candidates of Cap (I , ) for all the intervals I . The second\ncomponent is the data structures for storing the candidates with entrance prex uI (the candidates\nof Cap (I , )) in each I  I . Since, by Lemma 13, for each I  I all the candidates with entrance\nprex uI are maintained using O(1) words, all such candidates use O(|I |) = O(d + log m) words\n\n13\n\n\fof space. The third component is storing for each pattern interval I the ngerprint of the the\nprinciple period of uI , which takes a total of O(d + log m) words of space. In the following\n(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12) = O(d log m).\nLemma 14. (cid:80)\nlemma we prove that the total space usage of all of the lists LQI is O(d log m).\nI I\nProof. By Lemma 9, we know that |{(0), . . . , (k)}| = O(log m). For each (cid:96)  {(0), . . . , (k)}\n(cid:80)\nlet I(cid:96)  I be the sequence of all pattern intervals I  I such that (I ) = (cid:96). We show that\n(cid:80)\n|Cap (I , )| = O(|I(cid:96) | + d). Combining with property 4 of Lemma 9 which states that\nI I(cid:96)\n(cid:96){(0),...,(k)} |I(cid:96) | = |I | = O(d + log m) we have that:\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)\n(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12) =\n(cid:88)\nI I(cid:96)\nI I\n(cid:96){(0),...,(k)}\nO(|I(cid:96) | + d)\n(cid:88)\n=\n(cid:96){(0),...,(k)}\n(cid:96){(0),...,(k)}\n\n(cid:88)\n(cid:96){(0),...,(k)}\n\nO(d)\n\n=\n\nO(|I(cid:96) |) +\n\n= O(d + log m) + O(d log m)\nWe focus on intervals for which (cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)  3, since if (cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)  2 the bound is\n= O(d log m)\nstraightforward.\nLet [i , j  ] be the leftmost interval in I(cid:96) . By denition of I(cid:96) , we have j   i + 1 = (cid:96), and\nso by Property 3 of Lemma 9, there exists a string v of length (cid:96) containing only non-wildcard\ncharacters that is a substring of the length i prex of P . Let r be an arbitrary location of v\nin p0 . . . pi1 (since v could appear several times in the prex). For any [i(cid:48) , j (cid:48) ] = I (cid:48)  I(cid:96) the\nentrance prex (which does not contain wildcards) of each candidate in C (I (cid:48) , ) matches the i(cid:48)\nprex of P (which can contain wildcards), and in particular, the location which is r locations to\nthe left of any candidate in C (I (cid:48) , ) is a location of an occurrence of v in the text6 .\nSince we focus on intervals I  I(cid:96) for which |Cap (I , )|  3, then there exist three occurrences\nof v in the text in positions corresponding to a shift of r characters from locations of I s\ncandidates. These occurrences are within a substring of the text of length at most 2|v |, since\nall three candidates are in C (I , ) and so the distance between the rst and the last candidates\nis at most |I |  1  (cid:96)  1 = |v |  1. Thus, by Lemma 4, v must be periodic, and the distance\n(cid:104)(cid:83)\n(cid:105)\nbetween any two candidates in C (I , ) must be a multiple of v .\nC (I , )\nI I(cid:96)\nLet c = max\nbe the rightmost (largest index) candidate in the intervals\ncorresponding to pattern intervals in I(cid:96) . Since c is a candidate in some C (I (cid:48) , ) for I (cid:48)  I(cid:96) ,\nthen there is an occurrence of v at location c + r. Thus, tc+r . . . tc+r+(cid:96)1 = v . We extend this\noccurrence of v to the left and to the right in T for as long as the length of the period does not\nincrease. Let the resulting substring be tL+1 . . . tR1 . See Figure 8. If L  0 then the index L is\ncalled the left violation of v . Similarly, if R   then the index R is called the right violation of\nv . Notice that the period of v extends all the way to the beginning of the text if and only if\nL = 1, in which case there is no left violation. Similarly, the period of v extends all the way to\nthe current end of the text if and only if R =  + 1, in which case there is no right violation.\nFinally, notice that L < c + r  c + r + (cid:96)  1 < R, since v is a substring of tL+1 . . . tR1 .\n6Notice that this occurrence is well dened since i(cid:48)  i  r + |v |.\n\n14\n\n\fFigure 8: Positions L and R are the violations of the periodic substring that contains v . Notice\nthat it is possible that L  c, and similarly it is possible that R is in the entrance interval of c.\n\nFor a candidate c  text interval([i, j ], ) we dene the entrance interval of c to be\n[c, c + i  1]. In addition we denote ec = c + i  1, so the entrance interval of c is [c, ec ].\nClaim 1. For any candidate c  C (I , ) where I  I(cid:96) we have c + r  [c, ec ].\nProof. Let c be a candidate in C (I , ) for I = [i, j ]  I(cid:96) . Recall that C (I , )  text interval(I , ) =\n[  j + 1,   i + 1]. Since c is in this interval we have that   j + 1  c    i + 1. In\nparticular, ec = c + i  1    j + 1 + i  1 =   (j  i + 1) + 1 =   |I | + 1. By denition,\nsince I  I(cid:96) , we have that |I |  (cid:96) and so ec    (cid:96) + 1. Since tc+r . . . tc+r+(cid:96)1 = v , it must be\nthat c + r + (cid:96)  1  . Thus, c + r    (cid:96) + 1  ec . By the maximality of c, it is obvious that\nc  c  c + r. Hence, we have that c  c + r  ec .\nClaim 2. Suppose I = [i, j ]  I(cid:96) and |Cap (I , )|  3. Then for any candidate c  Cap (I , ) in\ntext interval(I , ) either L  [c, ec ] or R  [c, ec ].\nProof. For c  Cap (I , ) let u = u0 . . . ui1 be the entrance prex of c. Recall that L < c + r < R.\nBy Claim 1 it must be that c  c + r  ec and so we cannot have both L, R < c or both L, R > ec .\nAssume by contradiction that L < c  ec < R. We claim that there exists a text input T (cid:48)\nsuch that if we execute the algorithm with T (cid:48) as the text, then there exists some time  where\nC (I ,  ) contains three candidates with u as their entrance prex. Then, by Lemma 11 we deduce\nthat u = uI , in contradiction to the denition of Cap (I , ).\nRecall that the principle period length of tL+1 . . . tR1 is v . Since u = tc . . . tec is a substring\nof tL+1 . . . tR1 , it must be that u  v . Recall that Cap (I , ) contains at least three candidates.\nLet c1 , c2 , and c3 be three distinct candidates in Cap (I , ). Since c1 , c2 , and c3 are all occurrences\nof u then c3  c2 and c2  c1 are period lengths of u. Thus, u  min{c2  c1 , c3  c2}  c3c1\n2 \n|v |\n|I |\n2 . Therefore, by Lemma 5, u = v . Thus, u  ji\nji\nimplying that i + 2u  j  0.\n2 <\n2 =\nConsider a long enough (at least i + 2u  1) text T (cid:48) which is composed of repeated\n2\nconcatenation of u0 . . . uu1 . Notice that the substrings of T (cid:48) of length i starting at locations 0,\nu and 2u are all exactly the string u, which matches p0 . . . pi1 . Consider an execution of the\nalgorithm with T (cid:48) as the input text, and at time  = i + 2u  1 consider the set C (I ,  ). We\nhave that text interval(I ,  ) = [i + 2u  1  j + 1, i + 2u  1  i + 1] = [i + 2u  j, 2u ].\nBeing that i + 2u  j  0 then the interval [0, 2u ] is a subinterval of text interval(I ,  ),\nthen 0, u and 2u are all within this interval. Thus, these locations are candidates in C (I ,  )\n\n15\n\nTa+babcbabds entrance prefixViolations of ++1\fwith u as their entrance prex. Thus, by Lemma 11, it must be that u = uI , which contradicts\nc  Cap (I , ).\nLet C left\nap (I , ) be the set of candidates in Cap (I , ) whose entrance interval contains L, and\n(I , ) be the set of candidates in Cap (I , ) whose entrance interval contains R. C left\nlet C right\nap (I , )\nap\nand C right\nap (I , )  C right\n(I , ) are not necessarily disjoint. Notice that by Claim 2, C left\n(cid:12)(cid:12)(cid:12)C right\n(cid:12)(cid:12)(cid:12)C left\n(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d).\n(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d) and (cid:80)\n(I , )\nClaim 3. (cid:80)\nap\nap\ncontains all the candidates of Cap (I , ).\nI I(cid:96)\nI I(cid:96)\n(I , )\nap (I , )\nap\n(cid:12)(cid:12)(cid:12)C left\n(cid:12)(cid:12)(cid:12) from all sets C left\nNotice that the contribution to (cid:80)\nProof. Let I  I(cid:96) and let  denote the match relation between symbols in   {?}.\nI I(cid:96)\nap (I , )\nap (I , ) that have less than\ntwo candidates is at most O(|I(cid:96) |). Thus, we will prove that for any set C left\nap (I , ) with at least two\ncandidates, it must be that for any candidate c  C left\nap (I , ), except for possibly one candidate,\nwe have that pLc is a wildcard.\nap (I , ) contains at least two candidates and let cleft = max C left\nSuppose C left\nap (I , ) be the most\nrecent candidate in C left\nap (I , ). Let c < cleft be a candidate in C left\nap (I , ). Since c  C left\nap (I , )\nwe have that pLc  tc+Lc = tL (recall that both L and c are indices in the text). Similarly,\nsince cleft  C left\nap (I , ) we have that pLc  tcleft+Lc = tL+(cleftc) . Recall that the distance\nbetween any two candidates in C (I , ) is a multiple of v , since C (I , ) contains at least 3\ncandidates. In particular the distance (cleft  c) is a multiple of v and (cleft  c)  |I |  |v |.\nThus, tL (cid:54)= tL+(cleftc) since L violates the period of length v . Recall that tL  pLc  tL+(cleftc) ,\nand so pLc must be a wildcard. Therefore, each c  C left\nap (I (cid:48) , ) for all I (cid:48)  I(cid:96) , then the contribution to (cid:80)\nap (I , ), except for possibly cleft , is in\na position c such that pLc is a wildcard. Since L is the same for all of the candidates in all\n|C left\nof the C left\nap (I , )| of the candidates\n(cid:12)(cid:12)(cid:12)C left\n(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d). The proof that\nI I(cid:96)\nrecent candidates is at most O(|I(cid:96) |). Thus, (cid:80)\nthat are not the most recent in their set C left\nap (I , ) is at most d. The contribution of the most\n(cid:80)\nap (I (cid:48) , )\nI (cid:48)I(cid:96)\n(cid:12)(cid:12)(cid:12)C right\n(cid:12)(cid:12)(cid:12)C left\n(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d). Thus, we\n(cid:12)(cid:12)(cid:12) + (cid:80)\n(I , )| = O(|I(cid:96) | + d) is symmetric.\n|C right\n(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)  (cid:80)\nFinally, (cid:80)\nI (cid:48)I(cid:96)\nap\nI I(cid:96)\nI I(cid:96)\nI I(cid:96)\n(I , )\nap (I , )\nap\nhave completed the proof of Lemma 14.\n\n6 The Algorithm of Theorem 2\n\nThe algorithm of Theorem 1 for PMDW uses O(d) time per character and O(d) words of space.\nIn this section we introduce the algorithm of Theorem 2 which extends this result for a parameter\n0    1 to an algorithm that uses O(d1 ) time per character and O(d1+ ) words of space.\nAn overview of a slightly modied version (for the sake of intuition) of the tradeo algorithm\nis described as follows. Let P  be the longest prex of P such that P   d . The tradeo\nalgorithm rst nds all the occurrences of P  in T using a specialized algorithm for patterns\nwith bounded wildcard-period length. If P  = P then this completes the tradeo algorithm.\nOtherwise, let I = [i, j ] be the interval in the secondary partitioning of Theorem 1 such that\ni  |P  |  1  j . We rst divide I into two new intervals [i, |P  |  1] and [|P  |, j ]. If [|P  |, j ] = \nthen we discard [|P  |, j ]. It is straightforward to see that the properties of partitions that we\n\n16\n\n\f(a) The matrix M q\n\n(b) The oset patterns and q\n\n(c) The column pattern\nPq\n\nFigure 9: Example of the matrix representation for pattern P = abcab?abcabcabcabcabc and\nq = 5. Each color represents a unique oset pattern. The oset patterns P5,1 and P5,4 are equal\nand therefore they have the same id (column color). Since P5,3 contains a wildcard, it is not\nassociated with any id.\n\ndene in Lemma 9 are still satised. Let I  = [i = |P  |, j  ] be the interval immediately following\n[i, |P  |  1]. Each occurrence of P  in the text is inserted into the algorithm of Theorem 1 as\na candidate directly into QI  . Thus, the entrance prexes of candidates in the queues match\nprexes of P that are longer than P  and, by maximality of P  , these prexes of P have\nlarge wildcard-period length. This implies that the average distance between two consecutive\ncandidates that are occurrences of P  is at least d , and so, combined with a carefully designed\nscheduling approach for verifying candidates, we are able to obtain an O(d1 ) amortized time\ncost per character.\n\nIn Section 6.1 we describe the specialized algorithm for dealing with patterns\nOverview.\nwhose wildcard-period length is at most  , for some parameter  > 1. In Section 6.2 we complete\nthe proof of Theorem 2 by describing the missing details for the tradeo algorithm. In particular,\nthe proof of Theorem 2 uses the algorithm of Section 6.1 with  = d .\n\n6.1 Patterns with Small Wildcard-period Length\n\nLet P be a pattern of length m with d wildcards such that P <  . Let q be an integer,\nwhich for simplicity is assumed to divide m (see Appendix A.1 where we discuss how to get\nrid of this assumption). Consider the conceptual matrix M q = {mq\nx,y } of size m\nq  q where\nx,y = p(x1)q+y1 . An example is given in Figure 9. For any integer 0  r < q the r th column\nmq\nof M q corresponds to an oset pattern Pq ,r = pr pr+q pr+2q . . . pmq+r . Notice that some oset\npatterns might be equal. Let q = {Pq ,r | 0  r < q and (cid:48)?(cid:48) / Pq ,r } be the set of all the oset\npatterns that do not contain any wildcards. Each oset pattern in q is given a unique id. The\nset of unique ids is denoted by IDq . We say that index i in P is covered by q if the column\ncontaining pi does not contain a wildcard, and so Pq ,i mod q  q . The columns of M q dene a\n\n17\n\nabcabcbcab?acabcababcacabP5,0= acbacP5,1= P5,4= bacbaP5,2= cbacbP5,3= a?bac\"={acbac, bacba, cbacb}?P5,1P5,0P5,1P5,2\fcolumn pattern Pq of length q , where the j th character is the id of the Pq ,j column, or (cid:48)?(cid:48) if\nPq ,j / q (since Pq ,j contains wildcards).\nWe partition T into q oset texts, where for every 0  r < q we dene Tq ,r = tr tr+q tr+2q . . . .\nUsing the dictionary matching streaming (DMS) algorithm of Cliord et al. [14] we look for\noccurrences of oset patterns from q in each of the oset texts. We emphasize that we do not\nonly nd occurrences of Pq ,r in Tq ,r , since we cannot guarantee that the oset of T synchronizes\nwith an occurrence of P . When the character t arrives, the algorithm passes t to the DMS\nalgorithm for Tq , mod q . We also create a streaming column text Tq whose characters correspond\nto the ids of oset patterns as follows. If one of the oset patterns is found in Tq , mod q , then its\nid is the th character in Tq . Otherwise, we use a dummy character for the th character in Tq .\n\nFull cover. Notice that an occurrence of P in T necessarily creates an occurrence of Pq in\nTq . Such occurrences are found via the black box algorithm of Cliord et al. [11]. However, an\noccurrence of Pq in Tq does not necessarily mean there was an occurrence of P in T , since some\ncharacters in P are not covered by q . In order to avoid such false positives we run the process in\nparallel with several choices of q , while guaranteeing that each non wildcard character in P is\ncovered by at least one of those choices. Thus, if there is an occurrence of Pq at location i in Tq\nfor all the choices of q , then it must be that P appears in T at location i. The choices of q are\ngiven by the following lemma.\n\nLemma 15. There exists a set Q of O(log d) prime numbers such that any index of a non-\nwildcard character in P is covered by at least one prime number q  Q, and each number in Q is\nat most O(d).\n\nProof. The proof uses the probabilistic method: we show that the probability that the set\nQ exists is strictly larger than 0. Since our proof is constructive it provides a randomized\nconstruction of Q.\nIt is well known that for a prime number q , every integer 0  z < q denes a congruence\nclass which contains all integers i such that i mod q = z . For any two distinct natural numbers\nx, y  N, let Dx,y be the set of prime numbers q such that x and y are in the same congruence\nclass modulo q (i.e. x mod q = y mod q). Notice that in the interpretation of the pattern columns\nin the conceptual matrix, if q  Dx,y then px and py are in the same column of the conceptual\nmatrix M q . Recall that W is the set of occurrences of wildcards in P . Thus, if 0  j < m is an\nq > (cid:81)\nthe Chinese remainder theorem, |Dj,w | < log m (otherwise for  = (cid:81)\nindex such that j / W and if w  W such that q  Dj,w , then j is surely not covered by q . By\n2  m,\nFor any 0  j < m such that j / W , let Dj = (cid:83)\nwW Dj,w , so |Dj |  (cid:80)\nqDj,w\nqDj,w\nand so j mod  = w mod  implying that j = w).\nwW |Dj,w | <\n|W | log m = d log m. If 2d  m\nthen the proof is trivialized by choosing Q to contain only\nlog2 m\nthe smallest prime number which is at least m. If 2d > m\n, by Corollary 1 in [3], then there\nlog2 m\nare at least 2d log m prime numbers whose value are upper bounded by 2d log2 m. Let Q be the\nset of those prime numbers. For a random q  Q, the probability that a specic non-wildcard\n|Dj |\n| Q|  d log m\n2d log m = 1\npattern index j is not covered by q is at most\n2 . Let Q be a set of 2 log m\nrandomly chosen prime numbers from Q. The probability that a specic non-wildcard pattern\n22 log m  1\n1\nindex j is not covered by any of the prime numbers in Q is less than\nm2 . Thus, the\nprobability that there exists a non-wildcard pattern index j which is not covered by any of the\nm2  1\nprime numbers in Q is less than md\nm . Therefore, there must exist a set Q that covers all of\nthe indices of non-wildcard characters from P .\nFrom a space usage perspective, we need the size of |q | to be small, since this directly aects\nthe space usage of the DMS algorithm which uses O(k) space, where k is the number of patterns\n\n18\n\n\fFigure 10: For string S = s0 . . . s34 for pattern of length m = 18, S7 is marked by the blue\nrectangle and the green indices are the characters of S7,3,1 . Notice that S7,3,1 = S8,3,0 .\n\nin the dictionary. In our case k = |q |. In order to bound the size of q we use the following\nlemma.\nLemma 16. If P   then for any q  N we have |q | = O( ).\nProof. Since P   , there exists a string S = s0 . . . s2m2 with no wildcards that contains ( m\n )\noccurrences of P . Using the string S we show that |q | = O( ).\nFor each id in IDq we pick an index of a representative column in Mq that has this id, and\ndenote this set by Rq . Let r1 be the minimum index in Rq . For every index 0  i < m let\nSi = si . . . si+m1 (see Figure 10). For every 0  r < q let Si,q ,r = si+r si+r+q . . . si+mq+r , and\nso for any integer 0   < q  r we have Si,q ,r+ = Si+,q ,r . Notice that if Si matches P then\nPq ,r = Si,q ,r for each r  Rq .\nLet i be an index of an occurrence of P in S . For any distinct r, r (cid:48)  Rq , it must be\nthat Si,q ,r = Pq ,r (cid:54)= Pq ,r (cid:48) = Si,q ,r (cid:48) . In particular, for any r  Rq such that r > r1 , we have\nPq ,r1 = Si,q ,r1 (cid:54)= Si,q ,r = Si+rr1 ,q ,r1 . This implies that i + r  r1 cannot be an occurrence of P .\nHence, every occurrence of P in S eliminates |Rq |  1 locations in S from being an occurrence\nof P . We now show that the sets of eliminated locations dened by distinct occurrences are\ndisjoint. Assume without loss of generality that S contains at least two occurrences. Let\ni1 and i2 be two distinct occurrences of P in S , and assume by contradiction that an index\nj is eliminated by both of these occurrences. Since si1 . . . si1+m1 matches P , we have that\nSi1 ,q ,ji1 = Pji1 and j  i1  Rq . Similarly, we have that Si2 ,q ,ji2 = Pji2 and j  i2  Rq .\nBeing that Si1 ,q ,ji1 = Si2 ,q ,ji2 we have that Pji2 = Pji1 , contradicting the denition of Rq .\n|S |\n|Rq | = 2m1\nTherefore, the maximum number of occurrences of P in S is at most\n|Rq | . Since S\n  2m1\n|Rq | which implies that |q | = |Rq | \ncontains at least m\n instances of P , it must be that m\n2 = O( ).\n\nComplexities. For a single q  Q, the algorithm creates q = O(d) oset patterns and texts.\nFor each such oset text the algorithm applies an instance of the DMS algorithm with a dictionary\nof O( ) strings (by Lemma 16). Since each instance of the DMS algorithm uses O( ) words of\nspace [14], the total space usage for all instances of the DMS algorithm is O(d ) words. Moreover,\nthe time per character in each DMS algorithm is O(1) time, and each time a character appears\nwe inject it into only one of the DMS algorithms (for this specic q). In addition, the algorithm\nuses an instance of the black box algorithm for Tq , with a pattern of length q . This uses another\nO(q) = O(d) space and another O(1) time per character [11]. Thus the total space usage due to\none element in Q is O(d ) words. Since |Q| = O(log d) the total space usage for all elements in\nQ is O(d ) words, and the total time per arriving character is O(1). Thus we have proven the\nfollowing.\nTheorem 17. For any   1, there exists a randomized Monte Carlo algorithm for PMDW on\npatterns P with P <  in the streaming model, which succeeds with probability 1  1/poly(n),\nuses O(d ) words of space and spends O(1) time per arriving text character.\n\n19\n\nS70ss1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16s17s18s19s20s21s22s23s24s25s26s27s28s29s30s31S7s32s33s34\f6.2 Proof of Theorem 2\n\nIn this section we combine the algorithm of Theorem 1 with the algorithm of Theorem 17\nand introduce an algorithm for patterns with general wildcard-period length, thereby proving\nTheorem 2.\nPrior to Section 6.1 we presented an almost accurate description of the algorithm. The only\ntwo parts of the description that require elaboration are regarding how to insert occurrences of\nP  into the appropriate candidate-ngerprint-queue eciently, and how to schedule validations\nof candidates so that the amortized cost is low. We rst focus on how to insert candidates and\nlater we discuss the scheduling.\n\nDirect insertion of candidates. The challenge with inserting occurrences of P  into QI \nis that the candidate-ngerprint-queue data structure uses the SI of candidates, and so the\nstraightforward ways for providing this information together with the new candidates (which are\noccurrences of P  ) cost either too much time or too much space. In order to meet our desired\ncomplexities, we rst investigate the purposes of dierent parts of SI.\nThe SI for a candidate c in C (I = [i, j ], ) consists of the candidate ngerprint, (t0 . . . tc1 ),\nand the entrance ngerprint, (tc . . . tc+i1 ). The SI has two purposes. The rst is to validate a\ncandidate after a Dequeue() operation, in which case the algorithm makes use of both parts of\nthe SI in order to compute (tc+i . . . tc+j ) by combining the SI with the text ngerprint. The\nsecond purpose is to compute the next entrance ngerprints of candidates in order to distinguish\nbetween candidates that are stored as part of an arithmetic progression and candidates that are\nnot. The entrance ngerprint is obtained, via the sliding property, from the candidate ngerprint\nin the SI and the current text ngerprint.\nNotice that in order to validate c the algorithm only needs the ngerprint of (t0 . . . tci+1 ).\nAlso notice that entrance prexes are only used for candidates that are at some point part of\na stored arithmetic progression. Thus, for a specially chosen subset of strings   |P  | we\nprecompute all of the ngerprints of strings in . The set  is chosen so that for any occurrence\nof P  that is injected as a candidate c where c is at some point part of a stored arithmetic\nprogression, the occurrence of P  at location c is in . We use the DMS algorithm [14] to\nlocate strings from  in the text, and whenever such a string appears, we compute the SI\nfor the corresponding candidate in constant time from the stored ngerprint and the current\ntext ngerprint. We emphasize that not all of the candidates that correspond to strings in \nneed to necessarily at some point be a part of an arithmetic progression. However, in order to\nreduce the space usage, we require that  is not too large, and in particular || = O(d + log m).\nFor a candidate c that does not correspond to a string in , instead of maintain the SI of c,\nwe explicitly maintain the ngerprint of (t0 . . . tci+1 ) where c  C (I = [i, j ], ). Notice that\nwhenever such a candidate enters a new text interval, the text ngerprint at that time is exactly\nthe information which we need to store.\n\nCreating . Consider all pattern intervals I = [i, j ]  I with i  i . Notice that there are at\nmost O(d + log m) such pattern intervals. For each such interval I , let I be the prex of uI\nof length |P  |. Since, by Lemma 11, a candidate c  Cap (I , ) implies an occurrence of uI at\nlocation c, then I also appears at location c. Thus, we dene  to be the set containing I for\nall such pattern intervals I . Since any candidate in an arithmetic progression at time  must be\nin Cap (I , ) for some interval I , it is guaranteed that when c corresponded to an occurrence of\nP  , that occurrence must have been I , and so  has the required properties.\n\n20\n\n\fScheduling validations. Since the only bound we have proven on the number of pattern\nintervals I = [i, j ]  I with i  i is O(d + log m), if each time a new text character arrives we\nperform a Dequeue() operation for each one of the pattern intervals, then the time cost can be\nas large as O(d + log m) which is too much. The solution for reducing this time cost is to only\nperform a Dequeue() operation on QI when a candidate c actually leaves text interval(I , )\nand needs to be validated. This is implemented by maintaining a priority queue on top of\nthe pattern intervals, where the keys that are used are the next time a candidate exits the\ncorresponding text interval. Each time a candidate leaves a text interval, the key for the queue\nof that interval is updated to the time the next candidate leaves (if such a candidate exists).\nWhen a candidate entering a text interval is the only candidate of that text interval, then the\nkey for the queue of this text interval is also updated.\n\nComplexities. Recall that I  = [i , j  ] is a pattern interval such that i = |P  |, and that each\ntime the algorithm nds an occurrence of P  , the corresponding candidate is inserted into QI  .\nLet P (cid:48) be the prex of P of length j  + 1. By maximality of P  , it must be that P (cid:48) > d . We\npartition the time usage of the algorithm into three parts. The rst is the amount of time spent\non nding occurrences of P  using the algorithm of Theorem 17, which is O(1). The second is\nthe amount of time spent performing Enqueue() and Dequeue() operations on QI  , which is also\nO(1) since we perform O(1) operations on this queue per each arriving character. The third\nis the amount of time spent on Enqueue() and Dequeue() operations on QI for I = [i, j ] with\ni > j  . These operations only apply to candidates that are occurrences of P (cid:48) . For this part we\n(cid:109)\n(cid:108)\nuse amortized analysis.\nBy denition of wildcard-period length, for any string S of size 2|P (cid:48) |  1, we have d < P (cid:48) \n. Being that occ(S, P (cid:48) )  |P (cid:48) |, we have d < 2|P (cid:48) |\n|P (cid:48) |\nocc(S,P (cid:48) ) . Notice that for a text T of size\nocc(S,P (cid:48) )\nn  |P (cid:48) |, we must have occ(T , P (cid:48) ) < 2n\nd . This is because otherwise, if n  2|P (cid:48) |  1 then there\nexists a substring of n of length 2|P (cid:48) |  1 with at least 2|P (cid:48) |\nd occurrences of P (cid:48) , and if n < 2|P (cid:48) |  1\nthen we can pad T to create such a string. In both cases we contradict d < 2|P (cid:48) |\nocc(S,P (cid:48) ) for any\nstring S of length 2|P (cid:48) |  1.\nThe total amount of time spent on each occurrence of P (cid:48) is O(d), and so the total cost\nfor processing T on candidates that are also occurrences of P (cid:48) is at most O(occ(T , P (cid:48) )  d) =\nd d) = O(n  d1 ). Thus, the amortized cost per character is O(d1 ).\nO( 2n\nFor the space complexity, the most expensive part is the use of the algorithm of Theorem 17\nwhich takes O(d  d ) = O(d1+ ) words of space. This completes the proof of Theorem 2.\n\nReferences\n\n[1] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the\nfrequency moments. J. Comput. Syst. Sci., 58(1):137147, 1999.\n\n[2] Amihood Amir, Moshe Lewenstein, and Ely Porat. Faster algorithms for string matching\nwith k mismatches. J. Algorithms, 50(2):257275, 2004.\n\n[3] J Barkley Rosser and Lowell Schoenfeld. Approximate formulas for some functions of prime\nnumbers. Il linois J. Math, 6:6494, 1962.\n\n[4] Jean Berstel and Luc Boasson. Partial words and a theorem of ne and wilf. Theor. Comput.\nSci., 218(1):135141, 1999.\n\n21\n\n\f[5] Francine Blanchet-Sadri. Algorithmic Combinatorics on Words. Discrete mathematics and\nits applications. CRC Press, 2008.\n\n[6] Francine Blanchet-Sadri and Robert A. Hegstrom. Partial words and a theorem of ne and\nwilf revisited. Theor. Comput. Sci., 270(1-2):401419, 2002.\n\n[7] Dany Breslauer and Zvi Galil. Real-time streaming string-matching. ACM Transactions on\nAlgorithms, 10(4):22:122:12, 2014.\n\n[8] Dany Breslauer, Roberto Grossi, and Filippo Mignosi. Simple real-time constant-space\nstring matching. Theor. Comput. Sci., 483:29, 2013.\n\n[9] Sabin Cautis, Filippo Mignosi, Jerey Shallit, Ming-wei Wang, and Soroosh Yazdani.\nPeriodicity, morphisms, and matrices. Theor. Comput. Sci., 295:107121, 2003.\n\n[10] Peter Cliord and Raphael Cliord. Simple deterministic wildcard matching. Inf. Process.\nLett., 101(2):5354, 2007.\n\n[11] Raphael Cliord, Klim Efremenko, Benny Porat, and Ely Porat. A black box for online\napproximate pattern matching. Inf. Comput., 209(4):731736, 2011.\n\n[12] Raphael Cliord, Klim Efremenko, Ely Porat, and Amir Rothschild. From coding theory to\necient pattern matching. In Proceedings of the Twentieth Annual ACM-SIAM Symposium\non Discrete Algorithms, SODA, pages 778784, 2009.\n\n[13] Raphael Cliord, Klim Efremenko, Ely Porat, and Amir Rothschild. Pattern matching with\ndont cares and few errors. J. Comput. Syst. Sci., 76(2):115124, 2010.\n\n[14] Raphael Cliord, Allyx Fontaine, Ely Porat, Benjamin Sach, and Tatiana A. Starikovskaya.\nDictionary matching in a stream. In Proceedings of Algorithms - ESA 2015 - 23rd Annual\nEuropean Symposium, Patras, volume 9294 of Lecture Notes in Computer Science, pages\n361372. Springer, 2015.\n\n[15] Raphael Cliord, Allyx Fontaine, Ely Porat, Benjamin Sach, and Tatiana A. Starikovskaya.\nThe k -mismatch problem revisited. In Proceedings of the Twenty-Seventh Annual ACM-\nSIAM Symposium on Discrete Algorithms, SODA, pages 20392052, 2016.\n\n[16] Raphael Cliord, Markus Jalsenius, Ely Porat, and Benjamin Sach. Space lower bounds for\nonline pattern matching. Theor. Comput. Sci., 483:6874, 2013.\n\n[17] Raphael Cliord and Ely Porat. A ltering algorithm for k -mismatch with dont cares. In\nProceedings of String Processing and Information Retrieval, 14th International Symposium,\nSPIRE, pages 130136, 2007.\n\n[18] Raphael Cliord and Benjamin Sach. Pseudo-realtime pattern matching: Closing the gap.\nIn Proceedings of Combinatorial Pattern Matching, 21st Annual Symposium, CPM 2010,\npages 101111, 2010.\n\n[19] Richard Cole and Ramesh Hariharan. Verifying candidate matches in sparse and wildcard\nmatching. In Proceedings on 34th Annual ACM Symposium on Theory of Computing, pages\n592601, 2002.\n\n[20] Funda Ergun, Hossein Jowhari, and Mert Saglam. Periodicity in streams. In Proceedings of\nApproximation, Randomization, and Combinatorial Optimization. Algorithms and Tech-\nniques, 13th International Workshop, APPROX 2010, and 14th International Workshop,\nRANDOM 2010, pages 545559, 2010.\n\n22\n\n\f[21] Nathan J Fine and Herbert S Wilf. Uniqueness theorems for periodic functions. Proceedings\nof the American Mathematical Society, 16(1):109114, 1965.\n\n[22] Michael J Fischer and Michael S Paterson. String-matching and other products. Technical\nreport, DTIC Document, 1974.\n\n[23] Zvi Galil and Joel I. Seiferas. Time-space-optimal string matching. J. Comput. Syst. Sci.,\n26(3):280294, 1983.\n\n[24] Pawel Gawrychowski. Optimal pattern matching in LZW compressed strings. ACM\nTransactions on Algorithms, 9(3):25, 2013.\n\n[25] Monika Rauch Henzinger, Prabhakar Raghavan, and Sridar Ra jagopalan. External Memory\nAlgorithms, chapter Computing on data streams, pages 107118. American Mathematical\nSociety, Boston, USA, 1999.\n\n[26] Piotr Indyk. Faster algorithms for string matching problems: Matching the convolution\nbound. In Proceedings of 39th Annual Symposium on Foundations of Computer Science,\nFOCS, pages 166173, 1998.\n\n[27] Markus Jalsenius, Benny Porat, and Benjamin Sach. Parameterized matching in the\nstreaming model. In Proceedings of 30th International Symposium on Theoretical Aspects\nof Computer Science, STACS, pages 400411, 2013.\n\n[28] Adam Kalai. Ecient pattern-matching with dont cares. In Proceedings of the Thirteenth\nAnnual ACM-SIAM Symposium on Discrete Algorithms SODA, pages 655656. ACM/SIAM,\n2002.\n\n[29] Daniel M. Kane, Jelani Nelson, Ely Porat, and David P. Woodru. Fast moment estimation\nin data streams in optimal space. In Proceedings of the 43rd ACM Symposium on Theory\nof Computing, STOC, pages 745754, 2011.\n\n[30] Richard M. Karp and Michael O. Rabin. Ecient randomized pattern-matching algorithms.\nIBM Journal of Research and Development, 31(2):249260, 1987.\n\n[31] Donald E. Knuth, James H. Morris Jr., and Vaughan R. Pratt. Fast pattern matching in\nstrings. SIAM J. Comput., 6(2):323350, 1977.\n\n[32] Gad M. Landau and Uzi Vishkin. Ecient string matching with k mismatches. Theor.\nComput. Sci., 43:239249, 1986.\n\n[33] Lap-Kei Lee, Moshe Lewenstein, and Qin Zhang. Parikh matching in the streaming model. In\nProceedings of String Processing and Information Retrieval - 19th International Symposium,\nSPIRE, pages 336341, 2012.\n\n[34] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in\nTheoretical Computer Science, 1(2), 2005.\n\n[35] S. Muthukrishnan and H. Ramesh. String matching under a general matching relation. In\nProceedings of Foundations of Software Technology and Theoretical Computer Science, 12th\nConference, volume 652 of Lecture Notes in Computer Science, pages 356367. Springer,\n1992.\n\n23\n\n\f[36] Benny Porat and Ely Porat. Exact and approximate pattern matching in the streaming\nmodel. In Proceedings of 50th Annual IEEE Symposium on Foundations of Computer\nScience, FOCS, pages 315323, 2009.\n\n[37] Ely Porat and Ohad Lipsky. Improved sketching of hamming distance with error correcting.\nIn Proceedings of Combinatorial Pattern Matching, 18th Annual Symposium, CPM, pages\n173182, 2007.\n\n[38] William F. Smyth and Shu Wang. A new approach to the periodicity lemma on strings\nwith holes. Theor. Comput. Sci., 410(43):42954302, 2009.\n\nA Missing Details\n(cid:108) m\n(cid:106) m\n(cid:107)\n(cid:109)\nA.1 Dealing with q (cid:45) m\nIf q (cid:45) m, then the strings in q have two possible lengths; either\n. This implies that\nor\nq\nq\none string in q could be a proper sux of another string in q . So if the longer one appears in\nan oset text, then both ids need to be given to Tq - a situation in which it is not clear what to\ndo. So to avoid such scenarios, for each q  Q we run the algorithm twice, in parallel, where\none instance uses the DMS algorithm for one length while the other instance uses the DMS\nalgorithm on the other length. This creates two instances of Pq and Tq , one for each length of\ncolumns under consideration. Notice that in order for the algorithm to work, when considering\none specic length, all of the columns that correspond to the other length are treated as a (cid:48)?(cid:48) in\nthe appropriate instance of Pq .\n\n24\n\n\f", 
        "tag": "Data Structures and Algorithms", 
        "link": "https://arxiv.org/list/cs.DS/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nB\nD\n.\ns\nc\n[\n \n \n1\nv\n0\n7\n7\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nEnabling Smart Data: Noise ltering in Big Data\nclassication\n\nDiego Garca-Gila,, Julian Luengoa , Salvador Garcaa , Francisco Herreraa\n\naDepartment of Computer Science and Articial Intel ligence, University of Granada,\nCITIC-UGR, Granada, Spain, 18071\n\nAbstract\n\nIn any knowledge discovery process the value of extracted knowledge is directly\nrelated to the quality of the data used. Big Data problems, generated by mas-\nsive growth in the scale of data observed in recent years, also follow the same\ndictate. A common problem aecting data quality is the presence of noise,\nparticularly in classication problems, where label noise refers to the incorrect\nlabeling of training instances, and is known to be a very disruptive feature of\ndata. However, in this Big Data era, the massive growth in the scale of the\ndata poses a challenge to traditional proposals created to tackle noise, as they\nhave diculties coping with such a large amount of data. New algorithms need\nto be proposed to treat the noise in Big Data problems, providing high qual-\nity and clean data, also known as Smart Data. In this paper, two Big Data\npreprocessing approaches to remove noisy examples are proposed: an homoge-\nneous ensemble and an heterogeneous ensemble lter, with special emphasis in\ntheir scalability and performance traits. The obtained results show that these\nproposals enable the practitioner to eciently obtain a Smart Dataset from any\nBig Data classication problem.\n\nKeywords: Big Data, Smart Data, Classication, Class Noise, Label Noise.\n\n1. Introduction\n\nVast amounts of information surround us today. Technologies such as the\nInternet generate data at an exponential rate thanks to the aordability and\ngreat development of storage and network resources.\nIt is predicted that by\n2020, the digital universe will be 10 times as big as it was in 2013, totaling an\nastonishing 44 zettabytes [22]. The current volume of data has exceeded the\nprocessing capabilities of classical data mining systems [49] and have created\na need for new frameworks for storing and processing this data. It is widely\n\nCorresponding author\nEmail addresses: djgarcia@decsai.ugr.es (Diego Garca-Gil),\njulianlm@decsai.ugr.es (Julian Luengo), salvagl@decsai.ugr.es (Salvador Garca),\nherrera@decsai.ugr.es (Francisco Herrera)\n\nPreprint submitted to Journal of LATEX Templates\n\nApril 7, 2017\n\n\faccepted that we have entered the Big Data era [30]. Big Data is the set of\ntechnologies that make processing such large amounts of data possible [20],\nwhile most of the classic knowledge extraction methods cannot work in a Big\nData environment because they were not conceived for it.\nBig Data as concept is dened around ve aspects: data volume, data veloc-\nity, data variety, data veracity and data value [24]. While the volume, variety\nand velocity aspects refer to the data generation process and how to capture\nand store the data, veracity and value aspects deal with the quality and the\nusefulness of the data. These two last aspects become crucial in any Data Min-\ning process, where the extraction of useful and valuable knowledge is strongly\ninuenced by the quality of the used data. In Big Data, the usage of traditional\npreprocessing techniques [15, 33, 17] to enhance the data is even more time\nconsuming and resource demanding, being unfeasible in most cases.\nThe lack of ecient and aordable preprocessing techniques implies that the\nproblems in the data will aect the models extracted. Among all the problems\nthat may appear in the data, the presence of noise in the dataset is one of the\nmost frequent. Noise can be dened as the partial or complete alteration of the\ninformation gathered for a data item, caused by an exogenous factor not related\nto the distribution that generates the data. Learning from noisy data is an im-\nportant topic in machine learning, data mining and pattern recognition, as real\nworld data sets may suer from imperfections in data acquisition, transmission,\nstorage, integration and categorization. Noise will lead to excessively complex\nmodels with deteriorated performance [48], resulting in even larger computing\ntimes for less value.\nThe impact of noise in Big Data, among other pernicious traits, has not been\ndisregarded. Recently, Smart Data (focusing on veracity and value) has been in-\ntroduced, aiming to lter out the noise and to highlight the valuable data, which\ncan be eectively used by companies and governments for planning, operation,\nmonitoring, control, and intelligent decision making. Three key attributes are\nneeded for data to be smart, it must be accurate, actionable and agile:\n Accurate: data must be what it says it is with enough precision to drive\nvalue. Data quality matters.\n Actionable: data must drive an immediate scalable action in a way that\nmaximizes a business ob jective like media reach across platforms. Scalable\naction matters.\n Agile: data must be available in real-time and ready to adapt to the\nchanging business environment. Flexibility matters.\n\nAdvanced Big Data modeling and analytics are indispensable for discov-\nering the underlying structure from retrieved data in order to acquire Smart\nData. In this paper we provide several preprocessing techniques for Big Data,\ntransforming raw, corrupted datasets into Smart Data. We focus our interest\non classication tasks, where two types of noise are distinguished: class noise,\nwhen it aects the class label of the instances, and attribute noise, when it aects\n\n2\n\n\fthe rest of attributes. The former is known to be the most disruptive [38, 53].\nConsequently, many recent works, including this contribution, have been de-\nvoted to resolving this problem or at least to minimize its eects (see [14] for\na comprehensive and updated survey). While some architectural designs are\nalready proposed in the literature[51], there is no particular algorithm which\ndeals with noise in Big Data classication, nor a comparison of its eect on\nmodel generalization abilities or computing times.\nThereby we propose a framework for Big Data under Apache Spark for\nremoving noisy examples composed of two algorithms based on ensembles of\nclassiers. The rst one is an homogeneous ensemble, named Homogeneous\nEnsembe for Big Data (HME-BD), which uses a single base classier (Random\nForest [4]) over a partitioning of the training set. The second ensemble is an\nheterogeneous ensemble, namely Heterogeneous Ensembe for BigData (HTE-\nBD), that uses dierent classiers to identify noisy instances: Random Forest,\nLogistic Regression and K-Nearest Neighbors (KNN) as base classiers. For the\nsake of a more complete comparison, we have also considered a simple ltering\napproach based on similarities between instances, named Edited Nearest Neigh-\nbor for Big Data (ENN-BD). ENN-BD examines the nearest neighbors of every\nexample in the training set and eliminates those whose ma jority of neighbors\nbelong to a dierent class. All these techniques have been implemented under\nthe Apache Spark framework [19, 40] and can be downloaded from the Sparks\ncommunity repository 1 .\nTo show the performance of the three proposed algorithms, we have carried\nout an experimental evaluation with four large datasets, namely SUSY, HIGGS,\nEpsilon and ECBDL14. We have induced several levels of class noise to eval-\nuate the eects of applying such framework and the improvements obtained in\nterms of classication accuracy for two classiers: a decision tree and the KNN\ntechnique. Decision trees with pruning are known to be tolerant to noise, while\nKNN is a noise sensitive algorithm when the number of selected neighbors is\nlow. These dierences allow us to better compare the eect of the framework in\nclassiers which behave dierently towards noise. We also show that, for the Big\nData problems considered, the classiers also benet from applying the noise\ntreatment even when no additional noise is induced, since Big Data problems\ncontain implicit noise due to incidental homogeneity, spurious correlations and\nthe accumulation of noisy examples [11]. The results obtained indicate that\nthe framework proposed can successfully deal with noise. In particular, the ho-\nmogeneous ensemble is a suitable technique for dealing with noise in Big Data\nproblems, with low computing times and enabling the classier to achieve better\naccuracy.\nThe remainder of this paper is organized as follows: Section 2 presents the\nconcepts of noise, MapReduce and Smart Data. Section 3 explains the pro-\nposed framework. Section 4 describes the experiments carried out to check the\nperformance of the framework. Finally, Section 5 concludes the paper.\n\n1 https://spark- packages.org/package/djgarcia/NoiseFramework\n\n3\n\n\f2. Related work\n\nIn this section we rst present the problem of noise in classication tasks\nin Section 2.1. Then we introduce the MapReduce framework commonly used\nin Big Data solutions in Section 2.2. Finally, we provide an insight into Smart\nData in 2.3.\n\n2.1. Class noise vs. attribute noise\n\nIn a classication problem, several eects of this noise can be observed by\nanalyzing its spatial characteristics: noise may create small clusters of instances\nof a particular class in the instance space corresponding to another class, displace\nor remove instances located in key areas within a concrete class, or disrupt the\nboundaries of the classes resulting in an increased boundaries overlap. All these\nimperfections may harm data interpretation, the design, size, building time,\ninterpretability and accuracy of models, as well as decision making [52, 53].\nAs described by Wang et al. [47], from the large number of components that\ncomprise a dataset, class labels and attribute values are two essential elements\nin classication datasets. Thus, two types of noise are commonly dierentiated\nin the literature [53, 47]:\n Class noise, also known as label noise, takes place when an example is\nwrongly labeled. Class noise includes contradictory examples [42, 38] (ex-\namples with identical input attribute values having dierent class labels)\nand misclassications [53] (examples which are incorrectly labeled).\n Attribute noise refers to corruptions in the values of the input attributes.\nIt includes erroneous attribute values, missing values and incomplete at-\ntributes or do not care values. Missing values are usually considered\nindependently in the literature, so attribute noise is mainly used for erro-\nneous values [53].\n\nClass noise is generally considered more harmful to the learning process, and\nmethods for dealing with class noise are more frequent in the literature [53].\nClass noise may have many reasons, such as errors or sub jectivity in the data\nlabeling process, as well as the use of inadequate information for labeling. Data\nlabeling by domain experts is generally costly, and automatic taggers are used\n(e.g., sentiment analysis polarization [29]), increasing the probability of class\nnoise.\nDue to the increasing attention from researchers and practitioners, numerous\ntechniques have been developed to tackle it [14, 53, 15]. These techniques include\nlearning algorithms robust to noise as well as data preprocessing techniques that\nremove or repair noisy instances. In [14] the mechanisms that generate label\nnoise are examined, relating them to the appropriate treatment procedures that\ncan be safely applied:\n On the one hand, algorithm level approaches attempt to create robust\nclassication algorithms that are little inuenced by the presence of noise.\n\n4\n\n\fThis includes approaches where existing algorithms are modied to cope\nwith label noise by either being modeled in the classier construction [25,\n27], by applying pruning strategies to avoid overting as in [34] or by di-\nminishing the importance of noisy instances with respect to clean ones [32].\nRecent proposals exist which that combine these two approaches, which\nmodel the noise and give less relevance to potentially noisy instances in\nthe classier building process [3].\n On the other hand, data level approaches (also called lters ) try to develop\nstrategies to cleanse the dataset as a previous step to the t of the classier,\nby either creating ensembles of classiers [5], iteratively ltering noisy\ninstances [23], computing metrics on the data or even hybrid approaches\nthat combine several of these strategies.\n\nIn the Big Data environment there is a special need for noise lter methods.\nIt is well known that the high dimensionality and example size generate accu-\nmulated noise in Big Data problems [11]. Noise lters reduce the size of the\ndatasets and improve the quality of the data by removing noisy instances, but\nmost of the classic algorithms for noisy data, noise lters in particular, are not\nprepared for working with huge volumes of data.\n\n2.2. Big Data. MapReduce and Apache Spark\n\nThe globalization of the Big Data paradigm is generating a large response in\nterms of technologies that must deal with the rapidly growing rates of generated\ndata [39]. Among all of them, MapReduce is the seminal framework designed\nby Google in 2003 [8]. It follows a divide and conquer approach to process and\ngenerate large datasets with parallel and distributed algorithms on a cluster.\nThe MapReduce model is composed of two phases: Map and Reduce. The Map\nphase performs a transformation of the data, and the Reduce phase performs\na summary operation. Briey explained, rst the master node splits the input\ndata and distributes it across the cluster. Then the Map transformation is\napplied to each key-value pair in the local data. Once that process is nished\nthe data is redistributed based on the key-value pairs generated in the Map\nphase. Once all pairs belonging to one key are in the same node, it is processed\nin parallel. Apache Hadoop [45] [1] is the most popular open-source framework\nbased on the MapReduce model.\nApache Spark [19, 40] is an open-source framework for Big Data processing\nbuilt around speed, ease of use and sophisticated analytics. Its main feature is\nits ability to use in-memory primitives. Users can load their data into mem-\nory and iterate over it repeatedly, making it a suitable tool for ML algorithms.\nThe motivation for developing Spark came from the limitations in the MapRe-\nduce/Hadoop model [28, 12]:\n Intensive disk usage\n Insuciency for in-memory computation\n\n5\n\n\f Poor performance on online and iterative computing.\n Low inter-communication capacity.\n\nSpark is built on top of a distributed data structure called Resilient Dis-\ntributed Datasets (RDDs) [50]. Operations on RDDs are applied to each par-\ntition of the node local data. RDDs support two types of operations: trans-\nformations, which are not evaluated when dened and produce a new RDD,\nand actions, which evaluate all the previous transformations and return a new\nvalue. The RDD structure allows programmers to persist them into memory\nor disk for re-usability purposes. RDDs are immutable and fault-tolerant by\nnature. All operations are tracked using a lineage, so that each partition can\nbe recalculated in case of failure.\nAlthough new promising frameworks for Big Data are emerging, like Apache\nFlink [13], Apache Spark is becoming the reference in performance [18].\n\n2.3. From Big Data to Smart Data\n\nBig Data is an appealing discipline that presents an immense potential for\nglobal economic growth and promises to enhance competitiveness of high tech-\nnological countries. Such as occurs in any knowledge extraction process, vast\namounts of data are analyzed, processed, and interpreted in order to generate\nprots in terms of either economic or advantages for society. Once the Big Data\nhas been analyzed, processed, interpreted and cleaned, it is possible to access\nit in a structured way. This transformation is the dierence between Big and\nSmart Data [26].\nThe rst step in this transformation is to perform an integration process,\nwhere the semantics and domains from several large sources are unied under a\ncommon structure. The usage of ontologies to support the integration is a recent\napproach [9, 7], but graph databases are also an option where the data is stored\nin a relational form, as in healthcare domains [35]. Even when the integration\nphase ends, the data is still far from being smart: the accumulated noise in Big\nData problems creates problems in classical Data Mining techniques, specially\nwhen the dimensionality is large [10]. Thus, in order to be smart, the data\nstill needs to be cleaned even after its integration, and data preprocessing is the\nset of techniques utilized to encompass this task [15, 16].\nOnce the data is smart, it can hold the valuable data and allows interac-\ntions in real time, like transactional activities and other Business Intelligence\napplications. The goal is to evolve from a data-centered organization to a learn-\ning organization, where the focus is set on the knowledge extracted instead of\nstruggling with the data management [21]. However, Big Data generates great\nchallenges to achieve this since its high dimensionality and large example size\nimply noise accumulation, algorithmic instability and the massive sample pool\nis often aggregated from heterogeneous sources [11]. While feature selection,\ndiscretization or imbalanced algorithms to cope with the high dimensionality\nhave drawn the attention of current Big Data frameworks (such as Sparks ML-\nlib [31]) and researchers [37, 41, 36, 43], algorithms to clean noise are still a\n\n6\n\n\fchallenge. In summary, challenges are still present to fully operate a transition\nbetween Big Data to Smart Data. In this paper we provide an automated pre-\nprocessing framework to deal with class noise, enabling the practitioner to reach\nSmart Data.\n\n3. Noise ltering for Big Data\n\nIn this section, we present the framework for Big Data under Apache Spark\nfor removing noisy examples based on the MapReduce paradigm, proving its\nperformance over real-world large problems. It is a MapReduce design where\nall the noise lter processes are performed in a distributed way.\nFor the implementation of the framework, we have used some basic Spark\nprimitives. Here, we outline those more relevant to the algorithm:\n map: Applies a transformation to each element of a RDD and returns the\nresulting RDD.\n z ipW ithI ndex: Zips a RDD with its element indices.\n j oin: Return a RDD containing all pairs of elements with matching keys\nbetween two RDDs.\n f ilter : Return a new RDD containing only the elements that satisfy a\npredicate.\n\nWe have designed two algorithms based on ensembles. Both perform a k-fold\non the training data and identify noisy instances in the test partition. The rst\none is an homogeneous ensemble using Random Forest as a classier, named\nHME-BD (Section 3.1). The second one, named HTE-BD (Section 3.2) is a\nheterogeneous ensemble based on the use of three dierent classiers: Random\nForest, Logistic Regression and KNN. We have also designed a simple lter\nbased on the similarity between the instances using KNN as a classier, named\nENN-BD (Section 3.3).\n\n7\n\n\fAlgorithm 1 HME-BD Algorithm\n\n1: Input: data an RDD of type LabeledPoint (label, features)\n2: Input: P the number of partitions\n3: Input: nTrees the number of trees for Random Forest\n4: Output: the ltered RDD without noise\n5: kF old  kF old(data, P )\n6: f ilteredData \n7: map train, test  kF old\nrf M odel  randomF orest(train, nT rees)\n8:\nrf P red  predict(rf M odel, test)\nj oinedData  j oin(z ipW ithI ndex(test), z ipW ithI ndex(rf P red))\nf ilteredData \nmap orig , pred  j oinedData\nif label(orig) = label(pred) then\norig\n14:\nelse\n15:\nLabeledP oint(noise, f eatures(orig))\n16:\nend if\n17:\nend map\n18:\n19: end map\n20: return(f ilter(f ilteredData, label (cid:54)= noise))\n\n9:\n10:\n11:\n12:\n13:\n\n3.1. Homogeneous Ensemble: HME-BD\n\nAlgorithm 1 describes the noise ltering process in HME-BD. The homo-\ngeneous ensemble is based on a Cross-Validated Committees Filter [44]. The\nalgorithm lters the noise in a dataset by performing a k-fold on the input data.\nSparks kF old function returns an array of (train, test) for a given P . With a\nMap function we iterate through each train and test. We learn a Random For-\nest model using the train as input data and predict the test using the learned\nmodel. Then we join the test data and the predicted data by index using the\nz ipW ithI ndex operation in both RDDs in order to compare the classes. The\nnext step is to iterate using a Map function through each instance in order to\ncheck if the original class and the predicted one are the same. If the predicted\nclass and the original are dierent, the instance is marked as noise. Once all in-\nstances have been checked, marked ones are ltered and the dataset is returned.\nThe following are required as input parameters: the dataset, the number of\npartitions and the number of trees for the Random Forest.\n\n3.2. Heterogeneous Ensemble: HTE-BD\nThe noise ltering process in HTE-BD is shown in Algorithm 2. The hetero-\ngeneous ensemble is based on Ensemble Filter [5]. Like HME-BD, the algorithm\nlters the noise in a dataset by performing a k-fold on the input data. For each\nfold it learns three classication algorithms: Random Forest, Logistic Regres-\nsion and 1NN using the train as input data. Then it predicts the test data with\n\n8\n\n\fAlgorithm 2 HTE-BD Algorithm\n\n1: Input: data an RDD of type LabeledPoint (label, features)\n2: Input: P the number of partitions\n3: Input: nTrees the number of trees for Random Forest\n4: Input: vote the voting strategy (ma jority or consensus)\n5: Output: the ltered RDD without noise\n6: kF old  kF old(data, P )\n7: f ilteredData \n8: map train, test  kF old\nclassif iersM odel  learnC lassif iers(train, nT rees)\n9:\n(rf , lr, knn)  predict(classif iersM odel, test)\n10:\npredictions \n11:\nmap orig  test\n12:\ncount  0\n13:\nif rf (cid:54)= label(orig) then count  count + 1 end if\nif lr (cid:54)= label(orig) then count  count + 1 end if\nif knn (cid:54)= label(orig) then count  count + 1 end if\nif vote = maj ority then\nif count  2 then LabeledP oint(noise, f eatures(orig)) end if\nif count < 2 then orig end if\nelse\n20:\nif count = 3 then LabeledP oint(noise, f eatures(orig)) end if\n21:\nif count (cid:54)= 2 then orig end if\n22:\nend if\n23:\nend map\n24:\n25: end map\n26: return(f ilter(f ilteredData, label (cid:54)= noise))\n\n14:\n15:\n16:\n17:\n18:\n19:\n\nthe three learned models.\nIt iterates through each instance in the test data\ncomparing the three predictions and, depending upon the voting strategy, the\ninstance is marked as noise or as clean. Once all instances have been checked,\nthe data is ltered and returned. The following are required as input parame-\nters: the dataset, the number of partitions, the number of trees for the Random\nForest and the voting strategy.\n\n3.3. Similarity: ENN-BD\n\nThe noise ltering process in ENN-BD is simpler than that the in two previ-\nous algorithms. ENN-BD is based on Edited Nearest Neighbor [46] and follows\na distance between instances approach. This lter performs a 1NN using the\neuclidean distance and checks for each instance if its closest neighbor belongs to\nthe same class. If it has a dierent class, the instance is marked as noise. Once\nall instances have been checked, marked instances are removed from the train-\ning data. This process is described in Algorithm 3. The only input parameter\nrequired is the dataset.\n\n9\n\n\fAlgorithm 3 ENN-BD Algorithm\n\n1: Input: data an RDD of type LabeledPoint (label, features)\n2: Output: the ltered RDD without noise\n3: knnM odel  KN N (1, euclidean, data)\n4: knnP red  z ipW ithI ndex(predict(knnM odel, data))\n5: j oinedData  j oin(z ipW ithI ndex(data), knnP red)\n6: f ilteredData \n7: map orig , pred  j oinedData\nif label(orig) = label(pred) then\n8:\norig\n9:\nelse\n10:\nLabeledP oint(noise, f eatures(orig))\n11:\nend if\n12:\n13: end map\n14: return(f ilter(f ilteredData, label (cid:54)= noise))\n\nTable 1: Datasets used in the analysis\n\nDataset\n\nInstances\n\nAtts.\n\nTotal\n\nCL\n\nSUSY\nHIGGS\nEpsilon\nECBDL14\n\n5,000,000\n11,000,000\n500,000\n1,000,000\n\n18\n28\n2,000\n631\n\n90,000,000\n308,000,000\n1,000,000,000\n631,000,000\n\n2\n2\n2\n2\n\n4. Experimental Results\n\nThis section describes the experimental details and the analysis carried out\nto show the performance of the three noise lter methods over four huge prob-\nlems. In Section 4.1, we present the details of the datasets and the parameters\nused in the methods. We analyze the accuracy improvements generated by the\nproposed framework and the study of instances removed in Section 4.2. Finally,\nSection 4.3 is devoted to the computing times of the proposals.\n\n4.1. Experimental Framework\n\nFour classication datasets are used in our experiments:\n SUSY dataset, which consists of 5,000,000 instances and 18 attributes.\nThe rst eight features are kinematic properties measured by the parti-\ncle detectors at the Large Hadron Collider. The last ten are functions of\nthe rst eight features. The task is to distinguish between a signal pro-\ncess which produces supersymmetric (SUSY) particles and a background\nprocess which does not [2].\n HIGGS dataset, which has 11,000,000 instances and 28 attributes. This\ndataset is a classication problem to distinguish between a signal process\nwhich produces Higgs bosons and a background process which does not.\n\n10\n\n\fTable 2: Parameter setting for the noise lters\n\nAlgorithm Parameters\n\nClassier\n\nHME-BD P = 4, 5\n\nHTE-BD P = 4, 5\nVoting = ma jor-\nity, consensus\nENN-BD K = 1\n\nRandom Forest:\nfeatureSubsetStrategy = auto, im-\npurity = gini, maxDepth = 10 and maxBins = 32\n1NN, Random Forest: featureSubsetStrategy = auto,\nimpurity = gini, maxDepth = 10 and maxBins = 32\n\n-\n\nTable 3: Parameter setting for the classiers\n\nClassier\n\nParameters\n\nK = 1\nKNN\nimpurity = gini, maxDepth = 20 and maxBins = 32\nDecision Tree\n Epsilon dataset, which consists of 500,000 instances with 2,000 numerical\nfeatures. This dataset was articially created for the Pascal Large Scale\nLearning Challenge in 2008. It was further pre-processed and included in\nthe LibSVM dataset repository [6].\n ECBDL14 dataset, which has 32 million instances and 631 attributes (in-\ncluding both numerical and categorical). This dataset was used as a ref-\nerence at the ML competition of the Evolutionary Computation for Big\nData and Big Learning held on July 14, 2014, under the international\nconference GECCO-2014. It is a binary classication problem where the\nclass distribution is highly imbalanced: 98% of negative instances. For\nthis problem, we use a reduced version with 1,000,000 instances and 30%\nof positive instances.\n\nTable 1 provides a brief summary of these datasets, showing the number of\nexamples (Instances), the total number of attributes (Atts.), the total number\nof training data (Total), and the number classes (CL).\nWe carried out experiments on ve levels of uniform class noise [42]: for each\nlevel of noise, a percentage of the training instances are altered by replacing their\nactual label by another label from the available classes. The selected noise levels\nare 0%, 5%, 10%, 15% and 20%. In this case, a 0% noise level indicates that\nthe dataset was unaltered. We have conducted a hold-out validation due to the\ntime limitations of the KNN algorithm.\nIn Table 2 we can see the complete list of parameters used for the noise\ntreatment algorithms. In order to evaluate the eect of the number of partitions\non the behavior of the lters, we have selected 4 and 5 training partitions for\nHME-BD and HTE-BD. For the heterogeneous lter, HTE-BD, we also use two\nvoting strategies: consensus (same result for all classiers) and ma jority (same\nresult for at least half the classiers).\nTwo classiers, one MLlib classier, a decission tree, and one algorithm\n\n11\n\n\fTable 4: KNN test accuracy. The highest accuracy value per dataset and noise level is stressed\nin bold\n\nDataset\nP\nVote\n\nSUSY\n\nHIGGS\n\nEpsilon\n\nECBDL14\n\nNoise (%) Original HME-BD\n4\n\n5\n\nHTE-BD\n4\n5\n5\n4\nMa jority Consensus Ma jority Consensus\n\nENN-BD\n\n0\n5\n10\n15\n20\n\n0\n5\n10\n15\n20\n\n0\n5\n10\n15\n20\n\n0\n5\n10\n15\n20\n\n71.79\n69.62\n67.44\n65.27\n63.10\n\n61.21\n60.10\n58.97\n57.84\n56.69\n\n56.55\n55.71\n55.20\n54.54\n54.05\n\n74.83\n72.36\n69.86\n67.39\n64.90\n\n78.73\n78.68\n78.63\n78.62\n78.56\n\n64.26\n64.06\n63.83\n63.65\n63.53\n\n58.11\n58.64\n58.51\n58.39\n58.02\n\n76.06\n75.60\n75.31\n75.11\n74.82\n\n78.72\n78.69\n78.62\n78.61\n78.58\n\n64.25\n64.07\n63.84\n63.64\n63.40\n\n58.06\n58.60\n58.61\n58.41\n58.09\n\n76.03\n75.59\n75.32\n75.12\n74.83\n\n77.86\n77.68\n77.44\n77.19\n76.93\n\n63.94\n63.63\n63.29\n62.86\n62.55\n\n57.43\n57.47\n57.26\n57.00\n56.75\n\n75.12\n74.59\n74.19\n73.99\n73.70\n\n74.64\n73.38\n72.01\n70.52\n69.10\n\n62.30\n61.45\n60.65\n59.81\n58.89\n\n55.19\n55.47\n55.25\n55.00\n54.72\n\n73.54\n72.89\n72.50\n72.11\n71.89\n\n77.88\n77.68\n77.46\n77.20\n76.93\n\n63.93\n63.62\n63.24\n62.89\n62.55\n\n57.39\n57.39\n57.26\n57.02\n56.71\n\n75.14\n74.59\n74.19\n74.01\n73.70\n\n74.65\n73.39\n72.00\n70.53\n69.04\n\n62.23\n61.44\n60.66\n59.81\n58.85\n\n55.40\n55.41\n55.25\n55.03\n54.72\n\n73.46\n72.84\n72.47\n72.06\n71.90\n\n72.02\n69.84\n67.66\n65.28\n63.25\n\n60.65\n59.60\n58.56\n57.52\n56.45\n\n56.21\n55.43\n54.79\n54.30\n53.68\n\n73.94\n72.77\n71.40\n69.68\n67.64\n\npresent in Sparks community repository, KNN2 , are used to evaluate the ef-\nfectiveness of the ltering carried out by the two ensemble proposals and the\nsimilarity lter. The decision tree can adapt its depth to avoid overtting to\nnoisy instances, while KNN is known to be sensitive to noise when the number\nof selected neighbors is low. Prediction accuracy is used to evaluate the models\nperformance produced by the classiers (number of examples correctly labeled\nas belonging to a given class divided by the total number of elements). The\nparameters used for the classiers can be seen in Table 3.\nFor all experiments we have used a cluster composed of 20 computing nodes\nand one master node. The computing nodes hold the following characteristics:\n2 processors x Intel(R) Xeon(R) CPU E5-2620, 6 cores per processor, 2.00 GHz,\n2 TB HDD, 64 GB RAM. Regarding software, we have used the following con-\nguration: Hadoop 2.6.0-cdh5.4.3 from Clouderas open source Apache Hadoop\ndistribution, Apache Spark and MLlib 1.6.0, 460 cores (23 cores/node), 960\nRAM GB (48 GB/node).\n\n4.2. Analysis of accuracy performance and removed instances\n\nIn this section, we present the analysis on the performance results obtained\nby the selected classiers after applying the proposed framework. We denote\nwith Original the application of the classier without using any noise treatment\ntechniques, in order to evaluate the impact of the increasing noise level in the\nquality of the models extracted by the classication algorithms.\nTable 4 shows the test accuracy values for the four datasets and the ve\n\n2 https://spark- packages.org/package/JMailloH/kNN_IS\n\n12\n\n\flevels of noise using the KNN algorithm for classication. From these results\nwe can point out that:\n It is important to remark that the usage of any noise treatment tech-\nnique always improves the Original accuracy value at the same noise level.\nPlease note that the usage of the noise treatment technique allows KNN\nto obtain better performance at any noise level, even at the highest ones,\nthan Original at 0% level for every dataset. Since Big Datasets tend to\naccumulate noise, the proposed noise framework is able to improve the\nbehavior and performance of the KNN classier in every case.\n If we attend the best noise treatment strategy for KNN, we must point\nout that the homogeneous lter, HME-BD, enables KNN to obtain the\nhighest accuracy values.\n The dierent number of partitions used for HME-BD has little impact in\nthe accuracy values, which, in this respect, makes it a robust method.\n The heterogeneous ensemble lter, HTE-BD, is also robust to the number\nof partitions chosen, but its performance is lower than HME-BD. However,\nthe voting scheme is crucial for HTE-BD, as the consensus strategy will\nresult in worse accuracy for KNN, being close to 2% less accuracy for the\nconsensus voting strategy.\n The baseline noise ltering method, ENN-BD, is the worst option as KNN\nobtains the lowest accuracy values among the three noise treatment strate-\ngies. For ENN-BD, the accuracy drops around 2% for each 5% increment\nin noise instances. However, as mentioned earlier, ENN-BD is still prefer-\nable to not dealing with the noise at all. This is due to the noise sensitive\nnature of KNN.\n\nTable 5 gathers the test accuracy values for the three noise lter methods\nusing a decision tree. From these results we can point out that:\n Again, avoiding the treatment of noise is never the best option and using\nthe appropriate noise ltering technique will provide a signicant improve-\nment in accuracy. However, since the decision tree is more robust against\nnoise than KNN, not all the lters are better than avoiding ltering noise\n(Original ). When the lters remove too many instances, both noisy and\nclean, the decision tree is more aected since it is able to withstand small\namounts of noise while exploiting the clean instances. KNN was very af-\nfected by the noisy instances left, in a higher degree than the decision\ntree. Thus, a wrong ltering strategy will penalize the performance of the\ndecision tree. We will elaborate more on this later.\n In terms of the best ltering technique for the decision tree, for low levels\nof noise, the heterogeneous ensemble HTE-BD can perform slightly better\nthan the homogeneous HME-BD for some datasets. Nevertheless, from\na 10% noise level onwards, HME-BD outperforms HTE-BD, making it a\nbetter approach to deal with noise for the decision tree.\n\n13\n\n\fTable 5: Decision tree test accuracy. The highest accuracy value per dataset and noise level\nis stressed in bold\n\nDataset\nP\nVote\n\nSUSY\n\nHIGGS\n\nEpsilon\n\nNoise (%) Original HME-BD\n4\n\n5\n\nHTE-BD\n4\n5\n5\n4\nMa jority Consensus Ma jority Consensus\n\nENN-BD\n\n0\n5\n10\n15\n20\n\n0\n5\n10\n15\n20\n\n0\n5\n10\n15\n20\n\n80.24\n79.94\n79.15\n78.21\n77.09\n\n70.17\n69.61\n69.22\n68.65\n67.82\n\n62.39\n61.10\n60.09\n59.02\n57.73\n\n79.78\n79.99\n79.85\n79.81\n79.71\n\n71.16\n71.14\n71.06\n71.03\n71.05\n\n66.86\n66.64\n66.87\n66.62\n66.46\n\n79.79\n79.97\n79.84\n79.80\n79.73\n\n71.17\n71.11\n71.04\n70.99\n71.02\n\n66.19\n66.83\n67.00\n66.85\n66.79\n\n79.69\n80.07\n79.81\n79.32\n79.35\n\n69.61\n69.34\n68.95\n68.52\n68.18\n\n65.13\n65.32\n65.46\n65.33\n65.08\n\n80.27\n80.36\n80.04\n79.47\n78.95\n\n70.41\n69.98\n69.56\n69.04\n68.38\n\n66.07\n66.09\n66.11\n65.99\n65.69\n\n79.17\n80.10\n79.81\n79.61\n79.31\n\n69.68\n69.36\n68.97\n68.65\n68.35\n\n65.11\n65.33\n65.47\n65.29\n64.98\n\n80.29\n80.34\n80.22\n79.48\n79.41\n\n70.33\n69.92\n69.58\n69.06\n68.39\n\n66.02\n66.09\n66.10\n66.00\n65.65\n\n78.56\n77.49\n77.00\n75.81\n74.21\n\n68.85\n68.29\n67.52\n66.93\n66.05\n\n61.54\n60.41\n59.20\n58.09\n56.71\n\nECBDL14\n\n73.66\n74.62\n74.35\n74.51\n74.21\n74.38\n74.59\n73.98\n0\n73.48\n74.75\n74.25\n74.54\n74.16\n74.40\n74.64\n72.87\n5\n72.75\n74.63\n73.94\n74.51\n73.84\n74.25\n74.59\n71.67\n10\n71.68\n74.10\n73.98\n73.91\n73.82\n74.22\n74.61\n70.28\n15\n70.16\n73.86\n73.85\n73.82\n73.78\n74.18\n74.83\n68.66\n20\n Regarding the HTE-BD voting strategy, the consensus scheme achieves\nbetter results than the ma jority voting strategy. Please note that the\nopposite has been observed in KNN: since KNN is much more sensitive\nand demands cleaner class borders achieved with the ma jority voting, the\ndecision tree benets from a more accurate noise removal provided by the\nconsensus voting.\n The baseline method, ENN-BD, is achieving around 1% less accuracy\nthan the rest for low levels of noise, but this dierence increases to 5%\nless accuracy in higher noise levels.\n\nThe results presented have shown the importance of applying a noise treat-\nment strategy, no matter how much noise is present in the dataset, and the best\nstrategy overall: HME-BD. To better explain why HME-BD is the best ltering\nstrategy in the framework, we must study the amount of instances removed.\nIn Table 6 we present the average number of instances left after the appli-\ncation of the three noise ltering methods for the four datasets. In Figure 1\nwe can see a graphic representation of the number of instances for the sake\nof a better depiction. As we can expect, the higher the percentage of noise,\nthe lower the number of instances that remain in the dataset after applying\nthe ltering technique. However, there are dierent patterns depending on the\nltering technique used:\n For the homogeneous ensemble HME-BD, there is no eect in the number\nof partitions P chosen with respect to the amount of removed instances.\nOn average, HME-BD removes around 20% of the instances at a 0% noise\nlevel. At each noise level increment an average of 3% of the instances are\nremoved.\n\n14\n\n\fTable 6: Average number of instances for HME-BD, HTE-BD and ENN-BD\n\nDataset\nP\nVote\n\nSUSY\n\nHIGGS\n\nEpsilon\n\nNoise Original\n\nHME-BD\n4\n\n5\n\nHTE-BD\n4\n5\n5\n4\nMa jority Consensus Ma jority Consensus\n\n0%\n5%\n10%\n15%\n20%\n\n0%\n5%\n10%\n15%\n20%\n\n0%\n5%\n10%\n15%\n20%\n\n2,500,000\n2,500,000\n2,500,000\n2,500,000\n2,500,000\n\n5,500,000\n5,500,000\n5,500,000\n5,500,000\n5,500,000\n\n250,000\n250,000\n250,000\n250,000\n250,000\n\n1,984,396\n1,910,750\n1,837,604\n1,763,890\n1,691,290\n\n3,900,547\n3,787,000\n3,672,429\n3,554,120\n3,446,352\n\n164,222\n186,707\n180,489\n173,027\n166,191\n\n1,983,785\n1,911,317\n1,837,408\n1,764,176\n1,691,506\n\n3,900,035\n3,786,366\n3,672,553\n3,557,252\n3,443,459\n\n164,292\n186,839\n180,517\n173,114\n166,247\n\n1,974,018\n1,872,868\n1,801,616\n1,728,789\n1,657,323\n\n3,567,784\n3,484,271\n3,404,181\n3,324,547\n3,242,174\n\n194,252\n186,890\n180,296\n173,226\n166,394\n\n2,281,521\n2,241,766\n2,207,999\n2,174,051\n2,144,595\n\n5,048,874\n5,014,344\n4,972,401\n4,930,575\n4,888,991\n\n242,757\n239,200\n235,425\n231,962\n228,153\n\n1,973,587\n1,874,053\n1,800,276\n1,727,949\n1,657,035\n\n3,564,879\n3,484,274\n3,401,624\n3,323,465\n3,240,623\n\n194,037\n186,957\n180,332\n173,274\n166,285\n\n2,280,941\n2,242,598\n2,203,012\n2,175,876\n2,141,811\n\n5,051,498\n5,013,132\n4,973,794\n4,932,060\n4,886,961\n\n242,730\n239,200\n235,456\n231,997\n228,394\n\nENN-BD\n\n1,262,317\n1,260,781\n1,258,441\n1,256,611\n1,254,441\n\n2,765,831\n2,763,942\n2,760,547\n2,754,636\n2,756,382\n\n125,072\n124,983\n125,064\n124,980\n124,583\n\nECBDL14\n\n0%\n500,000\n387,815\n387,873\n393,242\n470,731\n393,273\n470,924\n367,101\n5%\n500,000\n370,991\n371,094\n377,451\n458,758\n377,239\n459,212\n344,717\n10%\n500,000\n357,565\n357,270\n361,587\n448,460\n361,614\n448,550\n324,674\n15%\n500,000\n344,363\n344,427\n346,454\n439,633\n346,633\n439,028\n306,832\n20%\n500,000\n330,694\n330,761\n331,552\n430,444\n331,511\n430,357\n292,000\n For the Epsilon dataset, at 20% nosie, HME-BD does not remove as many\ninstances as expected, but it is still the best option out of the two classi-\ners. A high instance redundancy in this dataset may cause homogeneous\nvoting to not discard as many instances as the other lters.\n Like HME-BD, HTE-BD is not aected by the number of partitions, but\nthe voting scheme does have a great impact on its behavior. While the\nma jority voting strategy achieves almost the same number of removed\ninstances as HME-BD, the consensus voting strategy is more conservative.\nConsensus voting removes 10% of the instances for 0% level of noise, and\nit is increasing a 3% on average as the level of noise increases, the same\nrate as HME-BD.\n ENN-BD is the lter that removes more instances. On average it removes\nhalf the instances of the datasets for 0% level of noise and then increases\naround 1% at each increment of noise level. This aggresive ltering hinders\nthe performance of noise tolerant classiers, such as the decision tree.\n In general, HME-BD is the most balanced technique in terms of instances\nremoved and kept. Although the amount of instances removed by HTE-\nBD with ma jority voting is very similar to HME-BD, the instances se-\nlected to be eliminated are dierent, severely aecting the classier used\nafterwards.\n\nIn view of the results, we can conclude that HME-BD is the most suitable\nensemble option in the proposed framework to deal with noise in Big Data\nproblems. Even when we did not introduce any additional noise, the usage\nof noise treatment methods has proven to be very benecial. As previously\n\n15\n\n\f(a) SUSY\n\n(b) HIGGS\n\n(c) Epsilon\n\n(d) ECBDL14\n\nFigure 1: Number of instances after the ltering process\n\nTable 7: Average run times for HME-BD, HTE-BD and ENN-BD in seconds\n\nDataset\nP\nVote\n\nHME-BD\n4\n\n5\n\nHTE-BD\n4\n5\n5\n4\nMa jority Consensus Ma jority Consensus\n\nENN-BD\n\nSUSY\nHIGGS\nEpsilon\nECBDL14\n\n513.46\n587.72\n1,868.75\n1,228.24\n\n632.54\n675.07\n2,021.14\n1,348.10\n\n5,511.15\n15,300.62\n4,120.79\n9,710.70\n\n5,855.66\n15,232.99\n7,201.05\n11,217.02\n\n6,701.62\n16,417.26\n5,179.09\n10,798.18\n\n6,399.32\n17,067.97\n5,664.06\n11,366.01\n\n8,956.71\n25,441.09\n2,718.97\n14,080.03\n\nmentioned, Big Data problems tend to accumulate noise and the proposed noise\nframework is a suitable tool to clean and proceed from Big to Smart Datasets.\n\n4.3. Computing times\n\nIn the previous section we have shown the suitability of the proposed frame-\nwork in terms of accuracy. In order to constitute a valid proposal in Big Data,\nthis framework has to be scalable as well. This section is devoted to present\nthe computing times for the two prosposed ensemble techniques, HME-BD and\nHTE-BD, and the simple similarity method, ENN-BD, used as a baseline.\nIn Table 7 we can see the average run times of the three methods for the\nfour datasets in seconds. As the level of noise is not a factor that aects the run\n\n16\n\n\fFigure 2: Run times chart\n\ntime, we show the average of the ve executions performed for each dataset. In\nFigure 2 we can see a graphic representation of these times.\nThe measured times show that the homogeneous ensemble, HME-BD, is\nnot only the best performing option in terms of accuracy, but also the most\necient one in terms of computing time. HME-BD is about ten times faster\nthan the heterogeneous lter HTE-BD and the similarity lter ENN-BD. This\nis caused by the usage of the KNN classier by HTE-BD and ENN-BD, which\nis very demanding in computing terms. As a result, HME-BD does not need\nto compute any distance measures, saving computing time and being the most\nrecommended option to deal with noise in Big Data problems.\n\n5. Conclusions\n\nIn this paper, we have tackled the problem of noise in Big Data classication,\nwhich is a crucial step in transforming such raw data into Smart Data. We have\nproposed several noise ltering algorithms, implemented in a Big Data frame-\nwork: Spark. These ltering techniques are based on the creation of ensembles\nof classiers that are executed in the dierent maps, enabling the practitioner to\ndeal with huge datasets. Dierent strategies of data partitioning and ensemble\nclassier combination have led to three dierent approaches.\nThe suitability of these proposed techniques has been analyzed using several\ndata sets, in order to study the accuracy improvement, running times and data\nreduction rates. The homogeneous ensemble has shown to be the most suitable\napproach in most cases, both in accuracy improvement and better running times.\nIt also shows the best balance between removing and keeping enough instances,\nbeing among the most balanced lter in terms of preprocessed training sets.\nThis work presents the rst suitable noise lter in Big Data domains, where\nthe high redundancy of the instances and high dimensional problems pose new\nchallenges to classic noise preprocessing algorithms. Thus, the presented frame-\nwork is a valuable tool for achieving the goal of Smart Data.\nIt also opens\npromising research lines in this topic, where the presence of iterative algorithms\n\n17\n\n\fand the usage of noise measures are also known as viable alternatives for dealing\nwith noise.\n\nAcknowledgment\n\nThis work is supported by the Spanish National Research Pro ject TIN2014-\n57251-P and the Foundation BBVA pro ject 75/2016 BigDaPTOOLS.\n\nReferences\n\nReferences\n\n[1] Apache Hadoop Pro ject, Apache Hadoop, http://hadoop.apache.org/\n(2016).\n\n[2] P. Baldi, P. Sadowski, D. Whiteson, Searching for Exotic Particles in High-\nEnergy Physics with Deep Learning, Nature Communications 5 (2014)\n4308.\n\n[3] C. Bouveyron, S. Girard, Robust supervised classication with mixture\nmodels: Learning from data with uncertain labels, Pattern Recognition\n42 (11) (2009) 26492658.\n\n[4] L. Breiman, Random forests, Machine Learning 45 (1) (2001) 532.\n\n[5] C. E. Brodley, M. A. Friedl, Identifying Mislabeled Training Data, Journal\nof Articial Intelligence Research 11 (1999) 131167.\n\n[6] C.-C. Chang, C.-J. Lin, Libsvm: A library for support vector machines,\nACM Transactions on Intelligent Systems and Technology (TIST) 2 (3)\n(2011) 27:127:27.\nURL http://doi.acm.org/10.1145/1961189.1961199\n\n[7] J. Chen, D. Dosyn, V. Lytvyn, A. Sachenko, Smart data integration by\ngoal driven ontology learning, in: Advances in Intelligent Systems and\nComputing, vol. 529, 2017, pp. 283292.\n\n[8] J. Dean, S. Ghemawat, Mapreduce: Simplied data processing on large\nclusters, Communications of the ACM 51 (1) (2008) 107113.\nURL http://doi.acm.org/10.1145/1327452.1327492\n\n[9] H. Fadili, C. Jouis, Towards an automatic analyze and standardization of\nunstructured data in the context of big and linked data, in: 8th Interna-\ntional Conference on Management of Digital EcoSystems, MEDES 2016,\n2016, pp. 223230.\n\n[10] J. Fan, Y. Fan, High dimensional classication using features annealed\nindependence rules, Annals of statistics 36 (6) (2008) 26052637.\n\n18\n\n\f[11] J. Fan, F. Han, H. Liu, Challenges of big data analysis, National Science\nReview 1 (2) (2014) 293314.\n\n[12] A. Fernandez, S. del Ro, V. Lopez, A. Bawakid, M. J. del Jesus, J. M.\nBentez, F. Herrera, Big data with cloud computing: an insight on the\ncomputing environment, mapreduce, and programming frameworks, Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery 4 (5)\n(2014) 380409.\nURL http://dx.doi.org/10.1002/widm.1134\n\n[13] A. Flink, Apache Flink, http://flink.apache.org/, http://flink.\napache.org.\n\n[14] B. Frenay, M. Verleysen, Classication in the presence of label noise: A sur-\nvey, IEEE Transactions on Neural Networks and Learning Systems 25 (5)\n(2014) 845869.\n\n[15] S. Garca, J. Luengo, F. Herrera, Data Preprocessing in Data Mining,\nSpringer Publishing Company, Incorporated, 2015.\n\n[16] S. Garca, J. Luengo, F. Herrera, Tutorial on practical tips of the most\ninuential data preprocessing algorithms in data mining, Knowledge-Based\nSystems 98 (2016) 129.\n\n[17] S. Garca, S. Ramrez-Gallego, J. Luengo, J. M. Bentez, F. Herrera, Big\ndata preprocessing: methods and prospects, Big Data Analytics 1 (1)\n(2016) 9.\nURL http://dx.doi.org/10.1186/s41044-016-0014-0\n\n[18] D. Garca-Gil, S. Ramrez-Gallego, S. Garca, F. Herrera, A comparison on\nscalability for batch big data processing on apache spark and apache ink,\nBig Data Analytics 2 (1) (2017) 1.\nURL http://dx.doi.org/10.1186/s41044-016-0020-2\n\n[19] M. Hamstra, H. Karau, M. Zaharia, A. Konwinski, P. Wendell, Learning\nSpark: Lightning-Fast Big Data Analytics, OReilly Media, 2015.\n\n[20] C.-H. Hsu, Intelligent big data processing, Future Generation Computer\nSystems 36 (2014) 16  18.\n\n[21] F. Iafrate, A journey from big data to smart data, Advances in Intelligent\nSystems and Computing 261 (2014) 2533.\n\n[22] IDC, The Digital Universe of Opportunities, http://www.emc.com/\ninfographics/digital-universe-2014.htm (2014).\n\n[23] T. M. Khoshgoftaar, P. Rebours, Improving software quality prediction by\nnoise ltering techniques, Journal of Computer Science and Technology 22\n(2007) 387396.\n\n19\n\n\f[24] D. Laney, 3D data management: Controlling data volume, velocity, and\nvariety, Tech. rep., META Group (February 2001).\nURL\nhttp://blogs.gartner.com/doug-laney/files/2012/01/\nad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.\npdf\n\n[25] N. D. Lawrence, B. Scholkopf, Estimating a kernel sher discriminant in\nthe presence of label noise, in: Proceedings of the Eighteenth International\nConference on Machine Learning, ICML 01, 2001, pp. 306313.\n\n[26] A. Lenk, L. Bonorden, A. Hellmanns, N. Roedder, S. Jaehnichen, Towards a\ntaxonomy of standards in smart data, in: Proceedings - 2015 IEEE Interna-\ntional Conference on Big Data, IEEE Big Data 2015, 2015, pp. 17491754.\n\n[27] Y. Li, L. F. Wessels, D. de Ridder, M. J. Reinders, Classication in the\npresence of class noise using a probabilistic kernel sher method, Pattern\nRecognition 40 (12) (2007) 33493357.\n\n[28] J. Lin, Mapreduce is good enough? if all you have is a hammer, throw\naway everything thats not a nail!, CoRR abs/1209.2191.\nURL http://arxiv.org/abs/1209.2191\n\n[29] B. Liu, Sentiment analysis: Mining opinions, sentiments, and emotions,\nCambridge University Press, 2015.\n\n[30] V. Mayer-Schonberger, K. Cukier, Big Data: A Revolution That Will\nTransform How We Live, Work, and Think, Houghton Miin Harcourt,\n2013.\n\n[31] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Free-\nman, D. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh,\nM. Zaharia, A. Talwalkar, Mllib: Machine learning in apache spark, Journal\nof Machine Learning Research 17 (34) (2016) 17.\n\n[32] Q. Miao, Y. Cao, G. Xia, M. Gong, J. Liu, J. Song, Rboost: Label noise-\nrobust boosting algorithm based on a nonconvex loss function and the\nnumerically stable base learners, IEEE Transactions on Neural Networks\nand Learning Systems 27 (11) (2016) 22162228.\n\n[33] D. Pyle, Data preparation for data mining, Morgan Kaufmann, Los Altos,\n1999.\n\n[34] J. R. Quinlan, C4.5: programs for machine learning, Morgan Kaufmann\nPublishers, San Francisco, CA, USA, 1993.\n\n[35] P. Ra ja, E. Sivasankar, R. Pitchiah, Framework for smart health: Toward\nconnected data from big data, Advances in Intelligent Systems and Com-\nputing 343 (2015) 423433.\n\n20\n\n\f[36] S. Ramrez-Gallego, S. Garca, H. Mourino-Taln, D. Martnez-Rego,\nV. Bolon-Canedo, A. Alonso-Betanzos, J. M. Bentez, F. Herrera, Data\ndiscretization: taxonomy and big data challenge, Wiley Interdisciplinary\nReviews: Data Mining and Knowledge Discovery 6 (1) (2016) 521.\n\n[37] S. Ramrez-Gallego, I. Lastra, D. Martnez-Rego, V. Bolon-Canedo, J. M.\nBentez, F. Herrera, A. Alonso-Betanzos, Fast-mrmr: Fast minimum re-\ndundancy maximum relevance algorithm for high-dimensional big data.,\nInternational Journal of Intelligent Systems 32 (2) (2017) 134152.\n\n[38] J. A. Saez, M. Galar, J. Luengo, F. Herrera, Analyzing the presence of\nnoise in multi-class problems: alleviating its inuence with the One-vs-One\ndecomposition, Knowledge and Information Systems 38 (1) (2014) 179206.\n\n[39] H. Singh, S. Bawa, A mapreduce-based scalable discovery and indexing of\nstructured big data, Future Generation Computer Systems (2017) http:\n//dx.doi.org/10.1016/j.future.2017.03.028.\n\n[40] A. Spark, Apache Spark: Lightning-fast cluster computing, http://spark.\napache.org/ (2016).\n\n[41] M. Tan, I. W. Tsang, L. Wang, Towards ultrahigh dimensional feature\nselection for big data, Journal of Machine Learning Research 15 (2014)\n13711429.\n\n[42] C.-M. Teng, Correcting Noisy Data, in: Proceedings of the Sixteenth Inter-\nnational Conference on Machine Learning, Morgan Kaufmann Publishers,\nSan Francisco, CA, USA, 1999, pp. 239248.\n\n[43] I. Triguero, S. del Ro, V. Lopez, J. Bacardit, J. M. Bentez, F. Herrera,\nRosefw-rf: the winner algorithm for the ecbdl14 big data competition: an\nextremely imbalanced big data bioinformatics problem, Knowledge-Based\nSystems 87 (2015) 6979.\n\n[44] S. Verbaeten, A. Assche, Ensemble methods for noise elimination in clas-\nsication problems, in: 4th International Workshop on Multiple Classi-\ner Systems(MCS 2003), vol. 2709 of Lecture Notes on Computer Science,\nSpringer, 2003, pp. 317325.\n\n[45] T. White, Hadoop: The Denitive Guide, OReilly Media, Inc., 2012.\n\n[46] D. L. Wilson, Asymptotic properties of nearest neighbor rules using edited\ndata, IEEE Transactions on Systems, Man, and Cybernetics 2 (3) (1972)\n408421.\n\n[47] X. Wu, Knowledge acquisition from databases, Ablex Publishing Corp.,\nNorwood, NJ, USA, 1996.\n\n[48] X. Wu, X. Zhu, Mining with noise knowledge: Error-aware data mining,\nIEEE Transactions on Systems, Man, and Cybernetics 38 (2008) 917932.\n\n21\n\n\f[49] X. Wu, X. Zhu, G.-Q. Wu, W. Ding, Data mining with big data, IEEE\nTransactions on Knowledge and Data Engineering 26 (1) (2014) 97107.\n\n[50] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J.\nFranklin, S. Shenker, I. Stoica, Resilient distributed datasets: A fault-\ntolerant abstraction for in-memory cluster computing, in: Proceedings of\nthe 9th USENIX Conference on Networked Systems Design and Implemen-\ntation, NSDI12, USENIX Association, Berkeley, CA, USA, 2012, pp. 22.\nURL http://dl.acm.org/citation.cfm?id=2228298.2228301\n\n[51] B. Zerhari, Class noise elimination approach for large datasets based on a\ncombination of classiers, in: Cloud Computing Technologies and Applica-\ntions (CloudTech), 2016 2nd International Conference on, IEEE, 2016, pp.\n125130.\n\n[52] S. Zhong, T. M. Khoshgoftaar, N. Seliya, Analyzing Software Measurement\nData with Clustering Techniques, IEEE Intelligent Systems 19 (2) (2004)\n2027.\n\n[53] X. Zhu, X. Wu, Class Noise vs. Attribute Noise: A Quantitative Study,\nArticial Intelligence Review 22 (2004) 177210.\n\n22\n\n\f", 
        "tag": "Databases", 
        "link": "https://arxiv.org/list/cs.DB/new"
    }, 
    {
        "text": "Knowledge Evolution in Physics Research: An Analysis of Bibliographic Coupling\nNetworks\nWenyuan Liu (\u0006),1 Andrea Nanetti,2 and Siew Ann Cheong1 , \n1School of Physical and Mathematical Sciences,\nNanyang Technological University,\nSingapore 637371, Singapore\n2School of Art, Design and Media,\nNanyang Technological University,\nSingapore 637371, Singapore\n(Dated: April 5, 2017)\n\nEven as we advance the frontiers of physics knowledge, our understanding of how this knowledge\nevolves remains at the descriptive levels of Popper and Kuhn. Using the APS publications data sets,\nwe ask in this letter how new knowledge is built upon old knowledge. We do so by constructing year-\nto-year bibliographic coupling networks, and identify in them validated communities that represent\ndierent research elds. We then visualize their evolutionary relationships in the form of alluvial\ndiagrams, and show how they remain intact through APS journal splits. Quantitatively, we see that\nmost elds undergo weak Popperian mixing, and it is rare for a eld to remain isolated/undergo\nstrong mixing. The sizes of elds obey a simple linear growth with recombination. We can also\nreliably predict the merging between two elds, but not for the considerably more complex splitting.\nFinally, we report a case study of two elds that underwent repeated merging and splitting around\n1995, and how these Kuhnian events are correlated with breakthroughs on BEC, quantum telepor-\ntation, and slow light. This impact showed up quantitatively in the citations of the BEC eld as a\nlarger proportion of references from during and shortly after these events.\n\nAccording to Karl Popper, science progresses through\nrepeated hypothesis testing [1]. Hypotheses contrary to\nempirical evidence must be rejected, while those consis-\ntent with data survive to be tested another day. In this\npicture of the scientic enterprise, our knowledge of the\nworld around us is always tentative, but becomes more\ncomplete over time. On the other hand, Thomas Kuhn\nbelieves that the accepted knowledge of a given time is\nthe result of consensus amongst scientists, based on evi-\ndences consistent with their theories [2]. However, when\ntoo many conicting evidences are found, a new con-\nsensus can form around new theories in what he called\na paradigm shift. Kuhn gives special relativity and\nquantum theory as examples of paradigm shifts. Look-\ning back, we realize these two theories have enormous im-\npacts on how we understand the world today. But could\nthere be paradigm shifts of various scales that have also\ncontributed to reshaping our knowledge of physics?\nMany historians of science have noted the strongly re-\nductionistic avor of scientic research in the last cou-\nple of centuries[3]. Starting as natural philosophy, the\nbody of scientic knowledge became separated disciplines\nof astronomy, biology, chemistry and physics. Within\nphysics itself, we also observe the emergence of high en-\nergy physics, condensed matter physics, biophysics, and\nphotonics. These are the results of the splitting of sci-\nence into more specialized elds. We also observe in par-\nallel the merging of elds, such as the merging of as-\ntronomy and physics to give astrophysics, biology and\nchemistry to give biochemistry, and others that arose\n\n cheongsa@ntu.edu.sg\n\nby division and recombination of specialties already ma-\ntured [2]. These developments have been discussed ex-\ntensively by philosophers and historians of science, but\nunlike our quantitative understanding of physics, our ap-\npreciation for the processes through which we acquired\nour knowledge of physics remains at a highly descriptive\nlevel. Some progress has been made in addressing this\nproblem [46]. In particular, the following three papers\nprovide the inspiration for our study. Chen and Red-\nner suggested that long-range connections can form be-\ntween disparate elds because of the development of a\nwidely applicable theoretical technique, or cross fertil-\nization between theory and experiment [7]. Visualizing\nthe cross citations between neuroscience journals, Ros-\nvall and Bergstrom traced the growth and maturation\nof neuroscience as a discipline [8]. Using embryology as\na specic example, Chavalarias and Cointet created a\nphylomemetic network visualization for the evolution of\nscience [9].\n\nWhile these previous studies point to the evolution of\nscientic knowledge, they do not identify the entity that\nis recognizably knowledge, or they do not study the in-\nteractions between such ob jects. To clarify what consti-\ntutes knowledge, we start with the bibliographic coupling\nnetwork (BCN) [10], proposed by Kessler and used ex-\ntensively in computer science [11, 12]. In a BCN, nodes\nrepresent papers, and if two papers share w common ref-\nerences, we draw an edge with weight w between them\n(see Fig. 1(a)). The BCN is suitable for our purpose for\ntwo reasons: (i) the BCN for a given year consists only\nof papers published that year and does not change after\nmore papers are published later, so features in the BCN\nrepresent the state of knowledge in that year; and (ii)\n\n7\n1\n0\n2\n \nr\np\nA\n \n4\n \n \n]\nh\np\n-\nc\no\ns\n.\ns\nc\ni\ns\ny\nh\np\n[\n \n \n1\nv\n5\n7\n8\n0\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\n\f2\n(cid:3); and\nn ) = (cid:2)Rn\n(cid:3) and Rt+1\n(cid:2)Rm\nm = R(C t\nas Rt\ncited by papers in C t\nm and C t+1\nm ) =\nn = R(C t+1\nn\n1 , ..., Rn\n1 , ..., Rm\nq\np\nm , ...}. N (element, list) is the number of\nRt = {Rt\n1 , ..., Rt\ntimes element occurs in list, and L(list) is the length of\nlist. In this denition, we assume each citation instance\nin t will be uniformly distributed over all instances of the\nsame citation in t + 1, while each citation in t + 1 receives\nequal contributions from all instances of the same citation\nmn (cid:54)= I b\nin t. In general, this index is asymmetric, i.e. I f\nmn ,\nbecause the references are not cited the same number of\ntimes in the two years, as illustrated in Fig. 1(b).\n\nWe visualize the sequence of TCs and their intimacy\nindices, the evolution of physics research they represent\nin the form of alluvial diagrams. For example, in Fig. 2\nwe can clearly see the birth of PRA, PRB, PRC and PRD\nfrom PR in 1970. Each journal consist of several TCs,\nwhich existed even in the PR era. The editorial decision\nto split PR is consistent with the self-organized TCs even\nthough it was done without classication analysis. We\nalso see the consistent birth of PRE from PRA in 1993\nIn SI Sec. III.\n\nMore importantly, from the alluvial diagram we can\nidentity the key interactions between TCs that are cor-\nrelated with important publications.\nIn Fig. S4 of SI\nSec. IV we showcase one such episode between 1991 and\n2000. At the beginning of the decade, we see two PRA-\ndominated TCs. Based on the papers they contain, we\ncan loosely associate one with quantum information (QI)\nand trapped atomic ions (BEC), and the other with quan-\ntum optics (QO). In 1993, the QI + BEC TC cited many\nQO papers, and in 1994, the QO TC cited many QI +\nBEC papers. Following this cross-fertilization, the two\nTCs merged in 1995, the same year Cornell et al.\n[15]\nand Ketterle et al.\n[16] published their seminal papers\ndemonstrating BEC in dilute atomic gases. In recogni-\ntion of their works, Cornell, Wieman, and Ketterle were\nawarded the 2001 Nobel Prize in Physics. The PRA-\ndominated TC split after 1996 to give one that is exclu-\nsively BEC, and another that is still a combination of QI\n+ QO. It was after Zeilinger demonstrated in 1997 exper-\nimental quantum teleportation [17] that the QI + QO TC\nsplit into a QI TC and a QO TC. After receiving more\ninuence from other PRB-dominated TC, the QO cluster\nproduced yet another breakthrough paper, in the form of\nultraslow light in hot atomic gases[18]. Without the data\nvisualization done here, few may suspect the existence of\nsuch connections between BEC, quantum teleportation\nand slow light.\n\nFrom Fig. 2 we see a diversity of inows and outows\nfrom one TC to another: some TCs are derived almost\nexclusively from one source, others receive strong contri-\nbutions from a small number of sources, or weak contri-\nbutions from a large number of sources. To quantify such\ndiversity, we construct a forward mixing degree of com-\nmunity C t\nm and backward mixing degree of C t+1\nanalo-\nn\n\nFIG. 1. (a) Building a BCN (lower) from a citation network\n(upper): circles with numbers are papers under consideration,\ncircles with letters are their references, and numbers on edges\nare weights. (b) Topical clusters in year t (left) and in year\nt + 1 (right), and their forward (left) and backward (right)\nintimacy indices, shown as ows.\n\nthe appropriate collective unit of knowledge is a commu-\nnity in the BCN instead of a few key papers or a journal.\nFor the American Physical Society (APS) data set, con-\nsisting of about half a million publications between 1893\nand 2013 [13], we show in Supplementary Information SI\nSec. I that the BCN edge weights are far more heteroge-\nneous than expected from an appropriate null model.\nThis heterogeneity can be explained by the presence\nof communities that we extracted using the modularity-\noptimized Louvain method [14]. Using the null model\nin SI Sec. I, we show that these communities are statis-\ntically signicant. We also test in SI Sec. II how likely\nthe most common PACS number in a community of n\npapers can appear with its observed frequency, within\nrandom collections of n papers. For most communities,\nthis is highly unlikely, so we conclude that the groupings\nof papers extracted are meaningful. We refer to these\nvalidated units of knowledge as topical clusters (TCs).\nTo study how knowledge evolves, we investigate how\nTCs {C t} in year t become {C t+1} in year t + 1. The\npapers published in dierent years are distinct, but they\ndo overlap in their references. Therefore we use this fact\nto dene a forward intimacy index I f\nmn and a backward\n(cid:1)\nN (cid:0)Ri , Rt+1\n(cid:88)\nN (cid:0)Ri , Rt\n(cid:1) / L (cid:0)Rt\n(cid:1) ;\nintimacy index I b\nmn :\nn\n(cid:88)\nN (Ri , Rt+1 )\n(cid:1) ,\n(cid:1) / L (cid:0)Rt+1\nN (cid:0)Ri , Rt+1\nm\nm\ni\nN (Ri , Rt\nm )\nI b\nN (Ri , Rt )\nmn =\nn\nn\ni\n(cid:8)C t+1\n(cid:9), and we denote the references\nto quantify how close C t\nm is to C t+1\nn . Here the TCs at\nt and t + 1 are C t = {C t\nu } and C t+1 =\n1 , ..., C t\nm , ..., C t\n, ..., C t+1\nn , ..., C t+1\nv\n1\n\nI f\nmn =\n\n(1)\n\n2111A1CBEDGFH2341234(a)0.70830.650.29160.3750.06250.150.56250.4512AABC345ACCCDD678AAACF91011CCCDE(b)tt+1\f3\n\nFIG. 2. The alluvial diagram of APS papers from 1965 to 1974. Each block in a column represents a TC and the height of\nthe block is proportional to the number of papers in the TC. Only communities comprising more than 100 papers are shown.\nTCs in successive years are connected by streams whose widths at the left and right ends are proportional to the forward and\nbackward intimacy indices. The dierent colors in a TC represent the relative contributions from dierent journals.\n\nnian pictures of the evolution of knowledge, where we ex-\npect incremental growth punctuated by abrupt paradigm\nshifts. Certainly, at the aggregate level of PR series\nof premier physics journals, the number of articles pub-\nlished has grown over the years. When we partition these\narticles into TCs, we naively expect that some clusters\nwill grow/shrink because of growing/declining interest in\ntheir topics. From the alluvial diagrams, we realize that\nthe real picture is far more complex because of recom-\nbinations between TCs. Therefore, instead of measuring\nthe growth rates of pure TCs, we need to measure the\ngrowth of recombined TCs. To do this, we assume that\nmn / (cid:80)\nm to the size of C t+1\nthe contribution of C t\nis proportional\nn\nto the size of C t\nm and also the normalized forward inti-\nmn / (cid:80)\nn ) = (cid:80)\nn I f\nmacy index I f\nmn , i.e.\nL(cid:48) (C t+1\nn I f\nm )(I f\nm L(C t\nmn ).\nWhen we plot the predicted sizes L(cid:48) (C t+1\nn ) against\nthe observed size L(C t+1\nin Fig. 3, we nd\nn )\n(L(cid:48) (C t+1\nn ), L(C t+1\nn )) scattered about about a straight line\nwith slope with 1.06, which is the annual growth rate of\nthe number of papers in APS journals. This tells us that\nthe growth of recombined TCs is also Popperian.\nNext, we consider the Kuhnian processes of splitting\nand merging. Unlike the prediction above, where we\nmade use of information from years t and year t + 1, we\nwould like to predict the splitting and merging of TCs us-\ning information only from year t. Specically, for merging\nm(cid:48) n / (cid:80)\nmn / (cid:80)\nm(cid:48) ) = (cid:80)\nevents, our ground truth will be the similarity\nn(cid:48)(cid:48) I f\nmn(cid:48) )(I f\nn(cid:48) I f\nS (C t\nm , C t\nn (I f\nm(cid:48) n(cid:48)(cid:48) )\n(4)\nbetween two TCs in year t, taking on values between 0\nand 1. If C t\nm and C t\nm(cid:48) merge perfectly into a single TC in\nyear t + 1, S = 1. On the other hand, if the osprings of\n\n(3)\n\nFIG. 3. (a) Plot of observed (y-axis) against predicted (x-\naxis) sizes of recombined TCs, showing a linear growth with\nslope 1.06 (dashed line). This linear growth is the same for\nTCs with (b) high (red) or (c) low (blue) backward mixing\ndegree.\n\n,\n\n.\n\n(cid:17)2\n(cid:16)\nmn / (cid:80)\nm = 1  (cid:80)\ngous to the Gini-Simpson index [19]:\n(cid:1)2\n(cid:0)I b\nmn / (cid:80)\nn = 1  (cid:80)\nn(cid:48) I f\nM f\nI f\nmn(cid:48)\nn\nM b\nm(cid:48) I b\nm(cid:48) n\nm\nA TC with low forward/backward mixing degree has\neectively one child/parent, whereas a TC with high\nforward/backward mixing degree undergoes/results from\nstrong splitting/merging. As shown in SI Sec. V, neither\nare frequent. It is more common to nd weak mixing be-\ntween TCs, which we believe is due to most papers citing\nsmall numbers of papers outside their elds.\nAt this point, let us recall the Popperian and Kuh-\n\n(2)\n\n1965196619671968196919701971197219731974PRPRLPRAPRBPRCPRDRMP0500100015002000250005001000150020002500(a)0500100015002000250005001000150020002500(b)0500100015002000250005001000150020002500(c)0.160.240.320.400.480.560.640.720.80\f4\n\nthe decile of most strongly splitting TCs, increasing the\nstandardized B by one standard deviation will decrease\nM f by about 0.05, whereas for the decile of the least\nstrongly splitting TCs, there is no obvious trend. In SI\nSec. VII we tested another index measuring a dierent\nnetwork aspect of the weight matrix and found the pre-\ndiction results are similar.\nFinally, we want to know the impacts of such merging\nand splitting events. We rst check for an increase in\n\nFIG. 5. Proportions of a TCs references published in dierent\nyears, relative to the year (0) of the TC. The black solid line\nis the proportions averaged over all TCs in the 1990s, while\nthe area shaded gray is up to one standard deviation away\nfrom the mean. Other color lines represent the distribution\nof four dierent BEC related TCs.\n\nthe number of publications after such events, but found\nan insignicant dierence in paper numbers in strongly\nand weakly mixing TCs (see Fig. 3(b) and (c)). We\nsuspected this is because our data set in conned to\nAPS publications, and a more careful check should in-\nclude other physics journals to capture any inuence\nspillover. We then check the highest, third quartile,\nmedian, average number of citations range two and ve\nyears after the events, but still see no signicant eects\n(SI Sec. VIII). Focusing on the highly productive chain\nof knowledge processes that led to experimental real-\nizations of BEC, quantum teleportation and slow light,\nwe checked the citation proles between 1995 and 1998.\nWhile the 1995 BEC+QI+QO TC cited a slightly lower\nproportion of 1995 papers than the APS 0-year average,\nthe 1996 BEC+QI+QO, the 1997 BEC TC, the 1998\nBEC TCs all cited signicantly more 0-year papers. The\nfull eect of this BEC breakthrough can be seen in the\nlarge proportions of 1996 papers cited by the 1997 and\n1998 TCs and the proportion of 1997 papers cited by the\n1998 TC (see Fig. 5). Indeed, we have provided early evi-\ndence suggesting that strongly mixing Kuhnian processes\nare associated with greater impact.\nWe thank Woo-Sung Jung for discussions.\n\nFIG. 4. (left) S (C t\nm , C t\nm(cid:48) ) of 16 TCs in 1991, computed using\nforward intimacy indices going from 1991 to 1992.\n(right)\nT (C t\nm , C t\nm(cid:48) ) of the same 16 TCs, using information from 1991\nonly. We use the same ordering of TCs in both matrices.\n\nC t\nm and C t\nm(cid:48) are distinct, S = 0. To do prediction using\nonly information from year t, we dene\n\nm )L(C t\nm(cid:48) )/(L(C t\nm , C t\nm(cid:48) ) = W (C t\nm , C t\nT (C t\nm(cid:48) )),\n\n(5)\n\nwhere W (C t\nm , C t\nm(cid:48) ) is the sum of weights of edges be-\ntween papers in C t\nm and C t\nm(cid:48) , normalized against the sizes\nof TCs involved. Fig. 4 shows that the two quantities are\nhighly correlated, with Spearmans rank correlation co-\necient of about 0.8 (see SI Sec. VI.) A high T between\ntwo TCs means they are likely to merge the next year.\nWe also tried to predict the splitting events. Here the\nsituation is more complex: when we examine the weight\nmatrix of a TC, we may nd a few large subcommunities\nor many small subcommunites. Naively, we expect the\ncriterion for splitting is the opposite to merging, i.e. the\neasier it is to tell one subcommunity from others, the\n(cid:80)\n(cid:80)\nA(j1 , j2 )/ (cid:80)\nhigher the chances for a split. The boundary index\n(cid:80)\n(cid:80)\nA(j1 , j2 )/ (cid:80)\nj1Ci1\ni1 (cid:54)=i2\nii (cid:54)=i2\nj2Ci2\ni L(Ci )L(Ci )\nj1 ,j2Ci\ni\n(6)\nmeasures how indistinct the subcommunities are in a TC.\nHere A(j1 , j2 ) is the weight of the edge between papers j1\nand j2 , and Ci is a subcommunity in the given TC. How-\never the picture we nd is not as simple as the merging\ncase. When we plot M f against B , we nd the expected\ndecreasing trend, but at the same time, the large scatter\nmakes it impossible to reliably predict a splitting event\nusing B . To better understand the relationship between\nM f and B , we use quantile regression[20] to nd that\nthe B has no prediction power when M f is small, but\nbecomes predictive when M f is large. We summarize\nthese ndings in SI Sec. VII The slopes show that for\n\nL(Ci1 )L(Ci2 )\n\nB =\n\n[1] K. R. Popper, Al l\nPress, 1999).\n[2] T. S. Kuhn, The Structure of Scientic Revolutions (1st\ned.) (University of Chicago Press, 1962).\n\nlife is problem solving (Psychology\n\n[3] D. Wootton, The invention of science: a new history of\nthe scientic revolution (Penguin UK, 2015).\n[4] J. Bollen, H. Van de Sompel, A. Hagberg, L. Bettencourt,\nR. Chute, M. A. Rodriguez,\nand L. Balakireva, PLoS\n\n0246810121402468101214S(Ctm,Ctm0)0246810121402468101214T(Ctm,Ctm0)0.000.080.160.240.320.400.480.560.640.00000.00150.00300.00450.00600.00750.00900.01050.01200.01359876543210Relative year0%5%10%15%20%25%30%Proportion of referencesAverage1995199619971998\fONE 4, e4803 (2009).\n[5] T. Kuhn, M. Perc, and D. Helbing, Physical Review X 4,\n1 (2014), arXiv:1404.3757.\n[6] T. Jia, D. Wang, and B. K. Szymanski, Nature Human\nBehaviour 1, 0078 (2017).\n[7] P. Chen and S. Redner, Journal of Informetrics 4, 278\n(2010), arXiv:0911.0694.\n[8] M. Rosvall and C. T. Bergstrom, PLoS ONE 5 (2010),\n10.1371/journal.pone.0008694, arXiv:0812.1242.\n[9] D. Chavalarias and J. P. Cointet, PLoS ONE 8 (2013),\n10.1371/journal.pone.0054847.\n[10] M. Kessler, IEEE Transactions on Information Theory 9,\n49 (1963).\n[11] E. Yan and Y. Ding, Journal of the American Society\nfor Information Science and Technology 63, 13131326\n(2012).\n[12] M. H. Huang, L. Y. Chiang, and D. Z. Chen, Sciento-\nmetrics 58, 489 (2003).\nSets\nData\n[13] APS\n\nResearch,\n\nfor\n\nsee\n\n5\n\nhttp://journals.aps.org/datasets, .\n[14] V. D. Blondel, J.-L. Guillaume, R. Lambiotte,\nand\nE. Lefebvre, Journal of Statistical Mechanics: Theory and\nExperiment 2008, P10008 (2008), arXiv:0803.0476.\n[15] M. H. Anderson, J. R. Ensher, M. R. Matthews, C. E.\nWieman, and E. A. Cornell, Science 269, 198 (1995).\n[16] K. B. Davis, M. O. Mewes, M. R. Andrews, N. J. van\nDruten, D. S. Durfee, D. M. Kurn,\nand W. Ketterle,\nPhysical Review Letters 75, 3969 (1995).\n[17] D. Bouwmeester, J.-W. Pan, K. Mattle, M. Eibl, H. We-\ninfurter, and A. Zeilinger, Nature 390, 575 (1997).\n[18] M. M. Kash, V. A. Sautenkov, A. S. Zibrov, L. Hollberg,\nG. R. Welch, M. D. Lukin, Y. Rostovtsev, E. S. Fry, and\nM. O. Scully, Physical Review Letters 82, 5229 (1999).\n[19] L. Jost, Oikos 113, 363 (2006).\n[20] J. Sienkiewicz and E. G. Altmann, Royal Society Open\nScience 3, 160140 (2016), arXiv:1605.07465.\n\n\fKnowledge Evolution in Physics Research: An Analysis of Bibliography Coupling\nNetworks\nSupplementary Material\n\n1\n\nI. NULL MODEL OF BIBLIOGRAPHY COUPLING NETWORK\n\nTo determine the statistical signicance of our empirical bibliographic coupling networks (BCNs) (Fig. S1(a)), we\nbuild a null model for comparison. In our null model, we x the out degrees and in degrees of all papers (citing and\ncited), but rewire the edges to get an ensemble of articial BCNs (Fig. S1(b)).\n\nFIG. S1. (a) Original citation network and its BCN. (b) A rewired citation network keeping in degrees and out degrees xed\nand its BCN. (c) Comparison of the degree and weight distributions of papers published in 1991, between the real BCN and\nthe null model. (d) Modularities of the best partitions extracted by the Louvain method for the real BCNs and the null model\nbetween 1991 and 2000. Results from null model are averaged over 10 dierent rewirings, and the error bars are much smaller\nthan the marker size.\n\nCompared to the null model, the real BCN has more high-weight edges. We suspect these are the most meaningful\nedges, arising from the papers content. If two papers focus on close topics, they will likely have high chance to have\nmore than one common reference, and this eect also manifest itself in the degree distribution: the null model has\na atter degree distribution at small degrees because the edges are drawn by chance, whereas in the real BCN this\ncoupling is based on content, meaning that papers will have edges mostly with papers that are trying to solve the same\nproblems, so the real BCN will have more low-degree nodes, fewer high-degree nodes compared the null model. The\nmost prominent feature of this content-sensitive citation is community structure: in the real BCN, papers focussed\non the same topic share more common references with each other than papers focussed on dierent topics, so that\nthe densities of edges within topics are much higher than between topics. Therefore the modularities of communities\nextracted by the Louvain method in the real network is much higher than in the null model, as shown in Fig. S1(d).\n\nII. VALIDATION OF BCN COMMUNITIES\n\nTo verify that the communities extracted are really focussed on closely related questions, we check the Physics and\nAstronomy Classication Scheme (PACS) numbers of members of the communities. Such numbers are provided by\nauthors to indicate the subelds of physics to which their papers belong. In our case, we only use the rst two digits\nof the PACS numbers, as a balance between accuracy and coverage. To test whether the PACS numbers appearing\n\n2111A1CBEDGFH2341234(a)311A1CBEDGFH2341234(b)100101102103Degree10-610-510-410-310-210-1Frequency(c)Real BCNNull model1991199219931994199519961997199819992000Year0.00.20.40.60.81.0Modularity(d)Real BCNNull model051015202530Weight10-510-410-310-210-1100FrequencyReal BCNNull model\fin the communities could have occurred by chance, we choose one year t, build its BCN, extracting the community\nstructure with sizes {s1 , s2 , ..., sn}, and then randomly assign papers in year t into n pseudo-communities of the same\nsizes, to remove any potential size eects. For a community of size s, we then identify the largest subset of papers\nsharing the same PACS number. This PACS number can represent the subeld of the community to a certain extent,\nand the fraction of papers in the largest subset reect the homogeneity of the community. On the other hand, the\nlargest subset of papers sharing the same PACS number in a random collection of s papers is typically small. Dividing\nthe sizes of the largest subsets in the empirical communities and in the random collections, we nd ratios are larger\nthan 1 for most cases. This means that the communities we extracted are meaningful (see Fig. S2).\n\n2\n\nFIG. S2. Comparison of PACS homogeneity between real BCN communities between 1991 and 2000 with more than 50 papers,\nand their corresponding random collections. (a) The red squares correspond to the sizes of the largest subsets of papers sharing\nat least one PACS number, nreal , in the empirical communities divided by the same quantity found in the corresponding\nrandom collections, nrand , as a function of the community size s. (b) The fraction of the largest subset of papers sharing at\nleast one PACS number as a function of s for real communities in the BCN and random collections. For clarity, the small error\nbars are not shown in the gures.\n\nIII. ALLUVIAL DIAGRAM FOR 1991-2000\n\nIn addition to Fig. 2 in the letter, we also plotted an alluvial diagram for 1991 to 2000, showing the splitting\nof PRA into PRA and PRE. As we can see from Fig. S3, before 1993, there were several PRA-dominated TCs.\nAfter the split in 1993, some PRA-dominated TCs remained PRA-dominated, whereas other PRA-dominated TCs\nbecame PRE-dominated. This means that even before 1993, papers in PRA were already divided into groups based\non dierent topics, some of which are predecessors of the PRE TCs.\n\nIV. CASE STUDY: QUANTUM OPTICS, QUANTUM INFORMATION AND BOSE-EINSTEIN\nCONDENSATION\n\nTo illustrate the utility our knowledge evolution framework can oer, we use as a case study the interesting inter-\nactions between quantum optics (QO), quantum information (QI), and Bose-Einstein Condensation (BEC). These\nthree elds experienced breakthroughs in the 1990s. In Fig. S4 we highlight the evolution of TCs which are related\nto these three topics and Table S1 shows the three most cited papers in these TCs. Key merging and splitting events\nare reported in the main paper, as are important publications these events are correlated with.\n\nV. KNOWLEDGE METABOLISM\n\nSome TCs have more references overlapping with those in the previous year, while other TCs have less. To quantify\nevolution of references, we count the sums of the forward and backward intimacy indices. These represent the\npercentage of references going to the next year, and the percentage of references inherited from the previous year,\nwhich we think of as the outow and inow respectively. As shown in Fig. S5(a) and (b), most outows and inows\n\n0500100015002000s012345678nreal/<nrand>(a)0500100015002000s0.00.20.40.60.81.0nreal/s or nrand/s(b)Real communityRandom community\f3\n\nFIG. S3. The alluvial diagram of APS papers from 1991 to 2000. Each block in a column represents a TC and the height of\nthe block is proportional to the number of papers in the TC. Only communities comprising more than 100 papers are shown.\nTCs in successive years are connected by streams whose widths at the left and right ends are proportional to the forward and\nbackward intimacy indices. The dierent colors in a TC represent the relative contributions from dierent journals.\n\nFIG. S4. The alluvial diagram of APS papers from 1991 to 2000, where we colored only TCs highly related to quantum optics,\nquantum information and Bose-Einstein condensation.\n\nare distributed within a narrow range, but there are exceptional cases as well: such as a single peak in Fig. S5(b),\nwhose references overlap signicantly less than normal with the previous year. In the context of birth, death, growth,\ndecay, split, and merge knowledge processes, we are inclined to call this event in 1993 the birth of a TC. Further\nanalysis shows that most common PACS codes are: 03 (Quantum mechanics, eld theories, and special relativity),\n42 (Optics) and 63 (Lattice dynamics). Looking at the references of this TC, we nd that most of these comes from\n1990, 3 year before. This interesting phenomenon is therefore more appropriately identied as a sleeping beautify[S1].\nEvery year, physicists absorb new references and drop old references as their elds progress. Although this\nmetabolism dier from TC to TC, the whole process is quite stable over all TCs, as shown in Fig. S5(c) and\n(d). This universal curve can be used as a benchmark for the test of scientic impact, as we have done in Fig. 5 of\nthe letter.\nAs we see from Fig. S3, there is a great diversity of processes acting on the TCs: some TCs are derived almost\nexclusively from one source, others receive strong contributions from a small number of sources, or weak contributions\nfrom a large number of sources. To quantify such diversity, we introduce forward mixing degree and backward mixing\ndegree (Eq. 2 in the letter) As shown in Fig. S5(e), (f ), strong splitting/merging or almost isolated development are\n\n1991199219931994199519961997199819992000PRLPRAPRBPRCPRDPRERMP1991199219931994199519961997199819992000\f4\n\nFIG. S5. The metabolic analysis of APS papers in the 1990s. (a) The distribution of outows of TCs. (b) The distribution\nof inows of TCs. (c) Proportions of APS papers references published in dierent years. (d) Proportions of APS papers\nreferences published in dierent years, relative of the year (0) of publication. (e) The distribution of forward mixing degree of\nTCs. (d) The distribution of backward mixing degree of TCs.\n\nrare, and in most case, TCs undergo weak information exchange.\n\nVI. PREDICTION OF MERGING\n\nAs mentioned in the letter, we found that the inter-TC connection is highly correlated with their mixing the next\nyear. High T (C t\nm , C t\nm(cid:48) ) leads with a large probability to a high S (C t\nm , C t\nm(cid:48) ). Analyzing APS papers in the 1990s, we\nfound a Spearmans rank coecient of 0.804 between T (C t\nm , C t\nm(cid:48) ) and S (C t\nm , C t\nm(cid:48) ) over all TCs (with at least 100\npapers). However, because the average Pearson correlation coecient is only 0.504, such a relation is not linear (see\nFig. S6)\n\n0.00.20.40.60.81.0Outflow 0% 5%10%15%20%25%30%Frequency(a)0.00.20.40.60.81.0Inflow 0% 5%10%15%20%25%30%35%Frequency(b)19801985199019952000Year 0% 5%10%15%20%25%Proportion of references(c)19911992199319941995199619971998199920009876543210Relative year 0% 5%10%15%20%25%Proportion of references(d)19911992199319941995199619971998199920000.00.20.40.60.81.0Forward mixing degree 0% 5%10%15%20%25%Frequency(e)0.00.20.40.60.81.0Backward mixing degree 0% 5%10%15%20%25%Frequency(f)\f5\n\nm , C t\nm(cid:48) ) and S (C t\nm , C t\nFIG. S6. The scatter plot between T (C t\nm(cid:48) ) among all TCs (with at least 100 papers) in 1990s.\n\nVII. HIERARCHY STRUCTURE OF COMMUNITY AND SPLITTING ANALYSIS\n\nTo predict a splitting event, we check the weight matrix of dierent TCs. Naively, we expect the components\nthat lead on to dierent TCs the next year to already form distinct subcommunities this year. However, when we\nuse the dendrogram extracted from the Louvain method to identify subcommunities, we found that dierent TCs\nhave dierent internal structures (see Fig. S7): some have a few large subcommunities, while others have many small\nsubcommunities.\nAssuming that splitting is the time reversal of merging, i.e. two distinct subcommunities becoming two distinct\ncommunities in the next year, we devise the boundary index B to measure how indistinct the subcommunities are\nin a TC. Quantile regression (QR) shows that the relation between B and M f depends on the decile, as shown in\nFig. S8(a), (b). For dierent decile, the relation is dierent.\n(cid:88)\nWe also try with fragmentation index\ni:j [i]\n\nwiS 2\nj [i]\n\nF =\n\n(S1)\n\nwhere wi is the size fraction of the top level subcommunity i, sj [i] is the relative size fraction of subsubcommunity j\ninside subcommunity i. The more fragmentation a community is, the closer F is to 0. Quantile regression between F\nand M f gives very similar results as B and M f , that is, for the decile of most strongly splitting TCs, increasing the\nstandardized F by one standard deviation will decrease M f by about 0.06, whereas for the decile of the least strongly\nsplitting TCs, there is no obvious trend as  close to 0, as shown in Fig. S8(c), (d).\n\nVIII. CORRELATION ANALYSIS BETWEEN CITATION AND MIXING\n\nWhen we think of high-impact research, we think of highly-cited papers. Therefore, to quantify the impact of\nstrongly-splitting events in the alluvial diagrams, we counted the citations of TCs resulting from splittings. As shown\nin Fig. S9, we did this for number of citations 2 years after the events, and also 5 years after the events. There were\nno obvious trends. The results of backward mixing degree, i.e. merging, are similar.\n\n[S1] Q. Ke, E. Ferrara, F. Radicchi, and A. Flammini, Proceedings of the National Academy of Sciences of the United States\nof America 2015, 40 (2015), arXiv:1505.06454.\n\n0.0000.0020.0040.0060.0080.0100.0120.0140.016T(Ctm,Ctm0)0.00.10.20.30.40.50.60.7S(Ctm,Ctm0)0.0000.0010.0020.000.020.040.060.080.10\f6\n\n1991\n\n1992\n\n1993\n\n10.1103/PhysRevA.44.5674\n\nLower\n\nUpper\n\n1994\n\n1995\n\n1996\n\n1997\n\nUpper\n\nLower\n\nUpper\n\nLower\n\nUpper\n\nLower\n\n10.1103/PhysRevLett.70.1244\n\n10.1103/PhysRevLett.70.1895\n\n10.1103/PhysRevLett.69.3314\n\nTABLE S1. The three most cited papers in quantum optics, quantum information theory, quantum computation and Bose-\nEinstein condensation related TCs.\nYear TC\nDOI\nTitle\n10.1103/PhysRevLett.67.661 Quantum cryptography based on Bells theorem\n10.1103/PhysRevLett.66.2593 Observation of electromagnetically induced transparency\n10.1103/PhysRevLett.67.1855 Enhancement of the index of refraction via quantum coherence\nAbove-surface neutralization of highly charged ions: The classical over-the-barrier\nmodel\n10.1103/PhysRevB.43.13401 Strong magnetic x-ray dichroism in 2p absorption spectra of 3d transition-metal ions\n10.1103/PhysRevLett.66.2601 Dynamic stabilization of hydrogen in an intense, high-frequency, pulsed laser eld\n10.1103/PhysRevLett.69.2881 Communication via one- and two-particle operators on Einstein-Podolsky-Rosen states\nObservation of the coupled exciton-photon mode splitting in a semiconductor quantum\nmicrocavity\n10.1103/PhysRevLett.68.580 Wave-function approach to dissipative processes in quantum optics\n10.1103/PhysRevLett.68.1943 X-ray circular dichroism as a probe of orbital magnetization\n10.1103/PhysRevLett.68.3535 High-order harmonic generation from atoms and ions in the high intensity regime\n10.1103/PhysRevLett.69.1383 Absorption of ultra-intense laser pulses\nTeleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen\nchannels\n10.1103/PhysRevA.47.4114 Threshold and resonance phenomena in ultracold ground-state collisions\nMeasurement of the Wigner distribution and the density matrix of a light mode using\noptical homodyne tomography: Application to squeezed states and the vacuum\n10.1103/PhysRevLett.71.1994 Plasma perspective on strong eld multiphoton ionization\n10.1103/PhysRevLett.70.1599 Above threshold ionization beyond the high harmonic cuto\n10.1103/PhysRevLett.70.774 High-order harmonic generation in rare gases with a 1-ps 1053-nm laser\n10.1103/PhysRevA.50.67\nSqueezed atomic states and pro jection noise in spectroscopy\n10.1103/PhysRevLett.72.3439 Statistical distance and the geometry of quantum states\n10.1103/PhysRevLett.73.58 Experimental realization of any discrete unitary operator\n10.1103/PhysRevA.49.2117 Theory of high-harmonic generation by low-frequency laser elds\n10.1103/PhysRevLett.73.1227 Precision Measurement of Strong Field Double Ionization of Helium\n10.1103/PhysRevA.50.1540 Modeling harmonic generation by a zero-range potential\n10.1103/PhysRevLett.75.3969 Bose-Einstein Condensation in a Gas of Sodium Atoms\n10.1103/PhysRevLett.74.4091 Quantum Computations with Cold Trapped Ions\n10.1103/PhysRevA.52.R2493 Scheme for reducing decoherence in quantum computer memory\n10.1103/PhysRevA.54.3824 Mixed-state entanglement and quantum error correction\n10.1103/PhysRevLett.77.1413 Separability Criterion for Density Matrices\n10.1103/PhysRevLett.77.2360 Collective Excitations of a Trapped Bose-Condensed Gas\n10.1103/PhysRevLett.78.985 Bose-Einstein Condensation of Lithium: Observation of Limited Condensate Number\n10.1103/PhysRevLett.78.586 Production of Two Overlapping Bose-Einstein Condensates by Sympathetic Cooling\n10.1103/PhysRevLett.78.5 Demonstration of the Casimir Force in the 0.6 to 6mum Range\n10.1103/PhysRevLett.78.5022 Entanglement of a Pair of Quantum Bits\nQuantum State Transfer and Entanglement Distribution among Distant Nodes in a\nQuantum Network\n10.1103/PhysRevLett.79.3306 Noiseless Quantum Codes\n10.1103/PhysRevLett.81.3108 Cold Bosonic Atoms in Optical Lattices\nAtomic Scattering in the Presence of an External Connement and a Gas of\nImpenetrable Bosons\n10.1103/PhysRevLett.81.742 Spinor Bose Condensates in Optical Traps\n10.1103/PhysRevA.57.120 Quantum computation with quantum dots\n10.1103/PhysRevLett.80.2245 Entanglement of Formation of an Arbitrary State of Two Qubits\nQuantum Repeaters: The Role of Imperfect Local Operations in Quantum\nCommunication\n10.1103/PhysRevLett.83.2498 Vortices in a Bose-Einstein Condensate\n10.1103/PhysRevLett.83.5198 Dark Solitons in Bose-Einstein Condensates\n10.1103/PhysRevLett.82.1975 Entanglement of Atoms via Cold Controlled Collisions\n10.1103/PhysRevLett.83.4204 Quantum Information Processing Using Quantum Dot Spins and Cavity QED\n10.1103/PhysRevB.59.2070 Coupled quantum dots as quantum gates\n10.1103/PhysRevLett.82.2417 Dynamical Decoupling of Open Quantum Systems\nUltraslow Group Velocity and Enhanced Nonlinear Optical Eects in a Coherently\nDriven Hot Atomic Gas\n10.1103/PhysRevLett.83.2845 Transmission Resonances on Metallic Gratings with Very Narrow Slits\n10.1103/PhysRevLett.83.967 Liquid-Crystal Photonic-Band-Gap Materials: The Tunable Electromagnetic Vacuum\n10.1103/PhysRevLett.84.806 Vortex Formation in a Stirred Bose-Einstein Condensate\n10.1103/PhysRevLett.85.1795 Stable 85Rb Bose-Einstein Condensates with Widely Tunable Interactions\n10.1103/PhysRevLett.85.3745 Regimes of Quantum Degeneracy in Trapped 1D Gases\n10.1103/PhysRevA.62.062314 Three qubits can be entangled in two inequivalent ways\n10.1103/PhysRevLett.84.2722 Inseparability Criterion for Continuous Variable Systems\nElectron-spin-resonance transistors for quantum computing in silicon-germanium\nheterostructures\n10.1103/PhysRevLett.85.5214 Double Resonant Raman Scattering in Graphite\n10.1103/PhysRevLett.85.154 Electronic Structure of Deformed Carbon Nanotubes\n10.1103/PhysRevB.62.13104 Carbon nanotubes, buckyballs, ropes, and a universal graphitic potential\n\n10.1103/PhysRevLett.78.3221\n\n10.1103/PhysRevLett.81.5932\n\n10.1103/PhysRevLett.82.5229\n\nUpper\n\nMiddle\n\n10.1103/PhysRevA.62.012306\n\nLower\n\nUpper\n\n10.1103/PhysRevLett.81.938\n\nUpper\n\nLower\n\nUpper\n\n1998\n\nLower\n\n1999\n\n2000\n\nMiddle\n\nLower\n\n\f7\n\nFIG. S7. Adjacency matrices of TCs in the 1990s. The blue lines indicate the boundaries of subsubcommunities, the red lines\nindicate the boundaries of subcommunities. The red lines are absent from some plots because such TC have only one level\nwhen the Louvain algorithm terminated.\n\n050100150200250300350050100150200250300350020040060080010000200400600800100001002003004000100200300400010020030040050060070080001002003004005006007008000100200300400500010020030040050002004006008001000020040060080010000100200300400500600010020030040050060001002003004005006007000100200300400500600700050010001500050010001500\f8\n\nFIG. S8. Relation between boundary index, fragmentation index and forward mixing degree of TCs in 1980s and 1990s. (a)\nEach dot corresponds to one TC, dash lines show QR results for quantiles  = 0.1, 0.2, ..., 0.9. (b)  coecients (slopes of QR in\nthe (a) as a function of  . The red arrows show low   ( = 0.1), half   ( = 0.5) and top   ( = 0.9), as, respectively,\nthe nock, a circle on the shaft, and the head of the arrow, the blue solid line represents 0. (c) Each dot corresponds to one TC,\ndash lines show QR results for quantiles  = 0.1, 0.2, ..., 0.9. (d)  coecients (slopes of QR in the (c) as a function of  . The\nred arrows show low   ( = 0.1), half   ( = 0.5) and top   ( = 0.9), as, respectively, the nock, a circle on the shaft,\nand the head of the arrow, the blue solid line represents 0.\n\n3210123456Standardized boundary index0.00.20.40.60.81.0Forward mixing degree(a)0.00.20.40.60.81.0tophalflow-0.03-0.06(b)321012345Standardized fragmentation index0.00.20.40.60.81.0Forward mixing degree(c)0.00.20.40.60.81.0tophalflow-0.03-0.06(d)\f9\n\nFIG. S9. The scatter plot between dierent citations received during 2 years and forward mixing degree among all TCs (with\nat least 100 papers) in 1990s. (a) Highest citation, (b) Third quartile citation, (c) Median citation, (d) Mean citation.\n\n050100150200250300350400450(a)Highest citation02468101214(b)3rd quartile citation0.00.20.40.60.81.001234567(c)Median citation0.00.20.40.60.81.00246810(d)Mean citationForward mixing degreeCitation\f", 
        "tag": "Digital Libraries", 
        "link": "https://arxiv.org/list/cs.DL/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nO\nC\n.\nh\nt\na\nm\n[\n \n \n1\nv\n6\n3\n8\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\nAB STRAC T. We introduce a generalization of the celebrated Lov asz theta number of a\ngraph to simplicial complexes of arbitrary dimension. Our generalization takes advantage\nof real simplicial cohomology theory, in particular combinatorial Laplacians, and provides\na semidenite programming upper bound of the independence number of a simplicial com-\nplex. We consider properties of the graph theta number such as the relationship to Hoff-\nmans ratio bound and to the chromatic number and study how they extend to higher dimen-\nsions. Like in the case of graphs, the higher dimensional theta number can be extended to a\nhierarchy of semidenite programming upper bounds reaching the independence number.\nWe analyse the value of the theta number and of the hierarchy for dense random simplicial\ncomplexes.\n\n1. IN TRODUC T ION\nThe theta number (G) of a graph G was introduced by L. Lov asz in his seminal paper\n[32], in order to provide spectral bounds of the independence number and of the chromatic\nnumber of G. In modern terms, (G) is the optimal value of a semidenite program, and\nas such is computationally easy; in contrast, the independence number (G) and the chro-\nmatic number (G) are difcult to compute. These graph invariants satisfy the following\ninequalities, where G denotes the complement of G:\n(G)  (G)  (G).\n(1)\nThe inequality (G)  (G) was one of the main ingredients in Lov asz proof of the\nShannon conjecture on the capacity of the pentagon [32]. More generally, this inequality\nplays a central role in extremal combinatorics, sometimes in a disguised form: to cite a few,\nthe Delsarte linear programming method in coding theory [8] and recent generalizations\nof Erd os-Ko-Rado theorems [7, 12, 13] can be interpreted as instances of this inequality.\nAnalogs of the theta number in geometric settings have lead to many advances in packing\nproblems (see [36] and references therein), in particular the very recent solutions to the\nsphere packing problems in dimensions 8 and 24 [5, 40].\nOur aim in this paper is to generalize this graph parameter to higher dimensions, in\nthe framework of simplicial complexes. Let us recall that an (abstract) simplicial complex\nX on a nite set V is a family of subsets of V called faces that is closed under taking\nsubsets. We refer to Section 1 for basic denitions and results about simplicial complexes.\nGraphs t in this framework, being simplicial complexes of dimension 1. In recent years,\nconsiderable work has been devoted to generalizing the classical theory of graphs to this\nhigher-dimensional setting. Much of the efforts have focused on the notion of expansion\n(see, e.g., [9, 15, 20, 27, 33, 38]), but other natural concepts such as random walks [37],\ntrees [11, 26], planarity [35], girth [10, 34], independence and chromatic numbers [14, 19]\nhave been extended to higher dimensions. Some of these notions were introduced and\nstudied previously in the context of hypergraphs. Pure k-dimensional simplicial complexes\n\nDate: April 7, 2017.\n\n1\n\n\f2\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\nare essentially (k + 1)-uniform hypergraphs, but the topological point of view brings the\nmachinery of algebraic topology such as homology theory to the subject.\nThe familiar graph-theoretic notions of independence number and of chromatic number\nextend in a natural way to this setting: For a k-dimensional simplicial complex X , an\nindependent set is a set of vertices that does not contain any maximal face of X , and\nthe independence number (X ) is the maximal cardinality of an independent set. The\nchromatic number1 (X ) is the least number of colors needed to color the vertices so that\nno maximal face of X is monochromatic, in other words, it is the smallest number of parts\nof a partition of the vertices into independent sets.\nIn order to dene the theta number k (X ) of a pure k-dimensional simplicial complex\nX , we will follow an approach that leads in a natural way to the inequality (X )  k (X ).\nThe main idea is to associate to an independent set S a certain matrix, and then to design\na semidenite program that captures as many properties of this matrix as possible. The\nmatrix that we associate to an independent set is (up to a multiplicative factor) a submatrix\nof the down-Laplacian of the complete complex. In the case of dimension 1, the down-\nLaplacian is simply the all-one matrix, and we end up with one of the many formulations\nof the Lov asz theta number.\nOur rst task will be to compare k (X ) to the eigenvalue upper bound of (X ) proved\nby Golubev in [19]. This upper bound involves for 0  i  k  1, the largest eigenvalues\n(cid:19)\n(cid:18)\ni of the i-th up-Laplacians of X and the minimal degrees di of the i-faces of X :\n1  (d0 + 1)(d1 + 2) . . . (dk2 + k  1)dk1\n(X )  n\n(2)\n.\n0 . . . k1\nWhen every possible (k  1)-face is contained in at least one k-face, i.e., when X has a\n(cid:18)\n(cid:19)\ncomplete (k  1)-skeleton, this inequality simplies to\n1  dk1\n(X )  n\nk1\n(cid:19)\n(cid:18)\nand can thus be seen as a natural generalization of the celebrated ratio bound for graphs\nattributed to Hoffman (see, e.g., [4, Theorem 3.5.2]). In that case, we will show that\n1  dk1\nk (X )  n\nk1\ntherefore k (X ) provides an upper bound of (X ) that is at least as good as (3).\nIn\nthe case of a non-complete (k  1)-skeleton, Golubevs bound and k (X ) turn out to be\nincomparable, as we will see in examples below.\nThe theta number of a graph has many very nice properties; some of them, although\nunfortunately not all of them, can be generalized to higher dimensions. Most of this paper\nis devoted to determining which of the properties of the graph theta number extend to our\nnotion of the theta number of simplicial complexes.\nThe relationship to the chromatic number generalizes only partially. Indeed, the in-\nequality (X )  k (X ) immediately leads to the inequality n/k (X )  (X ). How-\never, in the case of graphs, the stronger inequality (G)  (G) holds. We will see that\nits natural analog in the setting of k-complexes would be that k (X )  k(X ) and that\nthis inequality does not hold in general. Instead, we will introduce an ad hoc notion of\nchromatic number for simplicial complexes, denoted k (X ), and show that the inequality\nk (X )  k (X ) holds. While (X ) is dened using vertex colorings, the denition of\n1In the study of hypergraphs, the chromatic number (X ) is also known as the weak chromatic number while\n(X1 ), the chromatic number of the 1-skeleton, is known as the strong chromatic number.\n\n(3)\n\n,\n\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n3\nk (X ) is based on colorings of (k  1)-faces respecting orientations. Moreover, it is tightly\nrelated to a notion of homomorphisms between pure k-dimensional simplicial complexes\nthat we introduce and that may be of interest by itself.\nA very interesting benet of the theta number of a graph is that it is possible to expand\nit into hierarchies of semidenite upper bounds of the independence number; Lasseres\nhierarchy based on polynomial optimization principles is one of the most popular (see\n[29, 30]). We will see that a similar situation holds in higher dimensions: to a pure k-\ndimensional complex X we will associate a sequence (cid:96) (X ) for (cid:96) = k , . . . , (X ) such\nthat\n(X ) = (X ) (X )      (cid:96) (X )      k (X )  k (X ).\nIn order to dene (cid:96) (X ), we will proceed in two steps: in a rst step, we dene a natural\nsequence (cid:96) (X ) for (cid:96) = k , k + 1, . . . , (X ); in a second step, we modify the denition of\n(cid:96) (X ) slightly in such a way that the sequence of its values decreases.\nOur last results concern the theta number of random simplicial complexes X k (n, p)\nfrom the model proposed by Linial and Meshulam in [31]. This model is a higher-di-\nWe show that k (X k (n, p)) is of the order of (cid:112)(n  k)(1  p)/p for probabilities p\nmensional analog of the Erd os-R enyi model G(n, p) for random graphs and has gained\nincreasing attention in recent years (see [25] for a survey).\nsuch that c0 log(n)/n  p  1  c0 log(n)/n for some constant c0 . This result extends\nthe known estimates for the value of the theta number of the random graph G(n, p).\nThe paper is organized as follows: Sections 2 and 3 recall basic denitions and prop-\nerties of simplicial complexes and semidenite programming. Section 4 recalls proper-\nties of the theta number of a graph that serve as a guideline for the theta number of a\nk-dimensional simplicial complex, which is introduced in Section 5. Section 6 computes\nthe theta number of certain basic families of 2-dimensional simplicial complexes. Section\n7 discusses chromatic numbers and Section 8 the hierarchy of theta numbers. The nal\nSection 9 contains the analysis of the theta number of random simplicial complexes.\nLet V = {v1 , . . . , vn} be a nite set. We will use the notation (cid:0)V\n(cid:1) for the set of k-\n2. S IM P L IC IA L COM P LEX E S\nk\nsubsets of V . Let us recall that an (abstract) simplicial complex X on a vertex set V is a\nfamily of subsets of V (called the faces of X ), such that if F  X , then all subsets of F\nalso belong to X . The dimension of a face F  X is |F |  1, and we denote by Xi the set\nof i-dimensional faces of X , with the convention X1 = {}. Note that we do not require\nevery element in V to be a 0-face of X , so X0 can be a proper subset of V . The i-skeleton\nof X is the simplicial complex X1  X0      Xi .\nA simplicial complex X is said to be of dimension k  0, if k is the maximal dimension\nof any of its faces. For example, a graph is a simplicial complex of dimension 1. Going\nback to the general case, if X is of dimension k , and if moreover all maximal (with respect\nto inclusion) faces of X are of dimension k , then X is said to be pure. Unless explicitly\nmentioned, we will only consider pure complexes.\nA basic example of a pure k-dimensional simplicial complex is the complete k-complex\nn , whose faces are all the subsets of [n] = {1, . . . , n} that have at most (k + 1) elements.\nK k\nWe note that in order to dene a pure simplicial complex of dimension k , it is enough\nto specify its set of k-dimensional faces. In particular, the complementary complex X of a\npure simplicial complex of dimension k , is again a pure simplicial complex of dimension\nk , whose k-dimensional faces are those (k + 1)-subsets of V that do not belong to Xk\n\n\f4\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\n(we adopt the convention that the empty complex, whose set of faces is empty, is pure of\ndimension k for all k  0).\nLet X be a simplicial complex; we assume that every face of X is endowed with an\norientation, i.e., a local ordering of its vertices. Then, if F  Xi and K  Xi1 , an\noriented incidence number [F : K ]  {0, 1} can be dened. Often, the orientation\n(cid:26) (1)j\nof the faces is induced by a global ordering of the vertex set V ; in that case, if F =\n{x0 , x1 , . . . , xi } where x0 < x1 <    < xi with respect to this ordering,\nif K  F and F \\ K = {xj },\n[F : K ] =\notherwise.\n0\nThe vector space of functions from Xi to R is denoted by C i (X ; R) and its elements\nare called i-dimensional cochains of X with coefcients in R. The coboundary map i :\n(cid:88)\nC i (X ; R)  C i+1 (X ; R) is dened for 1  i < dim(X ) by\n[H : F ]f (F ).\n(i f )(H ) =\nF Xi\nThe image of i1 is the subspace B i (X ; R) of i-dimensional coboundaries, and the kernel\nof i is the subspace Z i (X ; R) of i-dimensional cocycles. Because the coboundary maps\nsatisfy i  i1 = 0, we have B i (X ; R)  Z i (X ; R). The quotient group\nH i (X ; R) := Z i (X ; R)/B i (X ; R).\nspaces C i (X ; R) are endowed with the standard inner product (cid:104)f , g(cid:105) = (cid:80)\nis then called the i-th cohomology group of X with coefcients in R.\nAnalogously, we can dene the homology groups of a simplicial complex. For this, the\nf (F )g(F )\nF Xi\n: C i+1 (X ; R)  C i (X ; R) is dened as the adjoint of\nand the boundary map i+1 = \n(cid:88)\nthe coboundary map i . We have, for F  Xi ,\ni\n(i+1 f )(F ) =\nHXi+1\nThe spaces of boundaries Bi (X ; R) := im i+1 and of cycles Zi (X ; R) := ker i are\nsubspaces of C i (X ; R) satisfying Bi (X ; R)  Zi (X ; R) and thus dene the i-th reduced\nhomology group of X\n\nHi (X ; R) := Zi (X ; R)/Bi (X ; R).\nMoreover, by duality we have that Zi (X ; R) = B i (X ; R) and Z i (X ; R) = Bi (X ; R) .\nThe following diagram summarizes these linear maps for 0  i  dim(X )  1:\ni1\n/ C i1 (X ; R)\n/ C i (X ; R)\nC i+1 (X ; R)\n\n[H : F ]f (H ).\n\ni\n\ni+1\n\ni\n\n\n\ni and i-th down-Laplacian L\nThe i-th up-Laplacian L\ni of X are the following self-\nadjoint and positive semidenite operators on C i (X ; R):\n\n\ni := i1i , L\nL\ni := i+1 i .\n\n\n\n\n\ni = 0. Furthermore, it is not hard to see that ker L\nBy denition, L\ni = L\ni L\ni L\ni =\n\n\n\ni = B i (X ; R), ker L\ni = Z i (X ; R), and im L\nZi (X ; R), im L\ni = Bi (X ; R). For\nHi (X ; R) := Zi (X ; R)  Z i (X ; R),\nwe have the Hodge decomposition of C i (X ; R) into pairwise orthogonal subspaces\nC i (X ; R) = Hi (X ; R)  B i (X ; R)  Bi (X ; R).\n\n/\no\no\n/\no\no\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\nIn particular, Hi (X ; R) (cid:39) H i (X ; R) (cid:39) Hi (X ; R).\nThe characteristic functions eF of faces F  Xi are called elementary cochains; they\nform an orthonormal basis of C i (X ; R). In order to express the matrices of the Laplacian\noperators in this basis we introduce the following notation: for F  Xi , let deg(F ) denote\nthe degree of F , i.e., the number of (i + 1)-faces of X that contain F . For (F , F (cid:48) )  X 2\ni ,\nsuch that |F  F (cid:48) | = i, let\n\n5\n\nand\n\n\ni )F,F (cid:48) =\n(L\n\n\u0001F,F (cid:48) := [F : F  F (cid:48) ][F (cid:48) : F  F (cid:48) ].\nWe note that, if F  F (cid:48)  Xi+1 , we can express \u0001F,F (cid:48) also as\n\u0001F,F (cid:48) = [F  F (cid:48) : F ][F  F (cid:48) : F (cid:48) ].\n(cid:26) i + 1\ni , such that |F  F (cid:48) | (cid:54)= i, we set \u0001F,F (cid:48) = 0. Then, it is easy to see that\nFor (F , F (cid:48) )  X 2\nif F = F (cid:48)\n\ni )F,F (cid:48) =\n(L\n deg(F )\notherwise\n\u0001F,F (cid:48)\nif F = F (cid:48)\nif F  F (cid:48)  Xi+1\n\u0001F,F (cid:48)\notherwise\n0\nwhere we use the same notations for the operators and for their matrices in the basis of\nelementary cochains.\nExample 2.1. In the case of the simplicial complex associated to a graph G = (V , E ),\n\ndened by X1 = {}, X0 = V and X1 = E , we nd that L\n0 = J is the all-ones matrix\n\n0 is equal to the combinatorial Laplacian L = D A where D is the diagonal matrix\nand L\nwith the degrees of the vertices as diagonal elements and A is the adjacency matrix of the\ngraph.\nn , and for 0  i  k  1, it is easy to verify\nExample 2.2. For the complete k-complex K k\nthat\n\n\ni + L\nL\ni = nI .\n\n\n\n\n\n\nof these operators gives the multiplicities of this eigenvalue, namely (cid:0)n1\n(cid:1) for L\ni .\ni and that (L\nTogether with the property L\ni = 0, we obtain that (L\ni )2 = nL\ni )2 = nL\ni L\n(cid:0)n1\n(cid:1) for L\nSo n is the only non zero eigenvalue of the up and down Laplacians. Computing the traces\n\ni and\n(cid:18)n  1\n(cid:19)\ni\n\ni . So we have\ni+1\n\n\ni  nI ) = im(L\n(cid:18)n  1\n(cid:19)\n,\ni ) = Bi ,\nker(L\ndim(Bi ) =\ni + 1\nand, as these dimensions add up to (cid:0) n\n(cid:1) = dim(C i ), Hi = {0}.\n\n\ni  nI ) = im(L\ndim(B i ) =\ni ) = B i ,\nker(L\ni\ni+1\nWe conclude this section by recalling the denition of the adjacency matrix of a k-\n\nk1 = D  A where D\n(cid:26) \u0001F,F (cid:48)\ndimensional simplicial complex X : it is the matrix A such that L\nis the diagonal matrix encoding the degrees of the (k  1)-faces. In other words,\nif F  F (cid:48)  Xk\notherwise\n0\nWe note that in dimension 1 this denition coincides with the usual notion of the adjacency\nmatrix of a graph.\n\nAF,F (cid:48) =\n\n,\n\n\f6\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\n3. S EM IDE FIN I TE PROGRAMM ING\nIn this section, we gather basic facts about semidenite programs. For further informa-\ntion we refer to standard references such as [2], [3], [39].\nSemidenite programs (SDP for short) are special cases of convex optimization pro-\ngrams that admit efcient algorithms, such as algorithms based on the so-called interior\npoint method. They generalize linear programs and have turned out to be very useful for\nproviding polynomial time approximations of hard problems in many areas, especially in\ncombinatorics (see, e.g., [18] and [1, Chapter 6]).\nFor a matrix A  Rnn we say that A is positive semidenite, denoted by A (cid:23) 0, if\nA is real-valued, symmetric, and if all its eigenvalues are nonnegative. If moreover none\nof its eigenvalues are equal to zero, A is positive denite (A (cid:31) 0). The set of all positive\nsemidenite matrices is a cone denoted by Rnn(cid:23)0 . The space of real symmetric matrices is\nendowed with the standard inner product (cid:104)A, B (cid:105) = trace(AB ).\nGiven (c1 , . . . , cm )  Rm and symmetric matrices A0 , . . . , Am of size n, the following\noptimization problem is a semidenite program in primal form:\np = sup{(cid:104)A0 , Z (cid:105) : Z  Rnn(cid:23)0 , (cid:104)Ai , Z (cid:105) = ci for all 1  i  m}.\nIn other words, this program asks for the supremum of a linear form, where this supre-\nmum is taken over the intersection of the cone of positive semidenite matrices with an\nafne space.\nA feasible solution of this program is a matrix Z that satises the required constraints:\nZ  Rnn(cid:23)0\nand (cid:104)Ai , Z (cid:105) = ci . It is an optimal solution if its objective value (cid:104)A0 , Z (cid:105) is\nequal to p . If there is no feasible solution, we let p = .\nThe following dual program is attached to the primal program:\nd = inf {c1x1 +    + cmxm : (x1 , . . . , xm )  Rm , A0 + x1A1 +    + xmAm (cid:23) 0}.\nThe terms primal and dual do not refer to a specic class of programs: Despite their\napparent difference, any of these programs can be put in the form of the other, and, as\nexpected, dualizing twice returns the initial program.\nThe inequality p  d , referred to as weak duality, always holds, and under some mild\nconditions even strong duality, i.e., p = d , holds. Strong duality is guaranteed if the\nSDP satises the so-called Slaters conditions, of which we will use the following version:\nIf an SDP has a strictly feasible primal solution, i.e., if there is a feasible solution Z of\nthe primal program such that Z (cid:31) 0, and a strictly feasible dual solution, i.e., there exists\n(x1 , . . . , xm ) such that A0 + x1A1 +    + xmAm (cid:31) 0, then strong duality holds and,\nmoreover, there are optimal solutions for both the primal and the dual program.\n\n4. TH E TH ETA NUMB ER O F A GRA PH\nIn this section, we introduce the theta number of a graph G = (V , E ). Our presentation\nwill serve as a guideline for the generalization to higher dimensional simplicial complexes.\nLet S be an independent set of G, i.e., a subset of V not containing any edges. The set\nS naturally denes a vector 1S  RV , namely its characteristic vector. We consider the\n(cid:26) 0\nS , whose entries are given by:\nmatrix Y S := 1S 1T\nif {v , v (cid:48)} (cid:42) S\notherwise.\n1\nThe following properties of Y S motivate the denition of (G): Y S is a positive semidef-\nv ,v (cid:48) = 0 if {v , v (cid:48)}  E . Furthermore, the cardinality of S can be\ninite matrix such that Y S\n\nY S\nv ,v (cid:48) =\n\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n\n7\n\nrecovered in two different ways from Y S : If I and J stand as usual for the identity matrix\nand the all-ones matrix, we have (cid:104)I , Y S (cid:105) = |S | and (cid:104)J, Y S (cid:105) = |S |2 . So, if we set\n(4) (G) = sup{(cid:104)J, Y (cid:105) : Y  RV V , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1, Yv ,v (cid:48) = 0 if {v , v (cid:48)}  E }\nthe matrix |S |1Y S is feasible for (4) and we get that |S |  (G).\nBecause (4) is a semidenite program, its optimal value (G) can be approximated\nnumerically up to arbitrary precision in polynomial time in the size of G. If, instead of a\nsharp numerical value, one aims for a rougher upper bound of (G), the dual formulation\nof (4) is often more convenient:\n(G) = inf {max (Z ) : Z  RV V , Z = J + T , Tv ,v (cid:48) = 0 if {v , v (cid:48) } / E }.\nHere, max (Z ) denotes the largest eigenvalue of Z .\nTo illustrate this principle we consider a classical example. For any matrix T such\nthat Tv ,v (cid:48) = 0 for all {v , v (cid:48)} / E , the dual formulation of (G) provides the inequality\n(G)  max (J + T ). A possible choice for T is a multiple of the adjacency matrix A of\nG, say T = tA. The best bound is obtained for t minimizing max (J + tA). For d-regular\ngraphs, the matrices J and A commute, so the eigenvalues of J + tA are easy to analyze.\nThe optimal choice of t then leads to the so-called ratio bound attributed to Hoffman (see,\ne.g., [4, Theorem 3.5.2]):\n\n(5)\n\n(6)\n\n(G)  |V |min (A)\nd  min (A)\n\n.\n\n5. TH E TH ETA NUMB ER O F A S IM PL IC IA L COM PL EX\nWe now move to higher dimensions and dene the theta number of a k-dimensional\n\nk1 is the matrix indexed by (cid:0)V\n(cid:1) that is dened by:\nsimplicial complex X . As suggested in the introduction, the down-Laplacian L\nk1 of the\ncomplete complex K k\nn will play the role of the all-ones matrix J in (4) and (5). Recall that\n\n(cid:26) k\nL\nk\nif F = F (cid:48)\n\nk1 )F,F (cid:48) =\n(L\notherwise\n\u0001F,F (cid:48)\nWe note that this matrix may not be the down-Laplacian of the complex X . Obviously,\nthis is the case if and only if X has a complete (k  1)-skeleton, otherwise the down-\n\nk1 . From now on, to avoid confusion, we\nLaplacian of X is a principal submatrix of L\n\n\n\nwill denote the matrices associated to X by L\ni (X ), L\ni (X ) and reserve the notations L\ni ,\n\ngraphs, we consider the following matrix Y S , indexed by (cid:0)V\n(cid:1):\ni for the complete complex.\nL\nLet S  V be an independent set of X . Following the same strategy as in the case of\n(cid:26) 0\nk\nif F  F (cid:48) (cid:42) S\n(7)\n(Y S )F,F (cid:48) =\n\notherwise.\nk1 )F,F (cid:48)\n(L\nWe have Y S = (S\n, where as a generalization of the characteristic vector of S , we\nk ) T\n(S\nk )\n(cid:26) 0\n(cid:0)(S\n(cid:1)\nk ) dened as follows:\nconsider the matrix (S\nk )\n(S )K,F\n\nif F (cid:42) S\notherwise,\n\nK,F =\n\n\f(10)\n\n+\n\n(9)\n\nand\n\n, the matrix Y S\n\nProposition 5.2. We have\n\n\n(cid:104)Y S , L\nk1 (cid:105) = k2\n\n(cid:1), F  (cid:0)V\nwhere K  (cid:0) V\n(cid:1) and  is the matrix of the boundary operator k2 with respect\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n8\nk1\nk\nto the basis of elementary cochains. The properties of Y S lead to the following denition\nof k (X ):\n\nDenition 5.1. Let X be a pure k-dimensional complex on V , and let L\nk1 be the down\nk (X ) := sup (cid:8)(cid:104)L\nLaplacian of the complete complex on V . Let:\n(8)\n\nk1 , Y (cid:105) : Y  R(V\nk ) , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1,\nk )(V\nYF,F (cid:48) = 0 if F  F (cid:48)  Xk ,\n\u0001F,F (cid:48) YF,F (cid:48) = \u0001F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F (cid:9)\nYF,F (cid:48) = 0 if |F  F (cid:48) |  k + 2,\n(X )  k (X ).\nProof. Let S be an independent set with |S | = (X ). As Y S = (S\nk ) T\n(cid:18)|S |\n(cid:19)\n(S\nk )\nis clearly positive semidenite. We have\n(cid:104)Y S , I (cid:105) = k\n(cid:18)|S |\n(cid:19)\nk\n(cid:88)\n1\n(cid:18)|S |\n(cid:18) |S |\n(cid:18)|S |\n(cid:19)\n(cid:19)\nk\n|F F (cid:48) |=k+1\nF F (cid:48)S\nk\nk + 1\nk\nMoreover, from the fact that S is an independent set, and from the denition of Y S (7), it\nis clear that (Y S )F,F (cid:48) = 0 if F  F (cid:48)  Xk , or if |F  F (cid:48) |  k + 2.\nThe conditions \u0001F,F (cid:48) YF,F (cid:48) = \u0001F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F  are satised by the\nTo sum up, we have proved that the matrix k1 (cid:0)|S |\n(cid:1)1\n\nentries of L\nk1 , so the matrix Y S inherits this property.\nY S is feasible for k (X ). Since\nits objective value is equal to |S |, we can conclude that (X )  k (X ).\n(cid:3)\nk\nNow we consider the dual program of (8), in order to obtain another formulation of\nk (X ), similar to (5).\nk (X ) = inf (cid:8) max (Z ) : Z = L\nProposition 5.3. We have\nTF,F = 0 for all F  (cid:0)V\n(cid:1)\n\n(cid:9)\nF F (cid:48)=H \u0001F,F (cid:48) TF,F (cid:48) = 0 if H  (cid:0) V\n(cid:1) \\ Xk\n(cid:80)\nk1 + T ,\n(11)\nk\nsame objective value because Slaters condition holds: Y = (cid:0)n\n(cid:1)1\nk+1\nProof. This is just a straightforward rewriting of the dual program. Both programs have the\nI is a strictly feasible\n(cid:3)\nk\nsolution of (8) and T = 0 gives rise to a strictly feasible solution of (11).\nas expected, k  k (X )  n. Indeed, the lower bound follows by taking Y = (cid:0)n\n(cid:1)1\nRemark 5.4. Let us make a few obvious observations about k (X ). The rst one, is that,\nI in\nk\n(8) while the upper bound follows by taking T = 0 in (11).\n\n(cid:19)\n\n= k2\n\n+ (k + 1)k\n\n= k\n\n|S |.\n\n\f9\n\n( deg(F )).\n\nTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\nplete k-complexes. Indeed, if X is the empty k-complex, the matrix Y = k1 (cid:0)n\n(cid:1)1\nThe second observation is that k (X ) is easy to determine for the empty and the com-\n\nprogram (8) has only one feasible solution which is Y = (cid:0)n\n(cid:1)1\nL\nk1\nk\nis feasible for (8) giving that k (X ) = n. If X is the complete k-complex, the semidenite\nI so k (X ) = k .\nk\nWe note that, in these trivial cases, the equality (X ) = k (X ) holds.\nThe benet of the formulation (11) is that any feasible matrix T leads to an upper\nbound of k (X ) and therefore to an upper bound of the independence number of X . Let us\nillustrate this principle by showing that we can recover the upper bound proved by Golubev\n[19] in the case of a k-dimensional simplicial complex X with complete (k  1)-skeleton.\n\nk1 (X )  Dk1 (X )) for some   R that will be chosen later.\nWe take T =  (L\nClearly T satises the conditions required by (11). Then\n\n\n\nk1 + T )  max (L\nk1 + L\nmax (L\nk1 (X )) + max\nF Xk1\n\n\nWe assume that X has complete (k  1)-skeleton, so we have L\nk1 (X ) and\nk1 = L\n\n\n\nk1 (X ) = 0. Let us denote by  the set of non zero eigenvalues of L\nk1 (X ).\nL\nk1L\n\n\nThen, the eigenvalues of the matrix L\nk1 (X ) are: n, associated to the eigenspace\nk1 + L\nB k1 , and , for   , corresponding to eigenvectors in Bk1 . For  =\n,\nn\n\nmax (L\nk1 (X ))\n(cid:19)\n(cid:18)\n\n\nwe have max (L\nk1 (X )) = n and we get:\nk1 + L\n1  degmin (X )\n(X )  k (X )  n\nWe note that, if X is regular, i.e., if deg(F ) is a constant number for F  (cid:0)V\n(cid:1), then this\n.\n\nmax (L\nk1 (X ))\nk\nupper bound is the exact analog of the ratio bound for graphs (6).\nWe have just seen that, in the case of a k-complex with complete (k  1)-skeleton,\nk (X ) is an upper bound of the independence number of X which is as least as good\nas the bound (2). The case of complexes with noncomplete (k  1)-skeleton turns out\nto be more tricky; indeed, in some cases k (X ) provides a good bound of (X ), even\na sharp one, and beats the bound (2) given by Golubev, while in other cases, Golubevs\nbound is better. We provide examples illustrating this situation in the next section, where\nwe explicitly work out the computation of 2 (X ) for certain families of 2-dimensional\ncomplexes. This will also yield counterexamples for certain properties of the theta number\nrelated to the chromatic number that we might expect (see Section 7).\nIt will also be\ninteresting to observe the prominent role plaed by the eigenvalues and eigenspaces of the\nLaplacian operators in these examples .\n\n6. TH E TH ETA NUMB ER O F CERTA IN FAM I L I E S O F 2-COM PL EX E S\n6.1. The complete tripartite 2-complex. To dene this complex, we let n = 3m and\npartition V = [n] into three subsets A, B , C of equal size m. As 2-dimensional faces\nwe select all triangles with exactly one vertex in each of these subsets; as 1-dimensional\nfaces all edges with at most one vertex in each of these subsets. A natural notation for\nm,m,m ) = 2m because A  B is a maximal\nthis complex is K 2\nm,m,m . It is clear that (K 2\nindependent set with 2m vertices. We will show that 2 (K 2\nm,m,m ) = 2m.\nWith the notations of (2), d0 = 2m, d1 = m, 0 = 3m, 1 = 3m and the bound in (2)\nequals (7m  1)/3, so this is an example where the theta number beats Golubevs bound.\n\n\f10\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\nm,m,m , we have 2 (K 2\nWe will also show that, for the complementary complex K 2\nm,m,m ) =\nm,m,m ). This complex has a complete 1-skeleton with d1 = 2m  2 and\n3 = (K 2\n1 = 3m, so Golubevs bound (2) equals (m + 2), which is not tight.\nProposition 6.1. We have 2 (K 2\nm,m,m ) = 2m and 2 (K 2\nm,m,m ) = 3.\nProof. To keep notations light we use the generic notation X for X = K 2\nm,m,m throughout\nthe proof. We will verify that 2 (X ) = 2m, by constructing a suitable matrix T feasible for\n(11). The matrix T will be constructed from the projection matrices associated to certain\n\n\neigenspaces of L\n1 (X ) and L\n1 (X ).\nWe denote by A  B the set of edges connecting one vertex in A and one vertex in B ,\nand similarly for the other kinds of edges. So, X1 = (A  B )  (B  C )  (C  A).\nWe choose the orientations of the triangular faces and of the edges of X following the rule\nA  B  C  A; this way, [G : F ] = +1 for all G  X2 and F  X1 .\n\n1 (X ) has three non zero eigenvalues, 3m, 2m and\nIt turns out that the up-Laplacian L\nm, respectively with multiplicity 1, 3(m  1), and 3(m  1)2 . We will need the projection\n\n\nJ3m2 /(3m2 ). The space VA = {(cid:80)\naA xa (1aB + 1aC ) : (cid:80)\nmatrices P\n3m and P\n2m associated to the eigenvalues 3m and 2m.\n\n\nThe all-one vector is clearly an eigenvector of L\n1 (X ) for the eigenvalue 3m, so P\n3m =\naA xa = 0} is easily\n\nseen to be an eigenspace of L\n1 (X ) for the eigenvalue 2m. Similarly, we have two other\n(m  1)-dimensional eigenspaces VB and VC , and these spaces are pairwise orthogonal.\n\nIn order to express the projection matrix P\n2m associated to the sum of these spaces, we\nintroduce the following notation: for (F , F (cid:48) )  X 2\n1 , we denote F  F (cid:48) if F and F (cid:48) both\n\nbelong to A  B (respectively to B  C , C  A). Then,\n2(m  1)\nif F = F (cid:48)\nif F  F (cid:48) and F  F (cid:48) = \n2\nif F  F (cid:48) and F  F (cid:48) (cid:54)= , F (cid:54)= F (cid:48)\n(m  2)\n1\nif F (cid:54) F (cid:48) and F  F (cid:48) = \n(m  1)\nif F (cid:54) F (cid:48) and F  F (cid:48) (cid:54)= \n\n1 (X ) has two non zero eigenvalues: 3m with multiplicity 2\nThe down Laplacian L\nand 2m with multiplicity 3(m  1). The vector space {1AB + 1BC + 1AC :\n\n++ = 0} is a two-dimensional space of eigenvectors for L\n1 (X ) and for the eigenvalue\n(cid:40)\n\n3m is given by:\n3m, and the corresponding projection matrix P\nif F  F (cid:48)\n\n3m2 \n2\n1\n3m )F,F (cid:48) =\n(P\n1\notherwise.\nC ). We now will consider matrices indexed by the whole set (cid:0)V\n(cid:1), therefore we extend the\nSo far the matrices that we have dened are indexed by X1 = (A  B )  (B  C )  (A \n2\nmatrices introduced above by adding zero rows and columns for the indices not belonging\nto X1 (we keep the same notation for the enlarged matrices). We are now ready to dene\nthe matrix T that will do the job for 2 (X ):\nLemma 6.2. With the previous notations, let\n\n\n2m + P\n3m + P\nT = 2m(P\nThis matrix satises the following properties:\n\n\n\n3m )  L\n1 (X ).\n\n2m2 \n1\n\n\n2m )F,F (cid:48) =\n\n(P\n\n\f11\n\nWe want to prove that\n\n(1) TF,F = 0 for all F  (cid:0)V\n(cid:1)\nTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n(2) TF,F (cid:48) = 0 for all F , F (cid:48) such that F  F (cid:48) (cid:54)=  and F  F (cid:48) / X2\n2\n\n(3) 2mI  L\n1  T (cid:23) 0.\nProof. Properties (1) and (2) follow by direct verication. In order to prove (3), we write\n\n\n\n\n\n\n1 (X )  L\n1 ,\n3m and W = L\n2m ), V = 2mP\n1 + T = U + V + W where U = 2m(P\n3m + P\nL\nand make the remark that the product of any two of these matrices is zero. Indeed, for U, V\nand for U, W it follows immediately from the property that the product of up and down\n\n3m is an eigenspace for\nLaplacians is zero; for V , W , it is due to the fact that the image of P\n\n\n1 . So, we need to prove that 2mI  U ,\nthe eigenvalue 3m not only for L\n1 (X ) but also for L\n2mI  V and 2mI  W are positive semidenite. For the rst two it is obvious because\n\n\n\n2m ) and 2mI  V = 2m(I  P\n2mI  U = 2m(I  P\n3m  P\n(cid:1) so that those in X1 = (A  B )  (B  C ) \nFor this, we arrange the elements of (cid:0)V\n3m ). So now the only missing\n\n\npiece is a proof that 2mI  (L\n1  L\n1 (X )) (cid:23) 0.\n(C  A) come before those in (A  A)  (B  B )  (C  C ), and we accordingly write\n2\n(cid:18)L\n(cid:19)\n\n1 by blocks:\nL\n\n\n1 (X ) M\nL\n1 =\n(cid:19)\n(cid:18) 2mI\nM T\nN\nM\n(cid:23) 0.\n2mI  N\nM T\nBy the Schur complement lemma, this is equivalent to 2mI  N  (2m)1M T M (cid:23) 0. A\ndirect computation shows that M T M = 2mN , so all boils down to mI  N (cid:23) 0, which is\n\nm ). (cid:3)\nindeed true because N is a block-diagonal matrix with three blocks equal to L\n1 (K 2\nNow, we turn our attention to K 2\nm,m,m = X . In order to prove that 2 (X ) = 3, we will\nuse the primal formulation (8) and apply a symmetry argument. In the next section we will\n(cid:18)Y1\n(cid:19)\nsee a second, simpler, proof, using chromatic numbers, see Example 7.6.\nWith the previous notations, a feasible matrix Y must be of the form:\n0\n I\n0\nwhere Y1 is supported on the diagonal and on the triangles that belong to X2 , i.e., the\ntriangles with one vertex in each of A, B , C . It is clear that the automorphism group of\nX permutes transitively the elements of X2 and of X1 , and that, by convexity, (8) has a\n\n1 (X )+  I.\nsymmetric solution. So, without loss of generality, we can assume that Y1 = L\nRestricting the semidenite program on this set of matrices leads to a linear program in the\nvariables  ,  ,  that can be easily solved and leads to the optimal value 3. We skip the\ndetails here.\nWe note that this approach would not work for 2 (X ) because X 2 has two orbits: the\ntriangles that are fully contained in one of the subsets A, B , C and the ones that have two\n(cid:3)\nvertices in one of these sets and one vertex in another one.\n\n.\n\nY =\n\n6.2. The complete bipartite 2-complex. Now n = 2m and V = [n] is partitioned in two\nsubsets A, B , of equal size m. As 2-dimensional faces we select the triangles that meet\nboth sets A and B , thus having two vertices in one of the parts and the third vertex in the\nother. We denote this complex by K 2\nm,m . It is clear that (K 2\nm,m ) = m since A is an\nindependent set with m vertices. This complex has a complete 1-skeleton and d1 = m,\n\n\f12\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\n1 = 2m so the bound (3) equals m, showing that 2 (K 2\nm,m ) = m and that the theta\nnumber agrees with Golubevs bound.\nm,m , which is nothing else than the disjoint union\nFor the complementary complex K 2\nm,m ) = 4. Golubevs bound is twice the\nof two complete complexes K 2\nm , we have (K 2\nvalue corresponding to K 2\nm , thus 4, and it is sharp again. As we will see know, 2 (K 2\nm,m )\nis much larger:\nm,m ) = 8m4\nProposition 6.3. We have 2 (K 2\nm,m ) = m and 2 (K 2\nm+1 .\nm,m has two orbits in X1 = (cid:0)V\n(cid:1): the set X in\nProof. We let X = K 2\nm,m . To compute 2 (X ), we again apply the symmetry principle,\nlike in the case of the complement of the tripartite complex. The automorphism group of\n1 of edges contained in A or in B , having\nK 2\n1 of crossing edges, with degree 2(m  1). It acts transitively\n2\ndegree m, and the set X out\non the 2-faces. So without loss of generality a feasible matrix Y of the primal formulation\nof 2 (X ) can be assumed to be\n\n\nL\n1 (X ) +  Iout +  Iin =\n\n\nY = L\n1 (X ) +  Iout +  Iin\nwhere Iout and Iin denote the 0  1 diagonal matrices associated to respectively X out\nand\n1\n\n1 . The expressions of (cid:104)I , Y (cid:105) and of (cid:104)L\n1 , Y (cid:105) are linear in the variables  ,  ,  , but the\nX in\n\nwill become clear if we write the matrices by blocks according to (cid:0)V\n(cid:1) = X in\ncondition that Y is positive semidenite is slightly more complicated because L\n1 (X ) does\nnot commute with Iout and Iin . In fact, this condition leads to quadratic constraints, as it\n1  X out\n(cid:19)\n(cid:18) mI\n1 . It is\n2\neasy to verify that\nM\n\n, M T M = mN  2J\nM T\n2mI  N\nL\n1 (X ) =\nand that N has two non zero eigenvalues: 2m, with multiplicity 1 and eigenvector the all-\none vector, and m, with multiplicity 2(m  1). Then, by the Schur complement lemma,\n(cid:18)(m +  )I\n(cid:19)\nthe condition\nM\n(2m +  )I  N\nM T\nleads to quadratic inequalities. It is a bit technical but not difcult to see that an optimal\nsolution satises  =  , and nally that it is\n1\n\n2\nL\n1 (X ) +\nY =\nm2 (m + 1)\nm(m + 1)\n\n1 , Y (cid:105) = (8m  4)/(m + 1).\nleading to the optimal value (cid:104)L\n7. CHROMAT IC NUMB ER S\nLet us rst review the case of graphs. For a graph G, the clique number (G) = (G)\nand the chromatic number (G) are related by the obvious inequality (G)  (G), and\nthe theta number (G) lies in between these numbers ([32, Lemma 3, Corollary 3]):\n(G)  (G)  (G).\n(12)\nMoreover, the inequality (G)  (G) is always at least as strong as the inequality\nn/(G)  (G); indeed, we know that n  (G)(G) from [32, Corollary 2].\nLet us consider the situation for pure k-dimensional simplicial complexes. By analogy\nwith graphs, the chromatic number (X ) of a complex X , is usually dened to be the least\nnumber of colors needed to color the vertices of X such that no k-face is monochromatic.\n\n(cid:23) 0\n\nI,\n\n(cid:3)\n\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n\n13\n\nWe remark that for the complete k-complex K k\nn , the color classes of an admissible coloring\nn ) = (cid:100)n/k(cid:101). So, for all\ncannot have more than k elements, and consequently that (K k\nk-dimensional complexes X , we have (X )  k(X ). Given that we have dened a\ngeneralization of the theta number to k-complexes, that satises (X )  k (X ), it is\nnatural to wonder if the inequality\nk (X )  k(X ).\n(13)\nis also satised. Unfortunately, this is not true in general.\nIndeed, from the results of\nSection 6, one can see that (13) is satised for the complete tripartite complex and for its\ncomplement, but fails for the complete bipartite complex K 2\nm,m , for which 2 (K 2\nm,m ) =\n(8m  4)/(m + 1) (Proposition 6.3) while (K 2\nm,m ) = 2.\nLet us now see if we can modify the denition of the chromatic number of a simplicial\ncomplex, so that it ts better with our theta number. To achieve this, we will adapt the con-\ncept of graph homomorphisms to simplicial complexes. Indeed, a nice way to understand\nthe notions of chromatic and clique numbers of graphs is through their connection to graph\nhomomorphisms, as we will recall now.\nA homomorphism f from a graph G to a graph G(cid:48) is a mapping from the vertices of G\nto the vertices of G(cid:48) that sends an edge of G to an edge of G(cid:48) . Then, the clique number\nand the chromatic number have the following interpretations: the clique number (G) is\nthe largest number (cid:96) such that there is a homomorphism from the complete graph K(cid:96) to\nG, and similarly (G) is the smallest number (cid:96) such that there is a homomorphism from\nG to K(cid:96) . Moreover, one can prove that, if there is a homomorphism from G to G(cid:48) , then\n(G)  (G(cid:48) ). The combination of these properties immediately leads to (12).\nIn order to follow a similar approach for simplicial complexes, we introduce an ad-hoc\nnotion of homomorphism.\nDenition 7.1. Let X and X (cid:48) be two pure k-dimensional simplicial complexes. A homo-\nmorphism f from X to X (cid:48) is a mapping f : Xk1  X (cid:48)\nk1 with the following property:\nThere exist orientations of X and X (cid:48) such that for every H  Xk , there is H (cid:48)  X (cid:48)\nk such\nthat\n(1) {f (F ) : F  Xk1 , F  H } = {F (cid:48)  X (cid:48)\nk1 : F (cid:48)  H (cid:48)},\n(2) [H (cid:48) : f (F )] = [H : F ] for all F  Xk1 with F  H .\nWe note that this denition coincides in dimension 1 with the usual notion of a graph\nhomomorphism as one can always nd suitable orientations.\nRemark 7.2. In this denition, it is important to understand that a homomorphism f may\nnot necessarily be induced by a global mapping f0 between the vertices, i.e., it may be the\n0 such that f (F ) = f0 (F ) for all F  Xk1 .\ncase that there is no mapping f0 : X0  X (cid:48)\nAs an example consider the 2-dimensional complex X depicted in Figure 1.\nFurthermore, condition (2) is not automatically fullled. The 2-dimensional complex X\ndepicted in Figure 2 possesses a map f : X1  (K 2\n3 )1 satisfying condition (1) but there is\n3 .\nno homomorphism from X to K 2\nProposition 7.3. Let X and X (cid:48) be two pure k-dimensional simplicial complexes, and let\nf be a homomorphism from X to X (cid:48) . Then,\nk (X )  k (X (cid:48) ).\n(14)\nProof. Our strategy will be to start with an optimal solution Y of the primal formulation\n(8) of k (X ), from which we construct a matrix Y (cid:48) , feasible for k (X (cid:48) ), and having the\nsame objective value as Y .\n\n\f14\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\nF IGURE 1 . The homomorphism of X to K 2\n3 is not induced by a vertex map.\n\nF IGUR E 2 . A complex X with no homomorphism to K 2\n3\n\nYF,F (cid:48)\n\nSo, let Y be primal optimal for the semidenite program dening k (X ). We remark\nthat, if F / Xk1 , then, for all F (cid:48)\n(cid:54)= F , F  F (cid:48) / Xk , and so YF,F (cid:48) = 0. As a\nconsequence, by the optimality of Y , we have YF,F = 0.\nFor (K, K (cid:48) )  X 2\n(cid:88)\nk1 , we set\nY (cid:48)\nK,K (cid:48) =\n(F,F (cid:48) )X 2\nk1\nf (F )=K, f (F (cid:48) )=K (cid:48)\nWe have trace(Y (cid:48) ) = (cid:80)\nK,K = (cid:80)\nwhere the sum is zero if K or K (cid:48) does not belong to the image of f .\nk ) Y (cid:48)\nYF,F = trace(Y ).\nF Xk1\nK(V\nBy the property 1) of homomorphisms, if K (cid:54)= K (cid:48) and K  K (cid:48) is not an element of X (cid:48)\nk ,\nand if K = f (F ) and K (cid:48) = f (F (cid:48) ), then F  F (cid:48) cannot belong to Xk , and so YF,F (cid:48) = 0.\nSo, we have that Y (cid:48)\nK,K (cid:48) = 0.\nThanks to property 2), if K  K (cid:48)  X (cid:48)\nk and K  K (cid:48) = K (cid:48)(cid:48)  K  , the required condition\nthat \u0001K,K (cid:48) Y (cid:48)\nK,K (cid:48) = \u0001K (cid:48)(cid:48) ,K Y (cid:48)\nK (cid:48)(cid:48) ,K  holds. So, we have proved that Y (cid:48) is primal feasible\nfor k (X (cid:48) ).\n\nk1 , Y (cid:48) (cid:105). We have\nIt remains to analyze the objective value (cid:104)L\n(cid:88)\n\n(cid:104)L\nk1 , Y (cid:48) (cid:105) = k trace(Y (cid:48) ) +\n\u0001K,K (cid:48) Y (cid:48)\nK,K (cid:48) .\nK,K (cid:48) : KK (cid:48)X (cid:48)\nk\n\nXK23X?XK23X\f15\n\n\u0001K,K (cid:48)\n\nYF,F (cid:48)\n\nBut\n\n\u0001K,K (cid:48) Y (cid:48)\nK,K (cid:48) =\n\n(cid:88)\nK,K (cid:48)\nKK (cid:48)X (cid:48)\nk\n\nTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n(cid:88)\n(cid:88)\n(cid:88)\nK,K (cid:48)\n(F,F (cid:48) )X 2\nk1\nKK (cid:48)X (cid:48)\nf (F )=K, f (F (cid:48) )=K (cid:48)\nk\n(F,F (cid:48) )X 2\nk1\nF F (cid:48)Xk\nwhere in the last equality we ignore the terms corresponding to F  F (cid:48) / Xk because they\n\n\nare equal to zero, and we apply the property 2). It follows that (cid:104)L\nk1 , Y (cid:48) (cid:105) = (cid:104)L\nk1 , Y (cid:105).\n(cid:3)\n\n=\n\n\u0001F,F (cid:48) YF,F (cid:48)\n\nDenition 7.4. Let X be a pure k-dimensional simplicial complex. Let k (X ) denote\nthe smallest number (cid:96) such that there exists a homomorphism from X to the complete\n(cid:96) .\nk-complex K k\nIt is not hard to see that k (X )  (X1 ) holds for any pure simplicial complex X\nas a vertex coloring with (cid:96) colors that is a proper graph coloring for X1 gives rise to a\nhomomorphism from X to K k\n(cid:96) . The complex X depicted in Figure 1 serves as an example\nthat the three notions of chromatic numbers considered here differ. It has 2 (X ) = 3,\n(X ) = 2 and (X1 ) = 4.\nProposition 7.5. We have\n\nk (X )  k (X ).\nProof. If there is f : X  K k\n(cid:96) then applying (14) leads to k (X )  k (K k\n(cid:96) ) = (cid:96) (see\n(cid:3)\nRemark 5.4).\nExample 7.6. Consider the complex X = K 2\nm,m,m dened in Section 6. Clearly, 2 (X ) =\n(X1 ) = 3, so we have 3 = (X )  2 (X )  2 (X ) = 3 and hence 2 (X ) = 3.\nA k-dimensional subcomplex C of a pure k-dimensional simplicial complex X is a\nconnected component of X if for every (k1)-face F of C any k-face of X that contains F\nis also in C . Note that this condition does not need to hold for lower dimensional simplices,\nso two distinct connected components can, e.g., share a common vertex. Further observe\nthat the connected components of X correspond to the connected components of the graph\nthat has the k-faces of X as vertices with two vertices forming an edge if the correponding\nk-faces intersect in a common (k  1)-face.\nAs different connected components do not share (k  1)-faces, the inequality k (X ) \n(X1 ) can actually be extended to the connected components of X .\nProposition 7.7. Let C be the collection of connected components of X . Then\nk (X )  max\nCC (C1 ).\nIt is well-known that a d-regular graph G has a bipartite connected component if and\nonly if the largest eigenvalue of the Laplacian is 2d.\nIn [23] Horak and Jost present a\ncombinatorial criterion that can be considered as a higher-dimensional analog of this: They\n\nshow that for a d-regular k-complex X the largest eigenvalue of the Laplacian L\nk1 (X )\nis (k + 1)d if and only if there is a connected component C of X and an orientation of the\nk-faces of X such that [H : F ] = [H (cid:48) : F ] for all F  Ck1 , F  H, H (cid:48) . Note that for a\nconnected graph the existence of such an orientation is equivalent to bipartiteness.\n\n\f16\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\nIf a k-dimensional simplicial complex X has chromatic number k (X ) = k + 1, this\nguarantees the existence of such an orientation. Hence, we have the following observation.\nProposition 7.8. Let X be a d-regular k-dimensional simplicial complex. If k (X ) =\n\nk + 1, then the maximal eigenvalue of the up-Laplacian L\nk1 is (k + 1)d.\nWe remark that these results extend to arbitrary complexes for a normalized version of\nthe Laplacian that we do not study here.\n\n8. A H I ERARCHY O F SEM ID E FIN I T E RE LAXAT ION S FOR TH E INDE PENDENCE\nNUMBER O F A k - S IM PL IC IA L COM PL EX\nIn this section, X is again a pure k-dimensional simplicial complex. We consider a\nstraightforward generalization of k (X ) that leads to higher order theta numbers (cid:96) (X ) for\n(cid:96) > k . We will see that all these numbers provide upper bounds of (X ), until (cid:96) = (X ),\nwhere (X ) = (X ). Finally, we will modify this sequence of theta numbers in order to\nget a decreasing sequence.\nindependence complex of X , and that it has complete (k  1)-skeleton, i.e., Indk1 = (cid:0)V\n(cid:1).\nIt will be convenient to denote by Indi the set of independent sets of dimension i.\nWe make the remark that Ind := Ind1      Ind(X )1 is a simplicial complex, the\nk\nFor (cid:96) > k , the matrices involved in the program dening (cid:96) (X ) are indexed by Ind(cid:96)1 .\nWe dene, for k  (cid:96)  (X ):\n(cid:96) (X ) = sup (cid:8)(cid:104)L\nYF,F (cid:48) = 0 if F  F (cid:48)  (cid:0) V\n(cid:1) \\ Ind(cid:96) ,\n(15)\n\n(cid:96)1 (Ind), Y (cid:105) : Y  RInd(cid:96)1  Ind(cid:96)1 , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1,\n\u0001F,F (cid:48) YF,F (cid:48) = \u0001F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F  (cid:9)\nYF,F (cid:48) = 0 if |F  F (cid:48) |  (cid:96) + 2,\n(cid:96)+1\n(cid:96) (X ) = inf (cid:8) max (Z ) : Z = L\nand its dual formulation:\n\n(cid:9)\n(cid:80)\n(cid:96)1 (Ind) + T ,\nTF,F = 0 for all F  Ind(cid:96)1 ,\nF F (cid:48)=H \u0001F,F (cid:48) TF,F (cid:48) = 0 if H  Ind(cid:96)\nThe above denition matches for (cid:96) = k with that of k (X ). Both primal and dual programs\nare strictly feasible: Y = I /(cid:104)I , I (cid:105) and respectively T = 0 give rise to strictly feasible\nsolutions. We note that, if (cid:96) = (X ), the feasible matrices of the primal program are\ndiagonal matrices and hence (cid:96) (X ) = (cid:96) = (X ). We have\nProposition 8.1.\n\n(16)\n\n(X )  (cid:96) (X ).\nProof. The same proof as the one of Proposition 5.2 works. For an independent set S such\n(cid:26) 0\nthat |S |  (cid:96), we dene Y S  RInd(cid:96)1  Ind(cid:96)1 by\n\n(cid:96)1 (Ind))F,F (cid:48)\n(L\nthat (cid:96)1 (cid:0)|S |\n(cid:1)1\nIt is then easy to verify, as every subset of an independent set S is also an independent set,\nY S is feasible for the primal program (15) and that its objective value is\nequal to |S |.\n(cid:3)\n(cid:96)\n\nif F  F (cid:48) (cid:42) S\notherwise.\n\n(Y S )F,F (cid:48) =\n\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n\n17\n\ny(F ) + (cid:96)((cid:96) + 1)\n\ny(H ).\n\nand\n(18)\n\nHowever, it is not clear that the sequence ((cid:96) (X ))k(cid:96)(X ) is decreasing, because the\nconstraints on the (cid:96)-sets involved in (cid:96)1 (X ) do not occur explicitly in (cid:96) (X ). We now\ndene a variant of (cid:96) (X ) that provides a decreasing sequence of upper bounds of (X ).\nTo start with, we note that, if a matrix Y is feasible for (15), then the value of \u0001F,F (cid:48) YF,F (cid:48)\nfor (F , F (cid:48) ) such that |F  F (cid:48) | = (cid:96) + 1 only depends on F  F (cid:48) . So, we can associate to Y a\nfunction y  RInd(cid:96) such that \u0001F,F (cid:48) YF,F (cid:48) = y(H ) if H = F  F (cid:48) . If we extend y to Ind(cid:96)1\nby y(F ) := YF,F , we see that y encodes every nonzero entry of Y . Said differently, we\nYF,F (cid:48) = 0 if F  F (cid:48)  (cid:0) V\n(cid:1) \\ Ind(cid:96) ,\nhave a one to one correspondence between RInd(cid:96)1  Ind(cid:96) and the set\nY(cid:96)1 = (cid:8)Y  RInd(cid:96)1  Ind(cid:96)1 :\n(cid:9)\nYF,F (cid:48) = 0 if |F  F (cid:48) |  (cid:96) + 2,\n(cid:96)+1\n\u0001F,F (cid:48) YF,F (cid:48) = \u0001H,H (cid:48) YH,H (cid:48) if F  F (cid:48) = H  H (cid:48)\n(cid:88)\nWe record for later use that, if y  RInd(cid:96)1  Ind(cid:96) corresponds to Y  Y as above, then\n(cid:104)I , Y (cid:105) =\n(17)\ny(F )\nF Ind(cid:96)1\n(cid:88)\n(cid:88)\n\n(cid:104)L\n(cid:96)1 (Ind), Y (cid:105) = (cid:96)\nHInd(cid:96)\nF Ind(cid:96)1\nNow, we introduce, for (cid:96)  2, a map (cid:96)1 : Y(cid:96)1  Y(cid:96)2 . It will be more convenient\nto dene (cid:96)1 on the corresponding functions y  RInd(cid:96)1  Ind(cid:96) , in the following way: let\n(cid:96)1 : RInd(cid:96)1  Ind(cid:96)  RInd(cid:96)2  Ind(cid:96)1\n(cid:55) (cid:96)1 (y) = z\ny\nwhere (cid:40)\n(cid:80)\n(cid:80)\nif K  Ind(cid:96)2\nz (K ) = 1\nF Ind(cid:96)1 : KF y(F )\n(cid:96)\nif F  Ind(cid:96)1\n(cid:96)((cid:96)1) y(F ) + 1\nz (F ) = 1\nHInd(cid:96) : F H y(H )\n(cid:96)1\nWe are now in the position to dene our strengthening of (cid:96) (X ): Let\n(cid:96) (X ) = sup (cid:8)(cid:104)L\n(19)\n\n(cid:96)1 (Ind), Y (cid:105) : Y  RInd(cid:96)1  Ind(cid:96)1 , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1,\nYF,F (cid:48) = 0 if F  F (cid:48)  (cid:0) V\n(cid:1) \\ Ind(cid:96) ,\ni  i+1      (cid:96)1 (Y ) (cid:23) 0 for all i = 1, . . . , (cid:96)  1,\n\u0001F,F (cid:48) YF,F (cid:48) = \u0001F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F  (cid:9).\nYF,F (cid:48) = 0 if |F  F (cid:48) |  (cid:96) + 2,\n(cid:96)+1\nTheorem 8.2. The numbers (cid:96) (X ), k  (cid:96)  (X ), satisfy:\n(1) (cid:96) (X )  (cid:96) (X )\n(2) (X ) = (X ) (X )  (X )1 (X )      k (X ).\nProof. That (cid:96) (X )  (cid:96) (X ) is clear since we have only added constraints on Y in the\ndenition of (cid:96) (X ).\nLet S be an independent set, with |S |  (cid:96). Let, like in the proof of Proposition 8.1,\n(cid:40)\n(cid:96)1  RInd(cid:96)1  Ind(cid:96)1 be dened by:\nY S\n0 if F  F (cid:48) (cid:42) S\n\n(cid:96)1 (Ind))F,F (cid:48) otherwise.\n(L\n\n(Y S\n(cid:96)1 )F,F (cid:48) =\n\n(20)\n\n\f18\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n(cid:96)1  RInd(cid:96)1  Ind(cid:96) corresponding to Y S\n(cid:96)1 is given by: yS\nThe element yS\n(cid:96)1 (F ) = (cid:96) if\n(cid:96)1 (H ) = 1 if H  S , and otherwise yS\nF  S , yS\n(cid:96)1 takes the value 0. We will need the\nfollowing lemma:\n\nLemma 8.3. We have\n\n(cid:96)1 (yS\n(cid:96)1 ) =\n\n|S |  (cid:96) + 1\n(cid:96)  1\n\nyS\n(cid:96)2\n\nz (K ) =\n\nfor yS\n(cid:96)1 as dened in (20).\n(cid:96)1 ). Let K  Ind(cid:96)2 . Every subset of S is independent so the\nProof. Let z := (cid:96)1 (yS\nnumber of F  Ind(cid:96)1 such that K  F  S is |S |  (cid:96) + 1. So,\n(cid:88)\n(cid:96)1 (F ) = |S |  (cid:96) + 1.\n1\nyS\n(cid:96)\nF Ind(cid:96)1 : KF\nNow let F  Ind(cid:96)1 . It is clear that, if F is not contained in S , z (F ) = 0. If F  S ,\n(cid:88)\n1\n1\n(cid:96)  1\n(cid:96)((cid:96)  1)\n1\nHInd(cid:96) : F HS\n|S |  (cid:96) + 1\n(|S |  (cid:96)) =\n(cid:96)  1\n\n1\n(cid:96)  1\n\n1\n(cid:96)  1\n\nz (F ) =\n\n(cid:96) +\n\n+\n\n=\n\n.\n\n(cid:3)\n\n(cid:96)1 ) is positive semidenite, and so, iteratively, that i \nLemma 8.3 shows that (cid:96) (Y S\ni+1      (cid:96)1 (Y S\n(cid:96)1 ) is positive semidenite for every i  (cid:96)  1. We conclude that Y S\n(cid:96)1\n(after a suitable rescaling) is feasible for (cid:96) (X ), and consequently that (X )  (cid:96) (X ).\nWe have already remarked that (X ) = (X ) so also (X ) = (X ).\nIt remains to prove that the sequence of (cid:96) is decreasing. For this, we start from an\noptimal solution Y of (cid:96) , and we show that Z := (cid:96)1 (Y ) is feasible for (cid:96)1 and that\n\n\n(cid:104)L\n(cid:96)1 (Ind), Y (cid:105) = (cid:104)L\n(cid:96)2 (Ind), Z (cid:105).\nIt is clear that Z  Y(cid:96)2 and that Z is positive semidenite, as well as i  i+1     \n(cid:96)2 (Z ) (cid:23) 0 for all i  (cid:96)  2. That (cid:104)I , Z (cid:105) = 1 follows easily from (17) and from the\ndenition of (cid:96)1 . It remains to take care of the objective value. Applying (18),\n(cid:88)\n(cid:88)\nF Ind(cid:96)1\nKInd(cid:96)2\ny(F ) + ((cid:96)  1)(cid:96)\n\n1\n(cid:96)((cid:96)  1)\nwhere in the sums we restrict to elements in Ind. Taking account of the fact that every\n(cid:88)\n(cid:88)\nsubset of an independent set is also an independent set, we obtain\n\n(cid:104)L\n(cid:96)2 (Ind), Z (cid:105) = (cid:96)\ny(H ) = (cid:104)L\nF Ind(cid:96)1\nHInd(cid:96)\n\n\n(cid:96)2 (Ind), Z (cid:105) = ((cid:96)  1)\n(cid:88)\n(cid:88)\nF : KF\nK\n\nz (K ) + (cid:96)((cid:96)  1)\n(cid:16)\n(cid:88)\nF\n\n(cid:88)\nH : F H\n\n\n(cid:96)1 (Ind), Y (cid:105).\n\n= ((cid:96)  1)\n\ny(F ) + (cid:96)((cid:96) + 1)\n\n1\n(cid:96)  1\n\n(cid:17)\n\ny(F ) +\n\ny(H )\n\n(cid:104)L\n\n1\n(cid:96)\n\nz (F )\n\n(cid:3)\n\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n\n19\n\n9. TH ETA NUMB ER S O F RANDOM COM P LEXE S\nA random model X k (n, p) for simplicial complexes of arbitrary xed dimension k was\n(k  1)-skeleton, and each element of (cid:0) [n]\n(cid:1) is added as a k-dimensional face of X k (n, p)\nintroduced by Linial and Meshulam [31] as a higher dimensional analog of the Erd os-\nR enyi model G(n, p) for random graphs. It has vertex set [n] = {1, . . . , n}, complete\nindependently with probability p. Here p = p(n) is a function of n, and we let q := 1  p.\nk+1\nIn this section we analyze the theta number of X k (n, p) for dense complexes, i.e., for p\nin the range [c0 log(n)/n, 1  c0 log(n)/n].\nwho proved that, in the case of constant probability p, (G(n, p)) = ((cid:112)nq/p) holds with\nThe study of the theta number of random graphs G(n, p) was initiated by Juh asz in [24]\nprobability tending to 1. In subsequent works, the range of probabilities for which Juh asz\nresult holds was extended, until in [6], Coja-Oghlan was able to cover c0 /n  p  1c0/n\nfor some sufciently large constant c0 .\nWe will restrict ourselves to the range c0 log(n)/n  p  1  c0 log(n)/n because we\nwill need the following estimates:\nTheorem 9.1 ([16, 22]). Let A denote the adjacency matrix of G(n, p). For every c > 0\nmax (pJ  A)  c(cid:48)(cid:112)pq(n  1)\nthere exists c0 > 0, c(cid:48) > 0, c(cid:48)(cid:48) > 0 such that, if c0 log(n)/n  p  1  c0 log(n)/n,\n(21)\n|min (A)|  c(cid:48)(cid:48)(cid:112)pq(n  1).\nand\n(22)\nwith probability at least equal to 1  nc .\nWith the above, it is rather straightforward to obtain:\nTheorem 9.2. For every c > 0 there exists c0 > 0, c1 > 0, c2 > 0 such that, if\n(cid:112)(n  1)q/p  (G(n, p))  c2\n(cid:112)(n  1)q/p.\nc0 log(n)/n  p  1  c0 log(n)/n,\n(23)\nc1\nwith probability at least equal to 1  nc .\nIndeed, following the method of Juh asz, the upper bound is obtained via the dual for-\nmulation for the theta number (5) and the matrix Z = J  A/p, where A is the adjacency\nmatrix of G(n, p), while the lower bound follows from the choice Y = Y (cid:48)/(cid:104)I , Y (cid:48) (cid:105) in the\nprimal formulation (4), where Y = A  min (A)I , A being the adjacency matrix of the\ncomplementary graph of G(n, p).\n9.1. The theta number of X k (n, p). We will establish the following similar result for\nrandom simplicial complexes X k (n, p):\nTheorem 9.3. For every k  1 and c > 0, there exists c0 > 0, c1 > 0, c2 > 0 such that, if\n(cid:112)(n  k)q/p.\n(cid:112)(n  k)q/p  k (X k (n, p))  c2\nc0 log(n)/n  p  1  c0 log(n)/n,\nc1\nwith probability at least equal to 1  nc .\nFor comparison, the independence number of X k (n, p) is of the order (log(nk p)/p)1/k\nIn the range c0 log(n)/n  p  1  c0 log(n)/n, the eigenvalues of the\n(see [28]).\nadjacency matrix of X k (n, p) have been studied in [21]. We will closely follow the\nmethods developed in [21], in particular the role played by the so-called links of X , an\n\n\f20\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\n(YK )F,F (cid:48) =\n\nK JK K .\n\nK JK K ,\n\nidea going back to the work of Garland [17]. By denition, for a k-dimensional simpli-\ncial complex X and a (k  2)-face K of X , the link lkX (K ) is the graph with vertices\n{v  V : K  {v}  Xk1}, and edges {{v , w} : K  {v , w}  Xk }. In view of the\nproof of Theorem 9.3, we will rst establish a relationship between the theta number of a\nsimplicial complex and that of its links.\nProposition 9.4. Let X be a k-dimensional simplicial complex with complete (k  1)-\nskeleton. Then\n(24)\n\n(lkX (K )).\n\nif K  F  F (cid:48)\notherwise.\n\nk (X )  k max\nKXk2\nProof. Let K  Xk2 . For a matrix Y  R(V\nk )(V\nk ) , we introduce its localization at K\n(cid:26) YF,F (cid:48)\ndenoted YK and dened by:\n0\nLet K  R(V\nk )(V\nk ) denote the diagonal matrix with [F : K ] as diagonal entries. Then we\n(cid:88)\nobserve that\n\n(25)\nL\nk1 =\nKXk2\n(cid:88)\nand that, if YF,F (cid:48) = 0 for all (F , F (cid:48) ) such that |F  F (cid:48) |  k + 2,\nYK  (k  1) diag(Y ).\n(26)\nY =\nKXk2\n(cid:88)\nk1 , Y (cid:105) = (cid:104)(cid:88)\nNow let Y be an optimal solution of (8). Taking account of (25) and (26),\n\n\nk (X ) = (cid:104)L\nYK (cid:105)  (k  1)(cid:104)L\nk1 , diag(Y )(cid:105)\n(cid:88)\nK\nK\n(cid:104)K JK K , YK (cid:48) (cid:105)  k(k  1).\n=\nK,K (cid:48)\n(cid:26) YF,F\n0\n\nif K  K (cid:48) = F\n(cid:104)K JK K , YK (cid:48) (cid:105) =\notherwise\n(cid:88)\n(cid:88)\n(cid:104)K JK K , YK (cid:105) =\nK\nK\nNow, the crucial observation is that the matrix K YK K gives rise to a feasible matrix\nof the semidenite program (4) dening the theta number of lkX (K ). Indeed, let ZK be\nthe matrix indexed by V \\ K and dened by (ZK )v ,w = (K YK K )K{v},K{w} . This\nmatrix inherits some properties of Y : The matrix ZK is positive semidenite, the entries\nof ZK associated to edges of lkX (K ) are equal to 0. With obvious notations, we have\nk (X )  (cid:88)\n(cid:104)JK , K YK K (cid:105) = (cid:104)J, ZK (cid:105) and (cid:104)I , ZK (cid:105) = (cid:104)I , YK (cid:105) so we obtain\n(cid:104)I , YK (cid:105)(lkX (K )).\nWe have (cid:80)\nK\nK (cid:104)I , YK (cid:105) = k(cid:104)I , Y (cid:105) = k so the announced inequality follows immediately.\n(cid:3)\n\n(cid:104)JK , K YK K (cid:105).\n\nIf K (cid:54)= K (cid:48) , we have\n\nso, since trace(Y ) = 1,\n\nk (X ) =\n\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n\n21\n\nProof of Theorem 9.3. For the upper bound, we apply Proposition 9.4. The link lkX (K )\nof a (k  2)-face K in a random complex X = X k (n, p) is an Erd os-Renyi random graph\non V \\ K with the same probability p. We can thus apply Theorem (9.2) and a union bound\nto obtain the result. We note that, since the number of such faces is of the order of nk1 ,\nfor the probability of the bad event to be, say, less than nc we need to apply Theorem\n(9.2) for the larger value c + k  1 instead of c, explaining the need for an arbitrary large\npower of n in the convergence speed of probabilities.\nIn order to nd a lower bound of k (X ), we consider the matrix Y = A  min (A)I\nk1 , Y (cid:105) = k(k + 1)|X k |  k(cid:0)n\n(cid:1)min (A). Moreover, (cid:104)L\n(cid:1)min (A), so\nhave (cid:104)I , Y (cid:105) = (cid:0)n\nwhere A denotes the adjacency matrix of the complementary k-complex X . The feasibility\nconditions of (8) are fullled by Y except for the normalization condition (cid:104)I , Y (cid:105) = 1. We\n(cid:18)\n(cid:19)\n\n(cid:0)n\n(cid:1)min (A)\nk\nk\n(k + 1)|X k |\nk (X )  k\n1 +\n.\n(cid:1)] with probability q . Hence, by a straightforward application of a Chernoff bound,\nin [(cid:0) n\nk\nThe number |X k | of k-faces of X = X k (n, q) is a random variable binomially distributed\nfor every c > 0, |X k | is at least of the order (cid:0) n\n(cid:1)q with probability at least 1  nc . It\nk+1\nk+1\nremains to upper bound |min (A)|. For this, we apply the localization procedure that we\n(cid:88)\nhave already encountered in the proof of Proposition 9.4:\nA =\nAK .\nKXk2\nk ) , if xK denotes the vector obtained from x by setting to 0\nThen, for every x = (xF )F (V\n(cid:88)\n(cid:88)\nthe coordinates of x associated to faces F not containing K ,\n(cid:104)Ax, x(cid:105) =\n(cid:104)AK x, x(cid:105) =\n(cid:104)AK xK , xK (cid:105).\nK\nK\nThe matrix AK has the same spectrum as K AK K . The latter is identical to the adjacency\nmatrix AlkX (K ) of the graph lkX (K ) on the entries indexed by {F = K {v}, v  V \\K },\n(cid:104)Ax, x(cid:105)  (cid:88)\nand zero elsewhere. So, its non-zero spectrum is that of AlkX (K ) and hence:\nmin (AlkX (K ) )(cid:104)xK , xK (cid:105).\nK\nThe links lkX (K ) are random graphs G(n  k + 1, q) so, applying (22) and a union bound,\n(cid:88)\n(cid:104)xK , xK (cid:105) = c(cid:48)(cid:48)k(cid:112)pq(n  k)(cid:104)x, x(cid:105).\n(cid:104)Ax, x(cid:105)  c(cid:48)(cid:48)(cid:112)pq(n  k)\nwe nd that, with probability at least equal to 1  nc , for a large enough constant c(cid:48)(cid:48) ,\nWe have obtained the desired upper bound |min (A)|  c(cid:48)(cid:48)(cid:48)(cid:112)pq(n  k). Putting every-\nK\n(cid:3)\nthing together, we obtain the announced lower bound for k (X ).\n9.2. The hierarchy of theta numbers of G(n, p). In this last subsection, we restrict our-\nselves to the case of random graphs G(n, p) and analyze the hierarchy of theta numbers\n(cid:96) (G(n, p)) for constant values of (cid:96). The restriction to random graphs, i.e., random com-\nplexes of dimension 1, is purely for simplicity. The assumption of constant (cid:96), however,\nis essential. Analyzing the complete hierarchy (cid:96) (X ) of a random complex X for non-\nconstant (cid:96) appears to be a difcult task. It would be interesting to know for which values\nof (cid:96) the theta number (cid:96) (G(n, p)) is close to the independence number. Unfortunately,\nsuch questions seem to be out of the reach of the methods we apply here.\n\n\fnq (cid:96)/p.\n\n1 +\n\n.\n\n22\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\nTheorem 9.5. For every (cid:96)  1 and c > 0, there exists c0 > 0, c1 > 0, c2 > 0 such that, if\n(cid:113)\n(cid:113)\nq (cid:96)  c0 log(n)/n and pq (cid:96)1  c0 log(n)/n,\nnq (cid:96)/p  (cid:96) (G(n, p))  c2\nc1\nwith probability at least equal to 1  nc .\nProof. We will sometimes use the expression with high probability for an inequality that\nholds with probability at least 1  nc for all c > 0, with appropriate constants depending\non c.\nFor an upper bound of (cid:96) (G(n, p)), we apply\n(cid:96) (G)  (cid:96) max\n(lkG (K )).\nK( V\n(cid:1) \\ Ind(cid:96) . If K is independent, this condition\nwith edges {v , w} if K  {v , w}  (cid:0) V\n(cid:96)1)\nHere, lkG (K ) is the graph on VK := {v  V : {v , k} / E (G(n, p)) for all k  K }\nsimply means that {v , w} is an edge of G, so lkG (K ) is the graph G[VK ] induced by G\n(cid:96)+1\non VK . If G = G(n, p), the number of vertices nK = |VK | is itself a random variable.\nSince |K | = (cid:96)  1, nK follows a binomial distribution with parameters (n  (cid:96) + 1) and\nq (cid:96)1 . For nK to be concentrated around its expected value q (cid:96)1 (n  (cid:96) + 1) we need\nq (cid:96)1  c0 log(n)/n for some c0 > 0.\nAssuming nK  cq (cid:96)1n for some c > 0, we have\n(G[VK ])  (G(cq (cid:96)1n, p))\nbecause G[VK ] can be viewed as an induced subgraph of G(cq (cid:96)1n, p). We would like to\napply Theorem 9.2. It requires p and q to be greater that c(cid:48)\n0 log(q (cid:96)1n)/(q (cid:96)1n) and holds\nwith probability at least 1  (q (cid:96)1n)c . All this will be ne if we assume:\npq (cid:96)1  c1 log(n)/n and q (cid:96)  c1 log(n)/n\n(cid:113)\nfor a sufciently large c1 . With a union bound we obtain with high probability:\n(cid:96) (G)  c\nnq (cid:96)/p.\nFor the lower bound, we consider the matrix Y = A  min (A)I where A is the\n(cid:17)\n(cid:16)\nadjacency matrix of the (cid:96)-skeleton of Ind and we apply (15). We obtain\n\n(cid:96) (X )  (cid:104)L\n(cid:96)1 (Ind), Y (cid:105)\n((cid:96) + 1)| Ind(cid:96) |\nIn order to estimate |min (A)| we use A = (cid:80)\nmin (A)| Ind(cid:96)1 |\n(cid:104)I , Y (cid:105)\n= (cid:96)\nAK and remark that AK has the\nKInd(cid:96)2\nsame non-zero eigenvalues as the adjacency matrix of the graph lkInd (K ), itself being the\n(cid:88)\n(cid:88)\ngraph G[VK ] induced by G on VK . We have\n(cid:104)Ax, x(cid:105) =\n(cid:104)AK x, x(cid:105) =\n (cid:88)\nKInd(cid:96)2\nKInd(cid:96)2\nmin (AK )(cid:104)xK , xK (cid:105)\n(cid:88)\nKInd(cid:96)2\n min\nmin (AK )\nKInd(cid:96)2\nKInd(cid:96)2\nmin (AK )(cid:96)(cid:104)x, x(cid:105),\n min\nKInd(cid:96)2\n\n(cid:104)AK xK , xK (cid:105)\n\n(cid:104)xK , xK (cid:105)\n\n\fTHE THETA NUMBER OF SIMPLICIAL COMPLEXES\n\n23\n\nso\n\nmin (A) = |min (A)|  (cid:96)  max\n|min (G[VK ])|.\nK\n|min (G[VK ])|  |min (G(cq (cid:96)1n, q))|  c(cid:48)(cid:112)\nLike for the upper bound we have with high probability nK  cq (cid:96)1n for some c > 0\nand thus\npq (cid:96)n\nfor some c(cid:48) > 0, under the same conditions on p and q .\nIt remains to deal with the ratio | Ind(cid:96) |/| Ind(cid:96)1 |. For this we will argue that Ind is\nalmost regular. To be more precise we apply double counting to the set\nD = {(A, B )  Ind(cid:96)1  Ind(cid:96) : A  B }.\nThe number of (cid:96)-subsets of B is (cid:96) + 1 so |D | = ((cid:96) + 1)| Ind(cid:96) |. For a given A, the number\nXA of B containing A follows a binomial distribution with parameters n  (cid:96) and q (cid:96) , with\nexpected value q (cid:96) (n  (cid:96)). With high probability (requires q (cid:96)  c log(n)/n) XA is larger\nthat c(cid:48) q (cid:96) (n  (cid:96)) and so\n| Ind(cid:96)1 |  c(cid:48) q (cid:96) (n  (cid:96))\n| Ind(cid:96) |\n(cid:113)\n(cid:96) + 1\nPutting everything together and applying another union bound we obtain\n(cid:96) (G)  c\n\nnq (cid:96)/p.\n\n.\n\n(cid:3)\n\nR E FER ENC E S\n\n[1] M. Anjos and J.B. Lasserre, Handbook on Semidenite, Conic and Polynomial Optimization, Springer, 2012\n[2] A. Ben-Tal and A. Nemirovski, Lectures on Modern Convex Optimization: Analysis, Algorithms, and Engi-\nneering Applications, SIAM, Philadelphia, 2001.\n[3] S. Boyd and L. Vandenberghe Convex Optimization, Cambridge University Press, 2004.\n[4] A.E. Brouwer and W.H. Haemers, Spectra of Graphs, Springer, New York (2012).\n[5] H. Cohn, A. Kumar, S.D. Miller, D. Radchenko, and M.S. Viazovska, The sphere packing problem in\ndimension 24, preprint, arXiv:1603.06518 [math.NT], 2016, 12pp.\n[6] A. Coja-Oghlan, The Lov asz number of random graphs, Combinatorics, Probability and Computing 14\n(2005), 439-465.\n[7] P.E.B. DeCorte, D. de Laat, and F. Vallentin, Fourier analysis on nite groups and the Lov asz theta-number\nof Cayley graphs , Experimental Mathematics 23 (2014), 146-152.\n[8] P. Delsarte, An algebraic approach to the association schemes of coding theory, Philips Research Repts\nSuppl. 10, (1973), 1-97.\n[9] D. Dotterrer, T. Kaufman, and U. Wagner, On Expansion and Topological Overlap, preprint,\narXiv.math:1506.04558.\n[10] D. Dotterrer, L. Guth, and M. Kahle. 2-complexes with large homological systoles, preprint,\narXiv.math:1509.03871.\n[11] A.M. Duval, C.J. Klivans, and J.L. Martin, Simplicial and Cellular Trees, in Recent Trends in Combina-\ntorics, The IMA Volumes in Mathematics and its Applications, Springer (2016), 713752.\n[12] D. Ellis and E. Friedgut, H. Pilpel, Intersecting families of permutations, Journal of the American Mathe-\nmatical Society 24 (2011), 649-682.\n[13] D. Ellis, Y. Filmus, and E. Friedgut, Triangle-intersecting families of graphs, Journal of the European\nMathematical Society 14 (2012), 841-885\n[14] S. Evra, K. Golubev and A. Lubotzky, Mixing properties and the chromatic number of Ramanujan complexes\nInternational Mathematics Research Notices 22 (2015), 11520-11548.\n[15] S. Evra, T. Kaufman, Bounded Degree Cosystolic Expanders of Every Dimension preprint,\narXiv.math:1510.00839.\n[16] U. Feige and E. Ofek, Spectral techniques applied to sparse random graphs, Random Structures Algorithms\n27 (2) (2005), 251275.\n[17] H. Garland, p-adic curvature and the cohomology of discrete subgroups of p-adic groups, Annals of Math-\nematics (2), 97(3) (1973), 375423.\n\n\f24\n\nCHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO\n\n[18] B. G artner and J. Matousek, Approximation Algorithms and Semidenite Programming, Springer, 2012\n[19] K. Golubev, On the chromatic number of a simplicial complex, to appear in Combinatorica.\n[20] M. Gromov, Singularities, expanders and topology of maps. Part 2: From combinatorics to topology via\nalgebraic isoperimetry, Geometric and Functional Analysis, 20 (2) (2010), 416526.\n[21] A. Gundert and U. Wagner, On eigenvalues of random complexes, Israel Journal of Mathematics,216 (2)\n(2016), 545582.\n[22] C. Hoffman, M. Kahle, and E. Paquette, Spectral gaps of random graphs and applications to random topol-\nogy, preprint, arXiv.math:1201.0425.\n[23] D. Horak and J. Jost, Spectra of combinatorial Laplace operators on simplicial complexes, Advances in\nMathematics, 244:303  336, 2013.\n[24] F. Juh asz, The aymptotic behaviour of Lov asz theta function for random graphs, Combinatorica 2 (2) (1982),\n153155.\n[25] M. Kahle, Random simplicial complexes, preprint, arXiv.math:1607.07069.\n[26] G. Kalai, Enumeration of Q-acyclic simplicial complexes, Israel Journal of Mathematics, 45 (1983), 337\n351.\n[27] T. Kaufman, D. Kazhdan and A. Lubotzky, Isoperimetric Inequalities for Ramanujan Complexes and Topo-\nlogical Expanders, Geometric and Functional Analysis, 26 (1) (2016), 250287.\n[28] M. Krivelevich, B. Sudakov, The chromatic numbers of random hypergraphs, Random Structures Algo-\nrithms, 12 (4) (1998), 381403.\n[29] J.B. Lasserre, An explicit equivalent positive semidenite program for nonlinear 0-1 programs, SIAM J.\nOptim. 12 (2002), 756769.\n[30] M. Laurent, A comparison of the Sherali-Adams, Lov asz-Schrijver and Lasserre relaxations for 0 1 pro-\ngramming, Math. Oper. Res. 28 (2003), 470496.\n[31] N. Linial and R. Meshulam, Homological connectivity of random 2-complexes, Combinatorica 26 (4)\n(2006), 475487.\n[32] L. Lov asz, On the Shannon capacity of a graph, IEEE Transactions on Information Theory 25 (1979), 17.\n[33] A. Lubotzky, Ramanujan complexes and high dimensional expanders, Japanese Journal of Mathematics 9\n(2) (2014), 137169.\n[34] A. Lubotzky and R. Meshulam, A Moore bound for simplicial complexes, Bulletin of the London Mathe-\nmatical Society, 39 (3) (2007), 353358.\n[35] J. Matousek, M. Tancer and U. Wagner, Hardness of embedding simplicial complexes in Rd , Journal of the\nEuropean Mathematical Society, 13 (2) (2011), 259-295.\n[36] F.M. de Oliveira Filho and F. Vallentin, Computing upper bounds for packing densities of congruent copies\nof a convex body, 30 pages, arXiv:1308.4893 [math.MG]\n[37] O. Parzanchevski and R. Rosenthal, Simplicial complexes: Spectrum, homology and random walks, Random\nStructures Algorithms, 50 (2) (2017), 225261.\n[38] O. Parzanchevski, R. Rosenthal, and R. J. Tessler, Isoperimetric inequalities in simplicial complexes, Com-\nbinatorica, 36 (2) (2016), 195227.\n[39] L. Vandenberghe and S. Boyd, Semidenite Programming, SIAM Review 38 (1996), pp. 4995.\n[40] M.S. Viazovska, The sphere packing problem in dimension 8, arXiv:1603.04246 [math.NT], 2016, 22pp.\n\nIN ST I TU T D E MATH EMAT IQU E S DE BORD EAUX , UMR 5251 , UN IVER S I T E D E BORD EAUX , 351 COUR S\nD E LA L IB ERAT ION , 33400 TA LENC E , FRANC E .\nE-mail address: christine.bachoc@u-bordeaux.fr\n\nMATH EMAT I SCHE S IN ST I TU T, UN IVER S I T A T ZU K O LN , W EY ERTA L 86 -90 , 50931 K O LN , G ERMANY.\nE-mail address: anna.gundert@uni-koeln.de\n\nIN ST I TU T D E MATH EMAT IQU E S DE BORD EAUX , UMR 5251 , UN IVER S I T E D E BORDEAUX , 351 COUR S\nD E LA L IB ERAT ION , 33400 TA LENC E , FRANC E .\nE-mail address: alberto.passuello@u-bordeaux.fr\n\n\f", 
        "tag": "Discrete Mathematics", 
        "link": "https://arxiv.org/list/cs.DM/new"
    }, 
    {
        "text": "Multi-Personality Partitioning for \n\n Heterogeneous Systems \n\nAnthony Gregerson, Aman Chadha, Katherine Morrow \nDepartment of Electrical & Computer Engineering \nThe University of Wisconsin  Madison \nMadison, WI  USA \nagregerson@wisc.edu, achadha2@wisc.edu,  kati@engr.wisc.edu \n \n\n \nAbstractDesign  flows  use  graph  partitioning  both  as  a \nprecursor  to  place  and  route  for  single  devices,  and  to  divide \nnetlists or  task  graphs among multiple devices.  Partitioners have \naccommodated  FPGA  heterogeneity \nvia  multi-resource \nconstraints,  but  have  not  yet  exploited  the  corresponding  ability \nto  implement  some computations in multiple ways (e.g., LUTs vs. \nDSP  blocks), which  could  enable  a  superior  solution.  This  paper \nintroduces  multi-personality \ngraph \npartitioning,  which \nincorporates  aspects  of  resource  mapping  into  partitioning.  We \npresent a modified multi-level KLFM partitioning algorithm that \nalso  performs  heterogeneous  resource  mapping  for  nodes  with \nmultiple  potential  implementations  (multiple  personalities).  We \nevaluate  several  variants  of  our  multi-personality  FPGA  circuit \npartitioner  using  21  circuits  and  benchmark  graphs,  and  show \nthat dynamic  resource mapping  improves  cut   size  on  average  by \n27% over static mapping for these circuits. We further show that \nit  improves  deviation  from  target  resource  utilizations  by  50% \nover post-partitioning resource mapping. \n\nKeywords partitioning; technology mapping \n\nINTRODUCTION \n\nI. \nNetlist  partitioning  is  a  critical  part  of  the  CAD  flow  for \nlarge circuit designs, both to achieve high-quality results and to \nreduce  the  CAD  flow  execution  time.  Existing  research  on \nmulti-resource  partitioning  for  heterogeneous  FPGAs  has \nfocused  on  partitioning  nodes with pre-assigned  resource  costs \n[1]  [2]  [3].  However, heterogeneity  presents  the  opportunity  to \nimplement  some  computations  in  multiple  different  ways, \nusing  different  combinations  of  resources.  For  example, \nmultiply-accumulate  operations  can  be  implemented  in  DSP \nblocks  or  CLBs.  We  refer  to  logic  with  multiple  possible \nimplementations  as multi-personality  logic,  and  the process of \nchoosing  an  implementation  for  that  logic  as  personality \nselection.  CAD  tools  perform  personality  selection  during \nsynthesis;  however,  allowing  a  post-synthesis  partitioner  to \nmodify  personality  selections  can  improve  partitioning  both  in \nterms of cut size and heterogeneous resource utilization. \n In  this  paper  we  define  the  multi-personality  partitioning \nproblem  and  propose  specific  modifications  to  multi-level \nKLFM  (Kernighan-Lin/Fiduccia-Mattheyses)  to  incorporate \npersonality  selection.  We  evaluate  our  modifications  using  a \ncollection  of  graphs  based  on  real-world  multi-FPGA  designs \nand  publicly-available  benchmarks.  We  demonstrate  that  they \nresult  in  better  cut  sizes  and  deviations  from  target  resource \nutilizations than partitioning of fixed-implementation netlists. \n\nII.  RELATED WORK \nCircuit  netlists  can  be  represented  as  hypergraphs,  where \nlogic  elements  (or  sometimes  groups  of  elements)  are  graph \nnodes  and  the  nets  that  connect  the  nodes  are  hyperedges. \nNetlist/Hypergraph partitioning based on variants of the KLFM \nalgorithm  [4]  often  optimize  for  metrics  such  as  channel \nwidth/congestion,  ease  of  routability,  and  operating  frequency \n[5].  Some  work  has  also  examined  multi-resource  constraints \nfor  heterogeneous  devices  for  KLFM  [2]  and  other  algorithms \n[3],  but  unlike  our  work,  they  do  not  integrate  personality \nselection  for  multi-personality  nodes  into  partitioning.  In \ngeneral,  our  proposed  changes  do  not  conflict  with  other \nKLFM  partitioning  extensions.  Furthermore,  some  of  our \ntechniques,  such  as  phase-based  implementation  rebalancing \nand  multi-level  implementation  control,  could  also  be  adapted \nto non-KLFM-based partitioning algorithms.  \n\nIII.  PROBLEM DEFINITION \nWe  define  graph  partitioning  for  heterogeneous  devices  as \nfollows:  Partition  a  graph  with  nodes  that  have  weights  in  R \nresources  such  that  all  R  resources  are  balanced  within  their \nspecified maximum  imbalance margins.  Each  nodes weight  is \nnow  described  by  an  R-entry  weight  vector  w.  If  we  define \nwr(v) as node vs weight in resource r, and I\nr  as the maximum \nweight imbalance between partitions for resource r, then  \n\n\nv V\n\ni\n1\n\n(\nw v\ni\nr\n\n)\n\n\n\n\nv V\n\nj\n2\n\n(\nw v\nr\n\nj\n\n)\n\n\n\nI\n\nr\n\n  \n\n r 1..\n =\nR\n\n       (1) \n\nThe  resulting  cut  sizes  for  multi-resource  partitioning  are \nusually higher  than  those of  single-resource partitioning due  to \nthe additional R1 constraints. \nFor  devices  like  FPGAs  a  given  node  may  have  multiple \nimplementations  using  different  resource  weights.  We  thus \ndefine  a  new  multi-personality  graph  partitioning  problem.  If \nwe define  all possible  implementations  of node v  as Pv  and  the \nselected personality as p(v), we can reformulate the partitioning \nconstraints as: \n\ni\n\n) )\n\n\n\n(\n(\nw p v\nr\n\n(\n(\nw p v\nr\n\n\n\nv V\nv V\n\n\ni\nj\n1\n2\nWhereas  multi-resource  partitioning  has  O(2N)  potential \nsolutions,  where  N  is  the  number  of  nodes,  multi-personality \npartitioning has O(C(P)2N) potential solutions, where \n\n   \n\n  r\n =\n\n1 . .\n\nR\n\nI\n\nr\n\n) )\n\nj\n\n\n\n   (2) \n\n\f)\n\n \n\n \n\n(3) \n\n(\nC P\n\n=    \nP\nv\nv V\n\nThe  number  of  personality  combinations  scales  non-\npolynomially  with  the  number  of  multi-personality  nodes, \nresulting  in  a  much  larger  solution  space.  Although  this \nflexibility  may  make  it  possible  to  find  superior  solutions,  it \nalso  increases  the  problem  complexity.  Furthermore,  within \neach  partition,  we  may  also  want  to  achieve  a  certain  relative \nutilization of resources, for example to find  the smallest usable \nFPGA  in  a  given  family  to  implement  the  netlists  partitions. \nWe  refer  to  this  as  the  resource  utilization  ratio  (RUR),  and \nmake it an additional goal for multi-personality partitioning. \n\nIV.  MULTI-PERSONALITY PARTITIONING ALGORITHM \nThe base KLFM algorithm starts with an initial partitioning \nsolution,  and  moves  nodes  between  partitions  in  multiple \npasses.  Each  pass,  nodes  swap  between  partitions  one-by-one \nuntil  all  that  can  move  have  moved,  and  at  the  end  of  each \npass,  the  best  solution  achieved  during  that  pass  is  kept  as  the \nstarting  point  of  the  next  pass.  In  single-resource  KLFM, \nstructures  called  gain buckets  are  used  to  find  the  highest-gain \nnode  (i.e.,  one  that  most  reduces  cutsize  or  other  cost \nmeasures)  that can move without  violating balance constraints. \nThe  partitioning  quality  often  decreases  for  very  large  graphs, \nhowever, so multi-level KLFM [6] hierarchically clusters graph \nnodes,  and  partitions  at  each  level  (starting  with  the  coarsest) \nbefore un-clustering at that level and partitioning the next. \nThis  section  describes  our  modifications  to  the  multi-level \nKLFM  partitioning  algorithm  to  support  multi-personality \ngraph  partitioning.  The  required  changes  affect  key  data \nstructures  and  introduce  unique  complexities.  As  part  of  our \nmodifications,  we  added  dynamic  personality  selection, \nexperimented  with  modifications  to  the  gain  buckets,  and \nintroduced  new  pass-level  global  remapping.  We  also \nexamined these techniques effects on multi-level partitioning. \nA.  Dynamic Gain Buckets \nMulti-personality  gain  buckets  also  perform  personality \nselection, with the goal of meeting balance and RUR goals. We \nexperimented with two different approaches, described here. \n1)  Multi-Personality  Buckets:  Multi-Personality  Buckets \nare  similar  to  standard  gain buckets,  but  each  nodes  entry  has \na  separate  set of  weights  (indicating  its  resource  requirements) \nfor  each  of  its  personalities.  Each  step  within  a  pass, we  use  a \ntournament-style  selection  policy  to  determine  which  node  to \nmove,  selecting  the  highest-gain  node  with  an  implementation \nthat  would  allow  the  node  to  move  without  violating \nconstraints.  For  all  of  that  nodes  implementations  that  do  not \nviolate constraints, we compute an imbalance score, defined as \nthe  root-mean-square  (RMS)  of \nthe  resulting  fractional \nimbalances  in  all  resources.  The  RMS  tends  to  emphasize \nlarge  imbalances  in  one  resource  over  small  imbalances  in \nmany  resources;  large  imbalances  are  more  likely  to  prevent \nfuture  high-gain  moves.  To  incorporate  an  RUR  goal,  we \ncompute  a  similar  resource-ratio  score  for  each  possible \nimplementation  as  the  RMS  of  the  percent  deviation  of  each \nresource from its target utilization, based on total utilization of \nall  resources  in  a  partition  and  the  target  utilization  ratio.  The \nnode  with  the  best  imbalance  and/or  resource  ratio  score  is \nselected for movement (depending on the partitioner policy).  \n\n2)  Resource-Affinity  Buckets:  Sometimes  it  is  beneficial  to \nmove  nodes  with  implementations  in  specific  resources. \nResource-Affinity  Buckets  separate  a  partitions  buckets  into \nseveral  resource-specific  queues,  making  it  easier  to  adjust  a \nspecific  resource  to  improve  RUR  or  alleviate  partition \nimbalances. Resource-Affinity  buckets  can be used  in place of \nMulti-Personality buckets, or combined in a hybrid approach at \nthe cost of additional computational overhead. \nB.  Pass-Level Implementation Remapping \nGlobal \nremapping  during  partitioning  can  modify \npersonality  selections  to  reduce  resource  imbalance  and/or \nimprove  RURs.  Because  it  is  a  relatively  expensive  operation \nwe  apply  O(N)-complexity  global  remapping  algorithms  once \nper pass, and thus do not increase the asymptotic complexity of \nthe  base  KLFM  algorithm.  We  experimented  with  two  global \nremapping  methods:  a  multi-phase  greedy  algorithm  that \nperforms  a  random  walk  on  all  nodes  and  changes  their \nimplementations  to  minimize  imbalance  and  RUR  scores,  and \na fractured integer linear program (ILP) method. \nC.  Multi-Level Partitioning \nImplementation  selection  is  a major problem  in multi-level \nKLFM, as each additional component node creates a geometric \nincrease  in  the  number  of  implementations  available  to  a \nsupernode.  We  use  a  similar  approach  to  that  described  for \npopulating  Resource  Affinity  Buckets  to  determine  the  subset \nof  implementations  for  each  supernode;  these  implementations \nprovide  the  partitioner  with  the  greatest  ability  to  adjust \nresource  balance.  We  also  select  implementations  with  RURs \nclosest  to  the  target  ratio,  since  achieving  a  similar  weight  for \nall nodes can at times lead to better partitioning results [7].  \nDuring  experimentation,  we  observed  that  the  partitioner \nrarely  changed  supernode  implementations  at  the  coarsest \nlevels  of  multi-level  partitioning,  and  that  enforcing  multi-\nresource  constraints  at  these  levels  often  led  to  poor  results \nboth at these and later levels. To address this, we relax resource \nconstraints  based  on  the  coarseness  of  the  partitioning  level  to \nincrease  freedom  and  improve  results at  the  coarsest  levels. As \nthe  algorithm  progresses  to  less  coarse  graphs,  stricter  balance \nconstraints are re-enforced.  \nD.  Multi-Personality Partitioning Strategies \nWe  developed  several  different  approaches  for  partitioning \nmulti-personality,  heterogeneous-resource  graphs  using  the \ntechniques  that  we  have  described  earlier  in  this  paper.  When \nchoosing  these  partitioning  strategies,  our  goal  was  to  select \nthose that were representative of the options currently available \nfor  partitioning  and  implementation  mapping  algorithms.  We \nalso aim to highlight tradeoffs in multi-personality partitioning. \n1)  Statically-Mapped Partitioning (SM):  \nThis  strategy \nis  based  on \nthe  Native  Multi-Constraint \nRefinement \nfor \nproposed \n[2] \npartitioning  method \nheterogeneous  FPGAs,  modified  to  use  Resource-Affinity \nBuckets.  We  first  apply  our  global  implementation  remapping \nalgorithms  to  achieve  the  target  RUR.  Implementations  are \nthen  fixed,  and  the  graph  is  partitioned  using  multi-constraint \nKLFM without dynamic personality selection or remapping. \n2)  Statically-Partitioned Remapping (SP):  \nStatically-Partitioned  Remapping  converts  as  many  nodes  as \npossible  into  the  commonly-usable  resource  (CLBs  in  the  case \n\n\fof  FPGAs)  before  partitioning.  After  partitioning,  it  globally \nremaps  each  partition  based  on  the  target  RUR  and  balance \nconstraints.  SP  is  conceptually  similar  to  the  Multi-Constraint \nIterative  k-way  Balancing  method  [6],  however,  it  exploits \nnodes  multiple  personalities  and  our  global  remapping \nalgorithms to enforce constraints. \n3)  Dynamic Multi-Personality Partitioning (DMP): \nDMP  uses  move-based \nimplementation  selection  using \nResource-Affinity  Buckets  and  pass-based  global  remapping \nusing  the  multi-phase  greedy  algorithm.  Implementation \nselection  is  based  on  imbalance  score.  It  uses  fractured  ILP \nglobal remapping to improve the RUR after partitioning. \n\nTABLE I : SUMMARY OF BENCHMARK CIRCUITS/GRAPHS. \n% BRAM \n% DSP \n% CLB \nTotal \n\nBenchmark \n\n144 \n598a \nblob \nboundtop \nbrack2 \ncti \ndiffeq1 \nfe_ocean \nfe_rotor \nfe_tooth \nfft128 \nisolation \njet \nm14b \nmcm l \nmemplus \nraygen \nrct \nsha \nwave \nwing \n\nNodes \n144649 \n110971 \n11842 \n29582 \n62631 \n16840 \n4292 \n143437 \n99617 \n78136 \n91590 \n187766 \n189579 \n214765 \n346248 \n17758 \n11457 \n241349 \n3669 \n156317 \n62032 \n\nNodes \n94 \n98 \n100 \n100 \n99 \n97 \n100 \n97 \n99 \n98 \n100 \n100 \n100 \n98 \n100 \n98 \n100 \n100 \n100 \n98 \n99 \n\nNodes \n20 \n18 \n24 \n0.5 \n1 \n14 \n38 \n3 \n2.5 \n4 \n9.5 \n1 \n32 \n3 \n15 \n2 \n25 \n31 \n15 \n6 \n4 \n\nNodes \n1 \n0.5 \n0 \n14 \n8 \n2 \n0 \n3 \n1.5 \n4 \n0.5 \n0 \n0 \n2 \n0.1 \n40 \n0.1 \n12 \n0 \n5 \n2.5 \n\n4)  Advanced Dynamic Multi-Personality (ADMP):  \nADMP  expands  on  DMP,  using  Hybrid  Buckets  for  move-\nbased selection and multi-level constraint relaxation. \n5)  DMP/ADMP with Fine-Grained Ratio Control  \n          (DMP-FR / ADMP-FR):  \nThis  modification  considers  RUR  score  during  move-based \npersonality  selection  and  pass-based  global \nremapping, \nimproving resource utilization at the cost of cut size. \n\nV.  EXPERIMENTAL METHODOLOGY \nWe  implemented  a  complex-constraint,  multi-level  KLFM \npartitioner  and  modified  it  to  include  the  multi-personality-\naware  features  described  in  Section  IV.  Our  base  algorithm \nincludes  common  KLFM  optimizations  such  as  LIFO  gain \nbucket  queues  and  randomized  initial  partitions  [4].  We \nvalidated performance of the base algorithm using  results  from \nthe Walshaw partitioning archive [7]. \nWe  then  compared  the  results  of  our  multi-personality \nalgorithm \nagainst \nnon-personality-aware \npartitioning \napproaches  in  both  cut  size  and  RUR  using  a  suite  of  21 \nbenchmark  graphs,  summarized  in  Table  I.  These  include \nseveral  heterogeneous  netlists  for  FPGAs  from  ERCBench  [8] \nand  VPR  [9];  graphs  from  the  Walshaw  graph  partitioning \narchive from domains such as communications, vibroacoustics, \nand  3D  meshes  [7];  several  FPGA  circuits  designed  for multi-\nFPGA  high-energy  physics  experiments  such  as  particle \n\nisolation  (isolation),  jet  reconstruction  (jet),  and  calorimetry-\nbased  triggering  (rct)  [10];  a  ray  tracing  circuit  (raygen)  [11]; \nand  a  circuit  for  Monte  Carlo  simulation  for  photodynamic \ncancer  therapy  (mcml)  [12].  Where  HDL  was  available,  we \nassigned  node  weights  and  personalities  using  experimental \nsynthesis  in  Xilinx  ISE  and  fixed  implementations  of  nodes \nalong  critical  paths,  since  these may  not  have  the  flexibility  to \nuse  alternate  personalities  due  to  timing  constraints.  For \nbenchmarks  distributed  as  graphs,  we  computed  a  range  of \npotential  resource  costs  for  functional  units  by  synthesizing  a \nrange  of  DSP  and  BRAM-based  units  and  assigned  weights \nwithin that range using a binomial distribution.  \nFor each experiment, we report the best result from 50 runs, \nsince  KLFM  partitioners  often  use  the  best  result  of  multiple \nruns.  We  applied  an  imbalance  limit  of  1%  for  all  resources. \nWe  set  a  target  ratio  of  1:1:1  for  the  percent  utilization  of  the \nthree  primary  FPGA  resources  and  computed  the  root-mean-\nsquare of the percent deviation from the target resource weight. \nIf  a  circuit  did  not  use  a  resource,  it  was  ignored.  Resource \ncapacity ratios are based on the Xilinx Virtex-7 2000T [13]. \n\nVI.  RESULTS \nFig.  1  shows  a  comparison  of  cut  size  results  for  2-way \nmulti-personality  partitioning,  normalized  to  the  result  of \nStatically-Mapped  Partitioning  (SM).  Cut  size  is  reported  as \nthe  sum  of  the  weight  of  all  wires  that  span  more  than  one \npartition.  Due  to  space  limitations,  the  chart  includes  a \nrepresentative  set  of  individual  results  for  nine  benchmarks, \nand the geometric mean of results for all 21 benchmarks. Fig. 2 \nshows  a  comparison  of  target  resource  utilization  deviation. \nThe  results  are  also  summarized  in  Table  II.  For  each  metric \nshown in these figures and this table, lower values are better. \nA.  Cut Size and Target Resource Utilization \nSM achieves worse cut  sizes  than other strategies due to its \ndifficult  task  of  balancing  multiple  resources.  Results  of  the \ndynamic  algorithms  demonstrate \nthat \nchanging  node \npersonalities  during  partitioning  can \nimprove  cut  sizes, \nresulting  in mean improvements of 27% over SM and 6% over \nSP  when  using  ADMP.  The  addition  of  dynamic  ratio  control \nharmed  cut  size  results  by  a  small  amount,  but  improved \ndeviations  from  the  target  RUR.  SM,  however,  results  in  the \nbest deviation from target RUR because its netlists are mapped \nprior to partitioning and each resource is limited to a maximum \nimbalance  of  1%  between  partitions.  However,  this  comes  at \nthe  aforementioned  cut  size  penalty.  SP  had  the  worst \ndeviationnot using personality  information during portioning \nmakes  it difficult  to balance heterogeneous  resource utilization \nafterwardswhich in almost all cases came from over-utilizing \nCLBs  and  under-utilizing  specialized  resources.  Results \ndemonstrate  that  the  dynamic  algorithms  offer  good  cut  sizes \nand a compromise in deviation between SP and SM. \nFor both  cut  size  and RUR deviation,  the magnitude of  the \ndifference  between  the  partitioning  strategies  varied  heavily \nfrom benchmark to benchmark, largely as a consequence of the \ntopology  and  the  quantity  and  distribution  of  resources  in  the \ngraph. The circuit for particle isolation, for example, has a very \nregular grid-based structure with uniform  resource distribution, \nso  almost  any  partitioning  would  lead  to  balanced  resource \nutilization.  Jet  and  boundtop,  on  the  other  hand,  had  most  of \n\n\f1.0\n\n0.8\n\n0.6\n\n0.4\n\nblob\n\nboundtop brack2\n\nfe_rotor fe_tooth\n\nfft128\n\nisolation\n\njet\n\nmcml\n\ngeomean\n\nSM\nSP\nDMP\nDMP+\nDMP-FR\nDMP+FR\n\nFig . 1.   Cut size results, norma lized to statically-mapped partitioning. The geometric mean includes data from all 21 benchmarks. Lower values are better. These \nresults are also summarized in Table I I. \n\n \n\n100%\n\n107%\n\n140%\n\n90%\n\n102%\n\n80%\n60%\n40%\n20%\n0%\n\nSM\nSP\nDMP\nDMP+\nDMP-FR\nDMP+FR\n\nblob\nboundtop brack2\nfe_rotor fe_tooth\nfft128\nisolation\njet\nmcml\ngeomean\n \nFig. 2.   RMS deviation from target resource utilization for a subset of  benchmarks. The geometric mean contains data from all 21 benchmarks. Lower values are \nbetter. These results are also summarized in Table II. \n\n \n\ntheir  non-CLB  resources  distributed  in  clusters,  allowing  for \nmore \nsignificant \ntradeoffs  between  balanced \nresource \nutilization  and  cut  size.  The  multi-level  constraint  relaxation \ntechnique  included  in DMP+ proved particularly effective with \nthe  highly  clustered  and  interconnected  topologies  of  large \nmemories,  as  evidenced  by  its  improvement  in  cut  size  for \nboundtop, which includes a large shared memory. \n \n\nTABLE II:  GEOMETRIC MEANS OF THE EVALUATED COST METRICS AND \nNORMAL IZED WALL-CLOCK RUN TIME FOR EACH STRATEGY. \nSP  DMP  ADMP  DMP-FR  ADMP-FR \nSM \n\n \nNorm. \n\nCut Size \n\nRUR \n\nDeviation \n\nNorm. \n\nRun Time \n\n1.00 \n\n0.79 \n\n0.78 \n\n0.73 \n\n0.83 \n\n0.80 \n\n1.6% \n\n61% \n\n24% \n\n19% \n\n4.6% \n\n4.0% \n\n1.0 \n\n0.6 \n\n1.3 \n\n1.7 \n\n1.3 \n\n1.7 \n\nB.  Run-Time Cost \nAlthough  our  modifications  do  not  alter  the  asymptotic \ncomplexity  of  the  base  multi-level  KLFM  algorithm,  they \nincrease  algorithm  run-time  linearly.  To  roughly  estimate  the \nimpact  of  various  techniques  on  run  time,  we  ran  the  multi-\npersonality  algorithm  with  different  combinations  of  enabled \nfeatures  over  a  subset  of  the  largest  benchmarks  and  recorded \nthe wall clock time. Table II presents the results, normalized to \nthe run time of Statically-Mapped Partitioning. \n\nVII.  CONCLUSION  \nAlthough  existing  partitioning  algorithms  can  handle \nmultiple  resources  on  heterogeneous  devices  in  a  primitive \nfashion,  they  do  not  exploit  the  overlapping  functionality \nprovided  by  some  of  these  resources.    Integrating  dynamic \nselection of node  implementations  into  the partitioner  achieves \nup  to  a 27% mean  improvement  in partition  cut  size  compared \nto  partitioning  a  statically-mapped  circuit  for  our  suite  of  21 \nbenchmarks. Dynamically selecting node personalities for ratio \ncontrol  can  achieve  up  to  a  15X  mean  improvement  in \ndeviation  in  target  resource  utilization  compared  to  post-\npartition  resource  mapping.  Combining  the  enhanced  dynamic \nalgorithm  with  fine-grained  resource  ratio  control  makes  it \npossible  to  achieve  better  cut  size  benefits  than  statically \npartitioned  enforcement-based  strategies  while  also  achieving \n\nmost  of  the  resource  utilization  advantages  of  statically-\nmapped partitioning approaches. \n\nREFERENCES \n\n[1] T. Taghavi, et  al ., \"Innovate  or perish: FPGA physical design,\" in \nISPD, 2004, pp. 148-155. \n [2] Selvakkumaran,  N.;   Ranjan,  A.;  Raje,  S.;  Karypis,  G.;,  \"Multi-\nresource  aware   partitioning  algorithms  \nfor  FPGAs  with \nheterogenous resources,\" in DAC, 2004, pp. 741-746. \n [3] H.  Liu,  K.  Zhu,  and  D.F.  Wong,  \"Circuit  partitioning  with \ncomplex  resource constraints  in FPGAs,\"  in FPGA, 1998, pp. 77-\n84. \n [4] S.  Hauck  and  G.  Borriello,  \"An  evaluation  of  bipartitioning \ntechniques ,\"  IEEE  Trans.  on  CAD,  vol .  16,  no.  8,  pp.  849-866, \n1997. \n [5] M.  Inagi ,  et  al.,  \"A  performance-driven  circuit  bipartitioning \nalgorithm  for  multi-FPGA  implementation with  time-multiplexed \nI/Os ,\" in FPT, 2006, pp. 361-364. \n [6] G.  Karypis  and  V.  Kumar,  \"Multilevel   Algorithms  for  Multi-\nConstraint  Graph  Partitioning,\"  University  of  Minnesota, \nMinneapolis, MN, Tech Report 98-019, 1998. \n [7] A.J.  Soper,  C.  Walshaw,  and  M.  Cross ,  \"A  Combined \nEvolutionary  Search  and  Multilevel   Optimisation  Approach  to \nGraph  Partitioning,\"  J.  Global  Optimization,  vol.  29,  no.  2,  pp. \n225-241, 2004. \n [8] D.  Chang  et  al.,  \"ERCBench:  An  Open-Source  Benchmark  Suite \nfor   Embedded  and  Reconfigurable  Computing,\"  in  FPL, Milano, \n2010, pp. 408-413. \n [9] V.  Betz  and  J.  Rose,  \"VPR:  A  new  packing,  placement   and \nrouting  tool  for  FPGA  research,\"  in  Field-Programmable  Logic \nand Applications, 1997, pp. 213-222. \n [10] A.  Gregerson,  et   al.,  \"FPGA  design  analysis  of  the  clustering \nalgorithm for the CERN Large Hadron Collider,\" in FCCM, 2009, \npp. 19-26. \n [11] J. Fender  and J. Rose, \"A high-speed ray  tracing engine built on a \nfield-programmable system,\" in FPT, 2003, pp. 188-195. \n [12] J.  Luu,  et   al.,  \"FPGA-based  Monte  Carlo  computation  of   light \nabsorption  for  photodynamic  cancer  therapy,\"  in  FCCM,  2009, \npp. 157-164. \n [13] P.  Dorsey,  \"Xilinx  stacked  silicon  interconnect  technology \ndelivers  breakthrough  FPGA  capacity,  bandwidth,  and  power  \nefficiency,\" Xilinx White Paper: Virtex-7 FPGAs , 2010. \n\n\f", 
        "tag": "Distributed, Parallel, and Cluster Computing", 
        "link": "https://arxiv.org/list/cs.DC/new"
    }, 
    {
        "text": "0.5 Petabyte Simulation of a 45-Qubit Quantum Circuit\n\nThomas H aner , Damian S. Steiger\n Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland\n\n7\n1\n0\n2\n \nr\np\nA\n \n4\n \n \n]\nh\np\n-\nt\nn\na\nu\nq\n[\n \n \n1\nv\n7\n2\n1\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstractNear-term quantum computers will soon reach sizes\nthat are challenging to directly simulate, even when employing\nthe most powerful supercomputers. Yet, the ability to simulate\nthese early devices using classical computers is crucial for cal-\nibration, validation, and benchmarking. In order to make use\nof the full potential of systems featuring multi- and many-core\nprocessors, we use automatic code generation and optimization\nof compute kernels, which also enables performance portabil-\nity. We apply a scheduling algorithm to quantum supremacy\ncircuits in order to reduce the required communication and\nsimulate a 45-qubit circuit on the Cori II supercomputer using\n8, 192 nodes and 0.5 petabytes of memory. To our knowledge,\nthis constitutes the largest quantum circuit simulation to this\ndate. Our highly-tuned kernels in combination with the re-\nduced communication requirements allow an improvement in\ntime-to-solution over state-of-the-art simulators by more than\nan order of magnitude at every scale.\n\n1. Introduction\n\nQuantum computers promise to solve problems which\nwould be impossible to tackle with classical machines.\nWhile such devices will not speed up every application,\nthere are certain areas which could be revolutionized by\nquantum computers. This includes quantum chemistry [1],\n[2], [3], material science [4], machine learning [5], [6], [7],\n[8], and cryptography [9].\nExperimental devices featuring close to 50 quantum bits\n(qubits) will soon be available and may be able to perform\nwell-dened computational tasks which would classically\nrequire the worlds most powerful supercomputers. Going\neven beyond these capabilities means entering the domain\nof Quantum Supremacy [10], [11]. While one of the compu-\ntational tasks proposed to demonstrate this supremacy  the\nexecution of low-depth random quantum circuits, see Fig. 1\n is not scientically useful on its own, running such circuits\nis still of great use to calibrate, validate, and benchmark\nnear-term quantum devices [10].\nTherefore, in addition to verifying quantum algorithms\nand carrying out studies of their behavior under noise, quan-\ntum circuit simulators may provide the means to carry out\nthese calibrations and benchmarks and thereby enable more\nefcient quantum hardware/software co-design. Quantum\ncircuit simulators are thus comparable to tools such as the\nstructural simulation toolkit (SST) [12], which allows to\nsimulate upcoming classical hardware.\n\nRelated work. There are many implementations of quantum\ncircuit simulators [13] available, most of them are meant to\nsimulate small systems on a single node. The most widely-\nused single-node simulator is Microsofts LIQU i |(cid:105) [14],\nwhich is implemented in F# and is thus not as fast as\nsimulators written in, e.g., C++ [15]. The massively parallel\nsimulator from [16], [17] was used to simulate 42 qubits\non the J ulich supercomputer in 2010, which set the new\nworld record in number of simulated quantum bits. Recently,\nIntels qHiPSTER [18] was specialized for the simulation of\nquantum supremacy circuits and then used to simulate these\ncircuits up to 42 qubits [10]. A topic similar to quantum\ncircuit simulation is emulation which, instead of simulating\nindividual gates, uses classical shortcuts in order to reduce\nthe time-to-solution for quantum operations whose action\nis known in advance. An example for such a shortcut is\nthe quantum Fourier transform, which can be emulated by\napplying a fast Fourier transform to the state vector [15].\nHowever, such emulation techniques are not applicable to\nquantum supremacy circuits.\n\nOur contribution. We improve the strong scaling behavior\nof the compute kernels underlying quantum circuit simu-\nlation in order to reduce time-to-solution when employing\nmulti- and many-core processors. In the multi-node domain,\nwe employ a communication scheme similar to [17] and\nintroduce an additional layer of optimization to reduce the\namount of communication required: We apply a clustering\nalgorithm to the quantum circuit in order to improve the\nscheduling of quantum gate operations. While this pre-\ncomputation terminates in 1  3 seconds on a laptop, it\ngreatly reduces the number of communication steps. We\nthen simulate quantum supremacy circuits of various sizes\nand report speedups of over one order of magnitude on all\nscales. Finally, we simulate a 45-qubit quantum supremacy\ncircuit on the Cori II supercomputing system using 0.5\npetabytes of memory and 8, 192 nodes. To our knowledge,\nthis constitutes a new record in the maximal number of\nsimulated qubits. The classical simulation of such circuits\nis believed to be impossible already for 49 qubits which,\naccording to [19], is the threshold for quantum computers\noutperforming the largest supercomputers available today at\nthe task of sampling from the output distribution of random\nlow-depth quantum circuits. While we do not carry out\na classical simulation of 49 qubits, we provide numerical\nevidence that this may be possible. Our optimizations allow\nreducing the number of communication steps required to\nsimulate the entire circuit to just two all-to-alls, making\n\n\fFigure 1: Low-depth random quantum circuit proposed by Google to show quantum supremacy [10]. We generated identical\ncircuits using the following rules: At clock cycle 0, a Hadamard gate is applied to each qubit. Afterwards, eight different\npatterns of controlled Z (CZ) gates are applied repeatedly until the desired circuit depth is achieved. See the 8 different\nCZ patterns above in clock cycles 1-8 for a 6  6 qubit circuit, where the CZ gates are represented by a line between two\nqubits. This pattern ensures that all possible two qubit interactions on this 2D nearest neighbor architecture are executed\nevery 8 cycles. In addition to the CZ gates, single qubit gates are applied to all qubits which in the previous cycle (but not\nin the current cycle) performed a CZ gate. The single qubit gates are randomly chosen to be either a T (red), X 1/2 (blue),\nor Y 1/2 (yellow) gate, except that the second single-qubit gate on each qubit (the rst is the Hadamard gate in cycle 0) is\nalways a T gate and when randomly choosing a single-qubit gate, it must be different from the previous single-qubit gate\non that qubit.\n\nit possible to use, e.g., solid-state drives if the available\nmemory is less than the 8 petabytes required.\n\n2. Basics of quantum computer simulation\n\nA quantum computer consists of quantum bits (qubits).\nA qubit is a two-level quantum systems whose state |(cid:105) can\nbe described by 2 complex numbers 0 and 1 as\n|(cid:105) = 0 |0(cid:105) + 1 |1(cid:105) ,\nwhere |0 |2 + |1 |2 = 1. The two computational basis states\n|0(cid:105) and |1(cid:105) are orthonormal and the qubit, when measured,\ncollapses onto either |0(cid:105) or |1(cid:105) with probability p = |0 |2\nand |1 |2 = 1  p, respectively. The measured qubit is then\njust a classical bit. To store |(cid:105) on a classical computer,\nit is more practical to choose two orthonormal vectors to\n(cid:18)0\n(cid:18)0\n(cid:18)1\n(cid:19)\n(cid:19)\n(cid:19)\nrepresent |0(cid:105) and |1(cid:105),\n|(cid:105) = 0\n1\n0\n1\nAny operation on this qubit can then be represented as a\ncomplex unitary 2  2 matrix. Applying, for example, a\n\n+ 1\n\n=\n\n.\n\n1 0 ), to the state |(cid:105),\nbit-ip operation, denoted by X = ( 0 1\n(cid:19)\n(cid:18)1\n(cid:19)\n(cid:19) (cid:18)0\n(cid:18)0\namounts to a matrix-vector multiplication:\n= 1 |0(cid:105) + 0 |1(cid:105) .\nX |(cid:105) =\n1\n=\n1\n0\n1\n0\n(cid:0) 1 1\n(cid:1), the T gate T =\n(cid:0) 1+i 1i\n(cid:1), and Y 1/2 = 1\n(cid:0) 1+i 1i\n(cid:1).\n(cid:1), X 1/2 = 1\n(cid:0) 1\nOther single-qubit gates used in quantum supremacy circuits\nare the Hadamard gate H = 1\n1 1\n2\n0\n1i 1+i\n0 ei/4\n1+i 1+i\n2\n2\nThe state of a two-qubit system |(cid:105) can be represented\nusing 4 complex coefcients giving the contribution of all\n0\n .\npossible classical states featuring two bits, i.e.,\n1\n2\n3\nand operations acting on the entire state can be represented\nby complex unitary matrices of dimension 44. An example\nfor a two-qubit gate which occurs in quantum supremacy\n\n|(cid:105) = 00 |00(cid:105) + 01 |01(cid:105) + 10 |10(cid:105) + 11 |11(cid:105) =\n\n(1)(2)(3)(4)(5)(6)(7)(8)\fC Z =\n\n1 0\n ,\ncircuits is the controlled-Z or CZ gate,\n0\n0\n0\n0\n0 1\n0\n1\n0 0\n0 1\n0 0\nwhich adds a (1)-phase to the |11(cid:105) basis state. Also, note\nthat this gate is symmetric in terms of qubits  it does\nnot matter which qubit is the control / target qubit. This\nsymmetry of the CZ gate can also be seen in Fig. 1.\nTo build the 4  4 matrix acting on the entire system\nwhen applying a single-qubit operation to just one qubit, one\nperforms a Kronecker product with a 2  2 identity matrix.\nApplying an X -gate to the rst qubit (with bit-index 0) can\nbe achieved as follows:\nX0 |(cid:105) = 12  X |(cid:105)\n= 01 |00(cid:105) + 00 |01(cid:105) + 11 |10(cid:105) + 10 |11(cid:105)\n= 00 |01(cid:105) + 01 |00(cid:105) + 10 |11(cid:105) + 11 |10(cid:105) .\nMore generally, the state of an n-qubit quantum computer\ncan be represented by a complex vector of size 2n\n|(cid:105) = 0 |0    00(cid:105) + 1 |0    01(cid:105) +    + 2n1 |1    11(cid:105)\nand operations on this state are 2n  2n unitary matrices.\nFinally, applying a single-qubit gate U to the i-th qubit of\nan n-qubit quantum computer amounts to multiplying the\nstate vector of coefcients i by the matrix\n(cid:123)(cid:122)\n(cid:123)(cid:122)\n(cid:125)\n(cid:125)\n(cid:124)\n(cid:124)\n U  12      12\n12      12\nni1\ni\nwhich is just a complex sparse matrix-vector multiplication\nof dimension 2n . Thus, for double-precision values, just\nstoring the state vector for 50 qubits would already require\n16 petabytes of memory.\n\n,\n\n3. Optimizations\n\nOur simulator was implemented and optimized using a\nlayered approach. The rst layer aims to improve the single-\ncore performance of our quantum gate kernels by employing\nexplicit vectorization using compiler intrinsics, instruction\nreordering, and blocking to reduce register-spilling. The\nsecond layer uses OpenMP to enable a good strong scal-\ning behavior on an entire node. The third and nal layer\nimplements the inter-node communication scheme using\nMPI. This allows to simulate up to 45 qubits on current\nsupercomputers, in addition to reducing the time-to-solution\nwhen executing quantum circuits featuring fewer qubits.\n\n3.1. Standard optimizations\n\nIn order to be able to simulate large systems,\nis\nit\nimportant not to actually store the 2n  2n matrix acting\non the state vector. Instead, one can exploit\nits regular\nstructure and implement methods which, given the state\nvector, mimic a multiplication by this matrix. A standard\n\nimplementation features two state vectors (one input, one\noutput). To determine one entry of the output vector, two\ncomplex multiplications and one complex addition have to\nbe carried out on two entries of the input vector when\napplying a general single-qubit gate. In total, there are thus\n2  (4[mul] + 2[add]) + 2[add] = 14 FLOP\nper complex entry of the output state vector. One complex\ndouble-precision entry requires 16 bytes of memory and the\ninput vector has to be loaded from memory and the output\nvector has to be written back to memory. The operational\nintensity is therefore less than 1/2, which shows that this\napplication is memory-bandwidth bound on most systems.\n\n3.2. Single-core\n\nIn order to reduce the memory requirements by a factor\nof 2x, this complex sparse matrix-vector multiplication can\nbe performed in-place, at the cost of a cache-unfriendly\naccess pattern. Moreover, k-qubit gates require more op-\nerations for larger k , allowing to better utilize hardware\nwith strong compute capabilities. In fact, the number of\noperations grows exponentially with k , since applying a\nk-qubit gate amounts to performing one scalar product of\ndimension 2k per (output) entry.\nTo apply a k-qubit gate (of dimension 2k  2k ) to a\nstate vector of size 2n , where n denotes the number of\nqubits, the entries corresponding to all 2k indices of the\ngate matrix have to be loaded into a 2k -sized temporary\nvector, which then gets multiplied by the matrix before it\nis written back to the state vector. The indices these state\nvector entries, when represented in binary, are bit-strings\nof the form cnk1xik1 ...cj ...xi1 ...c0 , where i0 , i1 , ..., ik1\ndenote the k qubits indices to which the gate is being\napplied. Extracting and combining the bits xij from the\nindex of an entry, i.e.,\n\nx = xik1 ...xi1 xi0 ,\nyields the index of this entry with respect to the tempo-\nrary vector. All 2k entries which have an identical c =\ncnk1 cnk2 ...c0 index substring are part of this matrix-\nvector multiplication. Once all entries have been gathered,\nmultiplied by the matrix, and stored back into the state\nvector, the next c(cid:48) = c + 1 index substring can be dealt\nwith. In total, this amounts to performing 2nk complex\nmatrix-vector multiplications of dimension 2k .\nA rst observation is that the same matrix is used 2nk\ntimes. One can thus permute the matrix entries before-hand\nin order to always have sorted qubit indices, which results\nin memory accesses to occur in a more local fashion.\nWhen applying the matrix-vector product, doing so in\n2k1(cid:88)\nthe usual manner, i.e.,\ni=0\nwould require all entries of the temporary vector v to be\nin register (and already loaded from memory). In order to\n\nml,i vi ,\n\nvl =\n\n\f(b) Rooine plot for one KNL node of Cori II.\n(a) Rooine plot for one Edison socket.\nFigure 2: Rooine plots illustrating the performance improvements from Sec. 3.2 and 3.3. Step 1 introduces lazy evaluation,\nmaking the application more compute-bound. Step 2 adds explicit vectorization and instruction re-ordering, followed by\nstep 3 which applies blocking for registers in addition to a pre-computation on the gate matrix, re-ordering and permuting\nthe complex-valued matrix entries to improve the FLOP/instruction ratio. An additional optimization specic to KNL is the\nblocking for MCDRAM, which is introduced in step 1.\n\nvl +=\n\nml,i(b,j ) vi(b,j ) ,\n\naddress this issue, we employ blocking of the computation\nand determine the block size using an automatic code-\ngeneration / benchmarking feedback loop. For each block\nB  1, all indices l of the temporary\nindex b = 0, 1, ..., 2k\n(cid:88)\noutput vector v are updated according to\nj<B\nwhere i(b, j ) = b  B + j , before moving on to the next\nblock.\nWe employ explicit vectorization to parallelize updates\nfor consecutive values of l. Since we are dealing with\ncomplex double-precision values, this theoretically allows\nto speed up the execution by a factor of 2x or even 4x\nwhen using AVX or AVX512, respectively. Denoting by aR\nand aI the real and imaginary parts of a, respectively, we\nnow inspect the update above more closely. Multiplying one\ncomplex entry vl = (vR , vI ) of the temporary vector v with\none complex entry of the gate matrix m = (mR , mI ) and\nsumming the result into the temporary output vector v can\nbe written as follows:\n(vR , vI ) += (vR  mR  vI  mI , vI  mR + vR  mI ) (1)\nYet, implementing this update results in wasted compute\nresources due to articial dependencies and additional per-\nmutes. However, these instructions can be re-ordered as\nfollows\n\n(vR , vI ) += (vR  mR , vI  mR )\n(2)\n(vR , vI ) += (vI  1  mI , vR  mI )\n(3)\nin order to increase the maximal achievable performance.\nNamely, having both (mR , mR ) and (1mI , mI ) available,\nthis update requires only two fused multiply-accumulate\ninstructions instead of several\nindividual multiplications,\nadditions, and permutations. This is an improvement in both\nFLOP/instruction and FLOP/FMA ratios.\n\nNote that vl can be permuted once upon loading (and\nthen kept in register), as it is re-used for 2k such complex\nmultiplications. Also, since the matrix m is used in 2nk\nmatrix-vector multiplications, the pre-computation to build\nup these two matrices consisting of (mR , mR ) and (1 \nmI , mI ) is essentially free.\n\n3.3. Single-node\n\nThe optimizations discussed above do not change the\nfact that the operational intensity for applying a 1-qubit gate\nis very low, making it harder to fully utilize the power of\nmulti- and manycore processors (see, e.g., Fig. 10). Yet,\nas mentioned previously, applying a k-qubit gate requires\nmore operations for larger values of k and as long as the\napplication remains memory bound,\nlarger gates can be\napplied in (almost) the same amount of time. The benet \nbesides increased operational intensity  is that larger gates\ncan be used to execute an entire sequence of single- and\ntwo-qubit gates at once. In particular, multiple gates acting\non k different qubits can be combined into one large k-qubit\ngate.\nWhich value of k to choose depends on the peak per-\nformance, the memory-bandwidth, the cache-size & asso-\nciativity of the system, and the circuit to simulate. The\ncache specications are important especially when gates are\napplied to qubits with larger indices, which cause memory\naccess strides of large powers of two. For low-associativity\ncaches, this causes conicts to arise already for small kernel\nsizes. Since 2k values need to be loaded from the state vector\n(which are at least 2m apart, where m is the lowest qubit\nindex) for each of the 2nk matrix-vector multiplications, a\n2k -way cache should map the corresponding cache-lines to\ndifferent locations, no matter how large m is. This allows to\ndirectly access these values from cache for the next matrix-\nvector multiplication. See Fig. 6 for experimental results.\n\n166.2 10 1000.54 1 10Performance [GFLOPS]Operational Intensity [FLOP/byte]Stream TRIAD BW (52 GB/s)Peak performance: 230.4 GFLOPS (12 cores, AVX)1-qubit kernel1.4-qubit kernel2.3.10100229.6442.7878.70.54 1 10Performance [GFLOPS]Operational Intensity [FLOP/byte]MCDRAM BW (460 GB/s)Peak performance: 3133.4 GFLOPS (68 cores, AVX512, FMA)1-qubit kernel1.4-qubit kernel2. (AVX)2. (AVX512)3.1/2 MCDRAM BW (230 GB/s)DRAM BW (115.2 GB/s)\fFinally, these k-qubit gate kernels are parallelized using\nOpenMP with NUMA-aware initialization of the state vector\nto ensure scaling beyond 1 NUMA node. Depending on\nthe qubits to which the gate is applied,\nthe outer-most\nloop may perform very few iterations, prohibiting a good\nstrong scaling behavior. The OpenMP collapse directive\nremedies this problem.\nPlease see Fig. 2a and Fig. 2b, which show the im-\nprovements in performance when applying all mentioned\noptimizations and running the kernels on one socket of\nEdison or Cori II, which feature one 12-core Intel R(cid:13) Xeon R(cid:13)\nProcessor E5-2695 v2 and one 68-core Intel R(cid:13) Xeon PhiTM\nProcessor 7250 (KNL), respectively.\n\n3.4. Multi-node\n\nThe simulation of quantum computers featuring many\nmore than 30 qubits requires multiple nodes in order for the\nstate vector to t into memory. We use MPI to communicate\nbetween 2g nodes, each node having its own state vector of\nsize 2l , where g and l denote the number of global and\nlocal qubits, respectively. Gate operations on local qubits,\ni.e., qubits with index i < l, require no communication.\nQubits with index i  l, on the other hand, do require\ncommunication.\nThere are two basic schemes which can be used to\nperform multi-node quantum circuit simulations. The rst\n[16] keeps global qubits global and applies global gates by\nemploying 2 pair-wise exchanges of half the state vector.\nThe second scheme [17] swaps global qubits with local\nones, applies gates to local qubits in the usual fashion and,\nif need be, swaps them again with global qubits. Note that\nswapping in a global qubit and then immediately swapping\nit back out requires the same amount of communication as\nthe rst scheme. We thus expect the global-to-local scheme\nto perform better and focus on this scheme.\n1-Qubit Example (see Fig. 3a). For the case of two ranks,\nswapping the highest-order qubit (highest bit in the local\nindex) with the global qubit (rst bit of the rank number)\ncan be achieved as follows: The rst block of rank 0 remains\nunchanged, since swapping 0 with 0 has no effect. Swapping\n0 (global) and 1 (local) for the second block requires sending\nthe entire block to rank 1, where these coefcients are\nassociated with the local qubit being 0. Proceeding in this\nmanner results in an exchange of the colored blocks, which\nis equivalent to an all-to-all.\n2-Qubit Example (see Fig. 3b). To swap two global qubits\nwith the two highest-order local qubits for the case of four\nranks, each rank sends its i-th quarter of the state vector\nto rank number i. Therefore, all identically-colored state\nvector parts are exchanged, which results again in one\nall-to-all.\n\nAdditionally, as done in [17], we generalize this scheme\nto swap multiple or even all global qubits with local ones.\nYet, in contrast to [17], we do not iteratively copy out parts\nof the state vector and carry out the pair-wise exchanges\n\n(a) Single-qubit swap.\n\n(b) Two-qubit swap.\n\nFigure 3: Illustration of a single- and multi-qubit global-to-\nlocal swap using one (group-) all-to-all. The blocks labeled,\ne.g., 01... represent the coefcients corresponding to the\nglobal basis state which starts with the bit-string r01, where\nr is the bit-representation of the rank (see text).\n\nmanually. Instead, we employ higher-level abstractions to\nachieve the same task, with the benet\nthat optimized\nimplementations for, e.g., specic network topologies are\nlikely to be already available. A q -qubit global-to-local\nswap, which exchanges q global with q local qubits, can be\nachieved using 1 group-local all-to-all for each of the 2gq\ngroups of processes. Therefore, turning all global qubits\ninto local ones amounts to executing one all-to-all on the\nMPI_COMM_WORLD communicator. This allows swapping\nthe k qubits with highest local index with k global ones.\nIn order to allow for arbitrary local qubits to be exchanged,\nwe rst use our optimized kernels to achieve local swaps\nbetween highest-index qubits and those to be swapped.\nWe then perform the group-local all-to-all and, if need be,\nanother local swap (with lower-index qubits) in order to\nimprove data locality in our k-qubit gate kernels.\n\n3.5. Global gate specialization\n\nWhile a general global gate always requires commu-\nnication,\nthere are a few common ones which do not.\nExamples include the controlled-NOT gate (or controlled-X)\nwhich, when applied to global qubits, causes merely a re-\nnumbering of ranks. The (diagonal) controlled-Z gate either\nturns into a conditional global phase or a local Z-gate which,\ndepending on the rank, is executed or not. Finally, the T-gate\nis also diagonal and results in a global phase, which can be\nabsorbed into the next gate matrix to be applied. Making\nuse of such insights allows to further reduce the number\nof global-to-local swaps without increasing the amount of\ncomputation performed locally.\n\nRank0Basis states0...1...10...1...Rank00Basis states00...01...10...11...0100...01...10...11...1000...01...10...11...1100...01...10...11...\fFor 36-qubit quantum supremacy circuits, this optimiza-\ntion enables a reduction of the required communication\nby another factor of 2x: Only one global-to-local swap is\nrequired to run the entire depth-25 circuit. For 42- and 45-\nqubit circuits, 2 global-to-local swaps are necessary, whereas\n3 are required without gate specialization.\n\n3.6. Circuit Optimizations: Gate scheduling and\nqubit mapping\n\nIn addition to performing implementation optimizations,\nalso the circuit requires optimization in order to reduce the\nnumber of communication steps and to use our highly-tuned\nkernels in a more efcient manner. We will demonstrate\nthe different optimizations for gate scheduling and qubit\nmapping using the quantum supremacy circuits from [10],\nfor which we also present performance results in the next\nsection. Our optimizations are general and can be applied to\nany quantum circuit. In fact, these quantum supremacy cir-\ncuits happen to be designed in a way that is least suitable for\nthese kinds of performance optimizations. We thus expect\neven larger improvements when employing these techniques\nfor the simulation of other circuits.\nThe construction of these random, low-depth quantum\ncircuits is shown in Fig. 1. These circuits are designed to\nbe run on a quantum computer architecture featuring a 2D\nnearest-neighbor connectivity graph. By design, all possible\ntwo qubit gates are applied within 8 cycles, which makes\nthe system highly entangled. Note that a simulator can skip\nthe initial Hadamard gates in cycle 0 and initialize the wave\nfunction directly to (2 n\n2 , ...)T , instead of starting in state\n|0...0(cid:105) = (1, 0, ..., 0)T . Furthermore, we do not simulate the\nnal CZ gates as they only alter the phases of the probability\namplitudes i , but not the probabilities pi = |i |2 which we\nare interested in.\n\n3.6.1. Gate scheduling. The most important optimization\non the quantum circuit is gate scheduling, as it drastically\nreduces the amount of communication in the multi-node\nsetting and also the number of k-qubit gate kernels on\nthe single-node level. The optimizations can be broken into\nthree steps:\n1. Minimize number of communication steps. In a rst\noptimization step, gate scheduling minimizes the number of\nglobal-to-local swaps which is the most important parameter\nin the multi-node setting. Executing every clock-cycle of the\ncircuit on its own requires at least one communication step\nfor every cycle which features a non-diagonal global gate.\nHowever, as explained in the multi-node strategy, it is\nbenecial not to execute those global gates but rather swap\nglobal qubits with local qubits and then execute these gates\nlocally. In order for this scheme to be most benecial, the\ngate scheduling algorithm reorders (if possible) the gates\ninto stages, where each stage consists of a sequence of\nquantum gates acting only on local qubits, see Fig. 4.\nGates acting on the same qubit never commute for quantum\nsupremacy circuits by design, making classical simulation\n\nharder. Nevertheless, we can reorder gates which act on\ndifferent qubits as they commute trivially. After completing\na stage, some local qubits are swapped with global qubits,\nand a new stage is started. This scheme reduces the number\nof communication steps signicantly. A depth-25 42-qubit\nsupremacy circuit requires only two global-to-local swaps,\nsee Fig. 5b. An important feature of our gate scheduling\nalgorithm is that the number of global-to-local swaps is\nmostly independent of the number of local qubits (29, 30,\n31, or 32). This allows for a good strong scaling behavior.\nFig. 5a shows how the number of global-to-local swaps\nbehaves as a function of circuit depth.\nWe decided to always swap global qubits with the\nlowest-order local qubits to arrive at an upper bound for the\nnumber of communication steps required. In addition, we\napply a cheap search algorithm to nd better local qubits\nto swap with. In case of a 36-qubit supremacy circuit, this\nresults in a 2x reduction in the number of global-to-local\nswaps, from two swaps to just one. Note that our stage-\nnding algorithm assumes the worst-case scenario, in which\nall randomly picked global single-qubit gates are dense,\nmeaning that we cannot apply our gate specialization for\nT gates to reduce the amount of communication.\n2. Minimize number of k-qubit gates. In a second step,\nwe schedule all the gates within a stage such that we can\nmerge sequences of consecutive 1- or 2-qubit gates into a\nk-qubit gate and execute this k-qubit gate instead of many\nsingle- and two-qubit gates. See Fig. 4, which shows how\nsuch a cluster with k = 3 can be built. We greedily try\nto increase the number of qubits k within a cluster while\nstill maintaining the condition that k < kmax , where kmax\nis determined by the largest k-qubit gate kernel which still\nshows good performance on the target system. To reduce\nthe over-all number of clusters, we perform a small local\nsearch in order to build the largest cluster with gates not\nyet assigned, before assigning the remaining gates to new\nclusters. We summarize the required number of clusters to\nexecute a quantum supremacy circuit in Table 1. Clearly,\neven for these circuits, more than k gates can be merged\ninto one k-qubit cluster on average.\n3. Local adjustments of global-to-local swaps. The last\nclusters within each stage tends to contain a lower number of\nsingle- and two-qubit gates. In order to increase the average\nnumber of gates in each cluster and thereby minimize the\ntotal number of clusters in the circuit, we try to remove the\nlast clusters of each stage by performing the global-to-local\nswap earlier if this is possible without increasing the total\nnumber of global-to-local swaps.\n\n3.6.2. Qubit mapping. Last, the bit-location of each qubit\nis optimized in order to reduce the number of clusters ex-\nperiencing the performance decrease resulting from the set-\nassociativity of the last-level cache. Since this performance\ndecrease only occurs if the gate is applied to high-order bit-\nlocations, this can be achieved by remapping. The following\nheuristic allowed for a 2x decrease in time-to-solution:\nAssign the qubit to bit-location 0 such that the number\nof clusters accessing bit-location 0 is maximal. From now\n\n\fNumber\nof Qubits\n\nNumber\nof Gates\n\nNumber of clusters\n\nkmax = 3\n\nkmax = 4\n\nkmax = 5\n\n30\n36\n42\n45\n\n369\n447\n528\n569\n\n82\n98\n111\n111\n\n46\n53\n58\n73\n\n36\n41\n46\n51\n\nTABLE 1: Rescheduling of the gates of 25-depth quantum\nsupremacy circuits into clusters (using 30 local qubits).\nClusters are build to contain k < kmax qubits using a\nheuristic which tries to maximize the number of gates\nmerged into one cluster. Clearly more than kmax individual\ngates can be combined into one single cluster on average.\nThese optimizations take less than 3 seconds using Python\nand can be reused for all instance of the same size.\n\n4.1. Cori II\n\nWe performed simulations of quantum supremacy cir-\ncuits featuring 30, 36, 42, and 45 qubits on the Cori II sys-\ntem at the Lawrence Berkeley National Laboratory (LBNL).\nCori II consists of 9,304 single-socket compute nodes, each\ncontaining one 68-core Intel R(cid:13) Xeon PhiTM Processor 7250\n(KNL) at 1.40GHz. The nodes are interconnected by a Cray\nAries high speed dragony [20] topology interconnect\nand offer a combined theoretical peak performance of 29.1\nPFLOPS and 1 PB of aggregate memory.\n\n4.1.1. Node-level performance. These experiments were\nrun on a single 68-core Intel R(cid:13) Xeon PhiTM Processor 7250\n(KNL) node of the Cori II supercomputing system in the\nquad/cache setting. For k  {1, 2, 3}-qubit gate kernels,\nfour threads per core were used, as this resulted in the best\nperformance. For k = 4 and k = 5, the best performance\nwas achieved when using two and one thread per core,\nrespectively. As mentioned in Sec. 3.3, the set-associativity\nof caches plays a crucial role in the performance of these\nk-qubit gate kernels. In particular, we nd the theoretical\npredictions from Sec. 3.3 to agree perfectly with observa-\ntions, see Fig. 6. The strong scaling behavior of executing\none k-qubit gate kernel on a state vector of 28 qubits can\nbe seen in Fig. 7.\n\n4.1.2. Multi-node performance. The strong scaling of our\nsimulator for a 36- and 42-qubit quantum circuit running on\n{16, 32, 64} and {1024, 2048, 4096} KNL nodes of Cori II,\nrespectively, is depicted in Fig. 8. Following these scaling\nexperiments, we ran a 45-qubit quantum supremacy circuit\nusing 8, 192 KNL nodes and a total of 0.5PB of memory.\nTo our knowledge, this is the largest quantum circuit simu-\nlation ever carried out. Averaged over the entire simulation\ntime (i.e., including communication time), this simulation\nachieved 0.428 PFLOPS. There are two reasons for this\ndrop in performance. First, the time spent in communication\nis 75%, and overlaying computation and communication\n\nFigure 4: Example of gate scheduling for a circuit with CZ\ngates and dense single-qubit rotations gates (R). Note that\nwe use gate specialization for CZ gates, which means we can\napply them without communication on global qubits. First,\ninstead of applying the gates cycle by cycle, we identify\nthe largest rst stage of gates which can be applied without\ncommunication. These are all the gates on the left of the\nsolid red line. Second, we schedule the gates within a stage\ninto clusters. For example, we can combine all the gates\non the left of the dashed green line into one 3-qubit gate\ninstead of applying 7 individual gates.\n\non, ignore all clusters which act on this qubit and assign bit-\nlocations 1, 2, and 3 in the same manner. Bit locations 4, 5,\n6, and 7 are assigned the same way, except that after each\nstep, only clusters acting on two of these four bit-locations\nare ignored when assigning the next higher bit-location. For\nnon-random circuits, it would pay off to perform a few\nlocal swaps between some bit-locations over the course of\nthe algorithm, in order to maximize the number of clusters\nacting on low-order qubits.\n\n4. Implementation and Results\n\nAll optimizations mentioned in the previous sections\nwere implemented in C++, except for the code generator for\nthe k-qubit kernels and the circuit scheduler/qubit mapper,\nwhich were both implemented in Python.\n\nRRRRRRRRRRRlocalqubitsglobalqubitscycles\f(b) Scaling of the required communication for increasing\nnumbers of qubits for quantum supremacy circuits with a xed\ncircuit depth of 25.\n\n(a) Scaling of the required communication for circuit depths 10\nto 50 for 42-qubit quantum supremacy circuits. The number of\nglobal-to-local swaps is mostly independent of the number of\nlocal qubits per node, which allows for a good strong scaling\nbehavior.\nFigure 5: Scaling of the required number of communication steps for quantum supremacy circuits as a function of circuit\ndepth (a) or number of qubits (b). The lower two panels show the number of global gates which require communication\nif executed individually as in [10]. In contrast, the top two panels show the number of global-to-local swaps required to\nexecute the full circuit when using our strategy of reordering gates and swapping global with local qubits. Note that one\nglobal-to-local swap (of all global qubits) requires the same amount of communication as one global gate. Averaged over\nthe different global qubits, executing a dense global gate takes approximately 1/2 of the time required to swap all global\nqubits with local qubits, because applying a dense gate to low-order global qubits is faster due to the increased locality of\nthe communication, see [10]. Note that the dashed lines are for worst case instances (only dense random gates on global\nqubits) and solid lines are for median hard instances, which we only consider in the two lower panels.\n\n#Qubits #Gates #Nodes\n6  5\n6  6\n7  6\n9  5\n\n8192\n\n4096\n\n569\n\n369\n\n447\n\n528\n\n1\n\n64\n\nTime [s] Comm.\n\nSpeedup\n\n9.58\n\n28.92\n\n79.53\n\n552.61\n\n0%\n37.6%\n67.4%\n75.0%\n\n14.8x\n\n12.8x\n\n12.4x\n\nN/A\n\nTABLE 2: Summary of all simulation results carried out\non Cori II. Circuit simulation time and speedup are given\nwith respect to the depth-25 quantum supremacy circuit\nsimulations performed in [10]. The comm.-column gives the\npercentage of circuit simulation time spent in communica-\ntion (i.e., all-to-all).\n\nwould not improve this behavior due to the low k-qubit\ngate times (less than 1 second). Second, the performance\nof our kernels suffers in the regime where only few k-\nqubit gates are applied before a global-to-local swap needs\nto be performed. This is due to the fact that blocking for\nMCDRAM requires a sequence of several gates acting on\n\nqubits below bit-location 29. While our mapping procedure\naims to maximize this number, the total number of gates\nbeing applied is not large enough. Yet, this is mainly due\nto the articial construction of random circuits and does\nnot occur in actual quantum algorithms, where interactions\nremain local over longer periods of time. As our 4-qubit\ngate kernel achieves 1/2 of the MCDRAM bandwidth which\ncorresponds to roughly 2x the bandwidth of DRAM (see\nFig. 2b), we expect a 2x drop in performance if memory\nrequirements exceed the MCDRAM size of 16GB. Aver-\naging the performance of our k-qubit kernels in Fig. 6\nand including this 2x reduction yields approximately 250\nGFLOPS per node. In total, we thus expect a performance of\n25%  8, 192  250 GFLOPS  0.5 PFLOPS, which agrees\nwith the measurement results given that we also apply a few\n3- and 2-qubit gate kernels for left-over gates.\nFor a summary of all runs carried out on Cori II, see\nTable 2. Our implementation for, e.g., 42 qubits behaves as\nexpected from Fig. 5a: For a depth-25 circuit, the communi-\ncation scheme used in [10] requires about 50 global gates,\nwhile our simulator performs 2 global-to-local swaps (of\n\n 1 2 3#Swaps29 local qubits30 local qubits31 local qubits32 local qubits 0 20 40 60 80 100 120 140 160 180 200101520253035404550#Global gatesCircuit depth 0 1 2#SwapsCircuit depth29 local qubits30 local qubits31 local qubits32 local qubits 0 20 40 60 80 100 120 1403036424549#Global gates#Qubits\fFigure 6: Decrease in performance when applying k-qubit\ngate kernels to qubits with large indices (high-order qubits)\nas opposed to low indices (low-order qubits). These experi-\nments were run on all 68 cores of a Cori II KNL node. As\nmentioned in Sec. 3.3, this performance drop occurs when\n2k is larger than the set-associativity of the last-level cache.\nWhile the L2-cache is 16-way set-associative, it is shared\nbetween 2 cores.\n\nFigure 7: Strong scaling for applying k-qubit kernels to a\n28-qubit system using 2p , p  {0, ..., 6} cores of the 68-\ncore Intel R(cid:13) Xeon PhiTM Processor 7250 and 4, 2, and 1\nOpenMP thread(s) per core for k  3, k = 4, and k = 5,\nrespectively.\n\nall global qubits). Including the fact that one such global-\nto-local swap requires the same amount of communication\nand that, averaged over all global qubits, a global gate is 2x\nfaster than if it is applied to the highest-order global qubit\ndue to the network bisection bandwidth (see [10]), yields a\nreduction in communication of\n\n50x\n2  2\n\n= 12.5x ,\n\nand since we achieve a similar reduction in time-to-solution\nfor the circuit simulation on each node, this is also the\nexpected overall speedup.\n\nFigure 8: Strong scaling of our simulator running a 36-\nand 42-qubit quantum supremacy circuit on {16, 32, 64} and\n{1024, 2048, 4096} nodes of Cori II, respectively.\n\n4.2. Edison\n\nIn order to be able to compare our results directly to [10],\nwe also ran 30- and 36-qubit quantum supremacy circuits on\nthe Edison system, also at LBNL. We used up to 64 sockets,\neach featuring a 12-core Intel R(cid:13) Xeon R(cid:13) Processor E5-\n2695 v2 at 2.4GHz. The 5, 586 2-socket Edison nodes are\ninterconnected by a Cray Aries dragony [20] topology\ninterconnect and the theoretical peak performance of the\nentire system is 2.57 PFLOPS.\n\n4.2.1. Node-level performance. The performance reduction\nfrom applying gates to high-order qubits due to the 8-way\nset-associativity of the L1- and L2-caches in Intel R(cid:13) Ivy\nBridgeTM processors can be seen in Fig. 9. These exper-\niments were run on an entire two-socket node on all 24\ncores with one OpenMP thread per core and using AVX\nvectorization.\nThe strong scaling of these k-qubit kernels with respect\nto the number of cores is depicted in Fig. 10. While the\n5-qubit gate kernel scales best to the full node, the perfor-\nmance drop when applying it to high-order qubits is much\ngreater than it is for 4-qubit gates. In addition, the 4-qubit\ngate kernel scales nearly perfectly to all 12 cores of a single\nsocket, which suggests to use 2 MPI processes per node in\nthe multi-node setting.\nRunning a single-socket simulation of a 30-qubit quan-\ntum supremacy circuit yields an improvement in time-to-\nsolution by 3x.\n\n4.2.2. Multi-node performance.\nto compare\nIn order\nthe present work directly to the state-of-the-art simulator\nin [10], we performed a simulation of a 36-qubit quantum\nsupremacy circuit using identical hardware: 64 sockets of\nthe Edison supercomputer. We calculated the entropy of\na depth-25 quantum supremacy circuit in 99 seconds, of\nwhich 90.9 seconds were spent in actual simulation and the\nremaining 8.1 seconds were used to calculate the entropy,\nwhich requires a nal reduction. This constitutes an im-\nprovement in time-to-solution of over 4x and indicates that\n\n 0 100 200 300 400 500 600 700 800 900 1000 1100 1 2 3 4 5Performance [GFLOPS]Kernel size [#qubits]High-orderLow-order 5 10 15 20 25 30 35 40 45 50 55 60 6514816 32 64SpeedupNumber of coresOptimal scaling5-Qubit gate4-Qubit gate3-Qubit gate2-Qubit gate1-Qubit gate 1 2 3 41x2x4xSpeedupNumber of nodes1024 nodes2048 nodes4096 nodes16 nodes32 nodes64 nodes36 qubits42 qubits\f5. Summary and outlook\n\nWe demonstrated simulations of up to 45 qubits using\nup to 8, 192 nodes. With the same amount of compute\nresources, the simulation of 46 qubits is feasible when using\nsingle-precision oating point numbers to represent the com-\nplex amplitudes. The presented optimizations are general\nand our code generator improves performance portability\nacross a wide range of processors. Extending the range of\nthe code generator to the domain of GPUs is an ongoing\nproject.\nAdditional optimizations on the quantum circuit descrip-\ntion allows to reduce the required communication by an\norder of magnitude. As a result, the simulation of a 49-qubit\nquantum supremacy circuit would require only two global-\nto-local swap operations. While the memory requirements\nto simulate such a large circuit are beyond what is possible\ntoday, the low amount of communication may allow the use\nof, e.g., solid-state drives. The simulation results may then\nbe used for verication and calibration of near-term quantum\ndevices.\n\nAcknowledgments\n\nWe thank Jarrod McClean for his outstanding help and\nsupport throughout the course of this project. Special thanks\ngoes to our advisor, Matthias Troyer, for giving us the\nopportunity to work on this project and for enlightening dis-\ncussions. Moreover, we would like to thank Ryan Babbush,\nSergio Boixo, Brandon Cook, Jack Deslippe, Sergei Isakov,\nand Hartmut Neven for helpful comments and discussions.\nThis research used resources of the National Energy\nResearch Scientic Computing Center, a DOE Ofce of\nScience User Facility supported by the Ofce of Science\nof the U.S. Department of Energy under Contract No. DE-\nAC02-05CH11231. This work was supported by the Swiss\nNational Science Foundation through the National Compe-\ntence Center for Research NCCR QSIT.\n\nReferences\n\n[1] A. Aspuru-Guzik, A. D. Dutoi, P. J. Love, and M. Head-Gordon,\nSimulated quantum computation of molecular energies, Science,\nvol. 309, no. 5741, pp. 17041707, 2005.\n[2] R. Babbush, J. McClean, D. Wecker, A. Aspuru-Guzik, and N. Wiebe,\nChemical basis of trotter-suzuki errors in quantum chemistry simu-\nlation, Physical Review A, vol. 91, no. 2, p. 022311, 2015.\n[3] M. Reiher, N. Wiebe, K. M. Svore, D. Wecker, and M. Troyer,\nElucidating reaction mechanisms on quantum computers, arXiv\npreprint arXiv:1605.03590, 2016.\n[4] B. Bauer, D. Wecker, A. J. Millis, M. B. Hastings, and M. Troyer,\nHybrid quantum-classical approach to correlated materials, arXiv\npreprint arXiv:1510.03859, 2015.\n[5] A. W. Harrow, A. Hassidim, and S. Lloyd, Quantum algorithm for\nlinear systems of equations, Physical review letters, vol. 103, no. 15,\np. 150502, 2009.\n[6] N. Wiebe, D. Braun, and S. Lloyd, Quantum algorithm for data\ntting, Physical review letters, vol. 109, no. 5, p. 050505, 2012.\n\nFigure 9: Performance decrease when k-qubit gate kernels\nare applied to high-order qubits instead of low-order ones\non a two-socket Edison node. The ndings again correspond\nto the set-associativity of the caches, which is 23 = 8 in\nthis case. For k  3, there is only a negligible drop in\nperformance, since all 2k entries are mapped to different\nlocations in the cache and the next 2k -sizes matrix-vector\nmultiplication can access the next 2k values directly from\ncache, see Sec. 3.3\n\nFigure 10: Strong scaling of the k-qubit kernels using up\nto 24 cores of a two-socket Edison node, which features\none 12-core Intel R(cid:13) Ivy BridgeTM processor per socket. Up\nto and including k = 4, the kernels are memory bandwidth\nlimited. This in combination with Fig. 9 suggests that k = 4\nis the best kernel size to use on this system (with 1 MPI\nprocess per socket).\n\nthe obtained speedups were not merely a consequence of a\nnew generation of hardware.\n\nThe kernels perform at an average of 47% theoretical\npeak, or 218 GFLOPS on every node during the execution\nof a 36-qubit quantum supremacy circuit. When including\ncommunication time, the entire simulation achieved 30%\nof the theoretical peak performance of 64 Edison sockets,\nwhich is 4.4 TFLOPS.\n\n 0 50 100 150 200 250 300 1 2 3 4 5Performance [GFLOPS]Kernel size [#qubits]High-orderLow-order 0 5 10 15 20 25 5 10 15 20SpeedupNumber of coresOptimal scaling5-qubit kernel4-qubit kernel3-qubit kernel2-qubit kernel1-qubit kernel\f[7]\n\n[8]\n\n[9]\n\nS. Lloyd, M. Mohseni, and P. Rebentrost, Quantum algorithms\nfor supervised and unsupervised machine learning, arXiv preprint\narXiv:1307.0411, 2013.\nP. Rebentrost, M. Mohseni, and S. Lloyd, Quantum support vector\nmachine for big data classication, Physical review letters, vol. 113,\nno. 13, p. 130503, 2014.\nP. W. Shor, Algorithms for quantum computation: Discrete loga-\nrithms and factoring, in Foundations of Computer Science, 1994\nProceedings., 35th Annual Symposium on.\nIEEE, 1994, pp. 124\n134.\n[10] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush, N. Ding,\nZ. Jiang, J. M. Martinis, and H. Neven, Characterizing quantum\nsupremacy in near-term devices, arXiv preprint arXiv:1608.00263,\n2016.\n[11] S. Aaronson and L. Chen, Complexity-theoretic foundations of\nquantum supremacy experiments, arXiv preprint arXiv:1612.05903,\n2016.\n[12] A. F. Rodrigues, K. S. Hemmert, B. W. Barrett, C. Kersey, R. Oldeld,\nM. Weston, R. Risen, J. Cook, P. Rosenfeld, E. CooperBalls et al.,\nThe structural simulation toolkit, ACM SIGMETRICS Performance\nEvaluation Review, vol. 38, no. 4, pp. 3742, 2011.\n[13] L. of QC simulators. (2017) http://www.quantiki.org/wiki/List of\nQC simulators (Last accessed 03/31/2017).\n[14] D. Wecker and K. M. Svore, LIQU i |(cid:105): A software design ar-\nchitecture and domain-specic language for quantum computing,\narXiv:1402.4467, 2014.\n[15] T. H aner, D. S. Steiger, M. Smelyanskiy, and M. Troyer, High\nperformance emulation of quantum circuits, Supercomputing 2016,\n2016.\n[16] D. B. Trieu, Large-scale simulations of error prone quantum compu-\ntation devices. Forschungszentrum J ulich, 2009, vol. 2.\n[17] K. De Raedt, K. Michielsen, H. De Raedt, B. Trieu, G. Arnold,\nM. Richter, T. Lippert, H. Watanabe, and N. Ito, Massively parallel\nquantum computer simulator, Computer Physics Communications,\nvol. 176, no. 2, pp. 121136, 2007.\n[18] M. Smelyanskiy, N. P. Sawaya, and A. Aspuru-Guzik, qhipster:\nThe quantum high performance software testing environment, arXiv\npreprint arXiv:1601.07195, 2016.\n[19] M. Mohseni, P. Read, H. Neven, S. Boixo, V. Denchev, R. Babbush,\nA. Fowler, V. Smelyanskiy, and J. Martinis, Commercialize quantum\ntechnologies in ve years. Nature, vol. 543, no. 7644, pp. 171174,\n2017.\n[20] J. Kim, W. J. Dally, S. Scott, and D. Abts, Technology-driven,\nhighly-scalable dragony topology, SIGARCH Comput. Archit.\nNews, vol. 36, no. 3, pp. 7788, Jun. 2008. [Online]. Available:\nhttp://doi.acm.org/10.1145/1394608.1382129\n\n\f", 
        "tag": "Emerging Technologies", 
        "link": "https://arxiv.org/list/cs.ET/new"
    }, 
    {
        "text": "Approximation of\nweighted automata with storage\n\nTobias Denkinger\n\nFaculty of Computer Science\n\nTechnische Universitat Dresden\n\n01062 Dresden, Germany\n\ntobias.denkinger@tu-dresden.de\n\n2017-03-30\n\nAbstract\n\nWe use a non-deterministic variant of storage types to develop a framework for\nthe approximation of automata with storage. This framework is used to provide an\nautomata-theoretic view on the approximation of multiple context-free languages.\n\nContents\n\n1 Introduction\n\n2 Preliminaries\n\n3 Automata with non-deterministic storage\n3.1 Non-deterministic storage types . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Automata with non-deterministic storage\n. . . . . . . . . . . . . . . . . .\n\n2\n\n2\n\n3\n3\n4\n\n7\n4 Approximation of automata with storage\n4.1 Superset approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.2 Subset approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.3 Approximation of weighted automata with storage . . . . . . . . . . . . . 11\n\n5 Approximation of multiple context-free languages\n\n13\n\n7\n1\n0\n2\n \nr\na\nM\n \n9\n2\n \n \n]\nL\nF\n.\ns\nc\n[\n \n \n1\nv\n0\n1\n9\n9\n0\n.\n3\n0\n7\n1\n:\nv\ni\nX\nr\na\n\n1\n\n\f1 Introduction\n\nIn the application of formal languages, approximation is a well-established concept (see\nNederhof [Ned00a] for an overview). For a context-free grammar it is common (but not\nexclusive [Ned00b, CPV+06]) to construct a pushdown automaton and then approximate\nthis automaton [KdT81, Pul86, LL87, BS90, PW91, Eva97, Joh98], e.g. by restricting\nthe height of the pushdown. Automata with storage [Sco67, Eng86, Eng14] are a gener-\nalisation of pushdown automata and were studied in recent literature [HV15, VDH16].\nMultiple context-free languages [SMFK91, VSWJ87] are among the language classes cap-\ntured by automata with storage [Den16] and their approximation was studied recently\n[BL05, vC12]. By attaching weights to the transitions of an automaton with storage,\nwe can model, for example, the multiplicity with which a word belongs to a language or\nthe cost of recognising a word. The resulting devices are called weighted automata with\nstorage [HV15, VDH16].\nWe develop a framework to study the approximation of weighted automata with ar-\nbitrary storage. To deal with non-determinism that arises due to approximation, we\nintroduce weighted automata with non-deterministic storage (Sec. 3). Our storage dif-\nfers from Engelfriets [Eng86, Eng14] in two aspects: As instructions we allow binary\nrelations instead of partial functions and each transition is associated with a weight\nfrom a semiring. Inspired by the storage simulation of Hoare [Hoa72 ] (also studied by\nEngelfriet and Vogler [EV86]), we formalise the strategy with which a storage type is\napproximated (the approximation strategy ) as a partial function (Sec. 4). In contrast to\nEngelfriet and Vogler [EV86], we do not utilise owcharts in our constructions. We use\nour framework to provide an automata-based approach to the approximation of multiple\ncontext-free languages (Sec. 5).\n\n2 Preliminaries\n\nThe set of {0, 1, 2, . . . } is denoted by N and N \\ {0} is denoted by N+ . The set {1, . . . , k}\nis denoted by [k ] for every k  N. Let A be a set. The power set of A is denoted by\nP (A). By ! we denote the quantier there is exactly one.\nLet A, B , and C be sets and let r  A  B and s  B  C be binary relations. We\ndenote {(b, a)  B  A | (a, b)  r} by r1 , {b  B | (a, b)  r} by r(a) for every a  A,\nand SaA r(a) by r(A ) for every A  A. The sequential composition of r and s is the\nbinary relation\nr ; s = {(a, c)  A  C | b  B : ((a, b)  r)  ((b, c)  s)}.\n\nWe call r an endorelation (on A) if A = B . A semiring is an algebraic structure\n(K, +, , 0, 1) where (K, +, 0) is a commutative monoid, (K, , 1) is a monoid, 0 is absorp-\ntive with respect to , and  distributes over +. We say that K is complete if it has a\nsum operation PI : K I  K that extends + for each countable set I [DKV09, Ch. 1].\nLet  be a partial order on K . We say that K is positively -ordered if  is preserved\nby +.\n\n2\n\n\fThe set of partial functions from A to B is denoted by A 99K B . The set of (total)\nfunctions from A to B is denoted by A  B . Note that every total function is a partial\nfunction and that every partial function is a binary relation. Let f : A 99K B be a partial\nfunction. The domain of f and the image of f are dened by dom(f ) = {a  A | b \nB : f (a) = b} and img(f ) = {b  B | a  A: f (a) = b}, respectively. Abusing the\nnotation, we may sometimes write f (a) = undened to denote that a / dom(f ).\n\n3 Automata with non-deterministic storage\n\nIn addition to the nite state control, automata with storage allow the checking and\nmanipulation of a so-called storage conguration that comes from a possibly innite\nset. We propose a syntactic extension of automata with storage where the set of unary\nfunctions (the instructions ) is replaced by a set of binary relations on the congurations.\n\n3.1 Non-deterministic storage types\n\nDenition 3.1. A non-deterministic storage type (short: nd storage type) is a tuple S =\n(C, P , R, ci ) where C is a set (of congurations ), P  P (C ) (predicates ), R  P (C  C )\n(instructions ), and ci  C (initial storage conguration ).\n(cid:3)\n\nIf every element of R is a partial\nConsider an nd storage type S = (C, P , R, ci ).\nfunction, we call S deterministic. The denitions of deterministic nd storage type\nin this paper and storage type in previous literature [Eng86, Eng14, HV15, VDH16]\ncoincide.\n\nExample 3.2. The deterministic nd storage type Count models simple counting ( Engel-\nfriet [Eng86, Eng14, Def. 3.4]): Count = (N, {true, iszero}, {inc, dec}, 0) where true = N,\niszero = {0}, inc = {(n, n + 1) | n  N} and dec = inc1 .\n(cid:3)\n\nExample 3.3. The following deterministic nd storage type models a pushdown stack\n(similar to Engelfriet [Eng86, Eng14, Def. 3.2]): PD = (  , Ppd , Rpd , ) where  is a\nnite set (pushdown symbols ); Ppd = {  , bottom}  {top |    } with\n\nbottom = {}\n\nand\n\ntop = {w | w   }\n\nfor every    ; and Rpd = {stay , pop}  {push , stay |    } with\n\nstay = {(w, w) | w   },\npush = {(w, w) | w   },\n\npop = {(w, w) | w    ,    },\nstay = {( w, w) | w    ,     }\n\nfor every    .\n\n(cid:3)\n\nExample 3.4. The nd storage type Count models counting, but does not distinguish\ndierent odd numbers: Count = ({2n | n  N}  {o}, {true, iszero}, {inc}, 0) where\ntrue = {2n | n  N}  {o}, iszero = {0}, and inc = {(2n, o), (o, 2n) | n  N}. Note that\n(inc )1 = inc and that Count is not deterministic.\n(cid:3)\n\n3\n\n\fWe call a nd storage type S = (C, P , R, ci ) nitely non-deterministic (short: nitely\nnd) if the set r(c) is nite for every r  R and c  C . The nd storage type Count , for\nexample, is not nitely nd since inc (o) = {2n | n  N} is not nite.\n\n3.2 Automata with non-deterministic storage\n\nDenition 3.5. Let S = (C, P , R, ci ) be a nitely nd storage type and  be a nite set.\nAn (S,  )-automaton is a tuple M = (Q, T , Qi , Qf ) where Q is a nite set (of states ), T\nis a nite subset of Q  (  {})  P  R  Q (transitions ), Qi  Q (initial states ), and\nQf  Q (nal states ).\n(cid:3)\n\nLet M = (Q, T , Qi , Qf ) be an (S,  )-automaton and S = (C, P , R, ci ). A conguration\nof M is an element of Q  C    . For every  = (q , v , p, r, q  )  T , the transition\nrelation of  is the endorelation  on the set of congurations of M that contains\n(q , c, vw)  (q  , c , w) for every w    and (c, c )  r with c  p. The run relation of\nM is M= S T  . The transition relations are extended to sequences of transitions\nby setting 1 k = 1 ; . . . ; k for every k  N and 1 , . . . , k  T . The set of runs of\nM is the set\nRM = (cid:8)  T  | q , q   Q, c, c  C, w, w     : (q , c, w)  (q  , c , w  )(cid:9).\n(1)\nLet w    . The set of runs of M on w is\nRM (w) = (cid:8)  T  | q  Qi , q   Qf , c  C : (q , ci , w)  (q  , c , )(cid:9).\nThe language accepted by M is the set L(M) = {w    | RM (w) 6= }.\nLet S be a nitely nd storage type,  be a nite set, and L    . We call L\n(S,  )-recognisable if there is an (S,  )-automaton M with L = L(M).\n\n(2)\n\nProposition 3.6. For every nitely nd storage type S there is a deterministic nd storage\ntype Sdet such that for every nite set  the class of (S,  )-recognisable languages is\nequal to the class of (Sdet ,  )-recognisable languages.\n\nProof. Let S = (C, P , R, ci ). Using a power set construction, we obtain the deterministic\nnd storage type Sdet = (P (C ), Pdet , Rdet , {ci }) where Pdet = {pdet | p  P } with pdet =\n{d  C | d  p 6= } for every p  P , and Rdet = {rdet | r  R} with rdet = {(d, r(d)) |\nd  C } for every r  R.\nLet M = (Q, T , Qi , Qf ) be an (S,  )-automaton and M = (Q , T  , Q\ni , Q\nf ) be an\n(Sdet ,  )-automaton. We say that M and M are related if Q = Q , Qi = Q\ni , Qf = Q\nf ,\nand T  = Tdet = {tdet | t  T } where tdet = (q , v , pdet , rdet , q  ) for each t = (q , v , p, r, q  ) \nT . Clearly, for every (S,  )-automaton there is an (Sdet ,  )-automaton such that both\nare related and vice versa.\nNow consider the (S,  )-automaton M = (Q, T , Qi , Qf ) and the (Sdet ,  )-automaton\nM = (Q, Tdet , Qi , Qf ) that are related. We extend ()det : T  Tdet to a function\ndet by point-wise application. We show for every   T  by induction\n()det : T   T \non the length of  that\nq , q   Q, c, c  C, w, w     :\n(q , c, w)  (q  , c , w  )  d  c: !d  c : (q , d, w) det (q  , d , w  )\n\n(3)\n\n4\n\n\fholds. The induction base is given by the sequence of length 0. For the induction step\nwe assume that (3) holds for all sequences of length n. Let q , q   Q, c, c  C, w, w     ,\n  T n , and   T . We distinguish two cases.\n\n(q , c, w)  (q  , c , w  ). Then there are c  C , u    , and  = ( q , v , p, r, q  ) \nCase 1:\nT with w = uvw  and (q , c, uvw  )  ( q , c, vw  )  (q  , c , w  ). By (3) we know that\nfor every d  c there is exactly one d  c with (q , d, uvw  ) det ( q , d, vw  ). It remains\nto be shown that ( q , d, vw  ) det (q  , d , w  ) for exactly one d  c . By construction,\nwe know that det = ( q , v , pdet , fdet , q  ). By denition of pdet , the fact that c  p, and\nc  d, we have that d  pdet . Then ( d, d )  rdet by the denition of det and d is\nunique by construction of rdet . By denition of rdet , the fact that (c, c )  r , and c  d,\nwe have that c  d . Finally, det clearly goes from state q to state q  and reads v .\nCase 2: (q , c, w)  (q  , c , w  ). This can have two reasons.\nCase 2.1: There is no ( q , c, w)  Q  C    such that (q , c, w)  ( q , c, w). Then it\nfollows by (3) that for every d  c there is no d  c such that (q , d, w) det ( q , d, w)\nand hence there is no d  c such that (c, d, w) ( )det (c , d , w  ).\nCase 2.2: There are ( q , c, u, v)  Q  C     (  {}) such that w = uvw  and\n(q , c, w)  ( q , c, vw  ), but for none of them exists a transition  = ( q , v , p, r, q) such\nthat ( q , c, vw  )  (q  , c , w  ). Then by (3) we know that for every d  c there is\nsome d  c such that (q , d, uvw  ) det ( q , d, vw  ). It remains to be shown that there\nis no d  c such that ( q , c, vw  ) det (q  , c , w  ). If such a d were to exist, then\nthere would exist a transition det = ( q , v , pdet , rdet , q  )  Tdet such that d  pdet\nand ( d, d )  rdet . Then by the construction there would be some c  d such that\nc  p and (c, c )  r which contradicts the assumption that there is no transition\n = ( q , v , p, r, q) such that ( q , c, vw  )  (q  , c , w  ).\n\nWe then obtain L(M) = L(M ) from (3) and from {ci } being the initial storage\nconguration of M .\n(cid:4)\n\nFor practical reasons it might sometimes be preferable to avoid the construction of\npower sets. The proof presented here, however, only shows containment rather than\nequality for the considered language classes.\n\nProposition 3.7. For every nitely nd storage type S there is a deterministic nd storage\ntype S  with the same set of congurations such that for every nite set  the class of\n(S,  )-recognisable languages is contained in the class of (S  ,  )-recognisable languages.\n\nProof. Let S = (C, P , R, ci ) be a nd storage type. We construct the deterministic nd\nstorage type S  = (C, P , R , ci ) where R is constructed as follows: Let r  R and\nr(c)1 , . . . , r(c)mr,c be a xed enumeration of the elements of r(c) for every c  C . Fur-\nthermore, let kr = max{|r(c)| | c  C }. We dene for each i  [kr ] an instruction r \ni by\ni (c) = r(c)i if i  mr,c and r \nr \ni (c) = undened otherwise. Let R contain the instruction\nr \ni for every r  R and i  [kr ].\nNow let M = (Q, T , Qi , Qf ) be an (S,  )-automaton. We construct the (S  ,  )-\nautomaton M = (Q, T  , Qi , Qf ) where T  contains for every transition t = (q , v , p, r, q  ) \n\n5\n\n\fT and i  [kr ] the transition t\ni = (q , v , p, r \ni , q  ). Then\nM = [tT\nt = [t=(q ,v,p,r,q  )T [i[kr ]\nand thus L(M) = L(M ).\n\nt\ni\n\n= [t T  t = M\n\n(cid:4)\n\nThe author conjectures that there are an nd storage type S , a nite set  , and an\n(S  ,  )-recognisable language L (where S  is dened as in the proof of Prop. 3.7) such\nthat L is not (S,  )-recognisable.\n\nProposition 3.8. Let  be a nite set, S = (C, P , R, ci ) be an nd storage type, and\nL be an (S,  )-recognisable language. If C is nite, then L is recognisable (by a nite\nstate automaton).\n\nProof. We will use non-deterministic nite-state automata with extended transition func-\ntion from Hopcroft and Ullman [HU79, Sec. 2.3] in a notation similar to that of automata\nwith storage (we leave out the storage-related parts of the transitions).\nLet M = (Q, T , Qi , Qf ). We construct the fsa M = (Q  C,  , T  , Qi  {ci}, Qf  C )\nwhere T  = {((q , c), v , (q  , c )) | (q , v , p, r, q  )  T , (c, c )  r, c  p}.\nIt remains to be shown that L(M) = L(M ). For this we rst show by induction on\nthe length of runs that\nq , q   Q, c, c  C, w, w     :\nM (q  , c , w  )  ((q , c), w) \n(q , c, w) \nM ((q  , c ), w  ).\nThe induction base is given by runs of length 0. For the induction step we assume that\n(4) holds for all runs of length n and let (q , c, w) \nM ( q , c, w) be witnessed by a run of\nlength n + 1. We derive\n\n(4)\n\n(q , c, w) \nM ( q , c, w)\n q   Q, c  C, w     : (q , c, w) \nM (q  , c , w  ) M ( q , c, w)\n c  C, w     , (q  , v , p, r, q )  T :\nM (q  , c , w  )  w  = v w  c  p  c  r(c )\n(q , c, w) \nM (q  , c , w  )  w  = v w\n w     , ((q  , c ), v , ( q , c))  T  : (q , c, w) \n(by construction of M )\nM ((q  , c ), w  )  w  = v w (by (4))\n w     , ((q  , c ), v , ( q , c))  T  : ((q , c), w) \n q   Q, c  C, w     : ((q , c), w) \nM ((q  , c ), w  ) M (( q , c), w)\n ((q , c), w) \nM (( q , c), w )\n\nFor the languages we then derive\n\nL(M) = {w    | RM (w) 6= }\n= {w    | q  Qi , q   Qf , c  C : (q , ci , w) \nM (q  , c , )}\n= {w    | q  Qi , q   Qf , c  C : ((q , ci ), w) \nM ((q  , c ), )}\n= {w    | RM (w) 6= }\n= L(M ).\n\n(by (1))\n\n(by (4))\n\n(cid:4)\n\n6\n\n\f4 Approximation of automata with storage\n\nAn approximation strategy maps an nd storage type to another nd storage type. It is\nspecied in terms of storage congurations and naturally extended to predicates and\ninstructions.\n\nDenition 4.1. An approximation strategy is a partial function A: C 99K C  where C\nand C  are arbitrary sets.\n(cid:3)\n\nDenition 4.2. Let S = (C, P , R, ci ) be an nd storage type and A: C 99K C  be an\napproximation strategy. The approximation of S with respect to A is the nd storage\ntype SA = (C  , PA , RA , A(ci )) where PA = {pA | p  P } with pA = A(p) for\nevery p  P , and RA = {rA | r  R} with rA = A1 ; r ; A for every r  R.\n(cid:3)\n\nLet S = (C, P , F , ci ) be a nitely nd storage type. We call an approximation strategy\nA: C 99K C  S -proper if SA is nitely non-deterministic.\n\nExample 4.3. Consider the approximation strategy Ao : N  {o}  {2n | n  N} that\nassigns to every odd number the value o and to every even number the number itself.\nThen Ao is not Count-proper since incAo (o) = decAo (o) = {2n | n  N} is not nite.\nNote that CountAo = Count (cf. Ex. 3.4).\nOn the other hand, consider the approximation strategy Aeo : N  {e, o} that returns\no for every odd number and e otherwise. This approximation strategy is Count-proper\nsince incAeo (e) = decAeo (e) = {o} and incAeo (o) = decAeo (o) = {e} are nite.\n(cid:3)\n\nExample 4.4. Consider the (Count, {a, b})-automaton M = ([3], T , {1}, {3}) and its\napproximation MAeo = ([3], T  , {1}, {3}) with\n\nT : 1 = (1, a , true , inc , 1)\n2 = (1, b, true , dec, 2)\n3 = (2, b, true , dec, 2)\n4 = (2,  , iszero, inc , 3)\n\nT  :  \n1 = (1, a , true\n, toggle, 1)\n2 = (1, b, true\n \n, toggle, 2)\n3 = (2, b, true\n \n, toggle, 2)\n \n4 = (2,  , iseven, toggle, 3)\n\nwhere true = trueAeo = {e, o} and iseven = iszeroAeo = {e} are the predicates\nof CountAeo , and toggle = incAeo = decAeo = {(e, o), (o, e)} is the instruction of\nCountAeo . The word aabb  {a, b} can be recognised by both automata:\n\n(1, 0, aabb) 1 (1, 1, abb) 1 (1, 2, bb) 2 (2, 1, b) 3 (2, 0, ) 4 (3, 1, )\n(2, e, )  \n(2, o, b)  \n(1, e, bb)  \n(1, o, abb)  \n(1, e, aabb)  \n(3, o, ).\n1\n1\n2\n3\n4\n\nOn the other hand, the word bb can be recognised by MAeo but not by M:\n\n(1, e, bb)  \n2\n\n(2, o, b)  \n3\n\n(2, e, )  \n4\n\n(3, o, ).\n\n(cid:3)\n\nDenition 4.5. Let M = (Q, T , Qi , Qf ) be an (S,  )-automaton and A be an S -\nproper approximation strategy. The approximation of M with respect to A is the\n(SA ,  )-automaton MA = (Q, TA , Qi , Qf ) where TA = {A |   T } with A =\n(q , v , pA , rA , q  ) for every  = (q , v , p, r, q  )  T .\n(cid:3)\n\n7\n\n\fObservation 4.6. Let M be an (S,  )-automaton with S = (C, P , R, ci ), and A1 : C 99K\nC and A2 : C 99K C  be approximation strategies. If A1 is S -proper and A2 is SA1 -proper,\nthen (MA1 )A2 = MA1 ;A2 .\n(cid:4)\nDenition 4.7. Let A1 : C 99K C and A2 : C 99K C  be approximation strategies. We\ncall A1 ner than A2 , denoted by A1 (cid:22) A2 , if there is a total approximation strategy A\nwith A1 ; A = A2 . We call A1 less partial than A2 , denoted by A1  A2 , if there is an\ninjective approximation strategy A with A1 ; A = A2 .\n(cid:3)\n\nNote that if A1 (cid:22) A2 , we also know that A1 and A2 are equal ly partial (i.e. A1  A2\nand A2 is less partial than A1 ). Similarly, if A1 is less partial than A2 , we know that A1\nand A2 are equal ly ne (i.e. A1 is ner than A2 and A2 is ner than A1 ). Consequently,\n(cid:22) and  are partial orders but not total orders.\n\n4.1 Superset approximations\n\nIn this section we will show that total approximation strategies (i.e. total functions)\nlead to superset approximations.\n\nTheorem 4.8. Let M be an (S,  )-automaton and A be an S -proper total approxima-\ntion strategy. Then L(MA )  L(M).\n\nProof. Let M = (Q, T , Qi , Qf ) and S = (C, P , R, ci ). We extend ()A from transitions\nto sequences of transitions by point-wise application. We show for every   T  by\ninduction on the length of  that\n\nq , q   Q, c, c  C, w, w     :\n(q , c, w)  (q  , c , w  ) = (q , A(c), w) A (q  , A(c ), w  )\n\n(5)\n\nholds. The induction base is given by the sequence of length 0. For the induction\nstep we assume that (5) holds for all sequences of length n. Let q , q   Q, c, c  C ,\nw, w     ,   T n , and   T such that (q , c, w)  (q  , c , w  ). Then there are u    ,\nv    {}, and a conguration ( q , c, vw  )  Q  C    such that w = uvw  and\n(q , c, uvw  )  ( q , c, vw  )  (q  , c , w  ). Hence  has the form ( q , v , p, r, q  ) for some p  P\nand r  R with c  p and (c, c )  r . By (5) we have (q , A(c), uvw  ) A ( q , A(c), vw  )\nand by Def. 4.5 we know that A = ( q , v , pA , rA , q  )  TA . Note that A(c )  C \nsince A is a total function. Now from Def. 4.2 we immediately obtain that A(c)  pA\nand (A(c), A(c ))  rA . Since the states and the read symbol (or ) are taken over in\nA , we therefore know that ( q , A(c), vw  ) A (q  , A(c ), w  ).\nThe claim then follows from (5) and the denition of MA .\n\n(cid:4)\n\nExample 4.9. Recall M and MAeo from Ex. 4.4. Their recognised languages are\nL(M) = {anbn | n  N+} and L(MAeo ) = {ambn | m  N, n  N+ , m  n mod 2}.\nThus L(MAeo ) is a superset of L(M).\n(cid:3)\n\nCorollary 4.10. Let M be an (S,  )-automaton, and A1 and A2 be S -proper approxi-\nmation strategies. If A1 is ner than A2 , then L(MA1 )  L(MA2 ).\n\n8\n\n\fL(MA1 )\n\n(Thm. 4.8)\n\n\n(cid:4)\n\nProof. Since A1 is ner than A2 , we know that there is a total approximation strategy\nA such that A1 ; A = A2 . We obtain\nL(cid:0)(MA1 )A (cid:1) (Obs. 4.6)\n=\nThe following example shows four approximation strategies that occur in the litera-\nture. The rst three approximation strategies approximate a context-free language by a\nrecognisable language (taken from Nederhof [Ned00b, Sec. 7]). The forth approximation\nstrategy approximates a context-free language by another context-free language. It is\neasy to see that the shown approximation strategies are total and thus lead to superset\napproximations.\n\n(Def. 4.7)\n= L(MA2 ).\n\nL(MA1 ; A )\n\nExample 4.11. Let  be a nite set and k  N+ .\n\n(i) Evans [Eva97] proposed to map each pushdown to its top-most element. The same\nresult is achieved by dropping condition 7 and 8 from Baker [Bak81]. This idea is\nexpressed by the approximation strategy Atop :      {@} with\n\nAtop () = @\n\nand\n\nAtop (w) = \n\nfor every w    and   \n\nwhere @ is a new symbol that is not in  .\n\n(ii) Bermudez and Schimpf [BS90] proposed to map each pushdown to its top-most k\nelements. The following approximation strategy implements this idea:\n\nAtop,k :  +  {w   + | |w|  k}\nAtop,k (w) = (w if |w|  k\nif w is of the form uv for some u   k and v   + .\nu\n(iii) Pereira and Wright [PW91] proposed to map each pushdown to one where no\npushdown symbol occurs more than once. To achieve a unique approximation of\neach pushdown, we provide an approximation strategy that retains exactly the\ntop-most (actually left-most) occurrence of each symbol: Consider\n\nAuniq :  +  Seqnr ( )\nAuniq (w) = (Auniq (uw  v)\nw\nwhere Seqnr ( ) denotes the set of all sequences over  without repetition.\n\nif w is of form uw   v with   \notherwise\n\n(iv) In their coarse-to-ne parsing approach for context-free grammars (short: CFG),\nCharniak et al. [CPV+06] propose, given an equivalence relation  on the set of\nnon-terminals N of some CFG G, to construct a new CFG G whose non-terminals\nare the equivalence classes of .1 Let  be the terminal alphabet of G. Say that\n\n1Charniak et al. [CPV+ 06] actually considered probabilistic CFGs, but for the sake of simplicity we\nleave out the probabilities in this example.\n\n9\n\n\fg : N  N/ is the function that assigns for a nonterminal of G its corresponding\nequivalence class; and let g  : (N   )  ((N/)   ) be an extension of g \n{(, ) |    }. Then g  is PDN  -proper and L(Mg  ) = L(G ) where M\nis the (PDN  ,  )-automaton obtained from G by the usual construction [HU79,\nThm. 5.3].\n(cid:3)\n\n4.2 Subset approximations\n\nIn this section we will show that injective approximation strategies lead to a subset\napproximation, this is proved by a variation of the proof of Thm. 4.8.\n\nTheorem 4.12. Let M be an (S,  )-automaton and A be an S -proper injective approx-\nimation strategy. Then L(MA )  L(M).\n\nProof. Let M = (Q, T , Qi , Qf ) and S = (C, P , R, ci ). We show for every   T  by\ninduction on the length of  that\n\nq , q   Q, c, c  img(A), w, w     :\n(q , c, w) A (q  , c , w  ) = (q , A1 (c), w)  (q  , A1 (c ), w  )\n\n(6)\n\nholds. The induction base is given by the sequence of length 0. For the induction\nstep we assume that (6) holds for all sequences of length n. Let q , q   Q, c, c \nimg(A), w, w     ,   T n , and   T such that (q , c, w) A A (q  , c , w  ). Then\nthere are u    , v    {}, q  Q, and c  img(A) such that w = uvw  and\n(q , c, uvw  ) A ( q , c, vw  ) A (q  , c , w  ). Hence A has the form ( q , v , pA , rA , q  )\nfor some p  P and r  R such that c  pA and (c, c )  rA . By (6) we have\n(q , A1 (c), uvw  )  ( q , A1 (c), vw  ) and by Def. 4.5 we know that A = ( q , v , p, f , q  ) \nT . Note that A1 (c ) is uniquely dened since c  img(A) and A is injective. Now\nfrom Def. 4.2 we immediately obtain that A1 (c)  p and (A1 (c), A1 (c ))  r . Since\nthe states and the read symbol (or ) are taken over in A , we therefore know that\n( q , A1 (c), vw  ) A (q  , A1 (c ), w  ).\nThe claim then follows from (6) and the denition of MA .\n\n(cid:4)\n\nCorollary 4.13. Let M be an (S,  )-automaton and A1 and A2 be S -proper approxi-\nmation strategies. If A1 is less partial than A2 , then L(MA1 )  L(MA2 ).\n\nL(MA1 )\n\n(Thm. 4.12)\n\n\n(cid:4)\n\nProof. Since A1 is less partial than A2 , we know that there is a injective approximation\nstrategy A such that A1 ; A = A2 . We obtain\nL(cid:0)(MA1 )A (cid:1) (Obs. 4.6)\n=\nThe following example approximates a context-free language with a recognisable lan-\nguage (taken from Nederhof [Ned00b, Sec. 7]). It is easy to see that the shown approxi-\nmation strategy is injective and thus leads to subset approximations.\n\n(Def. 4.7)\n= L(MA2 ).\n\nL(MA1 ;A )\n\n10\n\n\fExample 4.14. Let  be a nite set and k  N+ . Krauwer and des Tombe [KdT81],\nPulman [Pul86], and Langendoen and Langsam [LL87] proposed to disallow stacks of\nheight greater than k . This can be achieved by a partial identity:\nAbd,k (w) = (w\nif |w|  k\nundened otherwise.\n4.3 Approximation of weighted automata with storage\n\nAbd,k :  + 99K {w   | |w|  k}\n\n(cid:3)\n\nDenition 4.15. Let  be a nite set, S be a nitely nd storage type, and K be\na complete semiring. An (S,  , K )-automaton is a tuple M = (Q, T , Qi , Qf , ) where\n(Q, T , Qi , Qf ) is an (S,  )-automaton and  : T  K (transition weights ). We sometimes\ndenote (Q, T , Qi , Qf ) by Muw .\n(cid:3)\n\nConsider the (S,  , K )-automaton M = (Q, T , Qi , Qf , ). The congurations of M,\nthe run relation of M, and the set of runs of M on w for every w    are the same\nas for Muw . The weight of  in M is the value wtM () = (1 )  . . .  (k ) for every\n = 1    k with 1 , . . . , k  T . The weighted language induced by M is the function\nJMK:    K where for every w    :\nJMK(w) = XRM (w)\nLet S be a nitely nd storage type,  be a nite set, K be a semiring, and r :    K .\nWe call r (S,  , K )-recognisable if there is an (S,  , K )-automaton M with r = JMK.\nWe extend Prop. 3.6 to the weighted case, using ()det as dened in Prop. 3.6.\n\nwtM ().\n\n(7)\n\nProposition 4.16. The classes of (S,  , K )-recognisable and of (Sdet ,  , K )-recognis-\nable languages are the same for every nitely nd storage type S , nite set  , and semiring\nK .\nProof. Let M = (Q, T , Qi , Qf , ) and M = (Q , T  , Q\ni , Q\nf ,   ) be an (S,  , K )-automaton\nand an (Sdet ,  , K )-automaton, respectively. We call M and M related if Muw and\nM\nuw are related, and   (det ) = ( ) for every   T . Note that ()det is a bijection\nbetween T and Tdet . Clearly, for every (S,  , K )-automaton M there is an (Sdet ,  , K )-\nautomaton M such that M and M are related and vice versa. It remains to be shown\nthat JMK = JM K. For every w    , we derive\nJMK(w) = XRM\n= XRM\nwtM (det )\n= X RM\n= JM K(w)\nDenition 4.17. Let M = (Q, T , Qi , Qf , ) be an (S,  , K )-automaton, and A an\nS -proper approximation strategy. The approximation of M with respect to A is the\n(SA ,  , K )-automaton MA = (Q, TA , Qi , Qf , A ) where SA and TA are dened\nas in Def. 4.5, and A (  ) = P T :A=  ( ) for every    TA .\n(cid:3)\n11\n\n(by ()det being a bijection and (3))\n\n(by Def. of det )\n\n(by (7)) (cid:4)\n\nwtM ()\n\nwtM (  )\n\n(by (7))\n\n\fLemma 4.18. Let M be an (S,  , K )-automaton, A be an S -proper approximation\nstrategy,  be a partial order on K , and K be positively -ordered.\n(i) If A is total, then wtMA (  )  PRM :A=  wtM () for every    RMA .\n(ii) If A is injective, then wtMA (  )  PRM :A=  wtM () for every    RMA .\nProof. We prove (i) by induction on the length of   . For   = , we derive\nwtMA () = 1  1 = wtM () = XRM :A=\nwtM ()\nFor     RMA with    TA , we derive\nwtMA (   ) = wtMA ( )  A (  )\n (cid:16) XRM :A=  wtM ()(cid:17)  A (  )\n= (cid:16) XRM :A=  wtM ()(cid:17)  (cid:16) X T :A=  ( )(cid:17)\n= XRM , T :(A=  )(A=  )\nwtM ()  ( )\n X RM :( T )(( )A =    )\nwtM ()  ( )\nwtM ( )\n= X RM :( A=    )\nThe proof for (ii) can be obtained from the proof for (i) by replacing every occurrence\nof  by  and the use of (5) by (6).\n(cid:4)\n\n(by (5) and K being positively -ordered)\n\n(by induction hypothesis)\n\n(by distributivity of K )\n\n(by Def. 4.17)\n\n(by Def. 4.17)\n\nTheorem 4.19. Let M be an (S,  , K )-automaton, A be an S -proper approximation\nstrategy, and  be a partial order on K , and K be positively -ordered.\n\n(i) If A is total, then JMAK(w)  JMK(w) for every w    .\n\n(ii) If A is injective, then JMAK(w)  JMK(w) for every w    .\nProof. ad (i): For every w    , we derive\nJMA K(w) = X RMA (w)\nwtMA (  )\n X RMA (w) XRM : A=  wtM ()\n= X RMA (w) XRM (w) : A=  wtM ()\n XRM (w)\nwtM ()\n= JMK(w)\nFor (), we argue that, since A is total, we know that for every   RM (w), the\naddend wtM () occurs at least once on the left side of the inequality and exactly\nonce on the right side. Hence  is justied.\n\n(by Lem. 4.18 (i))\n\n(by Def. 4.5)\n\n(by (7))\n\n(by (7))\n\n(by ())\n\n12\n\n\fad (ii): For every w    , we derive\nJMA K(w) = X RMA (w)\nwtMA (  )\n X RMA (w) XRM : A=  wtM ()\n= X RMA (w) XRM (w) : A=  wtM ()\n XRM (w)\nwtM ()\n= JMK(w)\nFor (), we argue that since A is injective, we know that for every   RM (w), the\naddend wtM () occurs at most once on the left side of the inequality and exactly\nonce on the right side. Hence  is justied.\n(cid:4)\n\n(by Lem. 4.18 (ii))\n\n(by Def. 4.5)\n\n(by (7))\n\n(by ())\n\n(by (7))\n\n5 Approximation of multiple context-free languages\n\nIn Exs. 4.11 and 4.14 we recall some approximation strategies for pushdown automata\nfrom the literature. Due to the equivalence of pushdown automata and context-free\ngrammars [HU79, Thms. 5.3 and 5.4], those approximation strategies can be used for\nthe approximation of context-free languages. The framework presented in this paper\ntogether with the automata characterisation of multiple context-free languages [Den16,\nThm. 18] allows an automata-theoretic view on the approximation of multiple context-\nfree languages. The automata characterisation uses an excursion-restricted form of au-\ntomata with tree-stack storage.2\n\nDenition 5.1. Let  be a nite set. The tree-stack storage over  is the deterministic\nnd storage type TreeStack = (TS , Pts , Rts , ci,ts ) where\n\n TS is the set of tuples h , i where  : N\n99K   {@}, dom( ) is nite and\n+\nprex-closed,3   dom( ), and  ( ) = @ i  = ;\n\n ci,ts = h{(, @)}, i;\n\n Pts = {TS , bottom}  {top |    } with bottom = {h , i  TS |  = } and\ntop = {h , i  TS |  () =  } for every    ; and\n\n Rts = {down}  {upn , pushn, | n  N,    } with\n down = {(h , ni, h , i) | h , i  TS , n  N+ , n  dom( )},\n upn = {(h , i, h , ni) | h , i  TS , n  dom( )}, and\n pushn, = {(h , i, h  {(n,  )}, ni) | h , i  TS , n / dom( )}\nfor every n  N+ and    .\n2 See Denkinger [Den16] for details on the automata characterisation.\n3A set D  N\n+ is prex closed if for each w  D, every prex of w is also in D.\n\n(cid:3)\n\n13\n\n\fExample 5.2. Let  = {a, b, c}. Consider the (TreeStack{,#} ,  )-automaton M =\n([4], T , {1}, {4}) where T contains the transitions\n\n(1, a, TS , push1, , 1)\n(1,  , TS , push1,# , 2)\n(2,  , top# , down , 2)\n\n, down, 2)\n(2, b, top\n, 3)\n(2,  , bottom, up1\n\n, 3)\n(3, c, top , up1\n(3, , top# , down, 4)\n\nand note that L(M) = {anbn cn | n  N}.\n\n(cid:3)\n\nWe present two approximation strategies for multiple context-free languages from the\nliterature, using the framework of this paper.\n\nExample 5.3. Let  be a nite set.\n\n(i) Van Cranenburgh [vC12, Sec. 4] observed that the idea of Ex. 4.11 (iv) also applies\nto multiple context-free grammars (short: MCFG). The idea can be applied to\ntree-stack automata similarly to the way it was applied to pushdown automata in\nEx. 4.11 (iv). The resulting nd storage type is still a tree-stack storage.\n\n(ii) Burden and Ljunglof [BL05, Sec. 4] and van Cranenburgh [vC12, Sec. 4] proposed\nto split each production of a given MCFG into multiple productions, each of fan-\nout 1. Since the resulting grammar is of fan-out 1, it produces a context-free\nlanguage and can be recognised by a pushdown automaton. The corresponding\napproximation strategy in our framework is\n\nAcf, : TS    ,\n\nAcf, (( , n1    nk )) =  (n1    nk )     (n1n2 ) (n1 )\n\nfor every ( , n1    nk )  TS with n1 , . . . , nk  N+ . The resulting nd storage type\nis pushdown storage.\n(cid:3)\n\nExample 5.4. Let us consider the (TreeStack{,#} ,  )-automaton M from Ex. 5.2. The\ntransitions of the ((TreeStack{,#} )Acf, ,  )-automaton MAcf, (cf. Ex. 5.3) and the\n((TreeStack{,#} )Acf, ;Atop ,  )-automaton MAcf, ;Atop (cf. also Ex. 4.11) are shown\nbelow.\n\ntransitions of MAcf, :\n(1, a,  \n, 1)\n, push\n(1,  ,  \n, 2)\n, push#\n, 2)\n(2,  , top# , pop\n(2, b , top\n, 2)\n, pop\n(2,  , bottom, push  push# , 3)\n, push  push# , 3)\n(3, c , top\n(3,  , top# , pop\n, 4)\n\ntransitions of MAcf, ;Atop :\n, 1)\n(1, a, @ , {( , ) |   @}\n, 2)\n(1,  , @ , {( , #) |   @}\n(2,  , {#}, {( ,   ) |  ,    @}\n, 2)\n(2, b , {} , {( ,   ) |  ,    @}\n, 2)\n(2,  , {@} , {( ,   ) |   @ ,     }, 3)\n(3, c , {} , {( ,   ) |   @ ,     }, 3)\n(3,  , {#}, {( ,   ) |  ,    @}\n, 4)\n\nNote that MAcf ;Atop has nitely many storage congurations, and thus its language is\nrecognisable by a nite state automaton (by Prop. 3.8).\n(cid:3)\n\n14\n\n\fReferences\n\n[Bak81]\n\n[BL05]\n\nExtending lookahead for LR parsers.\nT. P. Baker.\nand System Sciences,\n22(2):243259,\n1981.\nof Computer\n10.1016/0022-0000(81)90030-1.\n\nJournal\nDOI:\n\nH. Burden and P. Ljunglof. Parsing linear context-free rewriting systems. In\nH. Bunt, R. Malouf, and A. Lavie, editors, Proceedings of the 9th Interna-\ntional Workshop on Parsing Technology (IWPT 2005), pages 1117, Strouds-\nburg, PA, USA, 2005. Association for Computational Linguistics. URL:\nhttp://dl.acm.org/citation.cfm?id=1654494.1654496.\n\n[BS90]\n\nM. E. Bermudez and K. M. Schimpf. Practical arbitrary lookahead LR pars-\ning. Journal of Computer and System Sciences, 41(2):230250, 1990. DOI:\n10.1016/0022-0000(90)90037-l.\n\n[CPV+06] E. Charniak, M. Pozar, T. Vu, M. Johnson, M. Elsner, J. Austerweil, D. Ellis,\nI. Haxton, C. Hill, R. Shrivaths, and J. Moore. Multilevel coarse-to-ne\nPCFG parsing. In Proceedings of the Main Conference on Human Language\nTechnology Conference of the North American Chapter of the Association of\nComputational Linguistics (NAACL HLT). Association for Computational\nLinguistics, 2006. DOI: 10.3115/1220835.1220857.\n\n[Den16]\n\nT. Denkinger. An automata characterisation for multiple context-free lan-\nguages. In S. Brlek and C. Reutenauer, editors, Developments in Language\nTheory (DLT 2016), pages 138150. Springer Berlin Heidelberg, 2016. DOI:\n10.1007/978-3-662-53132-7_12.\n\n[DKV09] M. Droste, W. Kuich, and H. Vogler. Handbook of weighted automata.\nSpringer, 2009. DOI: 10.1007/978-3-642-01492-5.\n\n[Eng86]\n\n[Eng14]\n\n[EV86]\n\n[Eva97]\n\nJ. Engelfriet. Context-free grammars with storage. Technical Report I86-11,\nLeiden University, 1986.\n\nJ. Engelfriet. Context-free grammars with storage. Computing Research\nRepository, 2014. arXiv:1408.0683 [cs.FL].\n\nJ. Engelfriet and H. Vogler.\nPushdown machines for the macro tree\ntransducer. Theoretical Computer Science, 42(0):251368, 1986. DOI:\n10.1016/0304-3975(86)90052-6.\n\nE. G. Evans. Approximating context-free grammars with a nite-state calcu-\nlus. In Proceedings of the 8th Conference of the European chapter of the As-\nsociation for Computational Linguistics (EACL 1997). Association for Com-\nputational Linguistics, 1997. DOI: 10.3115/979617.979675.\n\n[Hoa72]\n\nC. A. R. Hoare. Proof of correctness of data representations. Acta Informatica,\n1(4), 1972. DOI: 10.1007/bf00289507.\n\n15\n\n\f[HU79]\n\n[HV15]\n\nJ. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Lan-\nguages and Computation. Addison-Wesley, 1st edition, 1979.\n\nL. Herrmann and H. Vogler. A Chomsky-Schutzenberger theorem for\nweighted automata with storage.\nIn A. Maletti, editor, Proceedings of\nthe 6th International Conference on Algebraic Informatics (CAI 2015), vol-\nume 9270, pages 90102. Springer International Publishing, 2015. DOI:\n10.1007/978-3-319-23021-4_11.\n\n[Joh98] M. Johnson. Finite-state approximation of constraint-based grammars us-\ning left-corner grammar transforms. In Proceedings of the 17th International\nConference on Computational Linguistics. Association for Computational Lin-\nguistics, 1998. DOI: 10.3115/980451.980948.\n\n[KdT81]\n\nTransducers and grammars as the-\nS. Krauwer and L. des Tombe.\nTheoretical Linguistics,\nories of\nlanguage.\n8(1-3),\n1981.\nDOI:\n10.1515/thli.1981.8.1-3.173.\n\n[LL87]\n\nD. T. Langendoen and Y. Langsam. On the design of nite transducers for\nparsing phrase-structure languages. Mathematics of Language, pages 191235,\n1987.\n\n[Ned00a] M.-J. Nederhof.\nPractical experiments with regular approximation of\ncontext-free languages. Computational Linguistics, 26(1):1744, 2000. DOI:\n10.1162/089120100561610.\n\n[Ned00b] M.-J. Nederhof.\na grammatical\nRegular approximation of CFLs:\nIn H. Bunt and A. Nijholt, editors, Advances in Probabilistic\nview.\nand other Parsing Technologies, pages 221241. Springer, 2000. DOI:\n10.1007/978-94-015-9470-7_12.\n\n[Pul86]\n\nS. G. Pulman. Grammars, parsers, and memory limitations. Language and\nCognitive Processes, 1(3):197225, 1986. DOI: 10.1080/01690968608407061.\n\n[PW91]\n\nF. C. N. Pereira and R. N. Wright. Finite-state approximation of phrase\nstructure grammars. In Proceedings of the 29th annual meeting on Associa-\ntion for Computational Linguistics (ACL91). Association for Computational\nLinguistics, 1991. DOI: 10.3115/981344.981376.\n\n[Sco67]\n\nSome denitional suggestions for automata theory.\nD. Scott.\nnal of Computer and System Sciences, 1(2):187212,\n1967.\n10.1016/s0022-0000(67)80014-x.\n\nJour-\nDOI:\n\n[SMFK91] H. Seki, T. Matsumura, M. Fujii, and T. Kasami. On multiple context-\nfree grammars. Theoretical Computer Science, 88(2):191229, 1991. DOI:\n10.1016/0304-3975(91)90374-B.\n\n16\n\n\f[vC12]\n\ncontext-free\nEcient parsing with linear\nA. van Cranenburgh.\neditor, Proceedings of\nIn W. Daelemans,\nthe\nrewriting systems.\n13th Conference of\nthe European Chapter of\nthe Association for\nComputational Linguistics (EACL 2012), pages 460470, Stroudsburg,\nPA, USA, 2012. Association for Computational Linguistics.\nURL:\nhttp://dl.acm.org/citation.cfm?id=2380816.2380873.\n\n[VDH16] H. Vogler, M. Droste, and L. Herrmann. A weighted mso logic with stor-\nage behaviour and its buchichi-elgot-trakhtenbrot theorem. In A.-H. Dediu,\nJ. Janousek, C. Martn-Vide, and B. Truthe, editors, Proceedings of the 10th\nInternational Conference on Language and Automata Theory and Applica-\ntions (LATA 2016), pages 127139. Springer International Publishing, 2016.\nDOI: 10.1007/978-3-319-30000-9_10.\n\n[VSWJ87] K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. Characterizing structural\ndescriptions produced by various grammatical formalisms.\nIn Proceedings\nof the 25th Annual Meeting on Association for Computational Linguistics\n(ACL 1997), pages 104111, Stroudsburg, PA, USA, 1987. Association for\nComputational Linguistics. DOI: 10.3115/981175.981190.\n\n17\n\n\f", 
        "tag": "Formal Languages and Automata Theory", 
        "link": "https://arxiv.org/list/cs.FL/new"
    }, 
    {
        "text": "Kalman Filtering of Distributed Time Series \n \nDan Stefanoiu, Janetta Culita \n \nPolitehnica University of Bucharest, Dept. of Automatic Control and Computer Science \n313 Splaiul Independentei 060032, Bucharest,ROMANIA \nE-mails: danny@indinf.pub.ro, jculita@yahoo.com\n\nAbstract:  This  paper  aims  to  introduce  an  application  to  Kalman  Filtering  Theory,  which  is  rather \nunconventional. Recent experiments have shown that many natural phenomena, especially from ecology or \nmeteorology,  could  be  monitored  and  predicted  more  accurately  when  accounting  their  evolution  over \nsome  geographical  area.  Thus,  the  signals  they  provide  are  gathered  together  into  a  collection  of \ndistributed  time series. Despite  the common sense, such  time series are more or  less correlated each other. \nInstead  of  processing  each  time  series  independently,  their  collection  can  constitute  the  set  of measurable \nstates provided by some open system. Modeling and predicting the system states can take benefit from the \nfamily  of  Kalman  filtering  algorithms.  The  article  describes  an  adaptation  of  basic  Kalman  filter  to  the \ncontext of distributed signals collections and completes with an application coming from Meteorology.  \n\n \n\n1. INTRODUCTION AND PROBLEM STATEMENT \nKalman Filtering (KF) Theory was originated at early 60s by \nthe  works  of  R.E.  Kalman  and  R.S.  Bucy  (Kalman,  1960, \nKalman-Bucy,  1961).  One  can  say  that  Kalman-Bucys \napproach acted like a switch within the scientific community, \nbecause,  nowadays,  the  literature  on  this  topic  is  one  of  the \nrichest,  concerning  the  theory,  as  well  as  the  applications. \nMoreover,  new  and  sometimes  surprising  applications \ncontinue  to  keep  the  KF  field  alive.  For  example,  one  can \nmention  the  latest  results  from  avionics  (the  stellar  inertial \nnavigation  problem) \n(Kayton,  1997), \nfault  diagnosis \n(Hajiyev,  2003)  or  robotics  (Negenborn,  2003).  This  paper \nfocuses  on  the  problem  of  correlated  time  series  prediction. \nEvolution of some natural phenomena can be monitored with \nhigher accuracy if the observation and measurement take into \naccount  not  only  time  variation  of  some  parameter,  but  also \nits  distribution  over  a  geographical  area.  Take  for  example \nthe monitoring of minimum and maximum  temperatures over \na geographical area (see Fig. 1).  \n\n \n\n \n\n \n\n \n\nsuitable.  Sensors  could  thus  provide  several  time  series,  on \ndifferent \nlocations.  Such  data,  coming  from  different \nchannels, are in general more or less correlated. For example, \nin  Fig. 1,  one  can  easily  notice  the  strong  correlations \nbetween  the  fourth  temperature  variations,  since  the  two \ncities  are  close  to  each-other.  It  is  even  possible  that  hidden \ncorrelations  (that  cannot  be  perceived)  be  crucial  for \nmonitoring. Assume  that  the monitoring goal is to predict the \ntemperature.  It  is  very  likely  that  better  prediction  results  be \nobtained  when  considering  the  collection  of  all  data  sets, \nrather  than when  building  independent  prediction models  for \neach  channel  in  isolation.  The  problem  is  then  to  build  and \nestimate  multi-variable  identification  models,  in  view  of \nprediction.  \nThe  solution  introduced  within  this  paper  relies  on  the  idea \nthat  sensors  provide  direct  noisy  data  from  the  states  of  an \nopen  and  quasi-ubiquitous  system.  The  system  has  in  fact  a \ncontinuous collection of variable states. Placing a finite set of \nsensors  at  different  locations,  in  order  to  perform measuring, \nis  equivalent  to  sampling  the  system  both  in  time  and  space. \nThe  prediction  problem  of  each  data  set  (a  time  series,  in \nfact)  is actually a problem of state prediction and can  thus be \nsolved  in  context  of  KF  Theory.  Therefore,  an  adaptation  of \nbasic KF algorithm to the context of distributed time series is \npresented  next.  The  article  is  structured  as  follows.  Next \nsections  introduce  the  Markov-Kalman-Bucy  method  and \nsummarize the algorithm that allow the distributed prediction. \nThe  simulation  case  study  is  based  on  the  example  in  Fig.1. \nA conclusion and the references list complete the paper.  \n\n \n\nFig. 1. Temperature monitoring in 2 cities from Romania. \n\nWhen  using  a  stand  alone  sensor,  there  is  a  problem with  its \nlocation.  Obviously,  the  temperature  varies  both  in  time  and \nspace.  A  small  network  of  sensors  is  seemingly  more \n\n2. DISTRIBUTED PREDICTION FRAMEWORK \nThe  distributed  prediction  relies  on  state  representation \nbelow:  \n \n\n\n\n\n\nx\nk\n[\n1]\n+ =\ny\nk\n[ ]\n=\n\nF w\nk\n[\n]\nk\nD v\nk\n[ ]\nk\n\nk  N\n\n(1) \n\nA x\nk\n[ ]\nk\nC x\nk\n[ ]\nk\n\n+\n+\n\nB u\nk\n[ ]\nx\nk\nB u\nk\n[ ]\ny\nk\n\n+\n+\n\n,   \n\n, \n\n\fnx nx\n\n\nny nx\n\n\nny nu\n\n\n \n \nwhere:  \nxB R\nC R\nyB R\nA R\n, \n, \n, \n, \nnx nu\n\n \nk\nk\nk\nk\nF R\nD R\n  are  matrices  including  all \n  and \nny ny\nnx nx\n\n\nk\nk\nvariable  (but  already  estimated)  parameters  of  some \nn=D\nI y\nstochastic process; (usually, \n); \nk\nnxx R\n is the unknown state vector;  \n \nnuu R\n is the vector of (measurable) input signals;  \n \nnyy R\n is the vector of (measurable) output signals;  \n \nnxw R\n is the (unknown) endogenous system noise;  \n \nnyv R\n  is  the  (unknown)  exogenous  noise,  which  is \n \nusually corrupting the measured data.  \nWhenever  the  stochastic  process  cannot  be  stimulated \nartificially  (like  in  case  of  time  series),  the  input  vector  is \nw\nassigned  to  null values. Therefore,  the noise \n becomes  the \nv\nvirtual useful input, while the noise \n is parasite.  \n\nT\n\nT\n\n. \n\nv\n\n=\n\n=\n\nE\n\nE\n\nk\n[ ]\n\nk\n[ ]\n\nv\n\n(2) \n\n and \n\n}\nk\n[ ]\n\n}\nk\n[ ]\n\n{\nv\nk\n[ ]\n\n{\nw w\nk\n[ ]\n\nThe  following  noise  hypotheses  are  assumed  for  model  (1): \n(a) all noises are zero mean, Gaussian;  (b)  the  two noises are \nuncorrelated  each  other;  (c)  the  endogenous  noise  is  non \nauto-correlated,  but  its  compounds  could  be  correlated  at  the \nsame  instant;  (d)  the  compounds  of  exogenous  noise  are \nwhite  and  uncorrelated  each-other. According  to  the  last  two \nhypotheses, the covariance matrices of noises are mostly null, \n N\nexcepting  for  the  current  instant  k\n,  when  they  are \ndenoted by:  \n \nw\n \nv\nObviously,  the  matrix \n  is  diagonal.  This  is  perhaps  the \nmost restrictive condition, in general case.  \ny\n \nIn  case  of  distributed  time  series,  the  output  vector \nincludes  all  the  data  sets  (on  channels),  whereas  the  state \nx\n  encodes  the  invisible  correlations  between  them. \nvector \nNaturally,  there  is no reason  to consider  that  the white noises \ncorrupting  the  data  are  correlated  each  other.  The  number  of \nstates  is not necessarily equal  to  the number of  time series.  It \ndepends  on  the  size  of  multi-dimensional  ARMA(X)  model \nassigned to the global process, as shown in next section.  \nTwo  main  problems  can  be  stated  within  this  context.  The \nfirst  one  is  to  identify  a  rough  ARMA  model  (by  using  the \ntime series), to improve its accuracy and to transform it into a \nminimal  state  representation.  The  second  problem  is  to \npredict  the  states  via  an  adapted  version  of Kalman  filtering. \nEach problem is approached next.  \n\n3. FROM ARMA(X) TO STATE REPRESENTATION \nThe  rough  ARMA  model  can  be  constructed  by  simply \nconsidering  that  the  time  series  are  independent  each  other. \n N , \njy   (\nj\nn y )  of  length  N\nThus,  for  each  time  series \n1,\nthe corresponding ARMA model:  \n \n(\nA q\nj\n\n)\n(\nje\nC q 1\nj\n\n(3) \n\n\n\n)\n\n, \n\ny\n\n\n\n1\n\nj\n\n \n\n    \n\n \n\n(with known notations), is identified via Minimum Prediction \nError Method  (MPEM)  (Sderstrm and Stoica, 1989). After \nidentification,  the  estimated  polynomials  A j   and  C j   (for \nj\nn y ) yield the evaluation of prediction error:  \n1,\n\nj\n\nj\n\n1\n\n\n\ny\n\n)\n\n+\n\n\n\n(\n\nA q\nj\n\n(\n\n\n1 C q\n\nj\n\n \n\nv\n \nwhich  actually  stands  for  the  (approximated)  input \nnu\nny=\noverall stochastic process. So, \n, in this case.  \n\n,    \n\n\n\n\n)\n\n\ne\n\n\n\n1\n\nj\n\n(4) \n\nju   of \n\n1\n\n\n1\n\n\n\n\n1\n\n\n\n1\n\n\n\n1\n\n\n\n1\n\n\n\n1\n\n\n\n1\n\n1\n\nq\n\nq\n\nq\n\nq\n\nq\n\nq\n\nq\n\n)\n\n)\n\n(\n\n)\n\n(\n\n)\n\n(\n\n)\n\n(\n\n(\n\n)\n\n)\n\n(\n\n)\n\n(\n\n(\n\n)\n\n=\n\n=\n\nB\n\nA\n\nA\n\nG\n\nny ny\n\n1q\n\n\ne , \n\n & \n\n(5) \n\n,  (6) \n\ny B\n\n\n\nv C\n+\n\n(\nC 1q\n\n\nTo  refine  the  rough  model,  one  adopts  a  global  model,  of \nARMAX type:  \n \nA\n \n)1\n(\nA B C R\nwhere  the  polynomial matrices \n  have  to \n,\nq\n,\n\n\nbe  identified  from  the  time  series  as  output  data  and  rough \nprediction  errors  (4)  as  input  data.  The  two  system  functions \nof model (5), i.e.:  \n \nH\n \nencode  moreover  the  correlations  between  time  series. \nIdentification makes use of the same MPEM, but applied to a \nmulti-dimensional stochastic process. Implementation of such \na  method  is  non  trivial  and  involves  many  numerical \nproblems.  In  order  to  reach  for  a  suitable  tradeoff  between \nspeed  and  accuracy,  some  simplifications  are  necessary.  For \nexample,  in  MATLAB  environment,  the  following  general \nprinciple has been adopted: each output depends on  the  input \nsignals  and  noises  only.  With  another  words,  output  signals \n(\n)1q A\nare  not  mixed  each  other,  which  implies  the  matrix \n \nis  diagonal.  Another  simplification  is  related  to  inputs:  since \nj\n  should  not \ninput  signals  are  actually  noises,  the  channel \nju  (one for input and another one for \naccount twice the input \n)1q B\n(\nnoise).  Consequently,  the  matrix \n  has  null  diagonal. \nAlthough  the  resulting model  is  seemingly  less  accurate  than \nthe  one  obtained  by  applying  MPEM  at  once,  it  is  assumed \nthat  the  accuracy  is  acceptable.  The  great  facility  of  such  an \napproach  is  that  the  system  functions  (7)  are  directly \n.  A\ncomputed, without inverting the polynomial matrix \nOnce the system functions being estimated, the MIMO model \ncan  be  converted  into  a  state  representation  like  (1), \nfollowing  at  least  three  rationales.  First  of  them  is  based  on \nTheorem  of  Division  with  Reminder  and  atomic  ratios \ndecomposition \nidea \nthe \nfollowing \n(for  polynomials), \nintroduced  in  (Proakis,  1996).)  The  second  one  starts  from \nthe  linear  regression  form of ARMAX model and defines  the \nstate  by  concatenating \nthe  regressors  vector  and \nthe \nparameters vector (Niedwiecki, 2000). The number of states \ncan  increase  very  fast  in  case  of  MIMO  models.  This  is  the \n\n\f \n \nin \nintroduced \ntechnique \nthird  conversion \nthe \nreason \n(vanOverschee,  1996)  is  often  preferred,  since  it  leads  to  the \nminimum state representation.  \n\n, \n\n=\n\nE\n\n}T\n\n(7) \n\nVV\n{\n\n 0\n>\n\n4. ADAPTED KALMAN FILTER (PREDICTOR) \nThe  filter  is  aiming  to  predict  the  states  of  model  (1),  by \nusing  a  numerical  procedure  that  relies  on  the  recursive \nMarkov  estimator  (Stefanoiu,  2005).  Hereafter,  the  Markov \nestimator and the filter equations are described.  \nThe main  result  of Gauss-Markov Theorem  (GMT)  (Placket, \n1950)  can  be  implemented  through  a  recursive  procedure. \nOne  starts  from  the  canonic  form  of  linear  regression \nassociated to an identification model (like (3) or (5)):  \n \nY  V ,   \n\n+\n=\n \nNY R\nN\nwhere \n  is  the \n-length  output  (measurable)  data \n R\n  is  the  matrix  of  regressors  (either \nvector, \nN n \n  R \nn\n  is  the  vector  of \n  unknown \nmeasurable  or  not), \nn\nNV R\ntrue  parameters  and \n \nis \nthe  vector  of  (non \nN N R\nmeasurable)  noise data with \n  as  covariance matrix. \nOne  assumes  the  noise  is  Gaussian  with  zero  mean,  but  not \n\nnecessarily white  (i.e.  the matrix \n  could  be  non  diagonal). \nThe GMT states that the following Markov estimation:  \n \n) 1\n(\n\n\n=      Y  \nT\nT\n1\n1\n\n \nis  unbiased,  consistent  and  efficient.  Moreover, \nthe \nestimation  accuracy \ninverse  of \nis  also  provided  (the \nestimation error covariance matrix):  \n \n\n(8) \n\n\n \n\n\n\n\n) (\n\n\n \n\n\n\n\n)\nT\n\n=\n\n  1\nT\n\n. \n\n(9) \n\n \nThe  problem  is  to  design  an  efficient  numerical  procedure  to \n\n is usually \ncompute (8) and (9) recursively. Since not only \nunknown,  but  its  size  could  be  quite  big,  computing  its \ninverse  is  non  trivial.  To  solve  the  problem,  one  extends  the \ncanonical  model  (7)  by  the  following  virtual  model  (an \nidentity, also known as random walk model):  \n \n(cid:4)\n \n=\n\n(10) \n\n+\n\n, \n\n1\n\n\n\nP\n\n=\n\n{\n(\n\nE\n\n\n\n\n\n} 1\n\n\n\n\n\n(cid:4)\n)\n(\n \n(cid:8)(cid:11)(cid:9)(cid:11)(cid:10)\n\nW\n\n \nn(cid:4) R\n  is  the  Markov  estimation  from  previous \nwhere \nnW R\n  is  the  virtual  noise \ncomputation  stage,  whereas \n\n  is  non  measurable).  Interestingly,  the \n(unknown,  since \n(cid:4)\ncovariance  matrix  corresponding  to  virtual  noise,  P ,  is \nalready  available  from  the  previous  computation  too.  After \nfusing equations (7) and (10), the extended model is:  \n\n \n\n    \n\n, \n\n \n\n(11) \n\n=\n\n\n\n\n+\n\nY\n\n\n\n\n(cid:4)\n\n\n\n(cid:78)\n(cid:105)\nY\n\n\n\n\n\n\nI\n\n\n(cid:78)\n(cid:105)\n\n\nV\n\n\n\n\nW\n\n\n(cid:78)\n(cid:105)\nV\n\n \nV\n  is  not \nwith  natural  notations.  Whenever  the  noise \nW\ncorrelated  to  the  virtual  noise \n,  the  covariance  matrix  of \n(cid:105)V , includes two diagonal blocks only:  \nextended noise \n \n(cid:105)\n\n\n{\n(cid:105) (cid:105)\nV V\n\n\n(12) \n\n}T\n\nE\n\n=\n\n. \n\n\n= \n\n\n 0\n0 P(cid:4)\n\n\n\n\n\n1\n\n. \n\n \nWhen  computing  again  the  Markov  estimation  with  the \nextended  stochastic  process  (11),  after  some  manipulations, \nthe  direct  link  between  the  current  and  previous  values  of \nestimated parameters can be revealed:  \n \n(\n)\n(cid:4)\n(cid:4)\n(cid:4)\n1\n(\n) . \n\n\n     P\n  Y \nT\nT\n1\n1\n1\n\n\n\n\n= +\n+\n(cid:8)(cid:11)(cid:9)(cid:11)(cid:10)\n(cid:8)(cid:11)(cid:11)(cid:11)(cid:11)(cid:9)(cid:11)(cid:11)(cid:11)(cid:11)(cid:10)\n(cid:4)\n\n\n\n(13) \n\n(14) \n\n(cid:4)\nP   \nT\n1\n1\n\n\n+\n\n \nTwo interesting terms are outlined in (13). The first one is the \n(cid:4)\nN(cid:4) R\n,  based  on  previous  estimation   . \nprediction  error \n R\nThe  second  one  is  the  sensitivity  gain \n,  based  on \nn N\n1P(cid:4)\nprevious  accuracy \n.  Supplementary  manipulations  can \nlead  to  an  equivalent  expression  of  the  gain,  which  is  more \nsuitable  for numerical  evaluations  (only one matrix has  to be \ninverted instead of 4):  \n \n(cid:4)\n(cid:4)\n)T \n(\n P  P\nT\n+\n=\n \nThe covariance matrix of estimation error can also be updated \nwithout any matrix inversion (after computing the gain):  \n \n\nP\n \nnI\nn\nwhere \n.  Practically,  the \n  is  the  unit  matrix  of  size \nrecursive  equations  (14),  (15)  and  (13)  constitute  de  core  of \nimplementation  procedure \nfor  Markov  estimator.  An \n\n  can  be  obtained  by  using  the  prediction \nestimation  of \n(cid:4)\n(cid:4)\nV\n  instead  of  noise \n  (since,  actually, \n  is  the  current \nerror \nV\nestimation  of \n.)  The  most  time  consuming  operation  is \nN N\ncomputing  the  gain  (14),  because,  at  every  step,  a \n \nmatrix has to be inverted. To reduce the computational effort, \nthe  number  of  adaptation  data  should  be  used  instead  of \nN\n  is  the  number  of  data  between \nwhole  data  number.  So, \nsuccessive parameters upgrading (usually, no bigger than 10).  \nRecall  the  state  representation  (1)  and  assume  that  all \nparameters are already known at current instant  k  N . Then, \nthe final goal is to estimate/predict the state values at the next \n1k + ,  i.e.   [\n]+x\n1k\n,  depending  on  current  state  values \ninstant \n\n) (cid:4) , \n P\n\n(15) \n\n=\n\n\n\n=\n\n(\n\n)\n\nn\n\n\n\n(\n\nI\n\n1\n\n\n\fk\n\nv\n\n\n\n1\n\n)\n\n. \n\n\n\n\n\nE\n\n}\n\nT\n)\n\n) (\n\ndef\n=\n\n,   \n\n{\n(\n\nx\nk\n[ ]\n\nx\nk\n[ ]\n\n\nx\nk\n[ ]\n\n\nx\nk\n[ ]\n\n(16) \n\nk  N\n\n\n=\n(cid:4)\nP\n=\nk\n(cid:4)\nx\nk\n[ ]\n\n\nD C P C\nk\n[ ]\nT\nT\n+\nk\nk\nk\nk\n\n(\n\nP C D \nT\nk\nk\nk\n\n\nP  C P\n\nk\nk\nk\nk\n(\n\n y\nx\nk\nk\n[ ]\n[ ]\n+\n=\nk\n\n \n \n [ ]kx\n  and  newly  measured  data.  In  order  to  reach  for  this \ngoal, Markov estimator has to be employed as main tool.  \nThe  main  variables  of  Markov  estimator  are  identified  as \nfollows  from  the  last  equation  of  model  (1):  the  state  vector \n[ ]kx\n\nkC\n\n  is \n  and  the  output  matrix \n  is \n.  Consequently, \n(cid:4)\n\n [ ]kx\n [\n1k +x\n.  The  covariance \n  stands  for \n,  while \n  is \n]\nmatrix from (9) is defined as follows:  \n \n\nP\nk\n \nBefore  starting  the  Markov  estimation  procedure,  it  is \n[ ]kv\n  is  correlated  to \nnecessary  to  verify  whether  the  noise \n)\n(\n [ ]\n[ ]kx\nx\nx\nk\nk\nthe  error \n  or  not.  Since  the  current  state \n \n[ ]\n [ ]kx\nw\n \n and the estimated state \nonly depends on inside noise \n [\n1k x\n  (i.e.  by  the  values \nis  determined  by  the  former  state \n]\n1k \n  at  most),  the  non  correlation \nof  noises  at  instant \nrestriction  is  fully  verified  (under  the  noise  hypotheses  of \nsection  2).  This  allows  the  current  state  to  roughly  be \nestimated as follows:  \n \n\n\n\n\n\n \nkD\n.  In \nsince  the  exogenous noises are mixed  through matrix \nkP(cid:4)\n[ ]kx(cid:4)\n [\n1k +x\n  is  different \n  is  not  yet  equal  to \n  and \n(17), \n]\n\nk +P\nfrom \n  as  well,  because  the  first  equation  of  model  (1) \n1\nremained  untouched. They  are  only  rough  approximations  of \nthe  targeted  terms.  To  refine  the  approximations,  the  next \nstate  is  computed  from  the  first  equation  of  model  (1),  with \n[ ]kx(cid:4)\n [ ]kx\n0=w\n instead of \n and \n. Thus:  \n \n [\nx\n \nThe  next  covariance  matrix \nconsequence of equation (18):  \n \n\nP\nk\n \nafter  some  manipulations  where  the  non  correlation  between \n(\n)\n(cid:4)\n[ ]kw\nx\nx\nk\nk\n  and \n  played \nthe  main  role.  After \n[ ]\n[ ]\naggregating  equations  (17)-(19),  the  kernel  of  final  recursive \nprocedure related to Kalman-Bucy filtering is obtained:  \n \n\n\n=\n\nk\n \n\n\nx\nk\n[\n=\n\n\nP\n\n=\n\nk\n1\n+\nk  N . \n\n(\n)\n\n\nP C D \nC P C\nD\nk\n[ ]\nT\nT\nT\n+\nv\nk\nk\nk\nk\nk\nk\nk\n(\n\n\nC x\nA x\nB u\nA  y\nk\nk\nk\n[ ]\n[ ]\n[ ]\nx\n+\n+\n\nk\nk\nk\nk\nk\n)\n(\n\n\nA P\n C P A\nF \nF\nk\n[ ]\nT\nT\n\n+\nk\nk\nk\nk\nk\nk\nk\nk\n\n\nk +P   results  then  as  direct \n1\n\nk  N\n\nk  N\n\nk  N\n\n(cid:4)\nA P A\nk\nk\n\n(cid:4)\nA x\nk\n[ ]\nk\n\n\nC x\nk\n[ ]\nk\n\nB u\nk\n[ ]\ny\nk\n\nF\nk\n[ ]\nT\nk\n\ndef\n+ =\n1\n\nB u\n[\nk\n\n1]\n+ =\n\nk ,   \n]\n\n)k\n[ ]\n\n(19) \n\n(18) \n\n(20) \n\n(17) \n\nF \nk\n\nB u\ny\nk\n\nk\n[ ]\n\n,   \n\n,  \n\n1]\n\n\n\n\n\n+\n\n+\n\n, \n\n, \n\n. \n\n, \n\n)\n\nk\n\n\n\n+\n\nT\nk\n\n\n\n1\n\nw\n\nw\n\n \n\n    \n\n \n\nObviously,  the procedure  (20) can be  implemented only after \nthe  mixed  covariance  matrices  of  noises, \nestimating \nk  N .  This \nkwF \nkvD \nD\nF\n  and \n  at  any  instant \n[ ]\n[ ] T\nT\nk\nk\nk\nk\noperation involves 2 computation stages. At the first step, the \nmixed  exogenous  noise  is  estimated  with  the  help  of  the \nsecond equation of (1).  \n \n\nD v\nn\n[ ]\nn\n \nAt the second step, the covariance matrix is updated:  \n \nD \nk\n\nB u\nn n\n[ ]\n y\n\n\nD v\nn\n[ ]\nn\n\n\nC x\nn\n[ ]\nn\n\nn\n \n\nD\nk\n[ ]\n\nD\nn\n[ ]\n\n(21) \n\ny\nn\n[ ]\n\n\nn\n[ ]\n\n0,\n\n\nv\n\n=\n\n=\n\n\n\n=\n\n. \n\n, \n\nk\n\nT\nk\n\nT\n\n\nv\n\n \n\n(22) \n\n=\n\n1\n\n\nk\n1\n+ \nD \nk\nk\n1\n\n\n\nv\n\nk\n\nn\n0\n=\nk\n[\n\n=\n\n1\n+\n\n1\n\n(\n\nk\n\nT\nn\n\n\n\n\n\nD v\nk\n[ ]\nk\n\n\n\nD\n1]\n\nT\nk\n\n1\n\n\n+\n\nT\n\n\nv\n\n)\nD .T\nk\n[ ]\nk\n\n \nFor  the  endogenous  noise,  instead  of  repeating  the  steps \nabove, one can compute  the estimation more elegantly. Thus, \nit is easy to see from (20), (21) and (1) that:  \n \n\nA  D v\nk\n[ ]\nk\nk\nk\n \nSince  the  noises  estimations  are  so  correlated,  the  estimation \nkwF \nF\nof \n follows straightforwardly:  \n[ ] T\nk\nk\n\n\nF w . \nk\n[ ]\nk\n\n\nA x\nk\n[ ]\nk\n\nB u\nk\n[ ]\nx\nk\n\n1]\n+ \n\n(23) \n\n\nx\n[\n\n\n\n=\n\n=\n\nk\n\n \nF \nk\n\n\nw\n1\n+\n\n=\n\nk\n\n1\n\nF\nk\n[ ]\nT\nk\n\n=\n\nk\nF \nk\nk\n1\n\n\n\nw\n\n(\n\n1\n1\n+\nk\n[\n\nk\n\nn\n0\n=\nF\n1]\nT\n\nk\n1\n\n\n\n\nF w w\nn\n[ ]\nn\n\nT\n\nF\nn\n[ ]\nT\nn\n\n=\n\n+\n\n\nA  D v\nk\n[ ]\nk\nk\nk\n\n\nv\n\nT\n\n)\nD  A .\nk\n[ ]\nT\nT\nT\nk\nk\nk\n\n (25) \n\n \nEquations  above  can  be  gathered  together  into  a  numerical \nrecipe  aiming  to  predict  the  discrete  stochastic  states  and \noutputs of model (1). The main steps are as follows.  \n\n1\n\n\nv\n\n\nw\n\nnx\n\n(cid:190) Input  data:  a  small  collection  of  time  series  values  (the \n{\n}\n= y\ntraining set \n) yielding initialization.  \nn \n[ ] n\nD\n0\nN\n1,\n0\n1.  Initialization.  Produce  the  first  state  representation  (1). \nThen  complete  the  initialization  by  setting:  an  arbitrary \n\n0x\nP\nI\nstate  vector \n,  the  covariance  matrices \n  (with \n= \nnx\n0\n  R ), \nD \nF \nF\n0\n0\nD\n and \n.  \n[ 1]\n[ 1] T\nT\n\n\n=\n\n=\n1\n1\n1\n+\n\n\n\n\n2.  For \n:  0\nk \n2.1.  Estimate the exogenous mixed noise:  \n\n\nC x\nD v\ny\nB uk\nk . \nk\nk\nk\n[ ]\n[ ]\n[ ]\n[ ]\n y\n\n=\nk\nk\n2.2.  Update the covariance matrix of exogenous noise:  \n1\n(\n\n\nv\nD v\nD \nD \nD\nD\nDT\nk\nk\nk\nk\nk\n[ ]\n[ ]\n1]\n[ ]\n[\nT\nT\nT\nk\nk\nk\nk\nk\nk\n1\n\nk\n1\n+\n\nk=Q C P\n.  \n2.3.  Compute the auxiliary matrix: \nk\nk\nD Q C\nD \nR\n2.4.  Invert the matrix: \nk\n [ ]\nT\nT\n=\n+\nk\nk\nk\nk\nk\nv\n= Q R . \n2.5.  Evaluate the sensitivity gain: \nT\n1\n\nk\nk\nk\nk=S\nA  k\n2.6.  Compute the auxiliary matrix \n.  \nk\n\nR\n\nny ny\n\n\n\n\n. \n\n. \n\n=\n\n\n\n+\n\n)\n\n1\n\n\nny\n\n\nv\n\n\nv\n\n\f \n \n\n \n\nk\n\n. \n\n+\n\n.  \n\n]\n1\n\n2.7.  Update the covariance matrix of endogenous noise:  \n1\n)\n(\n\n\nD S\nS D v\nF \nF \nF\nF\nv\nk\nk\nk\nk\nk\n[ ]\n1]\n[ ]\n[ ]\n[\nT\nT\nT\nT\nT\n+\n\n=\n\n\nk\nk\nk\nk\nk\nk\nk\nk\n1\n1\nw\nw\n\n\nk\n1\n+\n2.8.  Update  the  covariance  matrix  of  estimation  error: \n(\n)\n\n\nA P  Q A\nF \nP\nF\n.  \nk\n[ ] T\nT\n+ =\n+\n\n\nk\nk\nk\nk\nk\nk\nk\nk\nw\n1\n2.9.  Predict the state: \n\n\n\nS D v\nB u\nA x\nx\n. \n]k\nk\nk\nk\n[ ]\n[ ]\n[\n[\n1]\nx\n+\n+\n+ =\nk\nk\nk\nk\n\n\nC x\nB u\ny\n2.10. Predict the output: \nk\nk\n[\n1]\n[\n1]\n[\ny\n+ +\n+ =\nk\nk\n{\n}\n+y\n2.11. Acquire new data: \n.  \nk\n[\n1]\n+ = \nD\nD\nk\nk\n1\n2.12. Update the state model.  \n(cid:190) Output data:  \npredicted time series values  {\n}\n [ ] kk\ny\nN ;  \n\nestimated covariance matrices  {\n}\nN .  \nvD \nD\nk\n [ ]\nk\n\nk\nThe  most  time  consuming  steps  of  algorithm  above  is  2.12 \n(state  model  matrices  updating),  followed  by  2.4  (matrix \ninversion).  The  algorithm  above  can  easily  be  adapted  to \nmulti-step  prediction,  thanks  to  Markov  estimator.  In  this \ncase, the algorithm has two stages. The first one is concerned \nwith  the  model  adaptation.  In  the  second  one,  multi-step \nprediction is performed. The main difference between the two \nalgorithms (one step and multi-step prediction) consists of the \nexogenous noise estimation. As long as the measured data are \navailable, equation (21) can successfully be employed. When \nthe measured  data  are missing,  the  exogenous  noises  have  to \nbe  estimated  by  a  different  technique.  For  example,  MIMO-\nARMA(X)  models  can  be  employed  in  this  aim;  the \nestimated  white  noises  can  directly  stand  for  mixed \nn=D\nI y\nexogenous noises (since, usually, \n).  \nn\n\nT\nk\n\nD \nD\nN\nk\nEstimations  of  covariance  matrices \n \n [\n]\nT\n+\nN k\nN k\ny\nv\ny +\n+\ny\nare  necessary  both  to  assess  the  prediction  performance  and \nto  estimate  the  SNRs.  Thus,  the  diagonal  of  each  matrix \nreturns  the  set  {\n}2\n j k\n,  whose  values  play  the  role  of \n\n,\nj\nny\n1,\nyN\nprediction errors variances on every channel. Here, \n is the \nk\nK\ndata  length  and \n  is  the  current  prediction  step  on  the \n1,\nprediction horizon. Then the following two types of SNR can \nbe  evaluated  (one  for  measured  data  and  another  one  for \npredicted data):  \n \nSNR\n \ny  are standard deviations of data on measuring \n, \nwhere \nK\njy\nj\nand  prediction  horizons,  respectively;  also, \n  is  the \nK\ny\n\ny\nj\nj\nstandard  deviation  of  prediction  error.  The  SNRs  (26)  allow \none to define the prediction quality (\nPQ) cost function below:  \n \n\nj\nn\n  y  (26) \n1,\n\n(\ndef\n= \n\n,1\n &\n2\nj\n\ndef\n= \n\nSNR\n\n)2\n\n/j\n\nK\ny\nj\n\nK\ny\nj\n\n(\n\n)\n\n\n\nK\nj\n\n2\ny\n\n\ny\n\nj\n\nj\n\n2\n\n/\n\n,\n\n\n\ndef\n\nPQ 100 / 1+\n=\n\nj\n\n\n\n\n\ne\n\nj\n\n\n\n\n2\nj k\n,\n\nK\n\nk\n1\n=\nSNR SNR\nj\n\n\n\n\n\n\n\nK\nj\n\n[%]\n\n, \n\nj\n \n\n1,\n\nny\n\n. \n\n(27) \n\n \n\n    \n\nPQ T\n\nny\n\n(cid:34)\n\nPQ\n\n= \n\n1PQ\n\n,  the better  the \n\nThe  bigger  the norm of \npredictor performance.  \nPQ\n \nFinding the structural indices that maximize the norm of \ncannot  be  realized  through  an  exhaustive  search.  The \np\nstructural  indices  are: \n    the  degree  of  polynomial  trend, \n   the orders of ARMA model and  nx    the number \nna nc\n, \nof  states  for  the  linear  system  (1). An  evolutionary  searching \ntechnique has to be employed in this aim.  \n\n5. SIMULATION RESULTS \nAn  application  coming \nfrom  Meteorology  has  been \nconsidered.  Daily  minimum  and  maximum  temperatures  of \ntwo  cities  have  been  monitored  and  predicted  (as  Fig.1 \nsuggests). The cities are 60 km far each other on a plain. The \ndata  block  consists  of  482  samples  on  4  channels.  Two \npredictors  are  compared  in  terms  of  PQ:  PARMA  and \nKARMA. The first one is based on ARMA prediction of each \nchannel  in  isolation.  The  second  one  relies  on  the  adapted \nKalman \nfilter  predictor.  Both  algorithms  have  been \nimplemented  within  MATLAB  environment.  In  order  to  find \noptimal  structural  indices,  the  technique  from  (Kennedy, \n1997)  has  been  adopted.  There  are  many  implementation \ndetails that cannot be described here. Just one word regarding \nKARMA:  numerical  stability  of  the  algorithm  required \nspecial attention at step 2.5.  \nFor  each  one  of  the  final  figures  (2-9),  three  variations  are \ndepicted: the original data with the deterministic model (trend \nand  seasonal variation,  if any) on  top,  the  residual noise with \nestimated  SNR  in  the  middle  and  the  performance  on  the \nprediction  horizon  at  bottom.  Although  the  predictability \nvaries  from  a  channel  to  another,  KARMA  succeeded  to \nperform  better  than  PARMA  (higher  PQ  and  SNR  values). \n(This  result  was  confirmed  by  other  data  blocks  as  well, \nwhere  correlation  between  channels  exits.)  However,  in \ngeneral,  PARMA  has  superior  performance  on  data  blocks \nwith  (almost)  uncorrelated  channels.  Below,  the  PQ  values \nand norms are shown:  \n \nPQ\n[46.00 62.65 65.12 63.23]\nT\nARMA =\nPQ\n[49.59 70.18 79.47 71.70]\nT\nKARMA =\n \n \nOnly  4  states  were  necessary  to  represent  the  linear  system \nnx\nny=\n  in  this  case  is  pure \nassociated  to  data.  The  fact \ncoincidence.  On  the  figures  corresponding  to  KARMA \nperformance  (Figs. 6-9),  the  only  purpose  of  ARMA  models \nis  to  estimate  the  input  colored  noises  that  stimulate  the \nsystem.  The  data  on  the  first  channel  are  seemingly  the  less \npredictable. This  is proven by  the modest PQ values returned \neven  by KARMA  (only  slightly  superior  to PARMA  one).  It \nseems  that  data  from  this  channel  are  less  correlated  to  data \nfrom  the  other  channels,  which  cannot  be  noticed  by  simply \ninspecting the data.  \n\nPQ\n;\n119.50\nARMA \nPQ\n. \n137.26\nKARMA \n(28) \n\n\n\n\n\n\f \n\n \n \nThe  prediction  accuracy  has  increased  at  the  expense  of \ncomputational complexity for KARMA. Therefore, if the data \nare  quite  uncorrelated  across  channels,  PARMA  should  be \nemployed as the first option.  \n\n6. CONCLUSION \nOne can  say  that KF  is a new and old  topic at  the same  time. \nConcerning  the  theory,  KF  has  drawn  the  bottom  line  long \ntime ago. The applications rejuvenate however this approach. \nThe KF-based algorithm  introduced  in  this article  is genuine. \nIts major contribution consists of noises estimation during the \nprediction. The most KF algorithms try to avoid this problem. \nThe simulation case study on natural data has proven that the \nprediction  quality  can  be \nimproved  when  considering \ncorrelations between channels.  \n\nREFERENCES \nHajiyev  Ch.  and  Caliskan  F.  (2003).  Fault  Diagnosis  and \nReconfiguration  in  Flight  Control  Systems.  Kluwer \nAcademic, Boston, USA.  \nKalman  R.E.  (1960).  A  New  Approach  to  Linear  Filtering \nand Prediction Problems. Transactions of ASME, Journal \nof Basic Engineering, Vol. 82D, pp. 35-45. \nKalman  R.E.  and  Bucy  R.S.  (1961).  New  Results  in  Linear \nFiltering  and  Prediction  Theory.  Transactions  of  ASME, \nJournal  of  Basic  Engineering,  Series  D,  Vol.  83, \npp. 95-108.  \nKayton  M.  and  Fried  W.R.  (1997).  Avionics  Navigation \nSystems. John Wiley & Sons Inc., New York, USA.  \nKennedy \nJ.,  Eberhart  R. \n(1995).  Particle  Swarm \nOptimization,  IEEE  International  Conference  on  Neural \nNetworks, Piscataway, USA, pp. 1942-1948.  \nNegenborn  R.  (2003).  Robot  Localization  and  Kalman \nFilters.  Ph.D.  Thesis,  Delft  University  of  Technology, \nDelft, NL.  \nNiedwiecki  M.  (2000).  Identification  of  Time-Varying \nProcesses. John Wiley & Sons, West Sussex, U.K.  \n(1996),  Subspace \nvanOverschee  P.  and  deMoor  B. \nIdentification \nof \nLinear \nSystems: \nTheory, \nImplementation,  Applications.  Kluwer  Academic \nPublishers, Holland.  \nPlackett  R.L.  (1950).  Some  Theorems  in  Least  Squares, \nBiometrika, No. 37, pp. 149157. \nProakis  J.G.  and  Manolakis  D.G.  (1996).  Digital  Signal \nProcessing.  Principles,  Algorithms  and  Applications., \nthird  edition,  Prentice  Hall,  Upper  Saddle  River,  New \nJersey, USA.  \nSderstrm  T.  and  Stoica  P.  (1989).  System  Identification, \nPrentice Hall, London, UK.  \nStefanoiu D., Culita J. and Stoica P. (2005). A Foundation  to \nSystem  Identification  and  Modeling.  PRINTECH  Press, \nBucharest, Romania.\n\n \n \n \n \n \n \n\n \n\n    \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\f \n \n\nFig. 2. PARMA performance on channel 1. \n\nFig. 3. PARMA performance on channel 2. \n\nFig. 4. PARMA performance on channel 3. \n\nFig. 6. KARMA performance on channel 1. \n\nFig. 7. KARMA performance on channel 2. \n\nFig. 8. KARMA performance on channel 3. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5. PARMA performance on channel 4. \n\nFig. 9. KARMA performance on channel 4. \n\n \n\n    \n\n\f", 
        "tag": "General Literature", 
        "link": "https://arxiv.org/list/cs.GL/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\na\nM\n \n0\n3\n \n \n]\nR\nG\n.\ns\nc\n[\n \n \n1\nv\n5\n0\n4\n0\n1\n.\n3\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAutocomplete3DSculptingMengqiPengJunXingLi-YiWeiTheUniversityofHongKongAbstractDigitalsculptingisapopularmeanstocreate3Dmodelsbutre-mainsachallengingtaskformanyusers.Thiscanbealleviatedbyrecentadvancesindata-drivenandproceduralmodeling,albeitboundedbytheunderlyingdataandprocedures.Weproposea3Dsculptingsystemthatassistsusersinfreelycre-atingmodelswithoutpredenedscope.Withabrushinginterfacesimilartocommonsculptingtools,oursystemsilentlyrecordsandanalyzesusersworkows,andpredictswhattheymightorshoulddointhefuturetoreduceinputlabororenhanceoutputquality.Userscanaccept,ignore,ormodifythesuggestionsandthusmain-tainfullcontrolandindividualstyle.Theycanalsoexplicitlyse-lectandclonepastworkowsoveroutputmodelregions.Ourkeyideaistoconsiderhowamodelisauthoredviadynamicworkowsinadditiontowhatitisshapedinstaticgeometry,formoreaccu-rateanalysisofuserintentionsandmoregeneralsynthesisofshapestructures.Theworkowscontainpotentialrepetitionsforanalysisandsynthesis,includinguserinputs(e.g.penstrokesonapres-suresensingtablet),modeloutputs(e.g.extrusionsonanobjectsurface),andcameraviewpoints.Weevaluateourmethodviauserfeedbacksandauthoredmodels.Keywords:workow,autocomplete,clone,beautication,sculpt-ing,modeling,userinterfaceConcepts:Human-centeredcomputingUserinterfacede-sign;ComputingmethodologiesShapemodeling;1Introduction3Dmodelingisubiquitousinvariousapplications,butdemandssig-nicantexpertiseandeffortstocreateoutputswithsufcientqual-ityandcomplexity.Thisisparticularlysoformodelsconsistingofrepetitivestructuresanddetails,ascommoninmanynaturalandman-madeobjects.Tohelpauthor3Dmodels,signicantresearchhasbeendevotedtomethodsbasedondata[Funkhouseretal.2004;Mitraetal.2013]orprocedures[Emilienetal.2015;Nishidaetal.2016].Thesemeth-odsmainlyfocusonwhatthemodelisshapedinsteadofhowitisauthored,andtheoutputscopeisdelineatedbytheunderlyingdataorprocedures.Theprocessesofhowmodelsareauthoredorcre-atedbyusers,termedworkows,containrichinformationthatcanfacilitateavarietyofmodelingtasksbasedonindividualuserstyles[DenningandPellacini2013;Chenetal.2014;Denningetal.2015;Salvatietal.2015].However,itremainsunclearwhetherandhowsuchworkowscanhelpuserscreate3Dmodels,especiallyunderinteractiveinterfacessuchasdigitalsculpting,apopularmeanstoauthororganicshapeswithindividualstyles.Weproposea3Dsculptingsystemthatassistsusersinfreelycre-atingmodelswithoutpre-existingdataorprocedures.Withacom-monbrushinginterface,oursystemanalyzeswhatusershavedoneinthepastandpredictswhattheymightorshoulddointhefuture,toreduceinputworkloadandenhanceoutputquality.Thepredic-tionsarevisualizedassuggestionsovertheoutputmodelwithoutdisruptinguserpractices.Userscanchoosetoaccept,ignore,ormodifythesuggestionsandthusmaintainfullcontrol.TheycanFigure1:Userinterfaceofoursystem.Theinterfaceconsistsofasculptingcanvas(left)andawidgetpanel(right).Thewidgetpanelprovidestheusualsculptingtools,brushparameterssuchassize,andmodecontrolsuniquetoourautocompletesystem.alsoselectpriorworkowsfromthemodelandcloneoverotherre-gions.Therichinformationcontainedintheworkowsallowsourmethodtooutperformpriormethodsbasedongeometry.Similartoexistingsculptingtools,ourinterfaceprovidessurfacebrushesforlocaldetailssuchasdisplacementsandfreeformbrushesforlargescalechangessuchasextrusions.Oursystemisintendedforuserswithvaryinglevelsofexpertiseandmodelsofdifferenttypeswith-outrequiringpre-existinggeometrydataorproceduralrules.Inad-ditiontosculptingstrokes,ourmethodalsoconsiderscameramove-ments,whichareoftenrepetitive,predictable,andcorrelatewellwiththeunderlyingshapesandbrushstrokes[Chenetal.2014].Inspiredbyrecentworksonpredictiveuserinterfacesthatanalyzeworkowstoassist2Ddrawings[Xingetal.2014]andanimations[Xingetal.2015],ourpremiseisthat3Dsculptingoftenconsistsofrepetitiveuseractionsandshapestructures,andthusboththeinputprocessandoutputoutcomecanbepredictable.However,unlike2Ddrawingsandanimationsinwhichthebasedo-mainisasimpleplanarcanvas,for3Dsculptingthebasedomainisa3Dobject.Thus,eventhoughsurfacebrushesplacelocaldis-placementsanalogoustopaintbrushesaddlocalcolors,theycanre-sideoversurfaceregionswithdifferentorientationsandcurvatures.Furthermore,unliketheplanarcanvaswhichremainsinvariantdur-ing2Dsketching,the3Dshapecanundergolargescalechangesbyfreeformbrushes.Suchchangesmightnotevenbefunctionsoverthedomainsurfacesandthuscompletelybeyondthemethodologiesin[Xingetal.2014;Xingetal.2015].Ourkeyideaistofactoroutthecontextualpartsofworkowpositionsviaproperlocalparam-eterizationsduringanalysis(e.g.repetitiondetection,cloneselect)andfactorbackthecontextsduringsynthesis(e.g.suggestion,clonepaste)whilekeepingtrackofgeometrysignaturessuchassurfacenormalandcurvatureallthetime.Formoreholisticanalysisandprediction,wecombinedifferentaspectsoftheworkows,includ-ing2Dinputs(e.g.penstrokesonapressuretablet),3Doutputs(e.g.brushesonanobjectsurface),andcameramovements.Weconductapilotuserstudytoshowthatoursystemcanhelp\fusersonbothobjectiveperformanceandsubjectivesatisfactionforavarietyofoutputmodels.Insum,themaincontributionsofthispaperinclude:Theideathatdynamicworkowscanaugmentstaticgeome-tryforbetterinteractive3Dmodeling;Anautocompleteuserinterfaceformorefriendlyandeffec-tive3Dsculptingviahint/suggestion,workowclone,cameracontrol,andotherfeatures;Methodsthatanalyzewhatusershavedoneinthepasttopre-dictwhattheymayand/orshoulddointhefuture,basedonasimilaritymeasureconsideringlocalframeandgeometry,andcombinationsof2Dinputs(e.g.onatablet),3Doutputs(e.g.extrusionsonanobjectsurface),andcameraviewpoints.2PreviousWorkData-drivenandproceduralmodelingCreatingmodelsfromscratchischallenging,butsimilarobjectsorpartsoftenalreadyexist.Analyzingexistingmodelgeometryfornovelsynthesishasbeenaveryactiveareaofresearch[Funkhouseretal.2004;Mi-traetal.2013]encompassingavarietyoftopics,suchassugges-tion[ChaudhuriandKoltun2010],repetition[Bokelohetal.2011],symmetry[Mitraetal.2006],style[Lunetal.2016],fabrication[Schulzetal.2014],andfunctionalinteraction[Huetal.2016].Commonmodelstructurescanalsobeabstractedintoproceduralrulesforofineorinteractivemodeling[Ebertetal.2002;Emilienetal.2015;Nishidaetal.2016].Theoutputsofthesemethodsarenaturallylimitedbythescopesoftheunderlyingdataandprocedures.Ourmethod,incontrast,aimstoassistuserstoexploreandcreatemodels[Cohen-OrandZhang2016]intheirindividualstylesandpreferences.ModelinginterfacePopularmodelinginterfaceshavebeende-signedformechanicmodels(e.g.AutoCAD)andorganicshapes(e.g.ZBrush).Avarietyofenhancementshavealsobeenproposedforstability[Umetanietal.2012],printability[Zehnderetal.2016],recongurability[Gargetal.2016],tiling[Guerinetal.2016],andcollaboration[Taltonetal.2009].Followingthelineofsuggestiveinterfacesfor3Ddrawingandsketching[IgarashiandHughes2001;Tsangetal.2004],oursys-temaimsforatraditionalsculptinginterfacewithenhancementinanalyzing,predicting,andsuggestingworkows.Workow-assistedauthoringWorkows[NancelandCock-burn2014]havebeeninvestigatedforvariousauthoringtasksin2Dimageediting[Chenetal.2011;Chenetal.2016],sketching[Xingetal.2014],andanimation[Xingetal.2015],aswellasin3Dmodelingsuchasvisualization[Denningetal.2011;Denningetal.2015],revisioncontrol[DenningandPellacini2013],viewselection[Chenetal.2014],andcollaboration[Salvatietal.2015].Theseworkowscanberecordedduringauthoring,orinferredaposteriori[Fuetal.2011;Huetal.2013;Tanetal.2015].Asreportedin[Santonietal.2016],evenforcomplexobjects,theworkowsoftenconsistofpredictablebrushchoicesandopera-tions.Ourmethodanalyzes3Dsculptingworkowstoautocom-pletepotentialrepetitions.3UserInterfaceTheuserinterface(Figure1)ofourprototypesculptingsystemfollowsthebrushmodelasinpopulardigitalsculptingtoolssuch(a)usersculpting(b)hints(c)acceptall(d)brushselectFigure2:Hintexample.Duringusersculpting(a),oursystemsug-gestspotentialfuturerepetitionsasdisplayedintransparentyellow(b).Userscanignorethehintandcontinuesculpting,acceptallhintsviaahotkey(c),orpartiallyselectasubsetofthehints(d)asshownintransparentblue.asBlender[BlenderFoundation2016],Sculptris[Pixologic2011],Zbrush[Pixologic2015],MeshMixer[SchmidtandSingh2010],etc.Oursystemsupportssurfacebrushesforlocaldetailsviadis-placement,andfreeformbrushesforlargescalechangesuchasex-trusion.Userscansculptasusualwhileoursystemsilentlyrecordsandanalyzesthesculptingworkows.Allbrushoperationscanbecombinedwiththemainfunctions:hint,workowclone,cameracontrol,aswellasotherfeaturessuchasworkowlock.3.1HintOursystemautomaticallyanalyzesuserssculptingworkowsontheyandpredictswhattheymightorshouldsculptinthenearfuture.Thesepredictionsaresuggestedashintsonouruserinter-face.Figure2showsanexamplewheretheuserissculptingde-tailedfeaturesonanobject.Asmorebrushesareadded,oursystemautomaticallyanalyzesthepaststrokesandpredictswhatbrushestheusermightwanttoperformnext,asshowninFigure2b.Thesuggestionsareshowntransparentlyovertheobjectsurface.Userscanignorethesuggestionsandcontinuesculptingasusual,acceptallthesuggestionsviaahotkey(Figure2c),orpartiallyacceptthesuggestionswithaselectionbrush(Figure2d).Thesuggestionsarecontinuouslyupdatedinreal-timeaccordingtouserinputs.3.2WorkowCloneTheclonetooliscommonamonginteractivecontentauthoringsystems.Priormethodsmostlyclonestaticcontentsuchasim-ages[Perezetal.2003],textures[Sunetal.2013],orgeometry[Takayamaetal.2011].Themethodsin[Xingetal.2014]canclonesketchingworkows.Oursystemallowsuserstoclonesculpting\f(a)rstcopysource(b)rstpastetarget(c)secondcopyandpaste(d)mergedcloneeffect(e)freeformcopysource(f)freeformpastetargetFigure3:Workowcloneexample.Thered/greenbrushmarkstheclonesource/target.Thecloneresultsarepreviewedasyellowin(b)and(c)fortheuserstopartiallyorfullyaccept.(c)showscloneappliedoverpriorclonedregionin(b),(d)showsthemergedeffect.Theclonecanbeappliedtobothsurfaceandfreeformbrushes.workowswithmoreinformationandoptionsthancloningstaticgeometry.Viaourbrushinginterface,userscanselectsourceandtargetregions,andparameterssuchaspositions,sizes,anddirec-tions.Similartopriorclonetools[Kloskowski2010],oursystempreviewsthecloneoutcomesforwhichuserscanacceptorignore.Furthermore,ourworkowclonecanbeappliedtoalreadyclonedregions,whichisdifculttoachieveviageometryclone.Anexam-pleisshowninFigure3.3.3CameraControl3Ddigitalsculptinginvolvesnotonlybrushstrokesbutalsocameramanipulations,whichcanalsobequitetediousandrepetitive.For-tunately,similartosculptingoperations,cameracontrolsalsotendtobepredictable[Chenetal.2014].Weprovideabasiccameracontrolmodethatautomaticallymovestheviewpointsalongwiththesuggestedhints(Section3.1).Thischoiceisinspiredbytheobservationin[Chenetal.2014]thatusersoftenviewthetargetsculptingregionsfrontal-parallel,andthusmorenaturalandlessdisorientingforusersthanotherformsofcameracontrol.Userscanquicklypressahotkeytoreturnto(a)before(b)afterFigure4:Cameracontrolexample.Thecameraautomaticallyad-juststheviewpointfrom(a)accordingtothesuggestedbrushesin(b).theoriginalviewpoint.Theycanalsoturnthismodeonoroffde-pendingontheirpreferences.OneautomaticviewpointexampleisshowninFigure4.3.4AdditionalFeatures(a)sculptingviewpoint(b)shiftedviewpointFigure5:Occlusionexample.Theblueandredstrokesarepredic-tionsfromthethreeprevioussculptingstrokes,withandwithoutcon-sideringocclusion.(a)istheoriginalsculptingview,and(b)shiftstheviewtoshowthepredictedstrokesunderneaththeocclusion.OcclusionOursystemconsidersgeometryocclusionforpredic-tionsandsuggestions.AsexempliedinFigure5,suggestionspre-dictedontheview-plane(likethecanvasfor2Dpredictionin[Xingetal.2014;Xingetal.2015])mayresideonthewrongpartoftheoccludinggeometryduetoviewpoint,asshownintheredstrokes.Oursystemconsiderstheunderlyingshapeandpropagatesthepre-dictionsmoreaccurately,asshowninthebluestrokes.SymmetrySimilarto[Calabreseetal.2016]andexistingmain-streamsculptingtools,weprovidesymmetrymodetomirrorallbrushstrokesontheothersideofthesamemodel.Userscanturnthesymmetrymodeonandoff,andwhenenabled,thepredictionsandsuggestionswillautomaticallyconsidersymmetry.LockLockingisanoptionundersomedigitalsculptingtools(e.g.[BlenderFoundation2016])tokeepsomepartsofthemodelxedwhileusersmanipulatingotherparts.Thisfeature,whileuseful,canbetediousanddifcultforusers,especiallynovices.Inadditiontofutureoperations,oursystemcanalsopredictwhatmightneedtobelockedbasedonworkows,asexempliedinFigure6b;suchscenarioscanbechallengingtoanalyzeviageometryonly.Thislockingmechanismcanbeappliedtonotonlyexistinggeometryasdescribedabovebutalsofuturegeometrypredictedbyoursystemwhichcannotbemanuallylocked,asshowninFigure6c.Userscaneasilyenableordisablethelockingeffectsviaahotkeyandthusmaintainfullcontrol.\f(a)initialmanualstroke(b)moremanualstrokesandhints(c)afteracceptinghintsFigure6:Workowlock.Afteramanualstrokein(a)underthesymmetrymode,theuserwentontoplacetwomorestrokesin(b).Theyellowpartsindicatesuggestedhints.Forcomparison,theleftsidehasnoworkowlock;noticehowearlierstrokescanbeunintentionallydeformedbythelaterstrokes.Ourworkowlockcanpreventthisfromhappeningforbothexistinggeometryandacceptedhints,asshownontherightsideof(c).Notethatthepredictedstrokes(yellow)arealwayscorrect,withorwithoutworkowlock.However,whentheuseracceptedthehints,theywillplayoutintheworkoworderasinmanualstrokes.Thus,withoutworkowlock,laterhintstrokescanstilldeformearlierhintstrokes.(a)brushing(b)stipplingFigure7:Autocompletesurfacepaintingexamples.Hintsareshowninyellowfortheuserstopartiallyorfullyaccept.PaintingAftersculpting,userscanpaintcolorsoverthemod-els,similartopaintingin[Pixologic2011].Oursystemcanalsoauto-completessurfacepaintingbyextendingthe2Dautocompletemethodin[Xingetal.2014]over3Dsurfaces.AnexampleisshowninFigure7.4MethodWedescribealgorithmsbehindourautocompletesculptinguserin-terfaceinSection3.4.1RepresentationBrushOursystemsupportstwomaintypesofbrushesasincom-mondigitalsculpting:surfacebrushesforsmallscaledisplace-ments(e.g.clay,crease,smooth),andfreeformbrushesforlargerscaleshapedeformation(e.g.extrusion[Santonietal.2016;Den-ningetal.2015],drag,grab).Theyarerepresentedasfollows.SampleWerepresenteachbrushstrokebasacollectionofpointsamples{s}.Eachsamplesisassociatedwithasetofattributesu:u(s)=(p(s),a(s),t(s))(1),wherep(s)isthe3Dpositionofs,a(s)isasetofappearancepa-rameters(suchassize,type,andpressure)andgeometrysignatures(suchasnormalandcurvature),t(s)indicatestemporalparametersabcde(a)surfacebrushabcde(b)freeformbrushFigure8:Brushtypes.Asurfacebrush(a)hasallsamplesontheobjectsurface,suchasthe5surfacesamplessa,sb,sc,sd,andse.Afreeformbrush(b)hastherstsamplesaontheobjectbuttherestfor3Dmovementssuchasextrusionsasbscsdse.thatincludetheglobaltimestampandasample-idfortherelativepositionwithinthestroke.AsshowninFigure8,forasurfacebrushb,itssamplesposi-tionsp(b)={p(s)}sballlayontheobjectsurface;whileforafreeformbrush,p(b)consistsoftwoparts:therstsampleisonthesurface,andtherest(kp(b)k1)samplesarein3Dfreespacewithmovementdirectionscontrolledbytheusers.MeshWeadoptamesh-basedrepresentationwithtwooperators,sculptcandmeshm,tosupportgeometryandtopologychanges.AmeshMisrepresentedbyasetofelementsincludingvertices,edges,andfaces.Eachsculptoperatorcappliesspecicgeometrytransformationtomeshelements,suchasvertexpositions,withinanitesupportdenedbythebrushradius.Ameshoperatormcanchangetheunderlyingmeshresolutionandtopologybyaddingorremovingmeshelements.TheresultofeachbrushstrokeoverMisthejointeffectofcwithm:M(cm)(M)(2),wherecombinescandmtoachieveBlender-Dyntopo-likeorSculptris-likeadaptivetessellationeffect,asshowninFigure9.4.2MeasurementAnalogoustopriormethodsinpredicting2Dsketch[Xingetal.2014]andanimation[Xingetal.2015],acorecomponentforour\fb1b2b3b4Figure9:Meshsculptingeffects.Asculptoperatorcsuchasdragcaninuencemeshgeometrybutnottopologyasshowninb1andb3withdifferentbrushradii.Ameshoperatormcanchangemeshresolutionandconnectivityasshowninb2andb4.methodistomeasuresimilaritybetween3Dbrushstrokesbasedontheirspatial-temporalneighborhoods.Thissimilarityinturnen-ablesourmethodtodetectrepetitions,suggestfutureedits,cloneworkows,andauto-lockbrushes.However,unlike[Xingetal.2014;Xingetal.2015]wheretheunderlyingdomainisaxed2Dplane(drawingcanvas),ourbasedomainisa3Dobjectunderdynamicmodication.Thus,alldenitionsofneighborhoodandsimilaritymustbeconditionedon3Dobjectsurfaces.NeighborhoodWedenetheneighborhoodn(s)ofasamplesasthesetofallsampleswithinitsspatial-temporalvicinityanal-ogoustothespatial-temporalneighborhoodsin[Maetal.2013].Eachspatialneighborhoodisorientedwithrespecttoalocalframeoassociatedwiths.Forsurface-brushsamples,allspatialdistancesarecomputedgeodesically(Figure8a),whileforfreeform-brushsamplesviatheirfreespacesampledistances(Figure8b).Thetem-poralneighborhoodiscausalandcontainsonlysamplesdrawnbe-fores.Brushstrokesarecomposedofsamplesandcouldcapturethehigh-levelrelationshipsbetweenoneanother.Thusanalogousto[Xingetal.2014;Xingetal.2015],weusebrushstrokesasthefunda-mentalunitsforsculptingworkowanalysisandsynthesis.Theneighborhoodofastrokebisdenedastheunionofitssampleneighborhoods:n(b)=[sbn(s)(3)SimilarityForeachneighborhoodsamples0n(s),wedeneitsdifferentialwithrespecttosas:u(s0,s)=wpp(s0,s)waa(s0,s)wtt(s0,s)(4),wherep,a,andtrepresentthesamplepairdifferentialsinpositionp,appearancea,andtemporalparameterstdenedinEquation(1),andwp,wa,wtarethecorrespondingscalarweightings.Wecomputethesamplepositiondifferentialsp(s0,s)via:p(s0,s)=p(s0)p(s)(5),wherep(s)isthelocalpositionofswithframeo(s)asdescribedinSection4.3andrelatestotheglobalp(s)viaacoordinatetrans-formation.FromEquation(4),wedenethedifferentialbetweentwostrokesb0andbviatheirconstituentsamples:u(b0,b)=(cid:8)u(s0,s)|s0=m(s)b0,sb(cid:9)(6),wheremisthematchingsamplecomputedviatheHungarianal-gorithmasin[Maetal.2013;Xingetal.2015].FromEquation(6),wecancomputethedistancebetweentwostrokeneighborhoodsn(bo)andn(bi)asfollows:kn(bo)n(bi)k2=ku(bo,co)u(bi,ci)k2+Xb0on(bo),b0in(bi)ku(b0o,bo)u(b0i,bi)k2(7),wherethersttermmeasuresthedistancebetweenthetwostrokesboandbiwithrespecttotheircentralsamplescoandci:u(b,c)={u(s,c),sb}(8),andthesecondtermcomputesthedistancesbetweentheirneigh-borhoodstrokeswithrespecttoboandbi.Thestrokepairsb0oandb0iarematchedviatheHungarianalgorithmaswell.4.3ParameterizationSurfacestrokeparameterizationWeextendthestrokeparame-terizationmethodin[Schmidt2013]foroursurfacebrushes.Eachsurfacebrushisparameterizedbythesurfacenormalasthez-directionandthestrokepathasthey-directionmeasuredbythearc-lengtht.Thex-directionismeasuredbythegeodesicdistanced.Wethenapplythesingle-passforwardpropagation[Schmidt2013]toestimatetheparametrizationforanysampleswithindistancerofthestroke,asillustratedinFigure10.p(s)=Ps(s)=(ts,ds)(9)zyxxyzr(ts, ds)Figure10:Strokeparameterization.Thesurfaceandfreeformstrokeparameterizationsareshowninredandbluewiththeirregionsingreenandyellow.FreeformstrokeparameterizationUnlikethesurfacebrushes,freeformbrushesdonotadheretotheobjectsurfaces.Thusthemethodin[Schmidt2013]cannotdirectlyapply.However,wecanextenditintothefreeformspaceasfollows.Weusethebrushpathasthez-directionsimilartothey-directionforthesurfacebrushes,parameterizedbyarc-length.Thecross-productofthez-directionandthecameralook-atdirection(non-parallelforsculpting)foreachbrushsamplepointformsthey-direction.ThisisillustratedinFigure10.Unlikesurfacestrokeparameterizationwhichis2D,thefreeformstrokeparameterizationis3D:p(s)=Pf(s)=(xs,ys,zs)(10)\f4.4SynthesisInordertosynthesizethepredictionsinteractively,weextendthetextureoptimizationmethodology[Kwatraetal.2005;Maetal.2013;Xingetal.2014].WithIasthecurrentsequencesofstrokesorderedbytheirtime-stamps,wesynthesizethenextstrokeboviathefollowingenergyformulation:E(bo;I)=minbiI|n(bo)n(bi)|2+(bo)(11),wherebirepresentsthecorrespondinginputstrokewithsimilarneighborhoodtobo.Thersttermevaluatestheneighborhoodsim-ilaritybetweenbiandboasexplainedinEquation(7).Thesecondtermdenotesoptional,applicant-dependentspecicationsthatcanbesuppliedbytheusers.Similarto[Xingetal.2014],foreachoutput,westartwithmulti-pleinitializations,andselectthemostsuitableoneviasearchandassignmentsteps.However,insteadofaplanarcanvasthatremainsinvariantforpainting,forsculptingtheunderlyingdomainisa3DobjectunderdynamicmodicationwithdifferentbrushtypesasintroducedinSection4.1.Furthermore,allcomputationsneedtosupportinteractiveresponses.InitializationWeinitializefuturestrokesbasedonlocalsimilaritywiththeexistingstrokes.Forthelastsculptedstrokeb0o,weiden-tifyacandidatesetofmatchingstrokes{b0i}.Eachb0iprovidesaninitializationbo,iviaitsnextstrokebi:u(bo,i,b0o)=u(bi,b0i)(12)Eachbo,iiscomputeddependingonthebrushstroketypesur-faceorfreeform,duetotheirdifferentparameterizationsasde-scribedinSection4.3.Forsurfacestrokes,Equation(12)iscom-putedonthelocalsurfaceparameterization.Forfreeformstrokes,Equation(12)iscomputedbyatwo-stepprocess:decidingthestart-ingpointonthesurface,followedbythefreeformspacemovementfromthestartingpoint.ThisisvisualizedinFigure11.b'1b'2b'3b'obo,1(a)surfacestrokeinitializationb'1b'2b'3b'obo,1(b)freeformstrokeinitializationFigure11:Synthesisinitialization.Forboth(a)and(b),thethreebluestrokesb01,2,3areplacedinorder,beforethecurrentstrokeb0oshowninred.Eachofb01,2,3canprovideapredictionbasedonitsnextstrokeb1=b02,b2=b03,b3=b0o,andb0oviaEqua-tion(12).Forexample,thegreenstrokebo,1ispredictedfromb01viau(bo,1,b0o)=u(b1=b02,b01).Forclarify,weuseboasthevariabletooptimizethechoiceofbo,ifordifferentmatching{b0i}ofb0o,byminimizingtheenergy:E(bo)=XsoboXs0ob0o(si,s0i)(cid:12)(cid:12)u(so)u(s0o)u(si,s0i)(cid:12)(cid:12)2si=m(so)bi,s0i=m(s0o)b0i(13),where(si,s0i)isaweightingparameterinspiredby[Maetal.2013]forGaussianfalloffwithpsetto10:(si,s0i)=exp |p(si)p(s0i)|2p!(14)Foreachinitializationbo,weoptimizeitbygoingthroughthesearchandassignmentstepsbelow,andtheonewhichhastheleastenergyinEquation(11)wouldbeconsideredasmostsuitableandselectedtobethepredictedstroke.SearchDuringthisstep,fortheinitializationboobtainedabove,withinitslocalspatial-temporalwindow,wesearchforthematch-ingstrokebiwhoseneighborhoodissimilarton(bo)bymea-suringtheneighborhoodsimilarityinEquation(7).Insteadofse-lectingonlyonematchingstroke,formorerobustoptimizationwesearchformultiplecandidates{bi}whoseneighborhooddissimi-larity|n(bo)n(bi)|2islowerthan2|n(bo)n(b00)|2,whereb00hasthelowestdissimilarityvalue.Foracceleration,similarto[Xingetal.2014],weperformtemporalmatchingfollowedbyspatialmatchinginsteadofmatchingwiththewholetemporal-spatialneighborhood.Intherststep,weconducttemporalmatchingtosearchthecandidatematchingstrokes,fromwhichweusespatialneighborhoodforfurtherltering.AssignmentTherstterminEquation(11)canbeexpandedviaEquation(7).Thesecondterm,,allowsuserstocongurevariousparame-tersettingsforvariouseffectssuchasdot,bumpy,orvarying-sizestrokesasin[Pixologic2015].Thiscanbeachievebyaddingcon-straintscforvarioussampleattributes:(bo)=Xsobo|u(so)c(so)|2(15)WedecidethenextstrokeviaminimizingEquation(11)withex-pansionsinEquations(7)and(15).4.5DeploymentBasedonthecommonframeworkinSection4.4,wenowdescribehowtosupportvariousmodesandoptionsinoursystem.FramechoiceForfreeformstrokesynthesis,bydefault,weuselocalframeforbettergeometryadaptation.Butforcertainsculptingtasks,usermightpreferglobalframetoachievespeciceffects.Oursystemthusallowsuserstoswitchtheframeoption.Figure12providesanexampleeffect.(a)localframe(b)globalframeFigure12:Framechoiceexample.Bydefaultoursystemsynthesizesinlocalframesasin(a)butuserscanalsooptfortheglobalframesasin(b).Theyellowpartsarehintedgeometry.\fHintThepredicted(Section4.4)andacceptedstrokesareren-deredinlighttransparentyellowandbluecolorstodistinguishthemfromtheexistinggeometry.WorkowcloneForworkowclone,wenormalizethestrokepa-rameterizationtosupportsourceandtargetregionsspeciedwithdifferentbrushlengths.Specically,asamplesinasurfacestrokewouldbenormalizedtobewithints[0,1],ds[1,1]via:tsts+rT+2rdsdsr(16),whereTisthestrokearclengthandristheparameterizationwidthrange,asillustratedinFigure10.Wealsonormalizethesample-idtofallwithin[0,1],where0and1representthestartingandendingpositionsofbrushb.A1B1A2B2Figure13:Workowlockbasedonspatial-temporalneighborhood.ThestrokesareplacedintheorderbAbB,andinasymme-trymodeforillustration.Theleftsideshowsgeneralsculptingeffectwithoutworkowlock,therightsideisaidedwithworkowlock.AnysamplesofbA1withinthespatial-temporalneighborhoodofbB1willbeautomaticallylocked,asexempliedintheyellowregionofthegreensampleinbB1.WorkowlockAutomaticallydeducingwhichpartsofthemodeltolockbasedongeometryalonecanbechallenging,asspatialinfor-mationmaynotconveyuserintention.Withworkows,ourmethodcandirectlyassociateallbrushstrokeswiththemodelgeometry,anddecidewhattolockbasedonworkowsimilarityasdescribedinSection4.2.Forexample,wecanlockpastworkowsampleswithinaspatial-temporalneighborhoodofthecurrentbrushstroke,asshowninFigure13.Basedonourexperiments,weadoptasimplestrategytolockallpastworkowswithaspatial-temporalneighborhoodofthecurrentbrushstroke.Thisstrategyworkswellwhenuserssculptinaspatially-temporallycoherentfashion,astheyoftendo.Thisisalsoonekeydifferencefrom[Xingetal.2014;Xingetal.2015]wherethesynthesizedsketchingstrokesarethenaloutputs.Incontrast,sculptingstrokescanaffectexistinggeometry.CameracontrolAsdescribedin[Chenetal.2014;Santonietal.2016],userbrushstrokestendtocorrelatewithcameramovementsandthuscanfacilitateviewpointselection.Oursystemstoresallcamerastates,includingpositionsandorientations,aspartofthesculptingworkows.Thus,ourmethodisabletopredictcameramovementsinadditiontosculptingbrushesasdescribedinSec-tion4.4.Inourexperimentswehavefoundthatexcessivecameraautomationcanbedisorientingtousers.Wethusexposeonlythebasicmodeofpilotingthecameraviewpointalongwiththepre-dictednextbrushstrokes.NeighborhoodandsearchwindowWesetrdynamicallytobe4thestrokeradius.Thespatial-temporalneighborhoodofabrushstrokeincludesitstwoprevioustemporalstrokesandnearbyspatialstrokesoverlappingitsparameterizationregion(Figure10).ForthesearchstepinSection4.4,wesearchwithinalocaltemporal-spatialwindowof20previoustemporalstrokes,andthesamespa-tialneighborhoodwindowasabove.NeighborhoodaccelerationToimprovequalityandspeed,weacceleratetheneighborhoodmatchinginEquation(7)byatwo-tieredsamplingprocessforthebrushstrokes.Specically,werstplacethreesamplesuniformlyovereachstroketoselectthemostsimilarcandidatestrokes,andcontinuewithallsamplestoselectthebestmatchesfromthecandidates.WeightsForEquation(4),wesetthepositionweightingwptobe1.Wesetwatobe1ifthereisnoterminEquation(11),oth-erwisewesetittobe0.1and0.9fortheneighborhoodandtermsThewtincludesglobaltimestampwt1andsample-idwt2.Wesetwt2tobe1,andwt1tobe100fortemporalneighborhoodmatchingtoenforcethesamesculptingorder,and0forspatialneighborhoodmatching.5UserStudyWehaveconductedapreliminaryuserstudytoevaluatetheus-abilityandefcacyofourassistedsculptingsystem.Thestudyconsidersthefollowingmodes:fullymanualauthoringasintra-ditionalsculptingtools,andourautocompletefunctionsincludinghint,workowclone,cameracontrol,andotherfeatures.SetupAlltaskswereconductedona13-inlaptopwithaWacomtablet.Thestudycontainsthreesessions:warm-up,targetsculpt-ing,andopencreation.Theentirestudytakesabout1.5hoursperparticipant.ParticipantsOurparticipantsinclude1experiencedsculptingmodelerand8noviceuserswithdifferentlevelsofsculptingandmodelingexperiences.WarmupsessionThewarm-upsessionisdesignedtohelppar-ticipantsgetfamiliarwiththesculptinginterfaceandvariousfunc-tionsofoursystem.Thetasksconsistofaddingdetailsviasurfacebrushes,performingextrusionsandcontourshapesviafreeformbrushes,andmovingtheuserviewpointsviacameracontrol.Oneoftheauthorsguidedtheparticipantsthroughouttheentireprocess.TargetsessionThegoalistomeasuretheusabilityandqualityofourassistedsystemcomparedtotraditionalsculptingtools.Weaskedourcollaboratingartiststocreateinitialinputandreferenceoutput(Figure14).Wethenaskedtheparticipantstostartfromtheinitialinputandreachthereferenceoutput.Eachparticipantperformedthistaskbothwithandwithoutassistedfunctions.OpencreationThegoalofthissessionistoobserveparticipantbehaviorsandidentitymeritsandissuesofoursystems.Partici-pantswerefreetoperformopen-endedsculptingusingoursystemwiththeonlyrequirementofcreatingstructures,eithersurfaceorfreeform,withatleastacertainamountofrepetition.Oneoftheauthorsaccompaniedtheparticipantsthroughthesessionanden-couragedthemtoexploredifferentfeaturesprovidedinoursystem.\f(a)frontofinput(b)backofinput(c)frontofoutput(d)backofoutputFigure14:Targetsculptingtask.Weprovidethebasemeshshapeshownin(a)and(b),andasktheparticipantstoperformstrokestoachievethedesiredappearancesin(c)and(d).6ResultsObjectiveperformanceForthefullmanualmode,onaveragetheparticipantstook23minutestocomplete504brushstrokes.Withourautocompletefunctions,theparticipantstook16minutestocomplete522brushstrokes,including230manualand292ac-ceptedfromourhintorclonemodes.Thus,oursystemcanhelpnoviceusersreducethetimeandeffortsbyabout30%and56%fortheparticulartargetstudyinFigure14.MoredetailedstatisticsarerecordedinAppendixA.1234567hintclonesymmetryhint/clonecameralockpaintingFigure15:Userfeedback.Subjectivefeedbackfrom8novicepartic-ipantsina7-Likertscaleforvariousmodesofoursystem,expressedasmeanstandarderror.SubjectivesatisfactionFigure15summarizesfeedbackfromthe8noviceparticipants.Eventhoughsomeparticipantshadexpe-riencesin2Dsketching/paintingviadigitaltablets,wehavefounditcrucialtocoachthemonpracticinghowtocontrolthetabletpenmorecorrectlyandeffectivelyfor3Dsculpting,especiallythestrength/pressureandcameraviewpointmanipulation,asthesetwofactorsheavilyinuencetheoutputvisualeffects.Overall,theparticipantswerepositiveaboutoursystem,andgavehighratingstothehint,clone,andlockmodesastheyarenew/interesting(P4,6),useful(P2,6,7,8),withoutinterruptingusualsculptingprocess(P1).Theypreferredthehintmodewithbrushselectformorecontrol.Theautomaticcameracontrolreceiveslowerratingbecauseitmaytaketimeforadjustmentandsomeparticipantswouldlikebettercontrol(P1)andbeinglessconser-vative(P4).Theyalsoprovidedsuggestionsforfutureimprove-ments,suchasaneraserfunctionforautomaticsmoothing(P2),moreexiblecontrolforhints(e.g.enlargethenumberofsugges-tionsP3),automaticstabilizingbrushstrokestrength(P5),beautifytheUIdesign(P1,6),morepowerfulpredictionformorerandomstrokes(P7).MoredetailedfeedbacksfromindividualparticipantsarerecordedinAppendixA.SampleoutputsFigure16showssampleoutputsfromouruserstudyparticipants.Pleaserefertooursupplementaryvideosforrecordedliveactions.7LimitationsandFutureWorkWepresentanautocomplete3Dsculptingsystemthatcanreducein-putlaborandenhanceoutputquality,anddemonstratethattheaddi-tionofdynamicworkowscaneffectivelyaugmentstaticgeometryforinteractivemodelcreation.Theautocompleteprototypetargetsrepetitiveoperations,andfallsbacktotraditionalsculptingfornon-repetitiveoperationssuchasinitialshapeformation.Assistingthelatterviaotherdata-drivenorproceduralmethodsincombinationwithourautocompletefunctioncanbeapotentialfuturework.Weproposeanautomaticcameracontrolmodefollowingworkowsuggestions.Thisisaverybasicmodeandyetconservativeenoughtoavoiduserdisorientation.Additionalautomaticcameracontrolsarepossiblefromthedataandmethodperspectives,butwarrantfurtheruserstudies.Ourcurrentprototypeprovidesbasicfunctionalityasaproofofcon-cept.Morefeaturesincommercialsculptingtoolscanbeadded.Wealsoplantoexploreothergeometryandtopologyfeaturesformoreaccuratecorrelationbetweenworkowandshape[Berkitenetal.2017].Tohelpprediction,insteadofmanuallycraftedalgorithms,weareinvestigatingamachinelearningapproachthatanalyzesuserfeed-backs(whethertheyaccept,ignore,ormodifythesuggestions)forcontinuoustrainingourpredictionmodel.Withinthescopeofthisprojectwehavefocusedonasingleuserwithinasinglemodelingsession.Thegeneralideasandspecicmethodscanbeextendedtomultipleusersforcrowdsourcingandtutorialsaswellasmultiplesessionsforindividualstylization.Wefocuson3Dsculptingasitisaverypopularandexibleformofmodelcreation.Apotentialfutureworkistoconsiderotherformsof3Dmodeling,suchasVRbrushingthatoperatesmoredirectlyin3Dfreespaceinsteadofmostlyoverthecurrentmodelsurfaceforsculpting.ReferencesBERKITEN,S.,HALBER,M.,SOLOMON,J.,MA,C.,LI,H.,ANDRUSINKIEWICZ,S.2017.Learningdetailtransferbasedongeometricfeatures.ComputerGraphicsForum.BLENDERFOUNDATION,2016.Blender.BOKELOH,M.,WAND,M.,KOLTUN,V.,ANDSEIDEL,H.-P.2011.Pattern-awareshapedeformationusingslidingdockers.ACMTrans.Graph.30,6(Dec.),123:1123:10.\f(a)362/332op,29min(b)431/322op,34min(c)278/302op,27min(d)261/396op,24min(e)169/230op,20minFigure16:Sampleoutputsfromourparticipants,allstartingfromasphere.Denotedwitheachoutputarethefollowingstatistics:numberofmanualsculptingstrokes,numberofautocompletestrokes,andtotalauthoringtimeinminutes.Pleaserefertothesupplementaryvideosforrecordedmodelingsessions.CALABRESE,C.,SALVATI,G.,TARINI,M.,ANDPELLACINI,F.2016.csculpt:Asystemforcollaborativesculpting.ACMTrans.Graph.35,4(July),91:191:8.CHAUDHURI,S.,ANDKOLTUN,V.2010.Data-drivensuggestionsforcreativitysupportin3dmodeling.ACMTrans.Graph.29,6(Dec.),183:1183:10.CHEN,H.-T.,WEI,L.-Y.,ANDCHANG,C.-F.2011.Nonlinearrevisioncontrolforimages.ACMTrans.Graph.30,4(July),105:1105:10.CHEN,H.-T.,GROSSMAN,T.,WEI,L.-Y.,SCHMIDT,R.M.,HARTMANN,B.,FITZMAURICE,G.,ANDAGRAWALA,M.2014.Historyassistedviewauthoringfor3dmodels.InCHI14,20272036.CHEN,H.-T.,WEI,L.-Y.,HARTMANN,B.,ANDAGRAWALA,M.2016.Data-drivenadaptivehistoryforimageediting.InI3D16,103111.COHEN-OR,D.,ANDZHANG,H.2016.Frominspiredmodelingtocreativemodeling.Vis.Comput.32,1(Jan.),714.DENNING,J.D.,ANDPELLACINI,F.2013.Meshgit:Difngandmergingmeshesforpolygonalmodeling.ACMTrans.Graph.32,4(July),35:135:10.DENNING,J.D.,KERR,W.B.,ANDPELLACINI,F.2011.Mesh-ow:Interactivevisualizationofmeshconstructionsequences.ACMTrans.Graph.30,4(July),66:166:8.DENNING,J.D.,TIBALDO,V.,ANDPELLACINI,F.2015.3dow:Continuoussummarizationofmesheditingworkows.ACMTrans.Graph.34,4(July),140:1140:9.EBERT,D.S.,MUSGRAVE,F.K.,PEACHEY,D.,PERLIN,K.,ANDWORLEY,S.2002.TexturingandModeling:AProceduralApproach,3rded.MorganKaufmannPublishersInc.EMILIEN,A.,VIMONT,U.,CANI,M.-P.,POULIN,P.,ANDBENES,B.2015.Worldbrush:Interactiveexample-basedsyn-thesisofproceduralvirtualworlds.ACMTrans.Graph.34,4(July),106:1106:11.FU,H.,ZHOU,S.,LIU,L.,ANDMITRA,N.J.2011.Animatedconstructionoflinedrawings.ACMTrans.Graph.30,6(Dec.),133:1133:10.FUNKHOUSER,T.,KAZHDAN,M.,SHILANE,P.,MIN,P.,KIEFER,W.,TAL,A.,RUSINKIEWICZ,S.,ANDDOBKIN,D.2004.Modelingbyexample.ACMTrans.Graph.23,3(Aug.),652663.GARG,A.,JACOBSON,A.,ANDGRINSPUN,E.2016.Com-putationaldesignofrecongurables.ACMTrans.Graph.35,4(July),90:190:14.GUERIN,E.,GALIN,E.,GROSBELLET,F.,PEYTAVIE,A.,ANDGENEVAUX,J.-D.2016.Efcientmodelingofentangleddetailsfornaturalscenes.ComputerGraphicsForum35,7,257267.HU,S.-M.,XU,K.,MA,L.-Q.,LIU,B.,JIANG,B.-Y.,ANDWANG,J.2013.Inverseimageediting:Recoveringasemanticeditinghistoryfromabefore-and-afterimagepair.ACMTrans.Graph.32,6(Nov.),194:1194:11.HU,R.,VANKAICK,O.,WU,B.,HUANG,H.,SHAMIR,A.,ANDZHANG,H.2016.Learninghowobjectsfunctionviaco-analysisofinteractions.ACMTrans.Graph.35,4(July),47:147:13.IGARASHI,T.,ANDHUGHES,J.F.2001.Asuggestiveinterfacefor3ddrawing.InUIST01,173181.KLOSKOWSKI,M.,2010.Cloningwithapreviewinphotoshopcs4.http://www.photoshop.com/tutorials/4305.KWATRA,V.,ESSA,I.,BOBICK,A.,ANDKWATRA,N.2005.Textureoptimizationforexample-basedsynthesis.ACMTrans.Graph.24,3(July),795802.LUN,Z.,KALOGERAKIS,E.,WANG,R.,ANDSHEFFER,A.2016.Functionalitypreservingshapestyletransfer.ACMTrans.Graph.35,6(Nov.),209:1209:14.MA,C.,WEI,L.-Y.,LEFEBVRE,S.,ANDTONG,X.2013.Dy-namicelementtextures.ACMTrans.Graph.32,4(July),90:190:10.MITRA,N.J.,GUIBAS,L.J.,ANDPAULY,M.2006.Partialandapproximatesymmetrydetectionfor3dgeometry.ACMTrans.Graph.25,3(July),560568.MITRA,N.J.,WAND,M.,ZHANG,H.,COHEN-OR,D.,ANDBOKELOH,M.2013.Structure-awareshapeprocessing.InEurographics2013-StateoftheArtReports.NANCEL,M.,ANDCOCKBURN,A.2014.Causality:Aconcep-tualmodelofinteractionhistory.InCHI14,17771786.NISHIDA,G.,GARCIA-DORADO,I.,ALIAGA,D.G.,BENES,B.,ANDBOUSSEAU,A.2016.Interactivesketchingofurbanproceduralmodels.ACMTrans.Graph.35,4(July),130:1130:11.PEREZ,P.,GANGNET,M.,ANDBLAKE,A.2003.Poissonimageediting.ACMTrans.Graph.22,3(July),313318.PIXOLOGIC,2011.Sculptris.PIXOLOGIC,2015.Zbrush.SALVATI,G.,SANTONI,C.,TIBALDO,V.,ANDPELLACINI,F.2015.Meshhisto:Collaborativemodelingbysharingandretar-getingeditinghistories.ACMTrans.Graph.34,6(Oct.),205:1\f205:10.SANTONI,C.,CALABRESE,C.,DIRENZO,F.,ANDPELLACINI,F.2016.Sculptstat:Statisticalanalysisofdigitalsculptingwork-ows.CoRRabs/1601.07765.SCHMIDT,R.,ANDSINGH,K.2010.Meshmixer:Aninterfaceforrapidmeshcomposition.InSIGGRAPH10Talks,6:16:1.SCHMIDT,R.2013.Strokeparameterization.ComputerGraphicsForum32,2pt2,255263.SCHULZ,A.,SHAMIR,A.,LEVIN,D.I.W.,SITTHI-AMORN,P.,ANDMATUSIK,W.2014.Designandfabricationbyexample.ACMTrans.Graph.33,4(July),62:162:11.SUN,Q.,ZHANG,L.,ZHANG,M.,YING,X.,XIN,S.-Q.,XIA,J.,ANDHE,Y.2013.Texturebrush:Aninteractivesurfacetexturinginterface.InI3D13,153160.TAKAYAMA,K.,SCHMIDT,R.,SINGH,K.,IGARASHI,T.,BOUBEKEUR,T.,ANDSORKINE,O.2011.Geobrush:In-teractivemeshgeometrycloning.InComputerGraphicsForum,vol.30,613622.TALTON,J.O.,GIBSON,D.,YANG,L.,HANRAHAN,P.,ANDKOLTUN,V.2009.Exploratorymodelingwithcollaborativedesignspaces.ACMTrans.Graph.28,5(Dec.),167:1167:10.TAN,J.,DVOROZNAK,M.,SYKORA,D.,ANDGINGOLD,Y.2015.Decomposingtime-lapsepaintingsintolayers.ACMTrans.Graph.34,4(July),61:161:10.TSANG,S.,BALAKRISHNAN,R.,SINGH,K.,ANDRANJAN,A.2004.Asuggestiveinterfaceforimageguided3dsketching.InCHI04,591598.UMETANI,N.,IGARASHI,T.,ANDMITRA,N.J.2012.Guidedexplorationofphysicallyvalidshapesforfurnituredesign.ACMTrans.Graph.31,4(July),86:186:11.XING,J.,CHEN,H.-T.,ANDWEI,L.-Y.2014.Autocompletepaintingrepetitions.ACMTrans.Graph.33,6(Nov.),172:1172:11.XING,J.,WEI,L.-Y.,SHIRATORI,T.,ANDYATANI,K.2015.Autocompletehand-drawnanimations.ACMTrans.Graph.34,6(Oct.),169:1169:11.ZEHNDER,J.,COROS,S.,ANDTHOMASZEWSKI,B.2016.De-signingstructurally-soundornamentalcurvenetworks.ACMTrans.Graph.35,4(July),99:199:10.AUserstudyusermanualautocompleteminopminopP12248413212/279P22450716229/282P32652918240/292P42954221262/312P52553319225/298P61848014217/272P71847215220/297P82048514235/301mean2350416230/292Table1:Objectiveperformancefrom8noviceparticipants.Statis-ticsincludetotalcompletiontimeinminutesandnumberofstrokesunderfullymanualmode;totalcompletiontimeandnumberofmanual/accepted-hintstrokeswithourautocompletefunctions.TheparticipantperformancehasbeenrecordedunderTable1.Be-lowaremoredetailedparticipantfeedbacksforouruserstudycon-ductedonJanuary13,2017.Q:Whatisyourscoreforthefollowingfunctions,thehigherthebetter?SeeTable2.userhintclonesymmetrylockcamerapaint-select+selecthint/cloneP16766665P26676655P36657565P46677755P56676655P65666654P76676665P85676655mean5.756.1256.56.2565.3754.875Table2:Subjectivefeedbackfrom8noviceparticipantsina7-Likertscaleforvariousmodesofoursystem.Q:Whatareyouropinionsandsuggestionsforourprototype,in-cludingthemainUIandvariousfunctions?P1:ItisgoodthatthehintsdonotinterrupttheusualprocesssoIcoulddecidetoacceptornot,IsuggestimprovingtheUI,andsomeiconstobettercontrolthecameraposition.P2:Overall,Ithinkitisprettygoodandusefulsoftwareformod-elling,butmaybecanaddaneraserfunction:Aftersculpting,Imightwanttosmoothacertainregion,butIdonotwanttousethesmoothfunctionagainandagain.MaybeifyouprovideafunctiontoletmeselecttheregionIwanttosmooth,andthenthewholeregioncouldautomaticallybecomesmoothatonego.P3:Itwouldbebetterifthehintsforthestrokesaremoreexible(e.g.,thenumberofsuggestionsofthepotentialstrokescanbelargeenough).P4:Thecameramovementcouldbemoreimproved,ascurrentoneisabitconservative.Overalltheexperienceisinterestingcomparedtomyusualmodelingwithouthints.P5:Makeusersstrengthmorestable:Assculptingeffecthaslargerelationshipwiththetabletstrength,Iusedtabletbeforeforpainting,butforthatcase,thestrengthisnotthatsensitiveandimportant.Iwonderwhetheryoucouldautomaticallymakesomerenementsforthosebadlycontrolledstrokes,sonoviceslikemecouldmoreeasilycontrolthestroke.P6:Itisaspecialexperienceformetotrythistool,IdoalotmodelingviaMaya,eventhoughitisverydifferentfromyours,thehintsandcloneareprovidedassuggestions,andwecouldpreviewthem.Ifmorefunctionsinothermodelingtoolscouldbeprovidedwouldbebetter,theUIcouldbedesignedtobemorebeautiful.P7:Ifthehintscouldworkformorerandomstrokes,itwouldbebetter;theautoandclonefunctionsareuseful.P8:Thesymmetrymodewithhintsonbothsideshelpsmereducemanualstrokes.Iwonderwhetherpaintingforsuchkindofmodel-ingwouldbeusefulenoughcomparedtootherfunctions.\f", 
        "tag": "Graphics", 
        "link": "https://arxiv.org/list/cs.GR/new"
    }, 
    {
        "text": "UNBIAS PUF: A Physical Implementation Bias Agnostic\nStrong PUF\nWei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta\nDepartment of Electrical Engineering, University of California, Los Angeles\nMentor Graphics\n\n7\n1\n0\n2\n \nr\na\nM\n \n1\n3\n \n \n]\nR\nA\n.\ns\nc\n[\n \n \n1\nv\n5\n2\n7\n0\n1\n.\n3\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstractThe Physical Unclonable Function (PUF) is a promising\nhardware security primitive because of its inherent uniqueness and low\ncost. To extract the device-specic variation from delay-based strong\nPUFs, complex routing constraints are imposed to achieve symmetric\npath delays; and systematic variations can severely compromise the\nuniqueness of the PUF. In addition, the metastability of the arbiter circuit\nof an Arbiter PUF can also degrade the quality of the PUF due to the\ninduced instability. In this paper we propose a novel strong UNBIAS\nPUF that can be implemented purely by Register Transfer Language\n(RTL), such as verilog, without imposing any physical design constraints\nor delay characterization effort to solve the aforementioned issues.\nEfcient inspection bit prediction models for unbiased response extraction\nare proposed and validated. Our experimental results of the strong\nUNBIAS PUF show 5.9% intra-Fractional Hamming Distance (FHD)\nand 45.1% inter-FHD on 7 Field Programmable Gate Array (FPGA)\nboards without applying any physical layout constraints or additional\nXOR gates. The UNBIAS PUF is also scalable because no characterization\ncost is required for each challenge to compensate the implementation bias.\nThe averaged intra-FHD measured at worst temperature and voltage\nvariation conditions is 12%, which is still below the margin of practical\nError Correction Code (ECC) with error reduction techniques for PUFs.\n\nI . IN TRODUC T ION\nHardware security has become an important aspect in modern\nIntegrated Circuit (IC) design industry because of the global supply\nchain business model. Identifying and authenticating each fabricated\ncomponents of a chip is a challenging task [1]. A Physical Unclonable\nFunction (PUF) has been a promising security primitive such that its\nbehavior, or Challenge Response Pair (CRP) [2], is uniquely dened\nand is hard to predict or replicate. A PUF can enable low overhead\nhardware identication, tracing, and authentication during the global\nmanufacturing chain.\nSilicon delay based strong PUFs have been studied intensively\nsince its rst appearance in [3] because of its low implementation\ncost and large CRP space compared with a weak PUF [4]. However,\nthere are still design challenges that restrain a strong PUF from\nbeing put in a widespread practical use. One of the major design\nchallenges for a silicon delay based PUF is the strict symmetric delay\npath layout requirement. The wire delays of the competing paths\nshould be designed and matched carefully to avoid biased responses,\notherwise low inter-chip uniqueness would make the PUF unusable\n[5, 6]. In addition to asymmetric routing, another source of the biased\nresponses for silicon based PUF is the systematic process variation,\nwhich can also degrade the quality of a PUF, such as uniqueness or\nunpredictability. Finally, the metastability issue of the arbiter circuit\nfor an Arbiter PUF can cause unstable PUF responses, making a\nportion of the CRP unusable due to their instabilities [7].\n\nA. Asymmetric Path Delay Routing\nFor a delay based PUF, the randomness should be contributed\nonly by the subtle variations between devices, so having biased delay\ndifferences due to asymmetric routing is detrimental to delay based\nPUFs, and such impact should be eliminated. However, a precise\ncontrol of the routing can be a difcult and time consuming task.\nAn implementation of an Arbiter PUF on Field Programmable Gate\nArray (FPGA) is considered much more difcult than a RO PUF\nbecause the connections to the arbiter circuit must also be symmetric\n\n[8], and performing completely symmetric routing is physically\ninfeasible in most cases [6], resulting small inter-Fractional Hamming\nDistance (FHD) for an Arbiter PUF [9]. One of the most common\nsolutions to the asymmetric routing is to use hard-macros in FPGA\ndesigns [10, 11], but it is not effective with Arbiter PUF, and some\nless commonly-used features of the FPGA would be required [12].\nOther approaches try to extract randomness by XORing the outputs\nof multiple Arbiter PUFs at the cost of large hardware overhead and\nless stability [13]. In [14], the authors proposed using middle bits\ninstead of the Most Signicant Bit (MSB) as the RO PUF response\nmeasurement. The measurement can effectively eliminate the biased\nresponses, but an efcient way of predicting the inspection bit is not\ndescribed, and the presented RO PUF is not a strong PUF. A RTL-\nbased PUF bit generation unit was proposed in [15], but to the best\nof our knowledge, a strong PUF that can be implemented efciently\nwithout any layout constraints has not yet been proposed.\n\nB. Systematic Process Variation\nThe existence of systematic process variation can degrade the\nquality of silicon based PUFs because the local randomness should\nbe the only desired entropy source of the delay based PUF [16].\nThe effect of systematic process variation is similar to having biased\nwire delay between two delay paths, which can also damage the\nuniqueness of the PUF. Another possible vulnerability caused by\nsystematic variation is the induced process side channel attack as\ndescribed in [17]. Due to intra-wafer systematic variation [18], PUFs\nfabricated at the same region on different wafers can have similar\nsystematic behavior, which can be exploited as a process side channel\nattack.\nTo account for systematic variations, a compensation technique is\nproposed in [10], which requires careful design decisions to compare\nRO pairs that are physically placed close to each other. In [16],\nthe systematic variation is modeled and subtracted from the PUF\nresponse to distill true randomness with the cost of model calculation.\nSimilarly, in [19], the averaged RO frequency is subtracted from the\noriginal frequency, where the multiple measurements of each RO can\nlead to large latency overhead. In [20], a method is proposed to extract\nlocal random process variation from total variation, however, a second\norder difference calculation is needed, and hard-macro technique\nmust be applied to construct symmetric delay paths.\n\nC. Metastability of the Arbiter Circuit\nThe idea of an Arbiter PUF is to introduce a race condition on\ntwo paths and an arbiter circuit is used to decide which one of the\ntwo paths reached rst. The arbiter circuit is usually a D ip-op or\na SR latch. If two signals arrive at an arbiter within a short time,\nthe arbiter circuit may enter a metastable state due to setup time\nviolation [21]. Once the arbiter circuit is in metastable state, the\nresponse becomes unstable. To eliminate the inconsistency caused\nby metastability of the arbiter circuit, existing approaches use the\nmajority vote or to choose the paths that have a delay difference larger\nthan the metastable window  at the cost of CRP characterization and\ndiscarding the unstable CRPs [7].\n\n\fD. Our Contributions\nIn this paper, we propose the physical implementation bias agnostic\n(UNBIAS) PUF that is immune to physical implementation bias. The\ncontributions of this paper include:\n We proposed the rst strong UNBIAS PUF that can be imple-\nmented purely by RTL without imposing any physical routing\nconstraints.\n Efcient inspection bit selection strategy based on intra-/inter-\nFHD prediction models are proposed and veried on the strong\nUNBIAS PUF.\n\nI I . PRO PO SED S TRONG UNB IAS PUF\nThe proposed strong UNBIAS PUF compares two delay paths to\ngenerate PUF responses. Similar to Arbiter PUF, each bit of the\nchallenge of the UNBIAS PUF species the path conguration of the\ndelay path. As shown in Figure 1, the challenge c1 and c2 specify\nthe path congurations, and an one-bit response is extracted from\nthe difference register, which can be of several bits long. Once a\nchallenge is given, a signal is applied at Trigger. Each of the Clock\ncounter begins to count the number of clock cycles of the system\nclock (CLK) whenever the the signal from Trigger is propagated to\nthe START input of the counter, and stops counting whenever the the\nsignal from Trigger is propagated to the STOP input of the counter.\nFor each challenge, the difference value of the two Clock counters is\nstored in the difference register for further response extraction, which\nis described in details in Section III.\n\nFig. 1. The proposed strong UNBIAS PUF. The Clock counter starts counting\nclock cycles of the system clock (CLK) when START arrives and stops when\nSTOP arrives. The difference of two Clock counters are stored in the difference\nregister for further response extraction.\n\nThe purpose of the ROs inserted between path congurations is to\nincrease the path delay so that it will take multiple clock cycles for\nthe signal to propagate to stop the clock counter. As shown in Figure\n2, each RO is associated with a RO counter that counts the number\nof oscillations of the RO. The RO counter starts counting when the\nsignal from its previous path conguration is arrived, and propagates\nthe signal to the next path conguration only when the count reaches\na certain threshold. All the ROs are composed of same number of\ninverters and neither congurations nor any layout constraints are\nneeded.\nUnlike the conventional Arbiter PUF, the strong UNBIAS PUF has\nno metastability issues caused by a D ip-op or a latch. The delay\ndifference of the two paths is transformed into counter values of the\nsystem clock. By judiciously extracting the response from the dif-\nference register, the physical implementation bias can be effectively\nmitigated, therefore the UNBIAS PUF can be implemented purely by\nRTL without any routing or layout constraints. Details of the response\nextraction are described in Section III.\n\nI I I . B IA S - IMMUN E R E S PON SE EX TRACT ION\nA. Inspection Bit on Unbiased/Biased Paths\nIn this section we describe how different selections of the in-\nspection bit can change the intra- and inter-FHD. Figure 3 shows\nan example of a distribution of values from difference registers of\n\nFig. 2. ROs are inserted between path congurations to increase the path delay\nfor better stability. The signal from previous path conguration is propagated\nonly when the count of the RO counter reaches a certain threshold.\n\nsymmetrically routed UNBIAS PUFs. The length of the difference\nregister is 22-bit, so the range of the register value is between 221\nand 221  1 as represented in 2s-complement. The large inter-chip\nmeasurement curve gives the distribution of the values across all\nPUFs. Since the PUF is unbiased, roughly half of the difference\nvalues would be greater than zero due to random local process\nvariation, therefore the inter-FHD of the UNBIAS PUFs would be\nclose to 50%. In this case, the inspection bit is simply the MSB,\nwhich divides the range of 22-bit difference value into two groups\nbin 1 and bin 0. All measurements fall into bin 1 on the left output\na 1; others output a 0. The small intra-chip measurement curve gives\nthe distribution of multiple measurements of the PUF on a same\nchip. Due to noise, the difference values could be different, so the\nintra-FHD of the difference register may not be a perfect 0%.\n\nFig. 3. For a symmetrically routed PUF, the inter-FHD would be close to\n50%. The intra-FHD may not be zero due to measurement noise.\n\nEven though symmetric UNBIAS PUF layout is much preferred,\nit is difcult and takes much effort and overhead to achieve such\nrequirement as described in Section I-A. In practice, if no layout con-\nstraints are imposed, the measurement distribution of the difference\nregister can be as shown in Figure 4, where most of the difference\nvalues across chips are greater than zero. In this case, using the MSB\nas the inspection bit would cause low inter-FHD of the PUFs because\nmost MSBs are 0s.\n\nFig. 4. For a biased PUF, most of the difference values across all chips could\nbe greater than zero, causing a low inter-FHD if the MSB is the inspection\nbit.\n\n\fFor the same biased distribution shown in Figure 4, if the ith bit is\nused as the inspection bit of the difference register as Figure 5 shows,\nthe range of the 22-bit difference value is divided into multiple bins\nwith width 2i , where the output of the measurement is decided by\nthe bin in which it resides. Note that in this case the response is\nnot an indicator of which delay is longer in the comparison. The\nsmaller the width of the bin is, the closer the inter-FHD is to 50%\nbecause roughly half of the outputs would reside in bin 1 even with\nbiased delay. On the other hand, the width of the bin should be large\nenough so that multiple measurements of a same PUF should always\nfall into the same bin. In other words, the width of the bin should be\nlarger than the variation of the intra-chip measurement distribution.\nTherefore, the choice of inspection bit is a tradeoff between inter-\nFHD and intra-FHD for a PUF with asymmetric routing.\n\nFig. 5. For an asymmetrically routed PUF with proper inspection bit, roughly\nhalf of the difference values across all chips would fall in bin 1, therefore\nthe inter-FHD would be close to 50%.\n\nFig. 6. Magnied view of Figure 5 with three bins. w is the bin width and the\nmeasurement ranges for challenges C1 and C2 are specied. The expected\nintra-FHD1 is 0% and the expected intra-FHD2 depends on the portion of\nmeasured values that fall in bin 1.\n\nshows. We rst prove that the worst-case inter-FHD happens when\n\u0001 = 0.5w , followed by the prediction model of the inter-FHD for the\nworst-case scenario.\n1) Worst-Case Inter-FHD Identication: Given a xed w , dene\nA1 (\u0001) and A0 (\u0001) to be the total underlying area in bin 1 and bin 0\nas functions of \u0001, respectively. For any Normal distribution, A1 (\u0001)\n(cid:88)\nand A0 (\u0001) are calculated as:\nF (\u0001 + 2nw + w)  F (\u0001 + 2nw)\nA1 (\u0001) =\nn=\nA0 (\u0001) = 1  A1 (\u0001, w)\n(3)\nwhere F () is the Cumulative Distribution Function (CDF) of the\nNormal distribution, and n is the index for bin area summation.\nThe ratio Ratio(\u0001) is dened as:\n\n(2)\n\nIV. IN S P EC T ION B I T ID EN T I FICAT ION\nA. Intra-FHD Prediction Model\nThe intra-FHD depends on the width of the bins w = 2i when\nthe inspection bit is biti . A straightforward way to determine the\nassociated intra-FHD for each inspection location is to gather multiple\nmeasurements of the same challenge on a same PUF, and simply\ncalculate the intra-FHD for each biti . A more efcient approach is\nto predict the intra-FHD without calculating it for each biti .\nTo predict intra-FHDk of a challenge Ck of an inspection bit, we\nrst obtain t measured difference registers of the challenge Ck of\na same PUF. Since the bin width and the range of the difference\nregister is known, the t difference values can be divided into two\ngroups (responses) according to the bins they reside in. Let the\nnumber of difference values fall in bin 1 be none , and number of\ndifference values fall in bin 0 be nzero . none and nzero represent\nthe number of responses of the challenge Ck to be one and zero\nduring the t measurements, respectively. Since the intra-FHD is\nessentially calculated from the response difference between any two\nmeasurements, the predicted intra-FHDk is calculated as:\n(cid:0) t\n(cid:1)\nnone  nzero\n 100%,\n2\nwhere the nal predicted intra-FHD would be the averaged intra-\nFHDk of all challenges.\nAs shown in Figure 6, the expected intra-FHD1 is 0% because\nall measurements fall in the same bin and none  nzero = 0. The\nexpected intra-FHD2 depends on the portion of measured values that\nfall in bin 1. With larger bin width w , it is more likely that all\nresponses would fall into the same bin\nB. Inter-FHD Lower Bound Prediction Model\nThe inter-FHD depends on the bin width w with a given inspection\nbit biti . Assume the distribution of inter-chip difference value is a\nNormal distribution N  (, 2 ). Dene \u0001 to be the distance between\nthe mean  and the closest bin boundary on the left as Figure 7\n\nintra-FHDk =\n\n(1)\n\n(4)\n\n(5)\n\n(6)\n\n,\n\n0 < \u0001 < w\n\nRatio(\u0001) =\n\nRatio(\u0001) =\n\nA1 (\u0001)\nA0 (\u0001)\nwhere the range of \u0001 is from 0 to w because of its periodic structure.\nThe closer the Ratio(\u0001) is to one, the closer the inter-FHD would\nbe to 50% because the two areas are closer to each other. We want\nto show that the largest (most unbalanced) ratio happens at \u0001 = 0.5w\nas Figure 7 shows.\nTo nd the extreme value of Ratio(\u0001) given a xed w , we take\nderivative with respective to \u0001 of Equation 4 and replace A0 (\u0001) by\n1  A1 (\u0001) from Equation 3:\nA(cid:48)\n1 (\u0001)\nd\n(1  A1 (\u0001))2\nd\u0001\nFrom Equation 5 we see that to nd the extreme value of Ratio(\u0001),\nit is equivalent to nd the solution of A(cid:48)\n(cid:88)\n1 (\u0001), which is given below:\nf (\u0001 + 2nw + w)  f (\u0001 + 2nw)\nd\nd\u0001\nn=\nwhere f () is the Probability Density Function (PDF) of the Normal\ndistribution. Equation 6 shows that A(cid:48)\n1 (\u0001) is the summation of\ndifferences between two PDF terms where one is a shifted version\nby w of another. Therefore, applying \u0001 = 0.5w to Equation 6, we get\na zero. Figure 7 shows that when \u0001 = 0.5w , each difference term in\nEquation 6 has its counter part at the mirrored location to the center,\nso that the summation becomes zero.\nTo conclude our derivation, given a w of an inspection bit, the\nextreme value of Ratio(\u0001) happens when \u0001 = 0.5w , and the inter-\nchip stander deviation  is needed for the Ratio(\u0001) calculation.\n2) Inter-FHD Lower Bound Prediction: To predict inter-FHD,\nwe calculate the probability of which any pair of chips produce\ndifferent responses. The inter-FHD prediction given the width w of\nthe inspection bit is:\n\nA1 (\u0001) =\n\ninter-FHD =\n\n2Ratio(\u0001)\n(1 + Ratio(\u0001))2\n\n(7)\n\n\fB. Prediction Model Validation\nThe inter-FHD is obtained from 7 FPGAs, and the intra-FHD\nis calculated by measuring each PUF 10 times. To show inter-\nchip variation and measurement noise of our experimental setup, we\nmeasure the frequency of a single RO across the chips 10 times, and\nthe inter-chip variation is 6.1% with 0.2% measurement noise.\nTo validate the intra-FHD prediction model, we follow the proce-\ndure described in Section IV-A with t = 10 measurements. Figure 8\nshows the results of the intra-FHD prediction of bit5 and bit10 . The\nintra-FHD of bit5 is much higher than bit10 because its bin width is\nmuch smaller.\n\nFig. 8. Strong UNBIAS PUF intra-FHD predictions of bit5 and bit10 of 7\nFPGAs. bit5 has much larger intra-FHD because its bin width is smaller.\n\nTo validate the inter-FHD prediction model, for each challenge,\nwe obtain an inter-chip standard deviation  from 7 FPGAs, and\nthe nal  used in the prediction model is the median of the \nfrom 120 challenges, which gives  = 521. The results shown in\nFigure 9 indicate that the inter-FHD lower bound prediction is well\nmatched with the measured data. To demonstrate that the inter-FHD\nprediction model does not require an accurate inter-chip  estimation,\nFigure 9 also shows the prediction range with   15% variation.\nWe can see that the differences of the predictions are limited, which\nindicates that the  can either be obtained from pre-layout simulation\nor measurements of a small number of chips. The prediction gap\nis relatively large when w is much larger than  . However, as w\nbecomes comparable to  , where potential inspection bits begin to\noccur, the prediction curve rises up quickly and matches the measured\ndata well. Figure 9 also shows that bit10 should be a proper inspection\nbit because the intra-FHD is low and the inter-FHD is close to 45%\n\nC. Uniqueness and Reliability Evaluation\nThe results of inter-FHD and intra-FHD with different inspection\nbit selections are shown in Figure 9. As we can see from the gure,\nusing bits closer to the MSB gives low intra-FHD but also low\ninter-FHD. This veries the fact that the delay paths are biased if\nno physical implementation constraints are imposed. On the other\nhand, using bits closer to the LSB gives 50% on both intra-FHD and\ninter-FHD because of the measurement noise. As predicted, the best\ninspection location appears at bit10 with 45.1% inter-FHD and 5.9%\nintra-FHD. The results also indicate that the systematic variation is\nmitigated because no constraints are imposed at all.\nTable I shows comparison results with previous work. With con-\nventional Arbiter PUF (APUF) shown in the second column, the\nresults from [9] show that the circuit is essentially a constant number\ngenerator with very little inter-FHD. The third column shows the 3-1\ndouble Arbiter PUF with XORs [13], where symmetric layout is still\nrequired, and the hardware overhead is 2X or 3X from the duplicated\ncircuits depending on the uniqueness requirement of the application.\nThe inter-FHD is close to 50% but the intra-FHD is high due to\nthe XORs. The fourth column shows the results from Path Delay\nLine (PDL) PUF [6]. Symmetric PDL and delay characterization for\neach CRP are required, which can cause scalability issues. Also the\n\nFig. 7. Worst Inter-FHD happens when the mean is at the middle of a bin.\n\nthe two areas are the same, resulting\nWith Ratio(\u0001) = 1,\na predicted 50% inter-FHD. Given a selected bit i, plugging in\n\u0001 = 0.5w to Equation 7 would give the predicted inter-FHD lower\nbound.\nthe inter-chip standard\ninter-FHD,\nto predict\nPlease note that\ndeviation  is needed because the calculation involves the CDF.\nHowever, the mean  does not affect the prediction because the\nextreme value is obtained by nding the worst-case \u0001. Also, since\nchanging the inspection bit results at least a 2x change of w , the\ninter-chip  does not have to be calculated with high accuracy. It can\nbe obtained by pre-layout simulation or measuring a small number\nof chips.\n\nC. Inspection Bit Selection\n\nGiven the Error Correction Code (ECC) specication correspond-\ning to the PUF design, the intra-FHD threshold can be dened.\nFrom the intra-FHD prediction model, choose a set of candidate bits\nthat would satisfy the intra-FHD threshold requirement. From the\ncandidate bits, a best inspection bit can be determined by applying\nthe inter-FHD prediction model given the standard deviation  of the\ninter-chip delay distribution.\nPlease note that only one chip is needed for the inspection bit\nselection since the measurement noise is similar for all chips from\nour experiment and the  is obtained from pre-layout simulation. The\nlocation of the nal inspection bit, which is a public information, is\npassed to all PUFs for the secret response generation.\n\nV. EX PER IM EN TA L R E SU LT S\n\nA. Strong UNBIAS PUF Implementation\n\nThe strong UNBIAS PUF structure is implemented on 7 Altera\nDE2-115 FPGA boards. In our implementation, no physical con-\nstraints, additional XORs, tunable delay units, or any systematic\nvariation compensation techniques are used. The design is purely\na RTL design.\nThe ROs inserted between path congurations are composed of\n19 inverters, and the signal will be propagated to the next path\nconguration when the RO counter associated to the RO reaches a\ncount of 50 thousand. The UNBIAS PUF has 10 path congurations,\ntherefore the length of the challenge is 10-bit long. The length of\nthe difference register is 19-bit, and the length of the nal response\nfor each challenge is one bit. For our experiment, 120 challenges are\napplied, and 120 bits of responses are obtained for each PUF within\na second. Please note that the RO structure and the count of the RO\ncounter are selected given the 50 MHz system clock of the FPGA.\nThe results are similar as long as no overow occurs at the 19-bit\ndifference register.\n\n\fdetermined efciently from the intra-FHD and inter-FHD prediction\nmodels.\nThe strong UNBIAS PUF is implemented on 7 FPGAs without\nimposing any physical layout constraints. Experimental results show\nthat the intra-FHD of the strong UNBIAS PUF is 5.9% and the inter-\nFHD is 45.1%, and the prediction models are closely tted to the\nmeasured data. The averaged intra-FHD of the strong UNBIAS PUF\nat worst temperature and voltage variations is about 12%, which is\nstill within the margin of conventional ECC techniques. The fact\nthat the proposed scheme is immune to physical implementation bias\nwould allow the strong UNBIAS PUF to be designed and integrated\nwith minimum effort in a high-level description of the design, such\nas during RTL design.\n\nIn Proc.\n\nR E FER ENC E S\n[1] S. U. Hussain, S. Yellapantula, M. Majzoobi, and F. Koushanfar. BIST-\nPUF: Online, Hardware-based Evaluation of Physically Unclonable\nCircuit Identiers. In Proc. ICCAD, 2014.\n[2] Roel Maes and Ingrid Verbauwhede. Physically Unclonable Functions: A\nStudy on the State of the Art and Future Research Directions. In Towards\nHardware-Intrinsic Security, pages 337. Springer Berlin Heidelberg,\n2010.\n[3] Blaise Gassend et al. Silicon Physical Random Functions.\nCCSC, 2002.\n[4] C. Herder et al. Physical Unclonable Functions and Applications: A\nTutorial. Proc. of the IEEE, pages 11261141, Aug 2014.\n[5] Daihyun Lim et al. Extracting secret keys from integrated circuits. IEEE\nTransactions on VLSI Systems, 2005.\n[6] D. P. Sahoo, R. S. Chakraborty, and D. Mukhopadhyay. Towards Ideal\nArbiter PUF Design on Xilinx FPGA: A Practitioners Perspective. In\nEuromicro Conference on DSD, 2015.\n[7] Teng Xu and M. Potkonjak. Robust and exible FPGA-based digital\nPUF. In International Conference on FPL, 2014.\n[8] Durga Prasad Sahoo et al. Architectural Bias: a Novel Statistical Metric\nto Evaluate Arbiter PUF Variants. Cryptology ePrint Archive, Report\n2016/057, 2016.\n[9] V. Gunreddy A. Maiti and P. Schaumont. A Systematic Method\nto Evaluate and Compare the Performance of Physical Unclonable\nFunctions. In Embedded Systems Design with FPGAs, 2013.\n[10] A. Maiti and P. Schaumont. Improving the Quality of a Physical Un-\nclonable Function using Congurable Ring Oscillators. In International\nConference on FPL, 2009.\n[11] Chongyan Gu and M. ONeill. Ultra-compact and Robust FPGA-based\nPUF Identication Generator. In IEEE ISCAS, 2015.\n[12] S. Morozov et al. An analysis of delay based PUF implementations\nIn Recongurable Computing: Architectures, Tools and\non FPGA.\nApplications. Springer Berlin Heidelberg, 2010.\n[13] Mitsugu Iwamoto Takanori Machida, Dai Yamamoto and Kazuo\nSakiyama. A New Arbiter PUF for Enhancing Unpredictability on\nFPGA. In The Scientic World Journal, 2015.\n[14] F. Kodtek and R. Lrencz. A Design of Ring Oscillator Based PUF on\nFPGA. In IEEE International Symposium on DDECS, 2015.\n[15] J. H. Anderson. A PUF design for secure FPGA-based embedded\nsystems. In Proc. ASP-DAC, 2010.\n[16] Chi-En Yin and Gang Qu. Improving PUF Security With Regression-\nbased Distiller. In Proc. DAC, 2013.\n[17] Wei-Che Wang, Yair Yona, Suhas Diggavi, and Puneet Gupta. LEDPUF:\nStability-Guaranteed Physical Unclonable. Functions through Locally\nIn IEEE International Symposium on HOST,\nEnhanced Defectivity.\n2013.\n[18] Lerong Cheng et al. Physically Justiable Die-Level Modeling of Spatial\nVariation in View of Systematic Across Wafer Variability. IEEE TCAD,\n2011.\n[19] Linus Feiten et al. Improving RO-PUF Quality on FPGAs by Incorpo-\nrating Design-Dependent Frequency Biases. IEEE ETS, 2015.\n[20] Qinglong Zhang et al. FROPUF: How to Extract More Entropy from\nTwo Ring Oscillators in FPGA-Based PUFs. IACR, 2015.\n[21] L. Portmann and Teresa H.-Y. Meng. Metastability in CMOS library\nelements in reduced supply and technology scaled applications. IEEE\nJSSC, 1995.\n[22] J. Guajardo, G.-J. Schrijen S. S. Kumar, and P. Tuyls. FPGA Intrinsic\nPUFs and Their Use for IP Protections. In CHES, Sep 2007.\n[23] M.-D. M. Yu and S. Devadas. Secure and Robust Error Correction for\nPhysical Unclonable Functions. In IEEE Des. Test, 2010.\n\nFig. 9.\nInter-, intra-FHD, and inter-FHD prediction using  = 521 with\ndifferent inspection bit selections of the strong UNBIAS PUF.\n\nTABLE I\nCOM PAR I SON B ETW E EN PREV IOU S ARB I TER PUF S AND STRONG\nUNB IAS PUF\n\ninter-FHD\nintra-FHD\nSymm. Layout\nCharacterization\n\nPDL\nXOR\nAPUF\n[13]\n[6]\n[9]\n50.6% 45.25%\n7.2%\n4.1%\n0.24% 11.8%\nYes\nYes\nNo\nNo\nNo\nYes\n\nUNBIAS\nPUF\n45.1%\n5.9%\nNo\nNo\n\nability of eliminating biased responses is limited because it depends\non the number of tuning stages inserted. The last column shows the\nproposed strong UNBIAS PUF. Its behavior is unique and stable, and\nmost importantly no symmetric layout at all.\n\nD. Temperature and Voltage Variations\nFor temperature and voltage variations, the reference responses\nare measured at 20C with standard voltage 12V. The reference\nresponses are then compared with responses measured at 20C and\n75C with 10% voltage variation. The results indicate the reliability\nof the PUF when it is enrolled at normal condition but veried at a\nhigh temperature environment with unstable voltage source.\nFigure 10 shows the intra-FHD using bit10 as the inspection bit.\nAll intra-FHD at 20C with 10% voltage variation is below 8%, and\nall intra-FHD at 75C with 10% voltage variation is below 14%,\nwhich is still within conventional ECC margin with error reduction\ntechniques for PUFs [22, 23]. Compared with RO PUF presented in\n[14], one possible explanation of smaller intra-FHD for our strong\nUNBIAS PUF is that with multiple RO delay units, the overall delay\nvariation is canceled out, where for the RO PUF, the variation of\neach RO is directly compared.\n\nFig. 10. Strong UNBIAS PUF intra-FHD under tempreature and voltage\nvariations.\n\nV I . CONCLU S ION S\nWe proposed the rst strong UNBIAS PUF that can be imple-\nmented purely by RTL without complex post-layout analysis or\nhand-crafted physical design effort. The proposed measurement can\neffectively mitigate the impact of biased delay paths and metastability\nissues to extract local device randomness. The inspection bit can be\n\n\f", 
        "tag": "Hardware Architecture", 
        "link": "https://arxiv.org/list/cs.AR/new"
    }, 
    {
        "text": " \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n \n\n Regular Research Paper  NS \n\n \nDeveloping a FPGA  Supported Touchscreen Writing / \nDrawing System for Educational Environments \n \nAslihan Tufekci \nGazi University, \nGazi Faculty of Education, \nDept. of Computer Instruction Technologies, Turkey \nasli@gazi.edu.tr  \n \nKamuran Samanci \nTurk Telekom, Turkey \nkamuransamanci@yahoo.com \n \nUtku Kose \nUsak University, \nComputer Sciences App. & Res. Center, Turkey \nutku.kose@usak.edu.tr  \n \n \n \n\nAbstract \n \nDevelopments  in  information  and  communication  technologies  have  been  greatly  influential  on \nthe practices  in all  fields, and education  is not an exception to this. To  illustrate with, computers \nwere  first  used  in  computer    assisted  education  in  order  to  increase  the  efficiency  of  teaching \nprocess.  Recently,  computer  has  contributed  more  to  the  field  through  interactive  and  smart \nclass  applications  that  are  specially  designed  for  classroom  use.  The  aim  of  this  study  is  to \ndevelop a  low  cost, portable and projection  supported touchscreen to be used  in educational \nenvironments  by  using  FPGA  technology  and  to  test  its  usability.  For  the  purposes  of  the  study, \nthe above mentioned system was developed by using the necessary hardware and software, and \nlater  it  was  tested  in  terms  of  usability.  This  usability  test  was  administered  to  teachers,  who \nwere  the  target  end    users  of  this  touchscreen  writing  /  drawing  system.  The  aim  of  this  test \nwas  to  determine  user    friendliness,  subservientness  and  usability  of  the  system.  Several \ntools  were  used  to  obtain  data  from  the  users  that  participated  in  the  study.  The  analysis  and \nevaluation of the data collected revealed that the system has achieved its objectives successfully. \n \nKeywords: FPGA, educational technologies, touchscreen writing / drawing system, usability \n \n \n \n1. INTRODUCTION \n\n \n\nEducation  has  always  been  a  field  of  study  making  use  of  technology  as  effectively  as  possible. \n\nTodays  popular  concept  educational  technologies  has  been  derived  from  the  process  of \n\nbenefiting  from the advantages provided by technology as much as possible  in order to  increase \n\nthe  effectiveness  and  efficiency  of  teaching  process  and  educational  studies.  In  other  words, \n\n \n\n60  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\neducational  technologies  (also  called  teaching  technologies)  are  defined  as  the  studies  and \n\nethical  applications  ensuring  facilitated  education  and  increased  performance  by  effectively \n\nusing,  managing  and  developing  appropriate  technological  processes  and  resources  (Richey, \n\n2008).  \n\nTechnology  has  always  supplemented  educational  practices  in  several  ways.  It  is  clear  that \n\nextending and introducing these practices to larger audiences, presenting visual simulations and \n\ngraphic  supports  for  students,  developing  various  audio    visual  educational  materials  and \n\nintroducing  many  evaluation  systems  and  tools  result  in  more  efficient  and  easier  educational \n\nprocesses.  Similarly,  the  findings  of  the  studies  conducted  in  educational  technologies  are  to  a \n\ngreat  extent  reflected  in  real  classrooms  in  the  form of practical  applications. To  illustrate with, \n\nmany  technological  tools  and  devices,  most  of  which  are  computer    supported,  are  used  for \n\neducational  processes  in  real  classroom  environments.  It  is  also  true  that  almost  all  the \n\neducational  materials  used  in  todays  classrooms  are  computer    controlled  or  computer   \n\noperated.  Therefore;  thanks  to  computer  support,  classrooms  have  now  acquired  smart \n\nclassroom  features.  Depending  on  the  developments  in  computer  technology,  the  users \n\n(students  or  teachers)  have  started  to  interact  with  the  devices  available  in  classrooms,  which \n\nhave  inevitably  led  to  the  emergence  of  the  concept  interactive  classroom  as  the  next \n\ngeneration version of smart classrooms.  \n\nIn  interactive  classrooms,  there  is  an  ongoing  direct  interaction  between  the  users  and  the \n\nsystem,  and  this  process  requires  a  computer  enabling  this   interaction.  The  studies  conducted \n\non  this  issue  show  that  almost  all  interactive  classroom  applications  require  the  use  of  at  least \n\none computer. Thus, the dense use of computers has become inevitable in educational processes \n\njust  like  in  every  field  of  life.  Used  for  measurement  and  evaluation  purposes  at  the  beginning, \n\ncomputers  later  have  been  used  in  computer    assisted  teaching  intensively  as  well  (Dogan, \n\n1997).  The  most  recent  example  of  this  new  function  of  computers  is  computer    supported \n\nboard, which  is  also  called  smart board. The  system  is  fully  controlled  by  a  computer,  and  the \n\ntexts  and  visuals  are  projected  on  a  board  via  a  projector.  These  visual  elements  can  be  altered \n\nby using specially designed pens called smart magic pen. \n\nThe  presence  of  hardware  does  not  suffice  for  computer    assisted  teaching  systems.  Some \n\nnecessary  software  supporting  the hardware must  also be  installed  into  the  system. Developing \n\nsuch  software  requires  hardwork  and  time,  so  the  total  system  can  be  expensive  since  this \n\nhardwork  and  time  spent  are  taken  into  consideration  while  determining  the  price  of  the \n\nproduct. Moreover,  it  is  true  that  computers  are multi    purpose  devices  and  not  used  only  for \n\neducational purposes. Depending on the particular needs of the customers, various software and \n\nhardware    not  only  for  educational  purposes    can  be  installed  in  standard  computers  and \n\n \n\n61  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nmarketed  by  computer  companies.    In  other  words,  the  consumers,  unfortunately,  have  to  pay \n\nfor the software and hardware that will not be used in educational processes too.  Therefore; the \n\nuse of computer in a system increases the overall cost considerably. In addition, since computers \n\nare  complex  devices,  it  is  necessary  to  employ  a  computer  technician  at  school  to  deal with  the \n\nmaintenance and repair of the computers.  \n\nBased on the explanations mentioned above, the current study aims at the following: Developing \n\nsimple,  low    cost  touchscreen  writing  /  drawing  system  for  educational  environments  which \n\ndoes not require the use of a computer and can be used as portable or stationary; and testing its \n\nusability  on  teachers,  who  are  the  real  target  users  of  this  touchscreen  writing  /  drawing \n\nsystem. In addition, the followings are targeted as secondary objectives: \n\n  To  assist  teachers, who  are  the  real  target  end    users,  in  teaching more  easily  and  give \n\nthem  an  alternative  to  continue  their  in    class  teaching  in  case  of  illnesses  and  even \n\ndisability  since  this  touchscreen  enables  them  to  teach  while  sitting  without  needing \n\nmuch physical activity or effort.  \n\n  To  ensure more  visibility  of  the  board    considering  the  fact  that  the  users might  cast  a \n\nshadow  on  some  parts  of  the  board  in  the  systems  like  smart  board  depending  on  the \n\nlight coming from the projector. \n\nTouchscreen writing  / drawing  system, which  is  likely  to meet  the  objectives mentioned  above, \n\nis  expected  to  be  an  alternative  tool  in  educational  processes  and  for  the  systems  requiring \n\ncomputers with expensive software and hardware. \n\n \n\n2. BACKGROUND \n\n \n\nDevelopments  and  improvements  in  the  context  of  especially  education  field  have  been  given \n\nrise  to  appearment  of  different  kinds  of  educational  technologies.  As  a  result  of  appearment  of \n\nthe related  technologies, supportive educational materials  for ensuring more quality  in  teaching \n\n learning processes has also been designed and developed in many scientific research works. At \n\nthis  point,  improvements  especially  within  computer  and  electronic  technologies  has  an \n\nimportant role on rapid developments in the related scope. In this sense; computers, computer  \n\nbased  systems  and  other  technologies  from  general  perspective  (especially  information  and \n\ncommunication technologies) have been widely used  in educational studies  in performing many \n\ndevelopments  and  improvements  (Deperlioglu  &  Kose,  2013;  Kose,  2010;  McCormack  &  Jones, \n\n1997). \n\n \n\n62  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nWhen  the  current  conditions  and  situations  have  been  examined,  it  can  be  seen  that  usage  of \n\ncomputer   based  and  electronic devices  in  teaching    learning processes  is widely preferred  in \n\norder  to  improve  quality  and  effectiveness  of  the  related  educational  approaches.  In  this  sense, \n\nwhen  this  issue  is  evaluated  from  students  perspective,  some  research  works  also  report  that \n\nthere  is  a  large  range  of  capacities  for  especially  information  and  communication  technologies \n\namong  students  in  higher  education  (Lee,  2003;  Palaigeorgiou  et  al.,  2005;  Van  Braak,  2004). \n\nRelated  to  the  discussed  issue,  such  reports  also  point  the  situation  of  remarkable  usage \n\npopularity of computer and electronic technologies among educational studies.  \n\nAs  being  parallel  with  the  research  subject  of  this  work,  usage  of  computer  and  electronic \n\ntechnologies  is  an  important  factor  that  must  be  taken  into  consideration  to  discuss  about  the \n\nbackground.  As  it  can  be  understood,  such  devices  based  on  computer  and   electronic \n\ntechnologies play active roles on improving teaching  learning processes and ensuring effective \n\nand  efficient  ways  for  achieving  the  educational  objectives.  Related  to  this  issue,  it  has  been \n\nargued  within  some  works  that  the  use  of  such  devices  (for  example,  laptops,  mobile  devices) \n\nduring  teaching    learning  processes  improves  students  grasp  and  comprehension  of  the \n\nsubject  by  providing  flexible  interaction  and  using  experiences  (Alsaggaf  et  al.,  2012;  Dexter  et \n\nal.,  1999;  Nilson  &  Weaver,  2005;  Tufekci  et  al.,  2013).  On  the  other  hand,  it  has  also  been \n\nreported  that  students  think  very  positive  about  using  such  materials  although  some \n\nresearchers also think that the related devices may affect the  learning process  in a negative way \n\n(Akbaba-Altun, 2006; Barak et al., 2006; Hembrooke & Gay, 2003; Ni & Branch, 2004). \n\nAs  it  was  also  mentioned  before  in  the  Introduction  section  of  this  article,  more  advanced \n\ncomputer  assisted materials  like smart board or interactive tablet have also been designed \n\nand  developed  as  a  result  of  rapid  developments  in  the  mentioned  technologies  combined  in  a \n\ncommon  scope  for  improving  educational  processes.  Nowadays,  different  kinds  of  educational \n\ntechnology  based materials  like smart boards, touchscreen devices,  interactive tablets, camera \n\n  based  viewing  devicesetc.  are  widely  used  for  teaching    learning  approaches.  Within  the \n\ncurrent  modern  life,  it  can  be  seen  that  the  touchscreen    based  system  are  used  usually  as \n\nkiosks  in  order  to  improve  standards  in  activities  in  airports,  train  stations,  grocery  stores, \n\nbanks  and  any  other workplaces  such  as  food  service,  retail  and  health  care  fields  (Astell  et  al., \n\n2010;  Chourasia  et  al.,  2013;  Newman  et  al.,  2012;  Schultz  et  al.,  1998;  Shervin  et  al.,  2011; \n\nWilson  et  al.,  1995).  As  being  parallel  with  the  expressed  points  and  because  of  the  main \n\nobjective  of  this  work  (designing  and  developing  touchscreen  writing  /  drawing  system)  and \n\nother  following  objectives;  previously  performed  research  studies    works  within  the  subject \n\nmust also be examined in order to enable readers to have more idea about the work.  \n\n \n\n63  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nIn  the sense of  the current  literature,  it can be seen  that  there  is not  too many studies regarding \n\nto design and develop of a touchscreen system for directly educational purposes. Because of this, \n\nsome  recent  studies    works  employing  design,  development  and  usage  of  touchscreen \n\ntechnologies  are  also  discussed  as  follows  in  order  to  give  ideas  about  touchscreen  based \n\napplications: \n\n \n\nIn their works, Raj et al. (2013) provides a research work based on design, development \n\nand  implementation  of  a  touchscreen  health  information  kiosk.  By  using  this  system, \n\npatients  at  St.  Johns  Medical  College  Hospital    Bangalore  have  ability  to  receive \n\ninformation  by  using  kiosk  systems  provided  in  the  context  of  specific  areas.  From  the \n\nview of receiving  information and  functions related  to especially feedback approaches, \n\nthis  system  can  also  be  evaluated  as  some  kind  of  learning    based  scientific  study \n\nproviding many benefits in the context of using touchscreen technologies. \n\n  Caviglia-Harris  et  al.  (2012)  has  performed  a  study  on  using  computer    assisted  data \n\ncollection approach for providing more accurate and effective way for performing survey \n\n based works.  In  this study,  touchscreen  laptop systems have been used  for performing \n\nsurveys  and  collecting  item  responses  in  this  way.  The  study  has  shown  that  using  the \n\nrelated devices rather than using the traditional method (paper and pencil interviewing) \n\nprovide  a  effective  way  on  data  collecting  by  reducing  mistakes  and  ensuring  many \n\nadvantages in the sense of time and place. \n\n \n\nIn  his  Master  Thesis  work,  Taylor  (2012)  proposes  a  near  touch  user  interface  for \n\ntouchscreen    based  systems.  This  system  has  been designed  and  developed  in   order  to \n\novercome  interaction  limitations  of  traditional  touchscreen  applications.  In  this  sense, \n\nTaylor  uses  some  supportive  devices  to  form  the  proposed  touchscreen  system   \n\napproach. \n\n \n\nIn the context of medical applications, Aguirre et al. (2012) provides a system developed \n\nas  a  touchscreen    based,  low  cost  electrocardiograph.  The  results  obtained  with  the \n\nwork  show  that  the  system  also  provides  a well  performance  within  applications  of  the \n\nelectrocardiogram. \n\n  Dixon  and  Prior  (2012)  have  provided  a  research  work  on  a  typical  touchscreen \n\nelectronic  queuing  system.  In  this  scope,  they  have  provided  a  work  on  design  and \n\ninitial usability  testing of  the anonymous electronic waiting room system  for  improving \n\npatient  satisfaction.  The  obtained  results  show  that  the  system  has  positive  affects  on \n\npatients and provided successful rates on other related factors like interaction. \n\n \n\n64  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n  Related to usage of touchscreen  based systems, Chourasia et al. (2013) have provided a \n\nwork  on  evaluating  effects  of  sitting  and  standing  on  performance  and  touch \n\ncharacteristics  during  a  digit  entry  touchscreen  task  in  individuals  with  and  without \n\nmotor    control  disabilities.  The  work  provides  a  good  evaluation  approach  for \n\nevaluating different  factors  (for  example  button  sizes  of  a  touchscreen)  in  order  discuss \n\nabout  the related effects and generally  the results points  that environmental conditions \n\nshould  also  be  considered  to  improve  accessibility  and  usability  of  touchscreen.  As \n\nbeing  similar  to  this  work,  some  previous  works  on  effect  of  touch  screen  interface \n\ndesign  on  performance  have  also  been  provided  (Colle  &  Hiszem,  2004;  Jin   et  al.  2007; \n\nMartin, 1988; Sesto et al. 2012) \n\n \n\nIn  their work,  Rahman  et  al.  (2012)  provides  a  touchscreen    based  automation  system \n\nto  control  electronic  devices  within  a  home.  In  the  context  of  the  proposed  system, \n\nelectronic  components  that  can  be  bought  from  any  available  local  market  have  been \n\nused in order to form a low cost device. \n\n  Another  remarkable work  regarding  to  development  of  touchscreen  approach  has  been \n\nprovided  by  Bi  et  al.  (2012).  In  this  work,  a multilingual  touchscreen    based  keyboard \n\napplication has been designed and developed by the authors.  In this sense,  the keyboard \n\nbutton  layout  has  been  optimized  according  to  usage  of  the  keyboard  for  different \n\nlanguages like French, Spanish, German, and Chinese. \n\n  Regarding  to  optimization  of  touchscreen  systems  and  applications,  another  work  has \n\nbeen performed by Bradley (2012), in order to determine factors affecting the adoption \n\nof  touchscreen  smartphones  among  individiuals  with  vision  loss.  According  to  the \n\nobtained  results  within  this  Master  Thesis  work,  more  user    friendly,  assistive \n\ntechnologies  can  be  performed  on mobile  devices,  in  order  to  fit  them  to  be  used  better \n\nby especially individiuals with vision loss. \n\nAfter  the  taking  a  brief  look  at  to  the  background  and  the  literature  status,  the  touchscreen \n\nwriting  /  drawing  system,  which  was  provided  within  this  work  must  be  examined  in  the \n\ncontext  of  design  process.  In  this way,  the  low  cost  aspects  of  the  system  and  its  using  features \n\nand functions can also be understood better. \n\n \n\n3. DESIGN PROCESS OF TOUCHSCREEN WRITING / DRAWING SYSTEM   \n\n \n\nAs  it  was  also  mentioned  before,  a  low  cost  projection    supported  touchscreen  writing  / \n\ndrawing  system, which  does  not  require  a  computer  and  can  be  used  as  portable  or  stationary, \n\n \n\n65  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nwas  developed  in  the  current  study  by  using  the  advantages  provided  by  FPGA  technology.  In \n\nthis sense, Figure 1 represents some  fotographs  taken  from  the designed and developed  system \n\n(as prototype). \n\n     \n\n    Figure 1. Some fotographs taken from the designed and developed system (as prototype). \n\n \n\nRegarding  to  the  touchscreen writing / drawing  system,  it  is necessary  to define  the design and \n\ndevelopment  process  of  touchscreen  writing  /  drawing  system  in  terms  of  software  and \n\nhardware  in  order  to  learn  more  about  the  study  and  have  a  clearer  idea  of  its  importance.  By \n\ndoing  so,  the  features  and  functions  of  the  system  are  also  explained  in  detail.  In  order  not  to \n\nconfuse the readers, technical details were omitted from the explanations.   \n\n \n\n3.1. Hardware Design of Touchscreen Writing / Drawing System  \n\n \n\nThe working mechanism of the system hardware is reprensented in the diagram under Figure 2. \n\nThis figure also presents the preview of the system software. \n\n \n\n66  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nFigure 2. System structure of the touchscreen writing / drawing system.  \n\n \n\n \n\n \n\nAs  seen  in  Figure  3.2,  X  and  Y  coordinates  of  the  writings  or  drawings  made  on  the  panel  with \n\nthe help of a special pen and a  touch   operated panel  located on  touchscreen GLCD panel (LTM \n\n  LCD  Touch  Module)  are  converted  into  digital  values  through  an  ADC   (Analog  to  Digital \n\nConverter)  chip.  These  digitally  converted  X  and  Y  coordinate  values  are  transferred  to  FPGA \n\ncontent  through ADC serial port  interface (SPI) control module which  is  located on FPGA chip.  \n\nThese  transferred  values  are  stored  in  an  internal  register  RAM  (Random  Access  Memory)  via \n\nSDRAM  or  Register  control  module  located  on  FPGA  chip.  Internal  register  is  used  in  this \n\ncurrent  system.  Coordinate  information  stored  in  the  register  again  via  the  same  module,  are \n\nsynchronously  sent  to  GLCD  control  module,  7    segment  display  module  and  VGA  control \n\nmodule.  GLCD  control  module  is  integrated  into  FPGA  chip  in  order  to  control  touchscreen \n\nGLCD  panel.  This  module  processes  the  coordinate  information  receieved  from  SDRAM  or \n\nRegister  Control  Module  and  later  enables  this  processed  information  to  be  converted  into \n\nimages.    7    segment  display  control module  is  used  to  control  six  7    segment  diplays. This \n\nmodule  seperates  X  and  Y  coordinate  values  as  3    segment  displays  that  are  recieved  from \n\nSDRAM  or  Register  Control  Module  and  realizes  the  visual  presentation.  On  the  other  hand, \n\nVGA Control Module is used to transfer the images to a computer screen (or a projector device) \n\n \n\n67  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nto be  connected  to main board with  the help  of  a VGA  (Video Graphics Array)  chip.  In  short,  the \n\nfunction of VGA control module is to process the coordinate information receieved from SDRAM \n\nor Register Control Module and to transfer the processes information to computer screen.   \n\n Touchscreen  Writing  /  Drawing  System  is  composed  of  two  hardware  units,  one  of  which  is \n\nLCD  touchscreen  panel  module  (also  called  LTM)  and  the  other  is  control  module  formed \n\nwith FPGA chip.  \n\n \n\n3.1.1. LCD touchscreen panel module \n\n \n\nAs  seen  in  Figure  3,  LCD  Touch  Panel  Module  (LTM)  includes  a  4.3  inch  graphic  LCD  with \n\n800X400 pixels and 24  bit color resolution and also 4.3 inch resistive touchscreen.  \n\nFigure 3 LTM module and the cable used for connection to main board. \n\n \n\n \n\nThis module  is mainly  used  for  system  control  and  image  transfer  function.  However,  it  can  be \n\nused  for  various  applications  as  well  with  its  high  resolution  screen  and  touch  panel  support. \n\nThe  aim  and  the  function  of  LTM  module  in  this  touchscreen  writing  /  drawing  system  are  as \n\nfollows:  Firstly,  the  things written  or  drawn  by  a  special  pen  on  the  active  area  of  the module \n\nare  converted  in  to  12    bit  X  and  Y  coordinate  values  via  an  ADC  chip  to  which  touch  panel  is \n\nconnected. Figure 4 shows the active area of the module and 12  bit X and Y coordinates. \n\n \n\n68  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nFigure 4. The display of coordinates X and Y on LTM module as 12  byte groups. \n\n \n\n \n\n \n\nThe connection between LTM panel and the main system is made by 40   pin socket connection. \n\nThe  connection  of  digitally  converted  X  and  Y  coordinate  values  to  the  main  board,  which  also \n\nincludes the FPGA chip, is also made via this socket (Figure 5).  \n\nSecondly,  the  data  converted  into  meaningful  formats  in  FPGA  chip  is  transferred  back  to  the \n\nGLCD  screen  found  on  that  module.  By  doing  so,  the  visual  transfer  of  the  drawings  made  on \n\ntouch panel is realized (Figure 5).  \n\nFigure 5. Data flow diagram on LTM panel. \n\n \n\n \n\n3.1.2. Control module \n\n \n\nThe  control  module  deals  with  the  following:  FPGA  chip,  which  receives  X  and  Y  coordinates \n\nfrom  GLCD  touch  panel  and  later  enables  the  conversion  of  these  processed  vaules  into  images \n\n \n\n69  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\non  the  LCD  display  of  GLCD  touch  panel  as  well  as  on  a  computer  screen  or  projection  device \n\nsimultaneously;  and  hardware  connections  of  other  chips  which  are  involved  in  the  process  as \n\nsupporting  components.  Figure  6  shows  FPGA  chip    which  is  the  main  component  of \n\ntouchscreen system main board  and other supplementary chips, sockets and connections. \n\nFigure 6. Connections between the chip, sockets and other components on the system main \n\n \n\n \n\nboard. \n\nThe FPGA chip used in touchscreen writing / drawing system developed for the purposes of this \n\ncurrent  study  is  called  EP2C70896C6N  chip,  which  is  the  most  advanced  chip  of  Cyclone  II \n\nfamily produced by Altera company.  \n\n \n\nIt is produced by using 90 nm technology. \n\n \n\nIt includes 68,416 logical elements (LEs). \n\n  Mbit embedded RAM. \n\n  Performance upto 260 MHz. \n\n  Compatible with high  speed mobile external disks such as DDR, DDR2 and SDR SDRAM. \n\n  Multi  volt multiple input/output voltage support (1.5V, 1.8V, 2.5V, 3.3V). \n\n  Configuration in less than 100ms thanks to quick  configuration opition. \n\n  16 sensitive clock input pins. \n\n  A total of 150 18 x 18 multiplexer. \n\n \n\nIt supports configurations in Active serial, Passive serial and JTAG mode.  \n\n \n\n70  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n  896  pin BGA socket structured. \n\n EP2C70896C6N  chip  is  found  to be  the  ideal one when  the  features mentioned above  and  the \n\npurposes  of  the  study  are  considered.  At  this  point,  learning  about  software  as  well  hardware \n\ndesign  of  the  touchscreen  writing  /  drawing  system  is  also  important  in  terms  of  being \n\nfamiliarized with its features and the operation process.  \n\n \n\n3.2. Software Design of Touchscreen Writing / Drawing System   \n\n \n\nSoftware  design  of  touchscreen writing  /  drawing  system mainly  depends  on  the  programming \n\nof  FPGA  chip.  It  is  clear  that  effective  and  efficient  operation  of  the  system  according  to  the \n\npredetermined  purposes  will  be  possible  only  if  the  chip  is  programmed   appropriately,  that  is \n\nsoftware infrastructure should be designed and integrated into the chip. This process also refers \n\nto the configuration of FPGA chip.   \n\nAs  for  the  programming  of  FPGA  chip,  Verilog  software  programming  language  was  used.  In \n\naddition,  Quartus  II  software,  produced  by  Altera  Company,  was  used  for  the  same  purpose.  In \n\nterms of software, FPGA chip is composed of a total of 7 software modules (Figure 7).   \n\nFigure 7. Software modules on FPGA chip. \n\n \n\n \n\nThe basic features and functions of software modules can be explained briefly as follows:  \n\n \n\n3.2.1. Main control module \n\n \n\nMain  control  module  refers  to  the  main  parts  of  software  system  on  FPGA  chip.  It  covers  all \n\nsoftware modules  in  the  chip  and  enables  the  connections  among  the modules.  In  addition,  it  is \n\n \n\n71  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nresponsible  for  the  connections  of  the modules with  external  world  and  the  necessary  controls \n\naccordingly.  \n\n \n\n3.2.2. Delay control module  \n\n \n\nDelay  control  module  enables  the modules  on  the  main  module  to  be  processed  after  a  certain \n\ndelay  time.  The  aim  of  this  function  is  to  ensure  harmonius  operation  between  software  and \n\nhardware processes and to avoid a potential system error accordingly.  \n\n \n\n3.2.3. ADC serial port interface control module \n\n \n\nADC serial port  interface control module  is  responsible  for  the  software controls  related  to ADC \n\nserial  port  interface  communication  during  the  system  operation  process with  regards  to  FPGA \n\nchip.  \n\n \n\n3.2.4. Graphic LCD control module \n\n \n\nGLCD  control  module  is  the  software  module  enabling  data  transfer  between  LCG  driver  chip \n\nand FPGA chip by establishing a protocol necessary for this transfer. This module consists of two \n\nsub   modules. One  of  them, which  is  GLCD  SPI  Control  Sub   module, was  formed  to  create  a \n\ncommon language between LCD driver chip and FPGA by forming the necessary protocol for the \n\ncommunication between  these  two chips. The other sub   module  GLCD Timing sub  module, \n\nis  a  software  module  that  determines  the  quality,  size,  resolution  and  speed  of  the  data  to  be \n\nsent to GLCD.  \n\n \n\n3.2.5. VGA control module \n\n \n\nVGA  control module  is  a  software module  enabling  VGA  signal  synchronization  on  both  vertical \n\nand  horizontal  coordinates  in  order  to  obtain  the  desired  image  levels  in  the  devices  such  as \n\nprojector or monitor that are connected to the system via VGA.  \n\n \n\n3.2.6. Seven  segment control module \n\n \n\nThis module  is  the  software module  that  realizes  the  processes  necessary  to  carry  out  accurate \n\n \n\n72  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\ntransfer of related data and to display it on a seven  segment display group. Here the aim is to \n\nconvert the coordinate data of the points touched on touch panel on X and Y coordinates. \n\n \n\n3.2.7. SDRAM / Flash / EEPROM control module \n\n \n\nBeing  the  last  software  module  in  FPGA,  SDRAM  /  Flash  /  EEPROM  control  module  is \n\nresponsible  for  the  communication between memory units  and  the  system depending  on which \n\ntouchscreen writing / drawing system is used.  \n\nTouchscreen  writing  /  drawing  system,  whose  software  and  hardware  design  has  been \n\nexplained  without  getting  into  technical  details,  was  planned  with  an  approach  to  enable  the \n\nusers  to  complete  the  actions  effectively  based  on  the  purposes  of  the  study.  At  this  point,  it  is \n\nnecessary  to make  an  evaluation  to  determine  the  effectiveness  of  the  system  and  its  adequacy \n\nto  meet  the  purposes  of  the  study.    To  achieve  this  aim,  the  following  evaluation  process  was \n\nplanned. \n\n \n\n4. EVALUATION PROCESS \n\n \n\nThe  evaluation  process  of  touchscreen  writing  /  drawing  system  mentioned  in  this  study \n\nincluded  the  following  phases;  observing  usability  processes  and  the  analysis  of  the  obtained \n\nopinion  data.  Accordingly,  the  details  related  to  evaluation  process  will  be  presented  in  the \n\nfollowing sub  titles and paragraphs.  \n\n \n\n4.1. Evaluation Method \n\n \n\nIn order to evaluate touchscreen writing / drawing system, an approach focusing on the usability \n\nof  the  system was  preferred.  In  order  to  achieve  this  purpose,  Heuristic  Evaluation Method was \n\nused.  This method  is  generally  applied  for  the  usability  evaluation  of  the  systems  that  are  based \n\non learning through trial  error (Nielsen & Molich, 1990).  \n\nIn  this  study    specific  touchscreen  writing  /  drawing  system,  there  are  not  any  written \n\ndocuments  except  the  names  of  the  buttons  on  the  device  activating  the  related  functions.  In \n\nother  words,  the  information  about  the  system  usage  is  limited  to  only  this  text    based \n\ninformation. According to Nielsen, a total of 5 subjects are sufficient to test such systems since 75 \n\n% of the problems can be determined by that number of participants (Nielsen & Landauer, 1993). \n\nSimilarly, in the related studies conducted in the field, the number of the problems identified was \n\n \n\n73  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nnot  found  to  increase  as  the  number  of  the  participants  increased.  The  graph  in  Figure  8 \n\nrepresents the problem  identification data according to the number of participants  in  the studies \n\nconducted by Nielsen and Landauer between 1990 and 1993 (Nielsen &   Landauer,  1993).  \n\nFigure 8. Problem identification rate according to the number of the subjects (Nielsen,  &  \n\n \n\nLandauer,  1993). \n\n \n\nHowever; the study conducted by Faulkner (2003) shows that five participants might not suffice. \n\nIn  the  study,  it  was  found  that  the  problems  identified  by  the  sub  groups  (five  participants) \n\nformed  out  of  a  larger  group  of  60  participants  corresponds  to  85%  of  the  problems  identified \n\nby  the whole  group. As  the  test  simulations  applied  increased,  the  usability  problems  identified \n\nby  groups  of  five  were  found  to  have  a  very  wide  range  of  match  or  mismatch  (between  55 % \n\nand  100 %)  with  the  problems  identified  by  the  whole  group.  This  wide  deviation  implies  that \n\nthe potential problems are very  likely  to be missed when groups of  five participants are used  in \n\nsimilar usability  studies. However, using  groups of  ten participants  increases  this percentage  to \n\nthe  average  95  %,  and  82  %  being  the  lowest  value.  Table  1  shows  the  changes  in  standard \n\ndeviation in brief. \n\n \n\n \n\n \n\n \n\nTable 1. The changes in standard deviation with regards to the related variables. \n\nUser Number \n\nDetermined Minimum \n% \n\nDetermined Mean % \n\nStd. \nDeviation \n\n5 \n\n10 \n\n15 \n\n20 \n\n30 \n\n40 \n\n50 \n\n55 \n\n82 \n\n90 \n\n95 \n\n97 \n\n98 \n\n98 \n\n85,55 \n\n94,69 \n\n97,05 \n\n98,40 \n\n99,00 \n\n99,60 \n\n100,00 \n\n9,30 \n\n3,22 \n\n2,12 \n\n1,61 \n\n1,13 \n\n0,81 \n\n0,00 \n\n74  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nWhen  above  mentioned  data  is  concerned,  a  group  of  10  users  as  the  participants  of  the  study \n\ncan be considered to be better in identifying the usability problems.   \n\n \n\n4.2. The Population and Sample for Evaluation \n\n \n\nThe  population  of  the  current  study  involves  teachers  working  in  primary  schools  regardless  of \n\ntheir  fields  of  teaching.  On  the  other  hand,  the  sample  of  this  study  is  randomly  selected  20 \n\nteachers from various fields of teaching who taught during 2011  2012 academic year.  \n\n \n\n4.3. Evaluation Material \n\n \n\nDue  to  the  limited  number  of  similar  studies  in  the  related  literature,  the  authors  developed  a \n\nsurvey  in  order  to  test  the  usability  of  this  touchscreen  writing/drawing  system.  After  several \n\nprior  analyses,  a  35    item  list  was  prepared  by  the  authors.  Later,  these  items  were  reviewed \n\nand  edited  by  an  expert  and  a  15    item  evaluation  survey  form  was  finalized,  which  aims  at \n\ndetermining  subservientness,  user    friendliness  and  usability  factors  regarding  the \n\nsystem. The distribution of these 15 items according to the factors is as follows: first five 5 items \n\nfor  subservientness,  the  next  5  for  user    friendliness  and  the  last  5  for  usability.  The \n\nparticipants of the study are asked to choose one of three options for each item; namely I agree \n\n(Yes), I partly agree (Partly) and I dont agree (No). \n\n \n\n4.4. Other Approaches used in the Evaluation \n\n \n\nPrior to the survey used as part of  the evaluation approach, the participants were asked  to  fill out \n\nan  information  form  in  order  to  learn  about  their  familiarity  with  the  study  topic.  The  data \n\nobtained  from  this  phase  revealed  that  the  participants  had  enough  knowledge  about  the  topic; \n\nand  therefore,  the  replies  to  the  survey  items  were  considered  to  be  quality  enough  for  the \n\npurposes  of  the  study.  The  details  of  this  form will  not  be  provided  here  so  as  not  to  confuse  the \n\nreaders and to focus on the survey study more. \n\nAnother  approach  used  for  the  evaluation  plan  is  the  observation  of  usability  processes, which  is \n\nbased  on  completing  the  tasks  given  regarding  the  use  of  touchscreen  writing  /  drawing  system \n\ndesigned and developed for the purposes of this study. Although the survey study seemed to have \n\nthe  primary  importance  with  regards  to  evaluation,  this  phase  was  also  crucial  since  it  provided \n\nvaluable  data  about  the  use  of  the  system  before  the  participants  filled  out  the  survey.  The  next \n\nsection presents the data obtained regarding the approach mentioned above. \n\n \n\n75  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n \n\n \n\n5. FINDINGS OF THE EVALUATION PROCESS \n\n \n\nAs  mentioned  in  the  previous  section,  the  process  was  observed  and  the  participants  were \n\nadministered  the  survey  developed  by  the  researchers,  and  later  the  necessary  data  was \n\ncollected  for  the  evaluation  purposes  of  touchscreen  writing  /  drawing  system  designed  and \n\ndeveloped  for  the purposes of  the current study. Therefore;  the data obtained will be presented \n\nin  the  next  sections  in  the  order  followed  while  collecting  the  data;  namely  the  data  regarding \n\nthe observations and the data obtained from the survey study respectively.  \n\n \n\n5.1. Findings Obtained from Usability Process \n\n \n\nThe  usability  process  deals  with  the  time  the  participants  of  the  study  spent  while  completing \n\nthe  tasks  given  as  well  as  the  problems  they  encountered  during  this  completion  process.  The \n\nparticipants were  not  provided  any  information  about  the  use  of  touchscreen writing  / drawing \n\nsystem prior to the process. They were only given information about the objectives of the system \n\nand brief introduction to its working principles.    \n\nAs  part  of  the  process,  the  participants  were  given  a  list  including  a  total  of  8  tasks.  The  time \n\nthey spent while working on the tasks was recorded by an observer as seconds. In addition, each \n\nparticipant was asked to decide on the difficulty level of each task by  marking a five point Likert \n\nscale available below each task in the list.   \n\nThe information and the findings obtained from this process are presented below:  \n\nAs  shown  in Table 2,  the  participants  spent  an  average  of  19  seconds  for  each  task  and  the  task \n\nlist,  which  includes  8  tasks,  was  completed  in  an  average  of  3  minutes  by  per  participant.  The \n\ntask that was completed  in the shortest time (15.1 second) was clear the screen and shut down \n\nthe  sytem  (Task  8), while  the  longest  time  spent was  23,60  seconds  for  the  task which writes: \n\nconnect  touch panel  to  the  screen  and  switch  it  on  in  adaptor mode  (Task  1). The  reason why \n\nTask 1 had the longest completion time might be explained by the fact that the participants used \n\nsuch  a  system  for  the  first  time.  In  other  words,  it  might  have  been  due  to  effect  of  first  time \n\nexperience and momentary panic and nervousness. \n\n \n\n \n\n \n\n \n\n \n\n76  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nTable 2. Usability process  task completion times (P1, P2.. P20= participants). \n\nTask Completion Times (sec.)  \n\nTask \n\nP \nP \nP \nP \nP \nP \nP \nP \nP \nP \nP \nP1 P2 P3 P4 P5 P6 P7 P8 P9 \n19 \n20 \n18 \n17 \n16 \n15 \n14 \n13 \n12 \n11 \n10 \n\nMean \n\n1. Connect touch panel to \nthe screen and switch it \non in Adaptor mode. \n\n2. After touch screen is \ndisplayed, shut it down \nand restart in Battery \nmode. \n\n3. Do some drawings on \nthe screen when draw \nmode is on. \n\n4. Switch to Erase \nmode and erase what \nyou have drawn. \n\n23 24 23 41 13 27 34 16 14 10 19 22 27 25 30 23 18 23 31 29 \n\n23,6\n0 \n\n27 34 35 38 19 23 27 21 21 19 21 18 16 21 17 20 13 18 15 16 21,95 \n\n10 19 15 25 17 12 15 18  7  12 17 26 21 14 23 16 19 17 20 18 17,05 \n\n15 21 19 23 17 18 16  9  12 10 14 19 23 32 28 17 16 26 18 22 18,75 \n\n5. Do drawings on the \nscreen when Draw Bold \n18 18 26 32 23 16 19 16 15 13 16 21 18 17 22 19 15 22 15 17 18,90 \nmode is on. \n\n6. switch to Erase Bold \nmode and erase what \nyou have drawn. \n\n7. Change the font color \nfrom Red to Blue. \n\n8. Clear the screen and \nshut down the system. \n\nTotal: \n\nOverall Mean: \n\n11 27 24 15 19 16 13 11 11 11 20 18 15 21 17 22 19 27 23 26 18,30 \n\n17 15 20 21 21 17 14  8  14 10 23 21 19 25 16 18 22 14 17 15 17,35 \n\n15 14 12 19 14 12  9  7  13 12 19 20 22 15 17 13 18 19 18 14 \n\n15,1\n0 \n\n13\n6 \n\n17\n2 \n\n17\n4 \n\n21\n4 \n\n14\n3 \n\n14\n1 \n\n14\n7 \n\n10\n6 \n\n10\n7 \n\n14\n97 \n9 \n\n16\n5 \n\n16\n1 \n\n17\n0 \n\n17\n0 \n\n14\n8 \n\n14\n0 \n\n16\n6 \n\n15\n7 \n\n15\n7 \n\n \n\n18,87 \n\n \n\n \n\n \n\nThe  Figure  9  represents  the  graph  of  the  average  times  spent  on  task  completion  as  part  of \n\nevaluation process. \n\n \n\n77  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nFigure 9. Usability process  average task completion times. \n\n \n\n \n\n \n\nTable  3  shows  the  data  regarding  the  difficulty  level  of  the  tasks  as  stated  by  the  participants \n\nduring the process. When the table is examined, it is seen that the  mean value for all tasks is 3,75 \n\n(easy). The most difficult task was found to be Task 2 After touch panel is switched on, switch it \n\noff and restart in battery mode with an average of 3,15 while the easiest task was Task 4 with a \n\nbalue of 4,45 which writes clear all drawings from touch panel by using the clear mode. \n\n \n\n \n\n \n\nTable 3. Usability process  difficulty levels of the tasks (P1, P2.. P20= participants).  \n\nTask \n\nDifficulty levels of the tasks  \n(1:very difficult; 2:difficult; 3:almost easy; 4:easy, 5:very \neasy) \n\nMean \n\nP \nP \nP \nP \nP \nP \nP \nP \nP \nP \nP \nP1 P2 P3 P4 P5 P6 P7 P8 P9 \n19 \n20 \n18 \n17 \n16 \n15 \n14 \n13 \n12 \n11 \n10 \n\n1. Connect touch panel to \nthe screen and switch it on \nin Adaptor mode. \n\n2. After touch screen is \ndisplayed, shut it down and \nrestart in Battery mode. \n\n3. Do some drawings on \nthe screen when Draw \nmode is on. \n\n4. Switch to Erase mode \nand erase what you have \ndrawn  \n\n3  4  3  4  3  4  2  3  2  4  4  3  4  3  4  2  3  2  4  4  3,25 \n\n2  3  3  4  3  4  4  3  3  4  2  3  4  3  2  4  3  2  4  3  3,15 \n\n5  4  5  4  4  4  4  3  3  5  4  5  4  4  4  4  3  3  5  4  4,05 \n\n5  5  5  4  5  4  5  4  3  5  5  5  4  5  4  5  4  3  5  4  4,45 \n\n78  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n5. Do drawings on the \nscreen when Draw Bold \nmode is on. \n\n6. Switch to Erase Bold \nmode and erase what you \nhave drawn. \n\n7. Change the font color \nfrom Red to Blue. \n\n8. Clear the screen and shut \ndown the system. \n\n3  3  4  3  4  3  4  4  4  5  3  4  3  4  3  4  4  4  5  3  3,70 \n\n3  4  5  4  4  4  5  4  5  5  4  5  4  4  4  5  4  5  5  3  4,30 \n\n4  4  4  4  4  3  4  3  4  5  4  3  4  4  3  4  3  4  5  3  3,80 \n\n3  4  3  4  3  3  4  4  3  3  4  3  4  3  3  4  4  3  3  4  3,45 \n\nTotal: \n\n28 31 32 31 30 29 32 28 27 36 30 31 31 30 27 32 28 26 36 28 \n\n \n\nOverall Mean: \n\n3,76 \n\n \n\n \n\nFor  all  the  tasks  in  the  process,  the  difficulty  levels  stated  by  the  participants  are  showed  in \n\nFigure 10, which shows that Task 4 was the easiest one and Task 2 the most difficult. \n\nFigure 10. Usability process  difficulty levels of the tasks (1:very difficult; 2: difficult; 3:almost \n\n \n\neasy; 4: easy, 5: very easy). \n\n \n\n5.2 Usability Findings Obtained from the Survey Study  \n\n \n\nAs mentioned  earlier,  a  total  of  20  participants were  administered  a  15    item  survey  after  the \n\nusability process, which aims at evaluating the  touchscreen writing / drawing system in terms of \n\nsubvertiness,  user    friendliness  and  usability.  In  this  regard,  basic  information  about  the \n\napproach and the findings obtained are presented as follows:  \n\n \n\n79  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nAs mentioned earlier, the reply options for the items in the survey were Yes, Partly and No. \n\nThe  answers  given  to  the  questions  in  the  survey  were  analyzed  through  frequency  (f), \n\npercentage (%), and standard deviation calculations. The  items and  the replies    feedback given \n\nto these items are presented in the tables in the following pharagraphs.  \n\n \n\n \n\n \n\nTable 4. The analysis of the replies provided for the statements determining the \n\nsubservientness level of the touchscreen writing / drawing system. \n\nStatements \nDo you think that \n\nyou can teach your lessons more \neasily and quickly by using the \ntouchscreen writing/drawing \nsystem?  \n\nyour current teaching \nperformance might increase by \nusing the touchscreen \nwriting/drawing system? \n\nyour teaching efficiency might \nincrease by using the touchscreen \nwriting/drawing system?  \n\nyou will be more helpful to your \nstudents by using the touchscreen \nwriting/drawing system? \n\nthe touchscreen writing/drawing \nsystemis a useful tool for your \nlessons? \n\nNo \n(1) \n\nPartly \n(2) \n\nYes \n(3) \n\nTotal \n\nf  % \n\nf  % \n\nf  % \n\nf  % \n\nMean \n \n\n2  10  7  35  11  55  20  100 \n\n2,45 \n\n2  10  5  25  13  65  20  100 \n\n2,55 \n\n3  15  5  25  12  60  20  100 \n\n2,45 \n\n4  20  6  30  10  50  20  100 \n\n2,30 \n\n1 \n\n5 \n\n8  40  11  55  20  100 \n\n2,50 \n\nOverall Mean: \n\n2,4  12  6,2  31  11,4  57  20  100 \n\n2,45 \n\nNo (1) 1,00  1,66 %0    %33 \nPartly (2) 1,67  2,33 %34    %66 \nYes (3) 2,34  3,00 %67    %100 \n\nWhen  Table  4  is  examined  and  overall  mean  values  are  considered,  it  is  seen  that \n\nsubservientness was given a high (subservient) value by the participants; that is 11,4 the highest \n\nout  of  20  (57 %).  Similarly,  the  overall mean  (2,45)  shows  that  the  system  is  subservient  since \n\nthe value is between 2,34 and 3,00.   \n\n \n\n80  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nWhen  Table  4  is  examined  in  terms  of  the  items,  the  following  implications  can  be  made: \n\nteachers  can  teach  their  lessons more  easily  and  quickly;  their  performances  and  their  teaching \n\nefficiency  can  be  affected;  touchscreen  writing  /  drawing  system  will  be  beneficial  for  their \n\nstudents; and this system will be a useful teaching tool.  \n\nThe  graphs  showing  the  data  regarding  subservientness  evaluation  results  are presented under \n\nthe Figure 11. \n\nFigure 11. Usability survey study  average values with regards to subservientness evaluation \n\n \n\n \n\nresults. \n\nThe results of  the survey with regards  to  user    friendliness  factor  are represented  in Table 5. \n\nWhen  Table  5  is  examined  and  overall mean  values  are  considered,  the  user    friendliness  of \n\nthe  system  is  found  to  be  evaluated  as  easy  with  the  highest  value  11,2  out  of  20  (56%). \n\nSimilarly,  the  overall mean with  a  value  of  2,37  is  between 2,34  and 3 points, which  shows  that \n\nuser  friendliness of the system is Yes, easy. \n\n \n\n81  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nWhen Table 5 is examined regarding to the items, the following can be concluded: touch panel is \n\nuser    friendly  and  easy    to    learn;  it  is  easy  to  teach  something  by  using  the  touchscreen \n\nwriting/drawing system; interaction with the screen is clear and simple; and finally it is easy to \n\ndo drawings on the screen.  \n\n \n\n \n\nTable 5. The analysis of the replies to the statements aiming at determining the user  \n\nfriendliness level of the touchscreen writing / drawing system. \n\nStatements \n\nDid you find it easy to use the \ntouchscreen writing/drawing \nsystem? \n\nWas it easy to learn how \ntouchscreen writing/drawing \nsystem worked? \n\nDo you think it will be easy to teach \nsomething by using touchscreen \nwriting/drawing system? \n\nDo you think the interaction with \ntouchscreen writing/drawing \nsystem is clear and simple (not \ncomplex)? \n\nWas it easy to make drawings you \nwanted to on touchscreen \nwriting/drawing system?  \n\nNo \n(1) \n\nPartly \n(2) \n\nYes \n(3) \n\nTotal \n\nf  % \n\nf  % \n\nf  % \n\nf  % \n\nMean \n \n\n3  15  4  20  13  65  20  100 \n\n2,50 \n\n4  20  5  25  11  55  20  100 \n\n2,35 \n\n5  25  5  25  10  50  20  100 \n\n2,25 \n\n2  10  5  25  13  65  20  100 \n\n2,55 \n\n5  25  6  30  9  45  20  100 \n\n2,20 \n\nOverall Mean: \n\n3,8  19  5  25  11,2  56  20  100 \n\n2,37 \n\nNo (1) 1,00  1,66 %0    %33 \nPartly (2) 1,67  2,33 %34    %66 \nYes (3) 2,34  3,00 %67    %100 \n\nThe graphs showing mean values regarding user  friendliness are presented under Figure 12.  \n\n \n\n82  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nFigure 12. Usability survey study  The average values regarding the findings of user  \n\nfriendliness evaluation. \n\n \n\n \n\n \n\nThe results of  the survey with regards  to usability  factor are showed  in Table 6. When Table 6 \n\nis examined and overall mean values are considered,  the  usability of  the system  is  found  to be \n\nevaluated  as  partly  usable  with  the  highest  value  10  out  of  20  (50%).  Similarly,  the  overall \n\nmean  with  a  value  of  2,36  is  between  2,34  and  3  points,  which  show  that  the  usability  level  of \n\ntouchscreen writing/drawing system is Yes, it is usable.    \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n83  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nTable 6. The analysis of the replies to the statements aiming at determining the usability levels \n\nof touchscreen writing / drawing system. \n\n \n\n \n\nStatements \n\nDo you thinktouchscreen \nwriting/drawing system is useful \nfor teaching your lesson? \n\nDo you think many features of the \nproduct are necessary and \nappropriate?  \n\nDid you find portability featureof \ntouchscreen writing/drawing \nsystemuseful? \n\nDo you think that being able to \nteach your lesson while sitting is a \nuseful feature? \n\nDo you think writing on a \ntuchscreeen is as easy as writing on \na board?  \n\nNo \n(1) \n\nPartly \n(2) \n\nYes \n(3) \n\nTotal \n\nf  % \n\nf  % \n\nf  % \n\nf  % \n\nMean \n \n\n2  10  6  30  12  60  20  100 \n\n2,50 \n\n1 \n\n5 \n\n8  40  11  55  20  100 \n\n2,50 \n\n2  10  7  35  11  55  20  100 \n\n2,45 \n\n1 \n\n5 \n\n6  30  13  65  20  100 \n\n2,60 \n\n3  15  6  30  11  55  20  100 \n\n2,40 \n\nOverall Mean: \n\n1,8  9  6,6  33  11,6  58  20  100 \n\n2,49 \n\nNo (1) 1,00  1,66 %0    %33 \nPartly (2) 1,67  2,33 %34    %66 \nYes (3) 2,34  3,00 %67    %100 \n\nWhen  Table  6  is  examined  in  terms  of  the  items  the  followings  can  be  inferred:  touchscreen  is \n\nusable  to  teach  lesson;  the  features  of  the  system  are  necessary  and  used  appropriately;  being \n\nportable  increases  the  usability  of  the  system;  and  it  is  as  usable  as  blackboards  since  it  can  be \n\nused to teach while sitting. \n\nThe graphs showing mean values regarding usability factor are presented under Figure 13. \n\n \n\n84  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nFigure 13. Usability survey study  The mean values regarding the findings of usability \n\n \n\nevaluation. \n\n \n\n6. CONCLUSIONS AND FUTURE WORK  \n\n \n\nThe  main  objective  of  the  current  study  is  that  the  touchscreen  writing  /  drawing  system \n\ndesigned  and developed within  the  framework of  this  study will  supplement education process. \n\nIn  addition,  this  system  is  expected  to  contribute  to  educational  technology  field  in  terms  of \n\nextending  the  field  and  giving  it  a multi    directional  feature.  In  this  respect,  the  authors  try  to \n\nintroduce  the  system  as  an  efficient  and  effective  one  so  as  to  integrate  education  and \n\ntechnology.   \n\nThe  literature  review  carried  out prior  to  the  study  revelaed  that  there  is  not  adequate number \n\nof  materials  and  teaching  tools  that  are  likely  to  contribute  to  increase  the  quality  of  activities \n\nused in teaching and to reduce the problems faced by the teachers while teaching. Therefore; the \n\ntouchscreen  writing/drawing  system  designed  and  developed  in  the  current  study  is  thought \n\n \n\n85  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nto be a solution to the needs mentioned.  \n\nIn  parallel  with  the  study,  software  and  hardware  design  process  of  this  system  was  explained \n\nwithout  mentioning  the  complex  technical  details.  In  addition,  the  study  was  evaluated  in \n\ngeneral  first  by  asking  the  teachers,  who  are  the  real  target  users  of  the  system,  to  use  the \n\nsystem  and  then  analyzing  the  feedback  provided  by  them.  Thanks  to  this  evaluation  approach, \n\nwhich involves usability process and the survey study following this process, the authors tried to \n\ndetermine to what extent the system was effective and efficient and the objectives were met.    \n\nWhen  the observations and the data obtained during evaluation process are considered,  it  is clear \n\nthat the touchscreen writing / drawing system is used by teachers in a positive way and therefore \n\nis  likely  to  contribute  to  classroom  teaching  processes  by  increasing  the  effectiveness  and \n\nefficiency of the lessons for both teachers and students. At this point, the usability process applied \n\nrevealed  that  touchscreen  could  be  used  in  a  positive  way  and  is  an  effective  tool  to  provide \n\nsolutions  to  the  limitations  mentioned  before.  The  survey  study  carried  out  after  the  usability \n\nprocess  aimed  at  evaluating  touchscreen  writing/drawing  system  with  regards  to  three \n\nvariables,  namely  subservientess,  user    friendliness  and  usability,  and  the  feedback  received \n\nregarding  these  factors  revealed  positive  opinions  about  the  sytem.  As  a  result,  touchscreen \n\nwriting/drawing  system  developed within  the  framework of  this study  is  thought  to be effective \n\nmaterial  device in meeting the objectives of the study and to contribute the related literature to a \n\ngreat extent.  \n\nThe positive  findings and  feedback obtained  in this study are quite encouraging  for the authors to \n\nconduct further studies on the topic. Accordingly, improving software and hardware infrastructure \n\nof  touchscreen writing/drawing  system  is planned within  the  scope of  these  further  studies.  In \n\nthis  regard,  certain  studies  for  improving  the  system  are  planned  to  have  a  more  effective, \n\ncomprehensive and interactive software in the future versions of the system. Finally, the following \n\nactions are also planned in the future: to develop interfaces to enable the interaction with different \n\nsystems; to use high storage memory units; and to develop touch panels in different sizes. \n\n \n\nACKNOWLEDGMENT \n\nPatent  application  of  the  designed  and  developed  touchscreen  writing  /  drawing  system  was \n\nprocessed  by  the  Turkish  Patent  Institute  (Application No:  2013/01298)   and  supported  by The \n\nScientific and Technological Research Council of Turkey (TUBITAK). \n\n \n\n \n\n \n\n \n\n86  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n \n\n \n\nREFERENCES \n\n \n\nAguirre,  E.  J.  F.  R.,  Moreno,  J.  R.  &  Franco,  M.  F.  F.  (2012).  Design  and  construction  of  an \n\nelectrocardiograph  prototype  with  touch  screen  interface  and  embedded  system  with  16-bit \n\nresolution, In Proocedings Simposio de Tratamiento de Seales, Imgenes Y Visin Artificial, Stsiva. \n\n \n\nAkbaba-Altun,  S.  (2006).  Complexity  of  integrating  computer  technologies  into  education  in \n\nTurkey, Educational Technology & Society, 9(1): 176-187. \n\n \n\nAlsaggaf,  W.,  Hamilton,  M.  &  Harland,  J.  (2012).  Mobile  learning  in  computer  sc ience  lectures, \n\nInternational Journal of e-Education, e-Business, e-Management and e-Learning, 2(6): 493-497. \n\n \n\nAstell,  A.  J.,  Ellis,  M.  P.,  Bernardi,  L.,  Alm,  N.,  Dye,  R.,  Gowans,  G.  &  Campbell,  J.  (2010).  Using  a \n\ntouch  screen  computer  to  support  relationships  between  people with  dementia  and  caregivers, \n\nInteracting with Computers, 22: 267-275. \n\n \n\nBarak,  M.,  Lipson,  A.  &  Lerman,  S.  (2006).  Wireless  laptops  as  means  for  promoting  active \n\nlearning in large lecture halls, Research on Technology in Education, 38(3): 245-63. \n\n \n\nBi, X., Smith, B. A., & Zhai, S. (2012). Multilingual touchscreen keyboard design and optimization. \n\nHumanComputer Interaction, 27(4), 352-382. \n\n \n\nBradley,  S.  L.  (2012).  Tactile media:  Factors  affecting  the  adoption  of  touchscreen  smartphones \n\namong  consumers  with  vision  loss.  Master  of  Sc.  Thesis,  Arts  (Radio,  Television  and  Film)   \n\nUniversity of North Texas, Denton, USA. \n\n \n\nCaviglia-Harris,  J.,  Hall,  S.,  Mullan,  K.,  Macintyre,  C.,  Bauch,  S.  C.,  Harris,  D.,  Sills,  E.,  Roberts,  D., \n\nToomey,  M.  &  Cha,  H.  (2012).  Improving  household  surveys  through  computer-assisted  data \n\ncollection: Use of touch-screen laptops in challenging environments, Field Methods, 24(1): 74-94. \n\n \n\nChourasia, A. O., Wiegmann, D. A., Chen, K. B., Irwin, C. B. & Sesto, M. E. (2013). Effect of sitting or \n\nstanding  on  touch  screen  performance  and  touch  characteristics,  Human  Factors: The  Journal  of \n\nthe Human Factors and Ergonomics Society, 55(4), 789-802. \n\n \n\n \n\n87  \n \n\n\f \n\nJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\nColle,  H.  A.  &  Hiszem,  K.  J.  (2004).  Standing  at  a  kiosk:  Effects  of  key  size  and  spacing  on  touch \n\nscreen numeric keypad performance and user preference, Ergonomics, 47: 1406-1423. \n\n \n\nDeperlioglu, O & Kose, U. (2013). The effectiveness and experiences of blended learning  \n\napproaches  to  computer  programming  education,    Computer  Applications  in  Engineering \n\nEducation, 21(2), 328-342. \n\n \n\nDexter, L., Anderson, E. & Becker, J. (1999). Teachers views of computers as catalysts for changes \n\nin their teaching practice, Journal of Research on Computing in Education, 31 (3): 221-239. \n\n \n\nDixon, M. & Prior, M.  (2012). Design and usability  testing of anonymous  touch -screen  electronic \n\nqueuing  system:  Towards \n\nimproving  patient  satisfaction,  In  Proocedings  International \n\nConference in Green and Ubiquitous Technology, Jakarta. \n\n \n\nDogan,    H.  (1997).  Program  and  Instruction  Design  in  Education,  (In  Turkish),  Onder \n\nPublishing, Ankara: Turkey, 87 -122. \n\n \n\nFaulkner,  L.  (2003).  Beyond  the  five-user  assumption:  Benefits  of  increased  sample  sizes  in \n\nusability testing. Behavior Research Methods, Instruments and Computers, 35 (3): 379-383. \n\n \n\nHembrooke,  H.  &  Gay,  G.  (2003).  The  laptop  and  the  lecture:  The  effects  of  multitasking  in \n\nlearning environments, Journal of Computing in Higher Education, 15(3): 46-64. \n\n \n\nJin, Z., Plocher, T. & Kiff, L. (2007). Touch screen user interfaces for older adults: Button size and \n\nspacing.  In  Universal  access  in  human  computer  interaction:  Coping  with  diversity  (pp.933941). \n\nBerlin, Germany: Springer-Verlag. \n\n \n\nKose,  U.  (2010).  Web  2.0  Technologies  in  E -learning.  In  Free  and  Open  Source  Software  for  E -\n\nlearning: Issues, Successes and Challenges  (pp. 1-23). Hershey, USA: IGI Global. \n\n \n\nLee,  A.  C.  K.  (2003).  Undergraduate  students  gender  d ifferences  in  IT  skills  and  attitudes, \n\nJournal of Computer Assisted Learning , 19(4): 488500. \n\n \n\nMartin, G. L. (1988). Configuring a numeric keypad for a touch screen, Ergonomics, 31: 945-953. \n\n \n\nMcCormack, C. & Jones, D. (1997). Building a Web -based Education System, Wiley, New York.  \n\n \n\n88  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n \n\n \n\nNewman, E. D., Lerch, V.,  Jones,  J. & Stewart, W.  (2012). Touchscreen questionnaire patient data \n\ncollection  in  rheumatology  practice:  Development  of  a  highly  successful  system  using  process \n\nredesign, Arthritis Care & Research, 64: 589-596. \n\n \n\nNi,  X.  &  Branch,  M.  (2004).  Experience  of  using  laptop  in  higher  education  institutions:  Effects \n\nwith  and  of  ubiquitous  computing  under  natural  conditions,  Proceedings  International \n\nConference of Association for Educational Communications and Technology, Chicago. \n\n \n\nNielsen,  J.  &   Landauer,  T.  K.  (1993).  A  mathematical  model  of  the  finding  of  usability \n\nproblems, Proceedings ACM/IFIP INTERCHI93  Conference , Amsterdam, 206-213. \n\n \n\nNielsen,  J.  &  Molich,  R.  (1990).  Heuristic  evaluation  of  user  interfaces,  Proceedings  ACM \n\nCHI'90 Conference, Seattle, 249-256. \n\n \n\nNilson,  L.  &  Weaver,  E.  (2005).  Enhancing  learning  with  laptops  in  the  classroom.  In  New \n\ndirections for teaching and learning, no. 101, Jossey-Bass, San Francisco. \n\n \n\nPalaigeorgiou,  G.  E.,  Siozos,  P.  D.,  Konstantakis,  N.  I.  &  Tsoukalas,  I.  A.  (2005).  A  computer \n\nattitude scale for computer science freshmen and its educational implications,  \n\nJournal of Computer Assisted Learning,  21(5): 330342. \n\n \n\nRahman,  M.  M.  &  Aktaruzzaman,  M.  (2012).  Design  and  development  of  a  simple  low -cost \n\ntouchscreen  to  control  home  automation  system,  International  Journal  of  Information \n\nTechnology and Computer Science, 11: 26-33. \n\n \n\nRaj,  T.  D.  S.,  Sarah,  T.,  Dhinagaran,  D.  &   Ugargol,  A.  P.  (2013).  Design,  development  and \n\nimplementation  of  a  touch-screen  health  information  kiosk  for  patients  at  the  outpatient \n\nwaiting area  in a  large  tertiary care hospital  in  India: An evaluation of user satisfaction,  Journal \n\nof Health Informatics in Developing Countries,  7(1): 8-19. \n\n \n\nRichey,    R.  C.  (2008). Reflections  on  the  2008  AECT  definitions  of  the  field,  TechTrends,  52:  24-\n\n25.   \n\n \n\nSchultz,  K.  L.,  Batten,  D.  M.  &  Sluchak,  T.  J.  (1998).  Optimal  viewing  angle  for  touch -screen \n\ndisplays: Is there such a thing?, International Journal of Industrial Ergonomics, 22: 343-350. \n\n \n\n89  \n \n\n\fJournal of Multidisciplinary Developments. 1(1), 60-90, 2016 \n\n \n\n \n\n     e-ISSN: 2564-6095 \n\nDevelop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. & Kose, U. \n\n \n\n \n\nSesto,  M.  E.,  Irwin,  C.  B.,  Chen,  K.  B.,  Chourasia,  A.  O.  & Wiegmann,  D.  A.  (2012).  Effect  of  touch \n\nscreen  button  size  and  spacing  on  touch  characteristics  of  users  with  and  without  disabilities, \n\nHuman Factors, 54(3): 425-436. \n\n \n\nShervin,  N.,  Dorrwachter,  J.,  Bragdon,  C.,  Shervin,  D.,  Zurakowski,  D.  &  Malchau,  H.  (2011). \n\nComparison  of paper  and  computer-based questionnaire modes  for measuring health outcomes \n\nin patients undergoing total hip arthroplasty, Journal of Bone and Joint Surgery, 93: 285. \n\n \n\nTaylor, N.  J.  (2012). A Near Touch User  Interface  for Touch  Screen Based  Systems. Master of  Sc. \n\nThesis,  Electronics  and  Computer  Systems  Engineering   Massey  University,  Palmerston  North, \n\nNew Zealand. \n\n \n\nTufekci,  A.,  Ekinci,  H.,  &  Kose,  U.  (2013).  Development  of  an  internet-based  exam  system  for \n\nmobile environments  and  evaluation of  its usability. Mevlana  International  Journal of Education, \n\n3(4), 57-74. \n\n \n\nVan  Braak,  J.  P.  (2004).  Domains  and  determinants  of  university  students  self-perceived \n\ncomputer competence, Computers & Education, 43(3): 299312. \n\n \n\nWilson,  K.  S.,  Inderrieden,  M.  &  Liu,  S.  (1995).  A  comparison  of  five  user  interface  devices \n\ndesigned for point-of-sale in the retail industry, Proceedings the Human Factors and \n\nErgonomics Society 39th Annual Meeting, Santa Monica. \n\n \n\n \n\nJOMUDE \nhttp://www.jomude.com \n \n\n \n\n90  \n \n\n\f", 
        "tag": "Human-Computer Interaction", 
        "link": "https://arxiv.org/list/cs.HC/new"
    }, 
    {
        "text": "Rhetorical Relations for Information Retrieval\n\nChristina Lioma\nComputer Science\nUniversity of Copenhagen\nDenmark\nc.lioma@diku.dk\n\nBirger Larsen\nRoyal School of Library and\nInformation Science\nCopenhagen Denmark\nblar@iva.dk\n\nWei Lu\nSchool of Information\nManagement\nWuhan University China\nreedwhu@gmail.com\n\n7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nR\nI\n.\ns\nc\n[\n \n \n1\nv\n9\n9\n5\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nABSTRACT\nTypically, every part in most coherent text has some plausi-\nble reason for its presence, some function that it performs to\nthe overall semantics of the text. Rhetorical relations, e.g.\ncontrast, cause, explanation, describe how the parts of\na text are linked to each other. Knowledge about this so-\ncalled discourse structure has been applied successfully to\nseveral natural language processing tasks. This work stud-\nies the use of rhetorical relations for Information Retrieval\n(IR): Is there a correlation between certain rhetorical rela-\ntions and retrieval performance? Can knowledge about a\ndocuments rhetorical relations be useful to IR?\nWe present a language model modication that considers\nrhetorical relations when estimating the relevance of a doc-\nument to a query. Empirical evaluation of dierent versions\nof our model on TREC settings shows that certain rhetorical\nrelations can benet retrieval eectiveness notably (> 10%\nin mean average precision over a state-of-the-art baseline).\n\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Retrieval\nModels; H.3.1 [Information Storage and Retrieval]: Con-\ntent Analysis and Indexinglinguistic processing\n\nKeywords\nRhetorical relations, discourse structure, retrieval model,\nprobabilistic retrieval\n\n1.\n\nINTRODUCTION\nAccording to discourse analysis, every part in most coher-\nent text tends to have some plausible reason for its presence,\nsome function that it performs to the overall semantics of\nthe text. Rhetorical relations, e.g. contrast, explana-\ntion, condition, are considered critical for text interpre-\ntation, because they signal how the parts of a text are linked\nto each other to form a coherent whole [23]. Unlike gram-\nmatical relations, which are generally explicitly manifest in\n\nPreprint of: Christina Lioma, Birger Larsen, and Wei Lu. Rhetorical re-\nlations for information retrieval. In: The 35th International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR\n12, Portland, OR, USA, August 12-16, 2012. Ed. by William R. Hersh,\nJamie Callan, Yoelle Maarek, and Mark Sanderson. ACM, 2012, pp. 931-\n940. isbn: 978-1-4503-1472-5. doi: 10.1145/2348283.2348407.\n.\n\nFigure 1: Rhetorical relations example (from [11]).\n\nlanguage, rhetorical relations may be unstated. The goal of\ndiscourse analysis is therefore to infer rhetorical relations,\nand specically to identify their span, constraints and func-\ntion.\nThere is a large body of research on both descriptive\nand predictive models of rhetorical structure and discourse\nanalysis in natural language text. For instance, annotation\npro jects have taken signicant steps towards developing se-\nmantic [12, 18] and discourse [5] annotated corpora. Some of\nthese annotation eorts have already had a computational\nimpact, making it possible to automatically induce semantic\nroles [15] and to automatically identify rhetorical relations\n[14], achieving near-human levels of performance on certain\ntasks [27].\nIn addition, applications of discourse analysis\nto automatic language processing tasks such as summarisa-\ntion or classication (overviewed in section 2) indicate that\nrhetorical relations can enhance the performance of well-\ntrained natural language processing systems.\nMotivated by these advances, this work brings perspec-\ntives from discourse analysis into Information Retrieval (IR)\nwith the aim of investigating if and how rhetorical relations\ncan benet retrieval eectiveness. Is there a correlation be-\ntween certain rhetorical relations and retrieval performance?\nCan knowledge about a documents rhetorical relations be\nuseful to IR? For example, consider the rhetorical relations\nof the text shown in Figure 1 (borrowed from [11]). Should\nsome of the terms in this sentence be given extra weight by\nan IR system, according to their rhetorical relations? Can\nsome rhetorical relations be considered more informative and\nhence more useful for IR ranking than others? These ques-\ntions have been posed before (see discussion in section 2),\nhowever to our knowledge this is the rst time that a prin-\ncipled integration of rhetorical relations into a probabilistic\nIR model improves precision by > 10%.\nReasoning about query - document relevance using the\nlanguage modeling formalism [9], we present a model that\nconditions the probability of relevance between a query and\n\n\fa document on the rhetorical relations occurring in that doc-\nument. We present an application of this model to an IR\nre-ranking task, where, given a list of documents initially\nretrieved for a query, the goal is to improve the ranking\nof the documents by rening their estimation of relevance\nto the query. Experimental evaluation of dierent versions\nof our model on TREC data and standard settings demon-\nstrates that certain rhetorical relations can be benecial to\nretrieval, with notable improvements to retrieval eective-\nness (> 10% in mean average precision and other standard\nTREC evaluation measures over a state-of-the-art baseline).\n\n2. RELATED WORK\nDiscourse analysis and rhetorical structures have been stud-\nied in the context of several automatic text processing ap-\nplications. This has been partly enabled by the availability\nof discourse parsers - see [11, 14] for up-to-date overviews\nof discourse parsing technology. Studies of discourse analy-\nsis in relation to IR and its broader applications are briey\noverviewed below. For a more general overview of discourse\nanalysis approaches, see Wang et al. [33], section 2.\nSun & Chai [28] investigate the role of discourse process-\ning and its implication on query expansion for a sequence\nof questions in scenario-based context question answering\n(QA). They consider a sequence of questions as a mini dis-\ncourse. An empirical examination of three discourse theo-\nretic models indicates that their discourse-based approach\ncan signicantly improve QA performance over a baseline of\nplain reference resolution.\nIn a dierent task, Wang et al. [33] parse Web user forum\nthreads to determine the discourse dependencies between\nposts in order to improve information access over Web fo-\nrum archives. They present three dierent methods for clas-\nsifying the discourse relationships between posts, which are\nfound to outperform an informed baseline.\nHeerschop et al. [16] perform document sentiment analy-\nsis (partly) based on a documents discourse structure. They\nhypothesise that by splitting a text into important and less\nimportant text spans, and by subsequently making use of\nthis information by weighting the sentiment conveyed by\ndistinct text spans in accordance with their importance,\nthey can improve the performance of a sentiment classier.\nA documents discourse structure is obtained by applying\nrhetorical structure theory on a sentence level. They re-\nport a 4.5% improvement in sentiment classication accu-\nracy when considering discourse, in comparison to a non-\ndiscourse based baseline. Similarly to this study, Somasun-\n[26] report improvements to opinion polarity\ndaran et al.\nclassication when using discourse, and Morato et al.\n[24]\nreport a positive dependence between classication perfor-\nmance and certain discourse variables. An overview of dis-\ncourse analysis for opinion detection can be found in Zhou\net al. [36].\nIn the area of text compression, Louis et al. [21] study the\nusefulness of rhetorical relations between sentences for sum-\nmarisation. They nd that most of the signicant rhetorical\nrelations are associated to non-discriminative sentences, i.e.\nsentences that are not important for summarisation. They\nreport that rhetorical relations that may be intuitively per-\nceived as highly salient do not provide strong indicators of\ninformativeness;\ninstead, the usefulness of rhetorical rela-\ntions is in providing constraints for navigating through the\ntexts structure. These ndings are compatible with the\n\nstudy of Clarke & Lapata [7] into constraining text com-\npression on the basis of rhetorical relations. For a more in-\ndepth look into the impact of individual rhetorical relations\nto summarisation see Teufel & Moens [30].\nIn domain-specic IR, Yu et al. [34] focus on psychiatric\ndocument retrieval, which aims to assist users to locate doc-\numents relevant to their depressive problems. They propose\nthe use of high-level discourse information extracted from\nqueries and documents, such as negative life events, depres-\nsive symptoms and semantic relations between symptoms, to\nimprove the precision of retrieval results. Their discourse-\naware retrieval model achieves higher precision than the vec-\ntor space and Okapi models.\nCloser to our work, Wang et al. [31] extend an IR ranking\nmodel by adding a re-ranking strategy based on document\ndiscourse. Specically, their re-ranking formula consists of\nthe original retrieval status value computed with the BM11\nmodel, which is then multiplied by a function that linearly\ncombines inverse document frequency and term distance for\neach query term within a discourse unit. They focus on one\ndiscourse type only (advantage-disadvantage) which they\nidentify manually in queries, and show that their approach\nimproves retrieval performance for these queries. Our work\ndiers on several points. We use an automatic (not man-\nual) discourse parser to identify rhetorical relations in the\ndocuments to be retrieved (not queries). We consider 15\nrhetorical relations (not 1) and we study their impact to re-\ntrieval performance using a modication of the IR language\nmodel.\nFinally, Suwandaratna & Perera [29] also present a re-\nranking approach for Web search that uses discourse struc-\nture. They report a heuristic algorithm for rening search\nresults based on their rhetorical relations. Their implemen-\ntation and evaluation is partly based on a series of ad-hoc\nchoices, making it hard to compare with other approaches.\nThey report a positive user-based evaluation of their system\nfor ten test cases.\n\n3. RANKING WITH RHETORICAL\nRELATIONS\nThere may be various ways of considering rhetorical rela-\ntions in an IR setting. In this work, we view rhetorical rela-\ntions as non-overlapping text spans, rather than a graph or\na tree with structure and overlapping nodes [27]. We select a\nprincipled integration of rhetorical relation information into\nthe retrieval model that ranks documents with respect to\nqueries. The goal is to enable evidence about the rhetorical\nrelations in a document to have a quantiable impact upon\nthe estimation of relevance of this document to a query, and\nto study that impact.\n\n3.1 Model Derivation\nLet q be a query, d a document, D a collection of docu-\nments, and g a rhetorical relation in the collection (so that\np(g |d) = 1). In probabilistic IR, each d in D can be\nPg\nranked by its probability p(d|q) of being relevant to q . Using\nBayes law:\n\np(d|q) =\n\np(q |d)p(d)\np(q)\n\nrank= p(q |d)\n\n(1)\n\nwhere the right-hand side of Equation 1 is derived as follows:\np(q) is dropped because it is xed for all documents, and\n\n\fp(d) can be dropped on the assumption that it is uniform\nin the absence of any prior knowledge about any document.\nUsing the language modeling approach to IR [9], p(q |d) can\nbe interpreted as the probability of generating the terms in\nq from a model induced by d, or more simply how likely it\nis that the document is about the same topic as the query.\np(q |d) can be estimated in dierent ways, for instance using\nDirichlet, Jelinek-Mercer, or two-stage smoothing [35].\nWe introduce into Equation 1 the probability of generat-\ning the query terms from a model induced by d and by its\nrhetorical relations   d as follows:\n\np(q |d, g )p(g |d)\n\np(q |d) = Xg\nWe now explain the two components in Equation 2. The\nrst component, p(q |d, g ), can be interpreted as the prob-\nability of generating the query terms from a model induced\nby d and g . We estimate p(q |d, g ) as a simple mixture of\nthe probabilities of generating q from d and g :\n\n(2)\n\np(q |d, g ) = (1  )  p(q |d) +   p(q |g )\n\n(3)\n\nwhere p(q |d) is the (baseline) probability of relevance be-\ntween q and d mentioned in the beginning of this section, \nis a free parameter, and p(q |g ) can be interpreted as the\nprobability of generating q from a model induced by the\nrhetorical relation g , or more simply, the likelihood of rel-\nevance between the terms in the query and the terms in the\nrhetorical relation.\nThe second component of Equation 2, p(g |d), is the prob-\nability of the rhetorical relation given the document. Simi-\nlarly to above, this can be interpreted as the probability of\ngenerating the terms in g from a model induced by d, or\nmore simply the likelihood of relevance between the terms\nin the rhetorical relation and the terms in the document.\n\n3.2 Model Induction\nTo make Equations 2-3 operational we need to compute\np(q |) and p( |d). One simple way of doing so is using the\nrespective maximum likelihood estimations:\n\nlog p(q |g ) =\n\n|q|\nXi=1\nwhere f (qi , g ) is the frequency of the query term qi in g ,\nand |g | is the number of terms in g .\n\nf (qi , g )\n|g |\n\n(4)\n\n(5)\n\nlog p(g |d) =\n\nf (gj , d)\n|d|\n\n|g |\nXj=1\nwhere f (gj , d) is the frequency of the rhetorical relation\nterm gj in d, and |d| is the number of terms in d. In this\nwork, we use the above equations and, to compensate for\nzero-frequency cases, we apply add-one smoothing.\nAlternative principled estimations of Equations 4-5 are\npossible (e.g. Dirichlet, Good-Turing) and could poten-\ntially improve the performance reported in this work. For\ninstance, one could discount the frequencies in Equations\n4-5 by a respective collection model using Dirichlet smooth-\nf (qi ,g )+p(qi |)\nlog ps(q |g ) = P|q|\ning:\nwhere  would\ni=1\n|g |+\nbe the smoothing parameter and  would be the collec-\ntion of all rhetorical relations in D. A similarly Dirichlet\n\nsmoothed alternative estimation of Equation 5 would be:\nlog ps (g |d) = P|g |\nf (gj ,d)+p(gj |D)\n. We choose to use\nj=1\n|d|+\nmaximum likelihood instead of Dirichlet to avoid introduc-\ning the extra Dirichlet smoothing parameter  when inves-\ntigating the eect of rhetorical relations upon retrieval.\nAnother alternative would be to use Good-Turing smooth-\ning, however doing so would scale down the maximum like-\nlihood estimations in Equations 4-5 by a factor of 1  E (1)\n|g |\nrespectively, where E (1)\n|g | (resp. E (1)\nand 1  E (1)\n|d| ) is the\n|d|\nestimate of how many items in the numerator of Equation 4\n(resp. Equation 5) have occurred once in the sample of the\ndenominator (see Gale & Sampson [13] for more on Good-\nTuring smoothing).\nIn eect, for Equation 4 this scaling\ndown would reduce the probability of the query terms that\nwe have seen in g , making room for query terms that we\nhave not seen. For our setting this would not be necessary,\nbecause in practice most queries and most rhetorical rela-\ntions correspond to rather short text spans. Good-Turing\nsmoothing might be better suited for larger samples [13].\nOverall, the model presented in this section can be seen as\na basic model for ranking documents (partly) according to\ntheir rhetorical relations. Dierent variations on this basic\nmodel are certainly possible, however we choose to use the\nsimple maximum likelihood version of this model for this\nexploratory investigation into the potential benets of using\nrhetorical relations for IR.\n\n4. EVALUATION\n\n4.1 Experimental Setup\nWe evaluate our model on the task of re-ranking an initial\nlist of documents, which has been retrieved in response to\na query. Re-ranking is a well-known IR practice that can\nenhance retrieval performance notably [19]. The baseline\nof our experiments consists of the top 1000 documents re-\ntrieved for each query using a state-of-the-art retrieval model\n(language model with Dirichlet smoothing1 [9]). Our ap-\nproach reranks these documents using Equation 2.\n\n4.1.1 Dataset and Pre-processing\nWe experiment with the TREC datasets of the Web 2009\n(queries 1-50) and Web 2010 (queries 51-100) tracks, that\ncontain collectively 100 queries and their relevance assess-\nments on the Clueweb09 cat. B dataset2 (50,220,423 web\npages in English crawled between January and February\n2009). We choose these datasets because they are used\nwidely in the community, allowing comparisons with state-\nof-the-art. We remove spam using the spam rankings of Cor-\nmack et al. [8] with the recommended setting of percentile-\nscore < 70 indicating spam3 .\nWe consider a subset of this collection, consisting of the\ntop 1000 documents that have been retrieved in response to\neach query by the baseline retrieval model on tuned settings\n(described in section 4.1.2) using the Indri IR system4 for\n\n1We also experimented with Jelinek-Mercer and two-stage\nsmoothing for the baseline retrieval model. Dirichlet and\ntwo-stage gave higher scores. We chose Dirichlet over two-\nstage because it includes one less parameter to tune.\n2http://lemurpro ject.org/clueweb09.php/\n3Note that removing spam from Clueweb09 cat B. is known\nto give overall lower retrieval scores than keeping spam [3].\n4http://www.lemurpro ject.org/\n\n\fTable 1: Examples of the 15 rhetorical relations (in bold italics) of our dataset, identied by the SPADE\ndiscourse parser [27]\nRhetorical relation Example sentences with rhetorical relations italicised and bold\n... the islands now known as the Gilbert Islands were settled by Austronesian-speaking people ...\nattribution\n... many whites had left the country when Kenyatta divided their land among blacks ...\nbackground\n... I plugged wives into the search box and came up with the fol lowing results ...\ncause-result\ncomparison\n... so for humans, it is stronger than coloured to frustrate these unexpected numbers ...\n... Conditional money based upon care for the pet ...\ncondition\n... voltage drop with the cruise control switch could cause erratic cruise control operation ...\nconsequence\n... Although it started out as a research project , the ARPANET quickly developed into ...\ncontrast\n... order accutane no prescription required ...\nelaboration\nenablement\n... The pro ject will also oer exercise programs and make eye care services accessible ...\n... such advances will be reected in an ever-greater proportion of grade A recommendations ...\nevaluation\n... the concept cal led as evolutionary developmental biology or shortly evo-devo ...\nexplanation\n... Fill current path using even-odd rule, then paint the path ...\nmanner-means\n... Safety Last, Girl Shy, Hot Water, The Kid Brother, Speedy (al l with lively orchestral scores) ...\nsummary\ntemporal\n... Take time out before you start writing ...\n... Director Mark Smith expressed support for greyhound adoption ...\ntopic-comment\n\nindexing and retrieval. For this subset, we strip HTML an-\nnotation using our in-house WHU-REAPER crawling and\nweb parsing toolkit5 . Rhetorical relations are identied us-\ning the freely available SPADE discourse parser [27]. Table\n1 shows the 15 types of rhetorical relations identied by this\nprocess, with examples taken from the re-ranking dataset.\n\n4.1.2 Parameter Tuning\nTwo parameters are involved in these experiments: the\nDirichlet smoothing parameter  of the retrieval model (used\nby both the baseline and our approach) and the mixture\nparameter  of our model. Both parameters are tuned using\n5-fold cross validation for each query set separately; results\nreported are the average over the ve test sets.  is tuned\nacross {100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,\n10000} (using the range of Zhai & Laerty [35]) and  is\ntuned across {0.1, 0.3, 0.5, 0.7, 0.9}.\nPerformance is reported and tuned separately for Mean\nAverage Precision (MAP), Binary Preference (BPREF), and\nNormalised Discounted Cumulated Gain (NDCG). These\nmeasures contribute dierent aspects to the overall evalua-\ntion: BPREF measures the average precision of a ranked list;\nit diers from MAP in that it does not treat non-assessed\ndocuments as explicitly non-relevant (whereas MAP does)\n[4]. This is a useful insight, especially for a collection as\nlarge as Clueweb09 cat. B where the chances of retrieving\nnon-assessed documents are higher. NDCG measures the\ngain of a document based on its position in the result list.\nThe gain is accumulated from the top of the ranked list to\nthe bottom, with the gain of each document discounted at\nlower ranks. This gain is relative to the ideal based on a\nknown recall base of relevance assessments [17]. Finally, we\ntest the statistical signicance of our results using the t-test\nat 95% and 99% condence levels [25].\n\n4.2 Findings\nFigure 2 shows the distribution of the rhetorical rela-\ntions in our re-ranking dataset as a percentage of the total\nnumber of rhetorical relations. Elaboration, attribution\nand background are the most frequent rhetorical relations,\nwhereas topic-comment is the most infrequent. This hap-\n\n5Freely available by emailing the third author.\n\npens because quite often in text a topic forms the nucleus\nof the discourse, which is then linked by a number of dier-\nent rhetorical relations, for instance about its background,\nelaborating on an aspect, or attributing parts of it to some\nentity. As a result, several types of other rhetorical rela-\ntions can correspond to a single topic-comment. Note that\nthe distribution of rhetorical relations reported here is in\nagreement with the literature, e.g. Teufel & Moens [30] also\nreport a 5% occurrence of contrast, albeit in the domain\nof scientic articles.\n\n4.2.1 Retrieval-Enhancing Rhetorical Relations\nTable 2 shows the performance of our model against the\nbaseline, for each rhetorical relation and evaluation measure.\nThe baseline performance is among the highest reported in\nthe literature for these setings; for instance Bendersky et al.\n[3] report MAP=0.1605 for a tuned language model baseline\nwith the Web 2009 track queries on Clueweb cat. B without\nspam.\nWe observe that dierent rhetorical relations perform dif-\nferently across evaluation measures and query sets. The four\nrhetorical relations that improve performance over the base-\nline consistently for all evaluation measures and query sets\n(shaded rows in Table 2) are: background, cause-result,\ncondition and topic-comment. Topic-comment is one of the\noverall best-performing rhetorical relations, which in simple\nterms means that boosting the weight of the topical part of\na document improves its estimation of relevance.\nA closer look at which rhetorical relations decrease per-\nformance presents a more uneven picture as no relations\nconsistently underperform for all measures and query sets.\nSome relations, such as explanation and enablement for\nWeb 2009, and summary and evaluation for Web 2010, are\namong the lowest performing, but are not under the baseline\nacross all measures and both query sets. This implies that\nseparating rhetorical relations into those that generally can\nenhance retrieval performance and those that cannot may\nnot be straight-forward. Even though exploring the fam-\nily likeness between useful relations and ones that give no\nmileage is an interesting discussion, in the rest of the pa-\nper we focus on those rhetorical relations that consistently\nimprove retrieval performance (for these datasets).\n\n\frhetorical relation\n\nMAP\n\nTable 2: Retrieval performance with rhetorical relations and without (baseline). * (**) marks stat. signif-\nicance at 95% (99%) using the t-test. Bold means > baseline. % shows the dierence from the baseline.\nShaded rows indicate consistent improvements over the baseline at all times.\nWeb 2010 (queries 51-100)\nWeb 2009 (queries 1-50)\nNDCG\nBPREF\nNDCG\nBPREF\n0.1625\n0.2920\n0.0986\n0.3893\n0.3230\n0.2240\n0.1654* +1.8% 0.3275** +1.4% 0.3927** +0.9% 0.0924\n-6.2% 0.2549** +13.8% 0.3008** +3.0%\n0.1646\n+1.3% 0.3291** +1.9% 0.3910\n+0.4% 0.1086* +10.2% 0.2623** +17.1% 0.3070** +5.1%\n+11.2% 0.3079\n+2.9% 0.2491*\n+0.2% 0.1015\n+0.1% 0.3255** +0.8% 0.3900\n0.1626\n+5.4%\n+1.9% 0.3040** +4.1%\n+3.1% 0.2282\n-0.4% 0.1017\n-0.9% 0.3251*\n0.1610\n+0.6% 0.3877\n+0.5%\n+1.3% 0.2470** +10.3% 0.2936\n+0.3% 0.0999\n+0.5% 0.3258** +0.9% 0.3903\n0.1632\n-0.5% 0.0945\n-1.4% 0.3250\n0.1602\n+0.6% 0.3874\n-4.1% 0.2377*\n+6.1% 0.2840**\n-2.7%\n+0.1% 0.1103* +11.8% 0.2531** +13.0% 0.3069** +5.1%\n-4.6% 0.3269** +1.2% 0.3897\n0.1549*\n0.1556*\n-4.2% 0.3292** +1.9% 0.3866\n-0.7% 0.0951\n-3.5% 0.2598** +16.0% 0.3005** +2.9%\n+2.5%\n+3.4% 0.2992*\n+2.4% 0.2316*\n-0.6% 0.1010\n+0.3% 0.3869*\n-1.4% 0.3240\n0.1601\n-17.4% 0.2313*\n-0.2% 0.0814**\n+0.4% 0.3886\n+0.5% 0.3242\n0.1632\n+3.3% 0.2902\n-0.6%\n+4.9% 0.2645** +18.1% 0.3069** +5.1%\n-2.1% 0.1034\n+0.9% 0.3813\n-4.9% 0.3259*\n0.1546\n-0.8%\n+3.7% 0.2897\n-\n-0.2% 0.0986\n+0.7% 0.3884\n-0.1% 0.3253*\n0.1623\n0.2324*\n+0.3%\n-12.6% 0.2220*\n-0.4% 0.0862\n+0.1% 0.3241\n0.1626\n+0.3% 0.3879\n-0.9% 0.2928\n0.1615\n-0.6% 0.3262** +1.0% 0.3887\n-0.2% 0.0921\n-6.6% 0.2546** +13.7% 0.3052\n+4.5%\n+3.1%\n+10.5% 0.3009\n+2.1% 0.1090* +10.5% 0.2476*\n+4.5% 0.3976*\n+3.0% 0.3375\n0.1673\n\nnone (baseline)\nattribution\nbackground\ncause-result\ncomparison\ncondition\nconsequence\ncontrast\nelaboration\nenablement\nevaluation\nexplanation\nmanner-means\nsummary\ntemporal\ntopic-comment\n\nMAP\n\nTable 3: Eect of the rhetorical relation to the re-\ntrieval model as indicated by parameter  (see Equa-\ntion 3), for the tuned runs of Table 2. Shaded\nrows indicate rhetorical relations that consistently\nimprove performance over the baseline at all times.\nrhetorical\nWeb 2009 (queries 1-50) Web 2010 (queries 51-100)\nMAP BPREF NDCG MAP BPREF NDCG\nrelation\n0.3\n0.5\n0.3\n0.1\n0.5\n0.1\nattribution\n0.3\n0.7\n0.3\n0.2\n0.6\n0.2\nbackground\n0.5\n0.7\n0.5\n0.3\n0.7\n0.3\ncause-result\n0.3\n0.5\n0.3\n0.4\n0.7\n0.4\ncomparison\ncondition\n0.3\n0.7\n0.3\n0.3\n0.5\n0.3\n0.5\n0.7\n0.5\n0.5\n0.7\n0.5\nconsequence\n0.3\n0.5\n0.3\n0.3\n0.7\n0.3\ncontrast\n0.3\n0.5\n0.3\n0.1\n0.5\n0.1\nelaboration\n0.3\n0.5\n0.3\n0.1\n0.9\n0.1\nenablement\n0.5\n0.7\n0.5\n0.5\n0.7\n0.5\nevaluation\nexplanation\n0.5\n0.7\n0.5\n0.5\n0.7\n0.5\n0.5\n0.7\n0.5\n0.5\n0.7\n0.5\nmanner-means\n0.3\n0.7\n0.3\n0.5\n0.7\n0.5\nsummary\n0.3\n0.5\n0.3\n0.1\n0.7\n0.1\ntemporal\ntopic-comment\n0.5\n0.5\n0.5\n0.5\n0.7\n0.5\n\ntopic-comment\nevaluation\n\nconsequence\n\nsummary\nenablement\n\nexplanation\ncomparison\n\nmanner-means\n\ncause-result\n\ntemporal\ncontrast\n\ncondition\n\nbackground\n\nattribution\nelaboration\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n% of all rhetorical relations\n\nFigure 2: % distribution of rhetorical relations in\nour dataset.\n\nImprovements over the baseline are generally higher for\nWeb 2010 than Web 2009, possibly because the former base-\nline is weaker, with potentially more room for improvement.\nAn interesting trend is that more rhetorical relations im-\nprove performance according to BPREF than according to\nMAP and NDCG. As BPREF is the only of these evaluation\nmeasures that does not consider non-assessed documents as\nnon-relevant, this indicates the presence of non-assessed doc-\numents in the ranking.\nThe scores shown in Table 2 are averaged over tens of\nqueries, meaning that they can be aected by outliers. Fig-\nure 3 presents a detailed per-query overview of the perfor-\nmance of each query in relation to the baseline for each of\nthe 15 rhetorical relations6 . The plotted points represent\nthe dierence in MAP between our approach and the base-\nline. Positive points indicate that our approach outperforms\nthe baseline. The points are sorted.\nWe observe that although the overall performance of the\nWeb 2010 query set is lower than that of the Web 2009 query\nset, the improvements over the baseline of the 2010 set are\nconsistently larger. Only in one case, topic-comment, do the\nplotted points clearly cross. Overall both query sets show\nsimilar plots with outliers at both ends of the scale. How-\never, the 2009 query set tends to have a somewhat larger\nproportion of negative outliers, which goes some way to-\nwards explaining the lower improvements over the baseline\nobserved for Web 2009. The Web 2010 set shows improve-\nments over the baseline for most of the rhetorical relations\nand for the ma jority of the queries.\n\n4.2.2 Quantifying the Contribution of Rhetorical\nRelations to the Ranking\nExactly how much impact each rhetorical relation has on\nthe ranking can be seen in Table 3. The table lists the  val-\nues for the best performing tuned runs from Table 2, where\nhigh  values mean that the rhetorical relations are given\nmore weight in the ranking (see Equation 3). We see that\nnone of the values are above 0.5 for MAP and NDCG, in-\ndicating that too much emphasis on the rhetorical relations\nmay not be benecial to performance. Consistent with Table\n2, BPREF follows a dierent trend than MAP and NDCG,\n\n6Similar trends are observed in the corresponding gures for\nBPREF and NDCG, which are not included here for brevity.\n\n\f0.08\n\n0\n\n0.02\n\n0.1\n\n0\n\n0.1\n\n0.2\n\n0\n\n0.1\n\n0.05\n\n0\n\n0.1\n\n0.1\n\n0\n\n0.1\n\nattribution\n\nbackground\n\ncause-result\n\n0.1\n\n0\n\n0.2\n\n0\n\ncomparison\n\ncondition\n\nconsequence\n\n0.1\n\n0\n\n0.1\n\n0.05\n\n0\n\n0.1\n\ncontrast\n\nelaboration\n\nenablement\n\n0.1\n\n0\n\n0.05\n\nevaluation\n\nexplanation\n\n0.1\n\n0\n\n0.2\n\n0.15\n\n0.05\n\n0\n\n0.05\n\n0\n\n0.1\n\nmanner-means\n\nsummary\n\ntemporal\n\ntopic-comment\n\n0.05\n\n0\n\n0.05\n\n0.05\n\n0\n\n0.05\n\nWeb 2009\nWeb 2010\n\nFigure 3: Sorted per-query dierence in MAP between the baseline and our model (y-axis), for each rhetorical\nrelation. The horizontal line marks the baseline. + and o mark the 2009 and 2010 query sets.\n\n\fwhich could be due to the fact that it is a dierent type\nof evaluation measure as discussed above in section 4.1.2.\nWith BPREF, unassessed documents are not explicitly pe-\nnalised in the evaluation (as in MAP and NDCG) - resulting\nin overall higher  values for best performing runs, typically\nof around 0.5-0.7.\nFurther we observe that the rhetorical relations that con-\nsistently improve performance over the baseline, as indicated\nin Table 2, dier in  values for their best performing runs.\nFor example,  = 0.2 - 0.3 for background and  = 0.5 for\ntopic-comment. This implies that, to use rhetorical rela-\ntions successfully for IR, it is not sucient to know which\nrhetorical relations should be considered in the ranking and\nwhich not; also knowledge about how much emphasis to put\non each rhetorical relation is needed for optimal IR perfor-\nmance.\nFinally, note that the frequency of rhetorical relations\ndoes not aect their impact to retrieval. For instance, the\nthree best performing rhetorical relations, topic-comment,\nbackground and cause-result constitute respectively ap-\nproximately >1%, 11% and 5% of all rhetorical relations, as\nshown in Figure 2.\n\n5. OPTIMISED RANKING WITH\nRHETORICAL RELATIONS\n\n5.1 Rhetorical Relation Selection\nThe ndings in section 4.2 show that some rhetorical re-\nlations can be more benecial to retrieval performance than\nothers. An ideal solution would not consider the lexical\nstatistics of all rhetorical relations in a document, but rather\nit would select to include in the ranking only those rhetorical\nrelations that have a higher likelihood of enhancing retrieval\nperformance. This can be formulated as nding the optimal\nrhetorical relation  that maximises the expected retrieval\nscores according to an evaluation measure (e.g. MAP) for a\nquery-document pair:\n = arg max\n\n\nE [y |q , d]\n\n(6)\n\nwhere E denotes the expectation and y the retrieval score\n(rest of notation as dened in section 3).\nBayesian decision theory allows to reason about this type\nof expectation, for instance see [32]. In this work, we treat\nthis as a problem of Bayesian posterior inference, where the\ngoal is to estimate the retrieval performance associated with\na rhetorical relation, given the observed retrieval scores it\nfetches on a number of queries. Then, we can consider\nthe rhetorical relation associated with the highest retrieval\nperformance as optimal. For this estimation, we split our\ndataset into dierent parts so that we use the observations\nfrom one to make inferences about the other (see section 5.2\nfor details).\nLet n = 15 be the rhetorical relations shown in Table 2,\nand xj be the number of queries for which retrieval with\nthe j th rhetorical relation gets a retrieval score yj . For now\nwe assume that all rhetorical relations may be expected to\nhave similar retrieval performance, with the j th rhetorical\nrelation having an average performance ratio per query j\n(estimated as yj\n). Various densities can be used to t simi-\nxj\nlar data [22], one of which is the Poisson distribution. Let us\nassume that, conditional on j , the retrieval scores yj have\nindependent Poisson distributions with means j xj . Let us\n\nfurther assume that the j are independent realisations of a\ngamma variable with parameters  and  , and that  itself\nhas a prior gamma distribution with parameters  and .\nThus\n\nf (y |) =\n\n (| ) =\n\nn\nYj=1\nn\nYj=1\n\n(xj j )yj\nyj !\n\nexj j\n\n1\nj\n()\n\nej\n\n ( ) =\n\n  1\n( )\n\ne\n\nso that the joint probability density of the retrieval scores\ny , the average performance ratios , and  is\n\nf (y |)f (| ) ( ) = c\n\nn\nYj=1\n\n{\n\nyj +1\nj\n\nej (xj + )}n+1 e\n\n(7)\n\nwhere c is a constant of proportionality.\nThe conditional density of  can be computed by vari-\nous numerical approximations, one of which is the Laplace\nmethod [2], which we use here. To nd the conditional den-\nsity of  we integrate over the j to obtain\n\nf (y ,  ) = c\n\nn\n{(xj + )(yj +)(yj +)}n+1 e (8)\nYj=1\nfrom which the marginal density of y is obtained by further\nintegration to give\n\n(9)\n\nf (y ) = c\n\neh( )d\n\n(yj + )  Z \n0\n\nn\nYj=1\nwhere h( ) =   (n +   1)log + P(yj + )log (xj +  ).\nLet I denote the integral in this expression. In this work,\nwe take an uninformative prior for  , with  = 0.1 and\n = 1 and use  = 1.87 . We then apply Laplaces method\nto I , resulting in the approximate posterior density for  ,\n ( |y ) = I 1eh( ) .\nTo calculate approximate posterior densities for j we\nintegrate Equation 7 over i , i 6= j and then we apply\nLaplaces method to the numerator and denominator inte-\ngrals of\n\n (j |y ) =\n\n\n\nyj +1\nej xj R \n0 ehj ( )d\nj\n(yj + ) R \n0 eh( )d\n\nwhere\n\nhj ( ) = ( + j )  (n +   1)log +Xi6=j\n(yi + )log (xi +  )\nThe resulting denominator is again I1 , while the numerator\nmust be recalculated at each of a range of values for j .\nThe output is the (posterior) expected retrieval performance\nassociated with each rhetorical relation.\n\n5.2 Experiments\n\n7These values are not tuned; they are the default values\nof this approach as illustrated in [10], chapter 11.3, pages\n603-604.\n\n\fTable 4: Retrieval performance with optimal rhetorical relations (inferred, observed) and without rhetorical\nrelations (baseline).\n(1)-(5) refers to the ve randomised samplings used to infer the optimal rhetorical\nrelations. Bold marks better than baseline.\nWeb 2010 (queries 51-100)\nWeb 2009 (queries 1-50)\nrhetorical relation\nNDCG\nBPREF\nMAP\nNDCG\nBPREF\nMAP\n0.0967\n0.2890\n0.2198\n0.3894\n0.1625\n0.3230\n+8.5% 0.1355 +40.1% 0.2859 +30.1% 0.3347 +15.8%\n0.1879 +15.6% 0.3503\n+8.5% 0.4224\n+7.9% 0.1285 +32.9% 0.2841 +29.3% 0.3394 +17.4%\n0.1948 +19.9% 0.3585 +11.0% 0.4202\n+9.3% 0.4169\n0.1984 +22.1% 0.3532\n+7.1% 0.1358 +40.0% 0.2906 +32.2% 0.3388 +17.2%\n+7.7% 0.4282 +10.0% 0.1360 +40.6% 0.2874 +30.8% 0.3336 +15.4%\n0.1952 +20.1% 0.3479\n0.1950 +20.0% 0.3528\n+9.2% 0.4287 +10.1% 0.1340 +38.6% 0.2865 +30.3% 0.3322 +14.9%\n0.2157 +32.7% 0.3660 +13.3% 0.4412 +13.3% 0.1474 +52.4% 0.2978 +35.5% 0.3569 +23.5%\n\nnone (baseline)\noptimalinf erred (1)\noptimalinf erred (2)\noptimalinf erred (3)\noptimalinf erred (4)\noptimalinf erred (5)\noptimalobserved\n\n5.2.1 Setup\nThe observations required to make the above inference\nare triples of rhetorical relation - query number - retrieval\nscore. To avoid overtting, we pool randomly 50% of the\nobservations from the 2009 Web query scores and 50% of\nthe observations from the 2010 Web query scores. We use\nthis pool to infer the expected retrieval performance of each\nrhetorical relation. We repeat this randomised pooling ve\ntimes, each time randomly pertrubing the data, producing\nve dierent sets of observations. We then use each set to\ninfer the expected best performing rhetorical relation per\nquery, in accordance to Equation 6. Following this, we use\nthe model introduced in section 3, Equation 2, to rank docu-\nments with respect to queries only for optimal (as inferred)\nrhetorical relations. We evaluate the above method using\nthe same experimental settings described in section 4.1.\n\n5.2.2 Findings\nTable 4 shows the runs corresponding to the ve dier-\nent inferences of the best rhetorical relation that use our\nmodel (optimalinf erred (1)-(5) respectively). We also report\nthe optimal retrieval performance actually observed in the\ndataset when using the best rhetorical relation per query\n(optimalobserved ). Optimal here means with respect to the\nchoice of rhetorical relation, not with respect to the Dirichlet\n parameter of the baseline retrieval model.\nTable 4 shows that our optimised ranking model for rhetor-\nical relations is better than the baseline for any of the ve\nrandom inferences on all three evaluation measures. The\nprobability of getting such a positive result by chance is\n1\n25 < 0.05, and thus the improvements are statistically sig-\nnicant. The improvements over the baseline are consider-\nable, a very promising nding given the relatively low num-\nber of observations used for optimising the choice of rhetor-\nical relations. Experiments involving larger query sets can\nbe reasonably expected to perform on a par with state-of-\nthe-art performance.\nMore generally, the improvements in Table 4 signal that\nrhetorical relations (derived automatically as shown in this\nwork) could potentially be useful features for linguistically-\nuninformed learning-to-rank approaches.\n\n6. DISCUSSION\n\n6.1 Rhetorical Relation Distribution\nThe distribution of the 15 rhetorical relations we identi-\ned in our dataset is not the same for all rhetorical relations\n(see Figure 2). Some types, e.g. topic-comment, tend to be\nvery sparse, whereas relations such as elaboration prevail.\n\nThis has no impact on the model presented in section 3, but\nit can bias the optimised inference of the model presented\nin section 5. The lower the occurrence of a rhetorical rela-\ntion in the dataset, the fewer the observations of retrieval\nperformance associated with it, and hence the weaker the\npredictions we can infer about whether it is optimal or not.\nA fairer setting would be to have the same number of query\n- retrieval performance observations for all rhetorical rela-\ntions - however that would imply ddling with the document\ndistribution of our dataset signicantly, potentially harming\nits quality as a test collection.\n\n6.2 Limitations\nA general limitation of discourse analysis is that not all\ntypes of text are susceptible to it. For instance, legal text,\ncontracts, or item lists often lack rhetorical structure. In this\nwork, we made no eort to identify and exempt such types\nof text from the discourse parsing. We reasoned that, as the\nSPADE parser includes a rst-step grammatical parsing, the\ninitial grammatical parsing of these types of text would ag\nout ill-formed parts (e.g. missing a verb, or consisting of ex-\ntremely long sentences), which would then be skipped by the\ndiscourse analysis. This was indeed the case, however at a\ncertain eciency cost. Overall processing speed for SPADE\nwas approximately 19 seconds per document (including the\ninitial grammatical parsing), on a machine of 9 GB RAM,\n8 core processor at 2.27GHz. One way of improving this\nperformance would be to update the rst-step grammatical\nparsing. Currently this depends on the well-known Charniak\nparser [6], which is one of the best performing grammatical\nparsers, however no longer supported. Other state-of-the-\nthe Stanford parser8 ,\nart faster grammatical parsers, e.g.\ncould be adapted and plugged into SPADE instead.\nThe choice of applying out model for re-ranking as op-\nposed to ranking all documents was closely related to the\neciency concerns discussed above. Our model is not spe-\ncic to re-ranking only, however, using SPADE on more than\n50 million documents was too expensive at this point. Im-\nproving the discourse parsers eciency is something we are\ncurrently working on, with the aim to apply our model for\nfull ranking and see if the conclusions drawn from this work\nhold.\nFinally, the accuracy of the discourse parser was not con-\nsidered in this work, apart from indications in the litera-\nture that SPADE is a generally well-performing parser [27].\nGiven that the default version of the parser we used is trained\non news articles, one may reason that its accuracy could\nimprove if we train it on the retrieval collection, or on doc-\n\n8http://nlp.stanford.edu/software/lex-parser.shtml\n\n\fuments of the same domain. Note that, parsing accuracy\naside, rhetorical relations assignment is not an entirely un-\nambiguous process, even to humans [23]. For the purposes of\nthis work, this type of ne-grained ambiguity may however\nnot be important to retrieval performance.\n\n6.3 Future Extensions\nFuture extensions include primarily making SPADE scal-\nable on large collections of documents as discussed above,\nas well as using more than one rhetorical relation per docu-\nment. For instance, the posterior probabilities estimated in\nsection 5.1 could be used to weight the text in each rhetor-\nical relation.\nIf those posteriors are too at, an exponent\ncould make them peakier. As the exponent goes to inn-\nity, the maximum relation model presented in section 5.2\nwould be recovered.\nIn addition, we intend to rene the\ndiscourse analysis by considering the nucleus (i.e. central)\nversus satellite (i.e. peripheral) rhetorical relations for IR,\nas well as to improve the eectiveness of the discourse parser\nby training it on data of the same domain. As discussed in\nsection 3.2, we will also investigate alternative estimations\nof Equations 2-3.\nAn interesting future research direction is the potential\nrelation between rhetorical relations and user context: for\ninstance, in a search session including several query refor-\nmulations, is there a correlation between the progression of\nthe information need of the user and the rhetorical rela-\ntions that the retrieval system should boost in a document\n(e.g. elaboration), as indicated by Sun & Chai [28]? An-\nother interesting future extension of this work is in relation\nto evaluation measures of graded relevance measures on an\ninter-document level, as investigated in XML retrieval [20]\nfor instance. If parts of a document can be regarded as more\nor less relevant, this may be reected to their discourse struc-\nture. This might be especially useful for multi-threaded doc-\numents, such as multiple-user reviews and opinions, where\nthe discourse relations tend to shift markedly. Finally, the\ncurrent operationalisation of our model is simplistic in the\nsense that the term rhetorical relation is coerced into mean-\ning non-overlapping text fragment and the actual relation\nbetween bits of text is discarded in the process. In future\nwork we could apply elded XML retrieval models in order\nto investigate nested structuring among rhetorical relations.\n\n7. CONCLUSIONS\nRhetorical relations, e.g. contrast, explanation, con-\ndition, indicate the dierent ways in which the parts of a\ntext are linked to each other to form a coherent whole. This\nwork studied two questions: Is there a correlation between\ncertain rhetorical relations and retrieval performance? Can\nknowledge about a documents rhetorical relations be use-\nful to IR? To address these, we presented a retrieval model\nthat conditions the probability of relevance between a query\nand a document on the rhetorical relations occurring in that\ndocument. We applied that model to an IR re-ranking sce-\nnario for Web search. Experimental evaluation of dierent\nversions of our model on TREC data and standard settings\ndemonstrated that certain rhetorical relations can be bene-\ncial to retrieval, with >10% improvements to retrieval pre-\ncision. Furthermore, we showed that these improvements\nover the baseline can improve signicantly, when the opti-\nmal rhetorical relation per document is selected for retrieval.\nOverall, three rhetorical relations were found to benet\n\nretrieval performance notably and consistently for dierent\nevaluation measures and query sets: background, cause-\nresult and topic-comment. In retrospect, this is perhaps\nnot surprising, since these are among the most salient dis-\ncourse relations on an intuitive basis:\nthe main topic or\ntheme of a text, its background, causes and results [21]. Fu-\nture extensions and research directions of this work include\napplying our model for ranking all documents (as opposed\nto re-ranking only) and experimenting with alternative esti-\nmations of its components.\n\n8. ACKNOWLEDGMENTS\nWe thank Kasper Hornbk, Jakob Grue Simonsen, Raf\nGuns, Qikai Cheng and the anonymous reviewers for help-\ning improve this paper. Work partially funded by the Dan-\nish International Development Agency DANIDA (grant no.\n10-087721) and the National Natural Science Foundation of\nChina (grant no. 71173164).\n\n9. REFERENCES\n\n[1] Proceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2011, 27-31 July 2011, John McIntyre Conference\nCentre, Edinburgh, UK, A meeting of SIGDAT, a\nSpecial Interest Group of the ACL. ACL, 2011.\n[2] A. Azevedo-Filho and R. D. Shachter. Laplaces\nmethod approximations for probabilistic inference in\nbelief networks with continuous variables. In R. L.\nde Mantaras and D. Poole, editors, UAI, pages 2836.\nMorgan Kaufmann, 1994.\n[3] M. Bendersky, W. B. Croft, and Y. Diao.\nQuality-biased ranking of web documents. In I. King,\nW. Nejdl, and H. Li, editors, WSDM, pages 95104.\nACM, 2011.\n[4] C. Buckley and E. M. Voorhees. Retrieval evaluation\nwith incomplete information. In M. Sanderson,\nK. Jarvelin, J. Allan, and P. Bruza, editors, SIGIR,\npages 2532. ACM, 2004.\n[5] L. Carlson, D. Marcu, and M. E. Okurowski. Building\na discourse-tagged corpus in the framework of\nrhetorical structure theory. In Current Directions in\nDiscourse and Dialogue, pages 85112. Kluwer\nAcademic Publishers, 2003.\n[6] E. Charniak. A maximum-entropy-inspired parser. In\nProceedings of the rst conference on North American\nchapter of the Association for Computational\nLinguistics, pages 132139, San Francisco, CA, USA,\n2000. Morgan Kaufmann Publishers Inc.\n[7] J. Clarke and M. Lapata. Discourse constraints for\ndocument compression. Computational Linguistics,\n36(3):411441, 2010.\n[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke.\nEcient and eective spam ltering and re-ranking for\nlarge web datasets. CoRR, abs/1004.5168, 2010.\n[9] W. B. Croft and J. Laerty. Language Modeling for\nInformation Retrieval. Kluwer Academic Publishers,\nNorwell, MA, USA, 2003.\n[10] A. C. Davison. Statistical Models. Cambridge\nUniversity Press, New York, 2009.\n[11] D. A. duVerle and H. Prendinger. A novel discourse\nparser based on support vector machine classication.\n\n\fAgreement among statistical signicance tests for\ninformation retrieval evaluation at varying sample\nsizes. In J. Allan, J. A. Aslam, M. Sanderson, C. Zhai,\nand J. Zobel, editors, SIGIR, pages 630631. ACM,\n2009.\n[26] S. Somasundaran, G. Namata, J. Wiebe, and\nL. Getoor. Supervised and unsupervised methods in\nemploying discourse relations for improving opinion\npolarity classication. In EMNLP, pages 170179.\nACL, 2009.\n[27] R. Soricut and D. Marcu. Sentence level discourse\nparsing using syntactic and lexical information. In\nHLT-NAACL, 2003.\n[28] M. Sun and J. Y. Chai. Discourse processing for\ncontext question answering based on linguistic\nknowledge. Know.-Based Syst., 20:511526, August\n2007.\n[29] N. Suwandaratna and U. Perera. Discourse marker\nbased topic identication and search results rening.\nIn Information and Automation for Sustainability\n(ICIAFs), 2010 5th International Conference on,\npages 119125, 2010.\n[30] S. Teufel and M. Moens. Summarizing scientic\narticles: Experiments with relevance and rhetorical\nstatus. Computational Linguistics, 28(4):409445,\n2002.\n[31] D. Y. Wang, R. W. P. Luk, K.-F. Wong, and K. L.\nKwok. An information retrieval approach based on\ndiscourse type. In C. Kop, G. Fliedl, H. C. Mayr, and\nE. Metais, editors, NLDB, volume 3999 of Lecture\nNotes in Computer Science, pages 197202. Springer,\n2006.\n[32] J. Wang and J. Zhu. On statistical analysis and\noptimization of information retrieval eectiveness\nmetrics. In F. Crestani, S. Marchand-Maillet, H.-H.\nChen, E. N. Efthimiadis, and J. Savoy, editors, SIGIR,\npages 226233. ACM, 2010.\n[33] L. Wang, M. Lui, S. N. Kim, J. Nivre, and\nT. Baldwin. Predicting thread discourse structure over\ntechnical web forums. In EMNLP [1], pages 1325.\n[34] L.-C. Yu, C.-H. Wu, and F.-L. Jang. Psychiatric\ndocument retrieval using a discourse-aware model.\nArtif. Intel l., 173:817829, May 2009.\n[35] C. Zhai and J. D. Laerty. Two-stage language models\nfor information retrieval. In SIGIR, pages 4956.\nACM, 2002.\n[36] L. Zhou, B. Li, W. Gao, Z. Wei, and K.-F. Wong.\nUnsupervised discovery of discourse relations for\neliminating intra-sentence polarity ambiguities. In\nEMNLP [1], pages 162171.\n\nIn Proceedings of the Joint Conference of the 47th\nAnnual Meeting of the ACL and the 4th International\nJoint Conference on Natural Language Processing of\nthe AFNLP: Volume 2 - Volume 2, ACL 09, pages\n665673, Stroudsburg, PA, USA, 2009. Association for\nComputational Linguistics.\n[12] C. J. Fillmore, C. F. Baker, and S. Hiroaki. The\nframenet database and software tools. In Proceedings\nof the 3rd International Conference on Language\nResources and Evaluation (LREC), pages 11571160,\n2002.\n[13] W. A. Gale and G. Sampson. Good-turing frequency\nestimation without tears. Journal of Quantitative\nLinguistics, 2(3):217237, 1995.\n[14] S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli.\nShallow discourse parsing with conditional random\nelds. In Proceedings of the 5th International Joint\nConference on Natural Language Processing\n(IJCNLP), pages 10711079, Chiang Mai, Thailand,\n2011.\n[15] D. Gildea and D. Jurafsky. Automatic labeling of\nsemantic roles. In ACL. ACL, 2000.\n[16] B. Heerschop, F. Goossen, A. Hogenboom,\nF. Frasincar, U. Kaymak, and F. de Jong. Polarity\nanalysis of texts using discourse structure. In\nProceedings of the 20th ACM international conference\non Information and know ledge management, CIKM\n11, pages 10611070, New York, NY, USA, 2011.\nACM.\n[17] K. Jarvelin and J. Kekalainen. Cumulated gain-based\nevaluation of ir techniques. ACM Trans. Inf. Syst.,\n20(4):422446, 2002.\n[18] P. Kingsbury and M. Palmer. From treebank to\npropbank. In Proceedings of the 3rd International\nConference on Language Resources and Evaluation\n(LREC), pages xx, 2002.\n[19] E. Krikon and O. Kurland. A study of the integration\nof passage-, document-, and cluster-based information\nfor re-ranking search results. Inf. Retr., 14(6):593616,\n2011.\n[20] M. Lalmas. XML Retrieval. Synthesis Lectures on\nInformation Concepts, Retrieval, and Services.\nMorgan & Claypool Publishers, 2009.\n[21] A. Louis, A. K. Joshi, and A. Nenkova. Discourse\nindicators for content selection in summarization. In\nR. Fernandez, Y. Katagiri, K. Komatani, O. Lemon,\nand M. Nakano, editors, SIGDIAL Conference, pages\n147156. The Association for Computer Linguistics,\n2010.\n[22] R. Manmatha, T. M. Rath, and F. Feng. Modeling\nscore distributions for combining the outputs of search\nengines. In W. B. Croft, D. J. Harper, D. H. Kraft,\nand J. Zobel, editors, SIGIR, pages 267275. ACM,\n2001.\n[23] W. C. Mann and S. A. Thompson. Rhetorical\nstructure theory: Toward a functional theory of text\norganization. Text, 8:243281, 1988.\n[24] J. Morato, J. Llorens, G. Genova, and J. A. Moreiro.\nExperiments in discourse analysis impact on\ninformation classication and retrieval algorithms. Inf.\nProcess. Manage., 39:825851, November 2003.\n[25] M. D. Smucker, J. Allan, and B. Carterette.\n\n\f", 
        "tag": "Information Retrieval", 
        "link": "https://arxiv.org/list/cs.IR/new"
    }, 
    {
        "text": "Study on a Low Complexity ECG Compression\nScheme with Multiple Sensors\nPengda Huang\n\n1\n\n7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nT\nI\n.\ns\nc\n[\n \n \n1\nv\n2\n1\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstractThe industry of wearable remote health monitoring\nsystem keeps growing. In the diagnosis of cardiovascular disease,\nElectrocardiography (ECG) waveform is one of the major tools\nwhich is thus widely taken as the monitoring objective. For the\npurpose of reducing bit expenditure in the monitoring systems,\nwe study the compression of ECG signal and propose a new\ncompressor in low complexity. Different from the traditional\nECG compressors, most of which are built on a single sensor,\nour compression scheme is based on multiple ECG sensors.\nThe multi-sensor based compression scheme is able to provide\nmore accurate sensing results. Besides the investigation into the\nstructure of the compressor, we also jointly optimize the period\nand the bit number per sample in the transmission of ECG signal.\nExperiments are performed on records in MIT-BIH Arrhythmis\ndatabase and European ST-T database. Experimental results\nshow that our method outperforms conventional ones with\nrespect to ECG reconstruction accuracy at the same bit rate\nconsumption.\n\nI . IN TRODUC T ION\nThanks to the development of mobile communication and\npositioning technologies [1][3] in the past several decades,\nremote health monitoring technology is near to practical\napplication in our everyday life. ECG signal is one of the\nmain tools of diagnosing cardiovascular diseases which are\nthe major mortality causes in current societies, especially\nin developed countries. Remotely monitoring ECG signal\nprovides an effective approach to avoiding the mortality caused\nby abrupt seizure of cardiovascular diseases.\nBasically, in a remote monitoring system a wearable device\ncollects biomedical information, and transmit the collected\ninformation to a remote data unit for prompt or delayed diag-\nnosis. The remote monitoring replies on the transmission of the\nbits which carry ECG signal. The bit transmission induces cost\ndue to consumption of resources provided by infrastructures in\nmobile communication systems. For the purpose of reducing\nthe cost, we investigate how to reduce the cost with respect to\nthe two aspects, lowering the complexity of ECG compressor\nand reducing the rate of bits conveying ECG signal. The cost\nreduction efforts are under the prerequisite that the accuracy\nof the received ECG signal at the remote data unit should be\nunder control and not degrade the diagnosis of cardiovascular\ndiseases.\nIn literature, a single sensor is widely used to monitor\nECG signal. In this paper, we consider multiple-sensor based\nECG compression scheme. Generally, a multiple sensors based\nmonitoring system provide more accurate and prompt sensing\nresults since the sensors equipped at different places of our\nbody are able to monitor the conditions of different parts of\na heart. Fig. 1 presents an example of our proposed ECG\n\nFig. 1. Demonstration of two ECG signal compression scheme\ncompressor built on two sensors, a primary ECG sensor and\na secondary sensor. As an arbitrary example shown in Fig. 1,\nthe secondary sensor is put on the wist which compresses\nECG signal and the transmit the compression results to the\nprimary sensor shown in the breast part. Battery capacity\nand computation capability of the secondary sensor are at a\nlower than the primary one since the targeted transmission\ndistance of the secondary sensor is shorter. The primary sensor\ncompresses and sends out the ECG signal from the secondary\none and itself to a remote data center.\nNo matter a single- or multiple-sensor based monitoring\nsystem, energy consumption is widely recognized as a major\nconcern [4]. The energy consumption is affected by diverse\nfactors, such as hardware chip, circuit board design, encoder,\nmodulation, or even selection of radio frequency (RF) antenna.\nThus, we can hardly evaluate the energy consumption in all\nterms of the mentioned and unmentioned factors. Independent\nof the diverse factors, rate of the bits carrying the ECG signal\nprovides us an effective approach to evaluating the energy\nconsumption at a high level.\nSingle-sensor based ECG compression scheme have been\nstudied in [5][14]. Basically, compression methods can be\ndivided into to two categories, direct and differential ECG\ncompression methods.\nUniform quantization is basic one of the direct ECG signal\ncompression methods. In [8], discrete cosine transform (DCT)\nwas used to compress ECG signal. Similarly, DCT is also used\nin ECG signal compression [9] while Huffman coding was\nused to compress DCT results further. Still as a direct compres-\nsion method, wavelet transform followed by run length coding\nwas taken to compress ECG signal in [10], [11]. Compressive\nsensing was utilized to compress ECG signal in [12][14].\n\n\fThe compression of ECG signal is implemented in differen-\ntial structures. Differential schemes built on a linear prediction\nmodel are used to compress ECG signal in [15], [16]. Multiple\nECG samples in the past are taken to predict ECG value in one\nstep ahead. Then, the difference between the prediction and\nits real value is quantized. In [7], [17], [18], adaptive signal\nprocessing methods are taken to update the coefcients of the\nlinear prediction model.\nWe observe that adjacent ECG samples are not independent\nto each other. The dependence means there exits redundant\ninformation between the samples. Differential compression\ncan effectively reduce the redundant information. After the\nredundant information reduction, less bits are needed for the\nquantization. Therefore, differential ECG compression is taken\nas one of the research objectives in this paper.\nThere is an important but not solved problem in existing\ndifferential ECG compression methods. As we know, coef-\ncients of an adaptive lter for predicting a stationary signal\ndo not change versus time. However, ECG signal\nis not\nstationary. Furthermore, ECG signals from a same person can\nbe signicantly different. Let us consider such a scenario that\none person, sitting on a bench for a long time, stands up to\nleave. The period of R-R waves in ECG waveform will be\ndifferent before and after his or her status transition, from\nsitting to walking. In this case, coefcients of the adaptive lter\nfor predicting his or her ECG waveform will also be different.\nTo keep ECG reconstruction at a high delity, the coefcients\nneed to be recalculated and retransmitted; otherwise, there will\nbe huge reconstruction error. To transmit the coefcients of\nan adaptive lter, a large number of bits will be consumed\nwhich is thus harmful to ECG transmission efciency. In [7],\n[15][18], adaptive lters based differential ECG compression\nschemes are investigated. In their compression schemes, either\nupdating or transmitting coefcients of adaptive lters may\ncause signicant increase of computation resources.\nDifferent\nfrom the existing ECG signal compression\nschemes, we proposed a new structure which is built on\nmultiple ECG sensors. The proposed ECG compressor is at\na low complexity. More specically, the contributions in this\npaper are presented as follow,\nFirst, we investigate ECG signal compression system with\nmultiple sensors. Simple superposition of multiple sensors is\nnot considered. From a same person, the ECG signals acquired\nby different ECG sensors at the same time instant usually\nhave similarity in waveform shapes. The similarity means the\nredundant information. After realizing the signal redundancy\nbetween ECG sensors, we design a new ECG compression\nscheme which effectively saves the bits by reducing the\nredundancy.\nSecond, we propose a novel differential ECG compression\nscheme which is implemented via comparison and addition\noperations, and free of multiplications. The traditional differ-\nential ECG compressors are built on adaptive lters which\nrely on the updating lter coefcients and thus increase\nresource consumptions. This problem does not exists in our\ncompression scheme. Furthermore, we optimize the codebook\nused for compressing the differential ECG signal.\nThird, we optimize compression ECG compression bit rates\n\nin two dimensions, the sampling period and the number of bits\nper sample. To my best knowledge, bit number per sample was\nconsidered in literature while the joint optimization is absent.\nThe remainder of this paper is organized as follows. In\nSection II, the potential problems of the existing compression\nmethods will also analyzed. In Section III, a novel ECG com-\npression scheme built on multiple sensors will be presented.\nThe joint optimization of bit rate over quantization level and\nsampling period will be performed in Section IV. Experiments\nand simulations will be presented in Section V which are\nfollowed by conclusions in Section VI.\n\nI I . R E LATED WORK AND POTENT IA L PROBL EM S\nIn this section, we investigate the potential problems in the\nexisting differential ECG compression schemes. Due to the\nlarge number of existing reports on ECG compression, our\nstudy will not cover all methods but only target at several\ntypical ones.\n\nA. Open-loop Predictive ECG Compression\n1) Open-loop based differential ECG compression method:\nFinite impulse response (FIR) predicator was widely used in\nthe open-loop based ECG compression. One example of the\nECG compressors is shown in Fig. 2.\n\nFig. 2. Block diagram of open loop differential ECG compressor\n\nLet x(t) denote the time continuous ECG signal to be\ncompressed and xi denote periodical samples of x(t), i  Z.\nAssume the FIR predicator is in the order of M . Let am ,\nm  {1, 2,    , M } denote coefcients of the predicator. At\nthe i-th time instance, estimation of ECG signal is denoted by\nx\nM(cid:88)\ni which is calculated as follows\nm=1\nEstimation error between x\ni and xi is determined by\ni  xi .\nei = x\nIn a differential ECG signal compression scheme, the esti-\nmation error ei should be encoded and transmitted to a remote\nreceiver. The receiver decodes the codewords and obtains the\nreconstruction of ei , which is denoted by ei . With ei , ECG\n\namxim .\n\nx\ni =\n\n(1)\n\n(2)\n\n\f(3)\n\nxi = ei +\n\nam xim .\n\nsignal is reconstructed by\n\nM(cid:88)\nm=1\nThe major concern for the open-loop based differential\nECG compressor is the stability at the decoder. If errors in\nquantizing ei will be accumulated, the compressor system is\nunstable. Unfortunately, there was no attention paid to the\nstability problem for open-loop compressors.\n2) Stability of open-loop based differential ECG compres-\nsion: An unstable open-loop compressor will accumulate\nquantization errors which will eventually cause the failure of\nECG signal reconstruction at the decoder. Therefore, we need\nto analyze the quantization error accumulation problem at the\ndecoder side. As dened in Section II-A1, ei is the difference\nbetween xi and its estimation x\ni . At the decoder side, the\ndifference between xi and its reconstruction xi is denoted by\ne\ni ,\ni = xi  xi .\ne\nFurthermore, we dene eqi as the difference between ei and\nei ,\n\n(4)\n\ne\ni\n\n= xi \n(b)\n\n= xi \n(a)\n\nximam + ei\n\nximam + ei  eqi\n\nei = ei + eqi ,\nwhere eqi is essentially the quantization error in compressing\nei .\nWe can realize that e\ni measures the bias of the reconstructed\nECG sample with respect to its real value. Only if e\ni stays\nwithin a small bounded range, the decoder is able to obtain\naccurate ECG samples. The quantization error eqi is the factor\nwhich may cause e\nto be outside of the bounded range.\ni\nTherefore, we construct e\ni as a function of eqi . Via analyzing\nthe stability of the function, we can understand whether the\n(cid:32) M(cid:88)\n(cid:33)\nECG compressor is stable. The function is derived as follows,\n(cid:33)\n(cid:32) M(cid:88)\nm=1\n(cid:32) M(cid:88)\nm=1\nm=1\n(xim  xim ) am + eqi\n\n= xi \n(c)\nM(cid:88)\nM(cid:88)\nm=1\nm=1\nwhere (a) follows (3); (b) follows (5); (c) follows (2); (d)\nfollows (1); and (e) follows (4).\nWe calculate z transform of (6) as follows,\n1  (cid:80)M\nZ{e }\n1\nZ{eq } =\n,\nHOLP =\nm=1 am zm\nwhere Z{} denotes the operator of Z transformation.\nThe stability of (7) depends on coefcients am , m  M.\nIndeed, shapes of ECG waveforms will differ with different\n\nximam + xi  x\ni  eqi\n\ne\nimam + eqi ,\n\n(cid:33)\n\n(d)\n=\n\n(e)\n=\n\n(5)\n\n(6)\n\n(7)\n\npeople or different health conditions. The change of ECG\nwaveform generates the different am . Furthermore, the incon-\nsistence of am means no guarantee of the stability in (7).\nAligning with the work in literature, we consider 4-\nth order FIR predictor. Under MMSE rule,\nthe two sets\nof {am } corresponding to No. 106 and No.118 ECG\nto {0.1436, 0.2120, 0.1582, 1.1548}\nrecords are equal\nand {0.2276, 0.2041, 0.2512, 1.1761} respectively. With\nthe calculated coefcients, HOLP |a and HOLP |b , are cor-\nrespondingly determined. Then, poles of\nthe two im-\npulse response functions are calculated which are equal to\npa = {0.9823, 0.0761  j 1.0866, 0.9908} and pa =\n{0.9868, 0.1200  j 1.0856, 0.9991} respectively. From the\npoles, we can easily realize that HOLP |a and HOLP |b are not\nnecessary to be stable which means there exists the risk of\ninducing the failure of ECG reconstruction at the decoder.\n\nB. Closed-Loop Predictive ECG Compression\nFrom Section II-A2, open-loop differential ECG compres-\nsors have the risk of being instable at the decoder. This prob-\nlem can be solved by adding a feedback to the quantization\nof ei .\n1) Closed-loop differential ECG compression method: The\ndifferential compressor with a feedback is called as closed-\nloop differential ECG compressor. Still M denotes the order\nof the linear model used to estimate the value of an ECG\nsample. When M = 1, the differential compressor degenerates\ninto Differential pulse code modulation (DPCM).\n\nFig. 3. Block diagram of closed loop differential ECG compressor\n\nFig. 3 plots the block diagram of closed-loop differential\ncompressors. Compared with open-loop compressor, the major\ndifference in the closed-loop one is that reconstruction is\nperformed at the encoder side, and the reconstructed sample is\ntaken as a reference of modifying the threshold for quantizing\nthe next ECG sample.\nLet xi denote the estimation of ECG sample at i-th time\nM(cid:88)\ninstance at the encoder side which is calculated by\ni=m\n\nam xim .\n\nxi =\n\n(8)\n\n\fThe estimation bias ei is determined as follows,\nei = xi  xi .\n(9)\nAfterwards, ei is rst quantized and the quantization result\nis denoted by ei , and the quantization error is stilled rep-\nresented by eqi . At the encoder side, the reconstruction of\nan ECG sample, denoted by xi , is obtained by adding the\nquantized ei to xi ,\n\n(10)\nxi = xi + ei .\nAs shown in (10), xi is feed back to the input of the linear\nlter. Since xi contains the error occurring in the quantization\nof the previous ECG sample, the feedback is benecial for\navoiding the accumulation of the quantization error.\n2) Stability of closed-loop based differential ECG com-\npression: Let e\nC i denote the difference between the ECG\nsample xi and its reconstruction at the decoder. For closed-\nloop compressor, the reconstructions of an ECG sample at\nboth the encoder and decoder are the same. Therefore, the\nreconstruction at the decoder is also denoted by xi . Due to\nthe same reason mentioned in Section II-A2, we calculate\ne\nC i as a function of eqi . Via analyze the stability of the\ncalculated function, we can understand whether there exists\nthe risk of accumulating quantization errors. The calculation\nof the function e\n(cid:32) M(cid:88)\n(cid:33)\nC i of eqi is presented as follows,\n= xi \n= xi  xi\n(cid:33)\n(cid:32) M(cid:88)\n(a)\nam xim + ei\ni=m\nam xim + ei  eqi\n(cid:32) M(cid:88)\nam xim + xi  M(cid:88)\ni=m\ni=m\ni=m\n\nam xim  eqi\n\n= xi \n\ne\nC i\n\n(cid:33)\n\n(11)\n\n= xi \n(b)\n\n= eqi\n\nwhere (a) follows (10); (b) follows (8) and (9).\nFrom (11), ECG reconstruction error in the closed-loop\ncompressor is fully determined by the error in quantizing ei .\nIn practice, quantization error is nite in a given quantizer.\nTherefore, the closed loop ECG compressor is always stable.\n\nI I I . PRO PO SED ECG COM PR E S S ION SCH EM E BA SED ON\nMU LT I P LE S EN SOR S\nBesides the absence of the stability analysis of ECG com-\npressor, there is another unsolved problem in the existing\nstudies, that is, only signal sensor is considered to compress\nECG signal. Indeed, more sensors are able to provide more\nobservations on the heart conditions since ECG signals ob-\ntained by sensors placed on different places of a body reect\nthe health conditions of different parts of a heart. Therefore,\nwe investigate the ECG signal compression based on multiple\nsensors.\nFor multiple sensors, independent quantization is an inef-\ncient practice since the redundancy between ECG signals from\nthe multiple sensors is not removed. The retaining redundant\ninformation induces more bits for quantization. We propose a\ncompression method used for multiple sensors.\n\nThe multiple sensors are divided into two tiers, that is,\none primary sensor is taken as the rst tier and the all the\nother sensors are at the secondary tier. The primary sensor has\nmore powerful computation and transmission abilities which\nis responsible for remotely transmitting the ECG signal. The\nsecondary sensors transmit their collected ECG signal to a pri-\nmary one and the transmission range is smaller than that for the\nprimary sensor. At the secondary sensor, conditional quantizer\nis used to compress ECG signal which can effectively reduce\nthe redundant information. For analysis simplicity, we consider\nthe case with one primary sensor and one secondary sensor.\n\nA. Structure of Multiple Sensors Based ECG Compression\nScheme\n1) System Overview: Fig. 4 presents the block diagram\nof the compression scheme built on the primary sensor and\nsecondary sensor. The secondary sensor transmits quantized\nECG signal xS to the primary one. The primary sensor\nquantizes xP to obtain xP and transmits the two quantized\nECG signals ( xS and xP ) to a remote data unit. In the scheme,\nwaveform features of xP are priorly known by the secondary\nsensor.\n\nFig. 4. Block diagram of double sensor quantization scheme\n\nWith the waveform feature of xP , we perform conditional\nquantization at the secondary sensor. The output from the con-\nditional quantizer at the secondary sensor is sent to the primary\none. The primary sensor takes differential compression scheme\nto quantize ECG signal.\nAfter introducing the functions of the modules in the\ncompression scheme, we present the details of how to im-\nplement the differential compression method at the primary\nsensor which is followed by the stability analysis. Then, the\nconditional quantization at the secondary sensor is introduced.\n2) Differential compression scheme at primary ECG sen-\nsor: Block diagram of our proposed differential ECG com-\npressor is presented in Fig. 5. Fig. 5 (a) and (b) describe the\nencoder and decoder respectively. Compared with conventional\nclosed-loop compressors, only addition and comparison opera-\ntions are needed, and multiplication is absent in the proposed\none. Furthermore, we will illustrate our compressor outper-\nforms the conventional ones in terms of ECG reconstruction\naccuracy.\nIn the new differential compressor, the difference between\ntwo adjacent ECG samples is rst calculated as follows,\nxi = xi  xi1 .\nNext, a modication factor, denoted by A, is added to\nxi . The factor A is designed to counteract the accumulation\n\n(12)\n\n\fthe following equation,\n\ne(cid:48)\ni = ei + eqi .\nFor analysis convenience, we simplify (16) into a form as\nfollows,\n\n(17)\n\n(18)\n\nA =  (xi1  xi1 ) ,\nwhere  is variable which absolute value is bounded into a\nsmall range and the sign of  is opposite to the sign of (xi1 \nxi1 ).\nNext, we determine the expression of e\ni as follows,\ni = xi  xi = xi  ( xi + xi1 )\ne\n= xi  (xi + A  eqi + xi1 )\n(a)\n= xi  (xi  xi1 + A  eqi + xi1 )\n(b)\n(cid:26)(1  | |) e\n= (xi1  xi1 )  ( (xi1  xi1 )  eqi )\n(c)\nfor xi1  xi1 > 0\ni1 + eqi\n(1 + | |) e\nfor xi1  xi1 < 0\n=\ni1 + eqi\n= (1  | |) e\ni1 + eqi .\nwhere (a) follows (17) and (13); (b) follows (12); (c) follows\n(18).\nThe Z -transformation of (19) is written as\nZ{e }\n1\n(20)\nZ{eq } =\n1  (1  | |)z1 .\nH (z ) =\nFrom (20), the pole is equal to p = 1  | | which locates in\ninner of a unit circle. Therefore, our proposed ECG processor\ncan avoid the accumulation of quantization error.\n\n(19)\n\nB. Quantizer Design in the Proposed ECG Compressor\nIn the optimum sense of minimizing average quantization\nerror at a given number of quantization levels, the statistics\nof the quantization objective affects the design of an optimum\nquantizer. Thus, we rst analyze the statistic features of the\ndifferential ECG signal. Afterwards, we present the details\nof how to design the differential ECG compressor. Then, the\nconditional quantization by the secondary sensor is introduced.\n1) Statistical Features of One-step Differential ECG Data:\nThere are two important issues determining statistical features\nof a signal, dynamic range of source and distribution of it. We\nwill numerically analyze the differential ECG signal at the two\naspects using two factors\nFig. 6 shows us the dynamical ranges of differential ECG\nand original ECG waveform which are calculated from 38\nrecords in MIT-BIH database.\nFor each record, we calculate the maximum and minimum\nvalues of both original and differential ECG signals. All the\nextreme values are plotted in Fig. 6.\nTo determine the dynamical range of differential ECG sig-\nnal, we rst calculate the upper bound of the maximum points\nand lower bound of minimum points via linear interpolation.\nAfterwards, we perform curve tting on the two bounds\nusing two horizontal lines. The two horizontal lines label the\nboundaries of the differential ECG dynamical range. The same\n\nFig. 5. Block digram of proposed ECG data compression scheme\n\nof quantization errors. After the addition of A, the adjacent\ndifference xi is derived into x(cid:48)\ni as follows,\nx(cid:48)\ni = xi + A.\n\n(13)\n\nAfterwards, x(cid:48)\nis quantized and quantization index is\ni\ndenoted by li . The quantization levels constitute a set ,\n = {l , l  L}, L = {1, 2, 3,    , L}, where N is equal\nto the total number of quantization levels. The elements in \nare ordered incrementally. As we know, the set  has impact\non the quantization performance. The details in designing \nwill be introduced in Section III-B2. After the quantizing x(cid:48)\ni ,\nthe compressor will perform modulation according to li .\nFrom the previous paragraph, the modication factor A is\nan important parameter. Next, we discuss the details of how\nto determine A.\nLet  xi denote the quantized x(cid:48)\ni . To reconstruct the ECG\nsample at i-th time instance ( xi ) at the encoder, we add the\n xi to xi1 ,\n\nxi = xi1 +  xi .\n\n(14)\n\nSince A is used to counteract the accumulation of quan-\ntization error, A is designed to be a function for reducing\nthe quantization error at\nthe previous time instance. The\nquantization error at the previous time instance is calculated\nby\nsi = xi1  xi1 .\n\n(15)\n\nA =\n\nTo avoid the accumulation of quantization error, A is set to\nbe a small positive value, when si < 0; and A is a negative\nvalue, when si  0. This correlation between A and si < 0 is\n\n(cid:16)\n(cid:17)\nmathematically described by\n(cid:17)\n (cid:16)\n lx(cid:48)\nif\n1\nlx(cid:48)\ni\ni\n+1  lx(cid:48)\nif\nsi < 0\nlx(cid:48)\ni\ni\ndenotes the index of the quantized x(cid:48)\nwhere lx(cid:48)\ni within\ni\nthe ordered set , and lx(cid:48)\nis the corresponding quantization\ni\nresult.\nStability Analysis: To analyze the stability of the proposed\ncompression scheme, we derive the compression bias e\ni as a\nfunction of the quantization error eqi due to the same reason\npresented in Section II-A2. The quantization error eqi satises\n\nsi > 0\n\n(16)\n\n,\n\n\fFig. 6. Dynamical range of differential ECG and original ECG signal\nmethod is also used to determine the dynamical range of the\noriginal ECG signal. From the calculation, we can observe that\ndynamical range of original ECG signal is approximately equal\nto 6. The differential ECG data has the dynamic range from -\n0.4854 to 0.6044. Since the dynamical range of the differential\nECG signal is smaller than that of the original signal, less bits\nare needed for quantizing the differential ECG signal at a given\nquantization accuracy.\nAfter analyzing the dynamical range, we study the dis-\ntribution of the differential ECG signal. First, we calculate\nhistogram of differential ECG signal which is plotted by\nthe blue stars in Fig. 7. With the calculated histogram, we\nuse the curve tting technology to abstract an approximated\nprobability model of the differential ECG signal.\n\nesqQ =\n\nx0\nl =\n\n,\n\nxl =\n\n.\n\n(22)\n\n(23)\n\n(21)\n\nis\n\nf (x)dx\n\nf (x)dx,\n\nquantizer is optimized in the sense of minimum mean square\n(cid:90) xl+1\nL1(cid:88)\n(cid:1)2\n(cid:0)x  x0\nof quantization error. The mean square error is calculated by\nl\nxl\nl=0\nwhere L is the number of total quantization levels and x0\nl\nquantization output at l-th quantization level.\nWe select Lloyd-Max algorithm [19] to determine each\nquantization zone (xl , xl+1 ) and the value of quantization\noutput x0\nl . According to Lloyd-Max algorithm, the parame-\n(cid:82) xl+1\nters are iteratively calculated as follows\n(cid:82) xl+1\nxf (x)dx\nxl\nxl\nl + x0\nx0\nl+1\n2\nIn a partial summarization of the quantization on the pri-\nmary sensor, the histogram of the rst order differential ECG\nsignal is calculated rst; second, via curve tting, a PDF in\nan explicit form is calculate to approximate the histogram;\nthird, the number of the bits for the quantization is determined;\nfourth, the codebook and quantization zones are determined\naccording to (22) and (23) respectively.\n3) Quantization on the Secondary ECG Sensor: The pro-\nposed differential ECG compression method at the primary\nsensor achieves the bit rate saving by reducing the redundancy\nbetween ECG samples from a same sensor. Besides,\nthe\nredundancy within the ECG samples from a single sensor,\nthere also exits inter-sensor redundancy which can be observed\nfrom the waveform similarities between the ECG signals from\ndifferent sensors. Without loss of generality, No. 100 ECG\nrecording in MIT-BIH arrhythmia database is plotted in Fig. 8\nwhich is taken as an example of showing the existence of\ninter-sensor redundancy. We will reduce to the inter-sensor\nredundancy to save the bit rate for the quantization on the\nsecondary sensor.\n\nFig. 7.\nfunctions\n\nApproximation of histogram via curve tting using exponential\n\nLet f (x) denote the probability. An exponential function\nwith peak clipping is used to represent the differential ECG\nhistogram. The red bold curve in Fig. 7 plots the probability\nfunction f (x) which takes a form of exponential function\nwith the exponent of -50. With the calculated probability\nmodel, we analytically study the quantizer design in the\nfollowing part.\n2) Quantization of differential ECG at primary sensor:\nAssume the quantizer in our analysis is labeled by Q. The\n\nFig. 8. Number 100 ECG recording in MIT-BIH Arrhythmia database\n\nThe ECG data collected by the primary sensor is denoted\nby xP , and xS is for the data from the secondary sensor. Let\nfP () and fS () denote the approximated PDFs from xP and\nxS respectively. At the secondary sensor, the approximated\nPDF of xP is priorly known. The quantization based on the\nprior information is denoted by Q(xS |fP ).\nSince there exists the connection between the waveforms\nof xP and xS , we build an afne relation between fP () and\n\n\ffS (). The afne is denoted by (cid:122) which is determined as\nfollows,\n\n(24)\n\n(cid:122) = {(a, b)| min(fP  a  fS  b)2},\nwhere a and b are constants for a group of ECG data from a\nsame person, such as the group consisting of xP and xS .\nWith the established afne relation, the conditional quan-\ntization and codebook can be calculated according to the\nfollowing four steps:\n First, a small number of bits, which number is denoted\nby N1 , are used to quantize the support area of fS ().\nSince bits number is smaller, the quantization bins are\n1i denote the i-th quantization\nsparse in the step. Let bS\n1i \nbin. Boundaries of bS\n1i are xS\n1i and xS\n1(i+1) , where xS\n1(i+1) and i  {0, 1, 2,    , 2N1 }.\n1i < xS\nbS\n We calculate the boundaries xP\ni from xS\ni according to the\n(cid:122) afne dened in (24). More explicitly, the calculation\nis presented below\n\ni = a  xS\n(25)\nxP\n1i + b.\n1i , i  {0, 1, 2,    , 2N1 }, we utilize\n Within each bin of bS\nLloyd-Max algorithm to calculate a sub-codebook which\nis denoted by cS\ni . Let N2 denote the number of bits used\nin the sub-level quantization.\n Using the calculated sub-codebooks, we quantize ECG\n1i , i  {0, 1, 2,    , 2N1 }. The\nsignal within all bins of bS\ncorresponding quantization indexes, denoted by I s , are\nthe nal outputs of the compressor on the secondary\nsensor.\nTo assist our explanation, Fig. 9 presents a toy example of\nthe conditional quantization method. In Fig. 9, the rst three\nsegments labeled with 1, 2, and 3 constitute a set. Near to the\nrst set, the 5 numbers ({1, 2, 3, 4, 5}) labeled ve segments\nform the second set. Beside the second one, the third set is\nconstituted in the same way. Each of three sets ({1, 2, 3},\n{1, 2, 3, 4, 5} and {1, 2, 3, 4, 5}) covers the range of a bS\n1i ,\ni  {1, 2, 3}, and all the three sets cover the full dynamic\nrange of xS without overlapping. In each bin of bS\n1i , sub-\ncodebook is calculated following the third step above. Then,\nthe quantization is performed in each bin according to the\ncalculated codebook and the numbers noted in Fig. 9 are the\nnal results of the compression on the secondary sensor.\n\nFig. 9. Demon of compression on secondary ECG sensor\n\nAfter the conditional quantization,\nthe secondary sensor\nsends the quantization results to the primary sensor where the\n\nreconstruction is performed. To reconstruct ECG signal at the\ndecoder, two steps are needed.\ni+1 ), 0  i  2N1 , we\n Among the set of sections [xP\ni , xP\ndetermine which section xP belongs to. For example, xP\nk  xP < xP\nbelongs to the k-th section, xP\nk+1 .\n We take the k-th sub-codebook cS\nk to determine the\nreconstruction corresponding to the quantization index I s .\nBased on the description above, we can realize that there are\nN2 bits used in the compression at the secondary sensor. In the\nexperiments of this paper, we will show that N2 is smaller than\nthe number of bits for direction quantization, N2 < W , where\nW denote the number of the bits used for direct quantization.\nIn this section, the structure of implementing the proposed\nECG compression scheme is presented which is at low com-\nplexity and thus easy to be implemented at less hardware\nresource cost. Furthermore, the new compression scheme saves\nthe consumed bits per sample. Indeed, besides the bits per\nsample, the sampling rate will also affect the accuracy of ECG\ncompression and the hardware resource consumption. In the\nnext section, we perform joint optimization with respect to the\ntwo aspects.\n\nIV. TWO D IM EN S IONAL B I T RAT E O PT IM I ZAT ION\nIn a remote health monitoring system, there are two factors\nsignicantly affecting the complexity and power consumption,\nthe quantization bits per sample and and transmission period.\nLet W denote the average number of the bits used for\nquantizing each sample, and Tt is the period of transmitting\nthe compressed ECG data. The bit rate r is calculated by\n. In the optimization, our objective is to minimize\nr = W\nTt\naverage square error of the reconstructed ECG signal. The\nminimization is under the constraint of a given bit rate. With\nrespect to the single dimension of bits number, the related\nminimization work refers to [20] The theoretic knowledge\nabout the two-dimension optimization refers to [21][23].\nFor an ECG sensor, let T0 denote the minimum sampling\nperiod. After the sampling, the ECG signal written as x(mT0 ),\nm  Z. In practice, the sampling frequency 1\nis over high for\nT0\nECG signal. Thus, the ECG signal tor be transmitted should\nbe down sampled. Let K denote the down sampling rate. After\nthe down sampling, the ECG signal is transmitted. Therefore,\nwe can realize that Tt = K T0 . Essentially, the optimization in\nthis section is performed with respect to K and W .\n\nA. Calculation of Bounds on Tt and W\n1) Upper bound on Tt : In general cases, upper bound on\nsampling period is determined according to Nyquist sampling\ntheorem. For sampling ECG signal, there are some differences.\nECG data is usually taken to assist diagnosis of cardiovas-\ncular diseases. In a heart beat period, a ECG signal consists\nof different waves, such as P wave, QRS wave and T wave.\nThese waves provide assisting information for diagnosing\ndifferent diseases. For example, ST segment depression or\nelevation accompanying with T wave inversion is used to\ndiagnose myocardial infarction and cardiogenic shock. QRS\nvoltage, ST-T wave, and R-wave changes are used to diagnose\nCardiomyopathy.\n\n\fWe can easily realize that ner sampling is able to keep\nmore information of ECG waveform. In general cases, dura-\ntions of different waves are not the same in a ECG waveform.\nThe wave having the smallest duration is most sensitive to\nsampling period. According to our observations, either Q-\nR segment or R-S segment has the smallest duration. Time\ninterval between Q and R is denoted by tQR . And interval\nbetween R and S is tRS . To avoid information loss of QRS,\nwe need to guarantee the smaller one between tQR and tRS\nis larger than the sampling period, min{tQR , tRS }  Tt .\nAs mentioned before, ECG signals signicantly changes\nfor different people and different health conditions. Thus, we\nstill use numerical method to investigate the smallest average\nduration of tQR and tRS . In the numerical analysis, we use\nECG data published by Michael Oeff [24] for higher accuracy.\nThe ECG data in [24] are sampled by 16 bits at the frequency\nof 10kHz. We estimate the durations of tQR and tRS of ECG\ndata from 549 persons. According to our calculation, average\nvalue of tRS is smaller than that of tQR . Furthermore, from the\n549 recordings, the smallest tRS is equal to 56.9ms. Therefore,\nwe need to guarantee sampling period Tt to provide the time\nresolution smaller than 56.9ms. Since sampling period of Tt\n2 , the upper bound on Tt is 113.8ms,\ngenerates resolution of Tt\nt = 0.1138s.\nT U\n2) Lower Bound on T0 : The ner sampling generates the\nmore accurate ECG signal while more hardware resources\nare consumed. In the joint optimization on bit rate, smallest\nsampling period at ADC is considered as the lower bound on\nT0 . In our analysis, 1/360s is taken as the low bound on Tt ,\n360 s.\nt = 1\nT L\n3) Upper Bound on W : Larger bit width means ner\nquantization which provides more accurate description on\nECG amplitude. For a given wearable device, bit width W\nis upper bounded by the implementable largest number of\nquantization levels. The number of largest bits varies for\ndifferent wearable devices. We consider 12 as the upper bound\non W , W U = 12.\n4) Lower Bound on W : As introduced in Section IV-A1,\nwaves in a beat rate period of ECG signal are used in diag-\nnosing different types of diseases. These waves have different\nsensitivities to bit number. To determine lower bound on bit\nwidth W , we need to nd out the wave which has the smallest\npeak average power ratio.\nWe still use data from [24] in the analysis of bit width lower\nbound. There are four steps in the calculation.\nFirst, we select the ECG signals in which all waveform fea-\ntures can be observed by a doctor in medicine. The waveform\nfeatures includes P, Q, R, S and T-waves.\nNext, we measure the waves summit-to-average distance,\nwhich are denoted by g ( includes an arbitrary member of\nthe alphabet group {P , Q, R, S }). Essentially, g is equal to\nthe distance between the locally maximum point of each wave\nto the base of ECG signals. Companying with each element\nof g , an envelope amplitude (distance between upper and\nlower envelope of a ECG recording) is measured. We use \nto denote the envelope amplitude companying with g .\nThird, at each ECG recording, we calculate the ratio of g\nover its corresponding  . The ratio is denoted by k , k  K\n\nwhere |K| is equal to the number of all calculated ratios.\nFinally, the k which has the smallest absolute value is\nselected to help us determine the lower bound on W . Let W L\n2W U +1 \ndenote the lower bound. We select W L such that\n1\nmin(|k |). According to our calculation, the lower bound on\nW is equal to 4, W L = 4.\n\nx(nK T0 )\n\nx(mT0 ) =\n\nB. Joint Optimization on Bit Rate\nAt the wearable device, the quantized ECG data are trans-\nmitted to a data server for storage and analysis. We assume\nthe time interval Tt\nto K T0 , Tt = K T0 . After\nis equal\nreceiving the quantized data, the data server reconstructs ECG\nsignal. The reconstructed ECG data is denoted by x which is\n(cid:20)\n(cid:21)\n+(cid:88)\ncalculated as follows,\nu((m  nK )T0 )\nu((m  (n + 1)K + 1)T0 )\nn=\n(26)\nWith the reconstructed ECG data x, we evaluate the recon-\nstruction accuracy in terms of average square error which is\n2(cid:88)\ndenoted by .  is calculated by\nM\nm= M\n2 +1\nIn the ECG data compression and transmission system, the\nbit rate budget is R which is essentially a upper bound on the\nactual bit rate r , that is,\n\n(x(mT0  x(mT0 )))2 .\n\n = lim\nM\n\n1\nM\n\n(27)\n\n.\n\nr =\n\n(28)\n\n R.\n\nlimM 1\nM\n\nW\nTt\nUnder the constraint shown in (28), we minimize average\nsquare error in reconstructing the ECG signal. The optimiza-\n(cid:80) M\ntion problem is formulated as\n(c(mT0  c(mT0 )))2\n2\nm= M\n2 +1\n R\nW\nK T0\n\nminimize:\nW,K\nsubject to:\n\n(29)\nIn the optimization shown in (29), the variables include\nthe average quantization number per sample (W ) and the\ntransmission period Tt = K T0 . Numerical methods are used\nto solve the optimization problem. Fig. 10 presents an example\nof solving optimization problem.\nIn Fig. 10 the colorful curves are contour of . The contours\nare plotted within a red dash rectangular. The boundaries of\nthe rectangular is formed by the calculated bounds on W and\nTt . The darker color means the smaller . The slope of a dot\ndash black lines are equal to an ideal bit rate budget R. A line\nwith markers indicates the actual bit rate. Since W and K are\nboth in discrete values, the actual bit rate lines with markers\ncan hardly exactly match the ideal lines.\nThere two steps to determine the minimum MSE under the\nconstraint of R. We rst draw a bit rate budget line with the\nslope of R. Next, we nd the contour curve which is tangent\nto the bit rate budget line. Then, the contour curve tangent to\nthe budget line informs us the minimum MSE achievable at\nthe bit rate of R.\n\n\fTABLE I\nCOM PAR I SON IN COM PUTAT ION COM P LEX I TY\nMul.\nAddl.\n(/sample) Memory U.\n(/sample)\n8\n5\n4\nLMS\n8\n5\n4\nLMS (no. coef.)\n39\n1\n1601\nDCT\n1216\n143\n68\nWavelet+SPIHT\n1\n2\n1\nDelta modulator\n2022\n183\n212\nCompressive Sensing\n0\n3\n2\nNew method\nof basis vectors. The volume of required multiplications is\nrelated to the length of a ECG segment. Besides the extensive\ndemand on multipliers, a large number of memory units are\nalso needed. LMS based compressor has lower computation\ncomplexity than the previous two compressors. From Table I,\nwe can easily nd that both DPCM based compressor and our\nmethod can be implemented in low complexity. Different from\nDelta modulator based compressor, our method does not need\nmultiplication operations.\nTo present an intuitive impression on the performance of\ncompression algorithms, we present the reconstructed ECG\nwaveforms by all the mentioned algorithms. Due the page\nlimits, the graphic performance comparison is performed on\ntwo ECG records, No. 112 record in MIT-BIH database and\nNo. 103 record in European ST-T database. The reconstruction\naccuracy comparison for the two records are presented in\nFig. 11 and Fig. 12 respectively. The computation is the\ncompressions are performed in 8-bits numbers.\n\nFig. 11. Comparison of the reconstructions of No. 112 record in MIT-BIH\narrhythmia database\n\nFrom Fig. 11, the reconstruction ECG via the DCT based\nmethod retains the key features, such as P, Q, R, S, and\nT waves. Wavelet based compression incurs some noise-like\ndistortion. In Fig. 11 (d), coefcients of adaptive lter are\nnot updated and we can observe ECG waveform distortion.\nIn Fig. 11 (e), the coefcients are adaptively updated which\ngenerate satisfying reconstruction accuracy. However, the bits\nused for updating coefcients are in a large number. The\naccuracy of the reconstructed ECG signal from the compres-\n\nFig. 10. ECG reconstruction ASE versus word length in bits and transmission\nperiod in second\nV. EX PER IM EN TA L V ER I FICAT ION S\nIn this section, experiments are performed to evaluate the\neffectiveness of the proposed method. We compare our method\nwith existing ones in three aspects, computation complexity,\nECG waveform distortion after reconstruction, and the ef-\nciency of compression method in saving bits. Second, we\ninvestigate the performance of the conditional quantization at\nthe secondary ECG sensor. Finally, experiments on joint bit\nrate optimization are performed.\nIn the experiments, our objective is to evaluate the effective-\nness of proposed ECG compression method. In our knowledge,\nthe compression is not as sensitive to the change of wave-\nform shapes as the algorithms for R-wave detection or other\ncardiovascular disease diagnosis. Therefore, the experiments\nare performed on the data from only two databases, MIT-BIH\nArrhythmia database [24] and European ST-T database [25].\n\nA. Complexity and Reconstruction Accuracy Comparison\nIn this subsection, we investigate the performance of the\nproposed differential ECG compression method. To evaluate\nthe performance, we compare our method with the ones in\nliterature. The ECG compression schemes based on DCT\nand wavelet are considered since they are widely adopted\nin ECG compression. Different from pure wavelet algorithm,\nwavelet compression by the set partitioning in hierarchical\ntrees algorithms is implemented. Since compressive sensing is\nwidely discussed and applied, ECG signal is compressed via a\ncompressive sensing algorithm. We also evaluate performance\nof two differential ECG compression methods, least mean\nsquare (LMS) based compression algorithm and DPCM based\none.\nTable I presents computation complexities of different ECG\ncompression methods. Average numbers of multiplications,\nadditions per data sample and required memory units are\ntaken as the metrics. From Table I, DCT and wavelet based\ncompressors need a large number of multiplications and mem-\nory units. The large number of multiplications are induced\nby the multiplication between ECG signal vector and groups\n\n\fFig. 12. Comparison of the reconstructions of No. 103 record in European\nST-T database\nsive sensing method is high while the computation burden is\nheavy. The key features of ECG signal can also be observed in\nDPCM based compressor (Fig. 11 (g)). However, we can nd\nunexpected uctuations between R and S. DPCM quantizes\nthe error occurring estimating current ECG value. When ECG\nwaveform changes fast, such as in the segment between R and\nS, DPCM is not able to keep tracking of the fast change. Thus,\nthe unexpected uctuations occur. Our method quantizes the\nECG amplitude change directly. Thus, our method is more\nrobust to the fast change. The similar phenomena can also be\nobserved in Fig. 12.\n\nFig. 13. MSE versus average number of bits per sample for records in\nMIT-BIH arrhythmia database\n\nFig. 11 and Fig. 12 illustrate the reconstructed ECG wave-\nform at a xed bit width of 8. Furthermore, we present the\nnormalized MSE of the reconstructed ECG at different bit\nwidths in Fig. 13 and Fig. 14 which curves are calculated from\nthe records in MIT-BIH Arrhythmia database and European\nST-T database respectively. From Fig. 13, wavelet based com-\npression induces the worst reconstruction accuracy. When 4\ncoefcients of LMS lters are not transmitted from a compres-\nsor, MSE does not decrease with bit number increasing. When\n\nFig. 14. MSE versus average number of bits per sample for records in\nEuropean ST-T database\nthe coefcients are transmitted, MSE decreases signicantly\nwith bit width increasing. However, the coefcient updating\nrequires more bits. Our new method achieve the smallest MSE\nat a given low bit rate. The advantage of the new method over\nthe existing ones can also be observed in Fig. 14.\n\nB. Simulation in Double Sensors Based ECG Compression\nIn this subsection, we investigate the performance of double\nsensors based ECG compression method. As discussed in\nprevious sections, the distribution of ECG signal from pri-\nmary sensor is priorly known by the secondary sensor. Thus,\nconditional quantization can be performed at the secondary\nsensor. The quantization results are transmitted to the primary\nsensor via a perfect channel. The primary sensor differentially\nquantize ECG signal acquired by itself. The results of condi-\ntional quantization and differential quantization are transmitted\nto a remote data center. At the data center, the ECG signal\nacquired by the primary sensor is rst reconstructed. Then,\nreconstruction of the ECG signal from the secondary sensor\nis performed.\nThe combination of differential quantization at primary\nsensor and the conditional quantization at the secondary sensor\nis called as hybrid quantization structure. For comparison, we\nalso consider other two quantization structures in the double\nsensors based compression. First, differential quantization is\napplied in both primary and secondary sensors. Second, differ-\nential quantization is taken at the primary sensor and uniform\nquantization used at the secondary one. The average MSE\nof the reconstructed ECG at both the primary and secondary\nsensors is taken as the accuracy metric.\nThe results of the two sensors based ECG compression are\npresented in Fig. 15. From the results, the proposed hybrid\nquantization method outperforms other two, the conventional\nquantization on original ECG signal (labeled as uniform\nquantization) and the two-independent-differential ECG quan-\ntization (labeled as twice diff. str). The advantage of the\nproposed two sensors based compression scheme is caused by\nthe reduction of redundant information between ECG signal\n\n\fof the joint optimization on saving bit rate can be clearly\nobserved.\n\nV I . CONC LU S ION\nWe investigate the compression of ECG signal which is im-\nportant for saving hardware and power consumption in health\ntelemonitoring systems. Different from the ECG compression\nwork in literature, compression scheme based on multiple ECG\nsensors is considered. Without loss of generality, we consider\nan example in which there are two ECG sensors, a primary and\na secondary ECG sensor. At the primary one, we use a novel\ndifferential structure to compress ECG signal which effectively\nreduces the redundant\ninformation between adjacent ECG\nsamples. At the secondary ECG sensor, conditional quantizer\nis proposed to compress ECG signal which utilizes the inherent\nconnection between the shapes of ECG signals from the two\nsensors. Experiments verify the advantage of our proposed\ncompression scheme both in complexity and reconstruction\naccuracy.\n\nR E F ER ENC E S\n\n[1] Y. B. Lin, Y. W. Lin, C. M. Huang, C. Y. Chih, and P. Lin, Iottalk: A\nmanagement platform for recongurable sensor devices, IEEE Internet\nof Things Journal, vol. PP, no. 99, pp. 11, 2017.\n[2] P. Huang and Y. Pi, An improved location service scheme in urban\nenvironments with the combination of gps and mobile stations, Wireless\nCommunications and Mobile Computing, vol. 14, no. 13, pp. 1287\n1301, 2014. [Online]. Available: http://dx.doi.org/10.1002/wcm.2232\n[3] , Wireless internet assisting satellite position in urban environ-\nments, in 2011 6th International ICST Conference on Communications\nand Networking in China (CHINACOM), Aug 2011, pp. 262267.\n[4] J. Pandey and B. Otis, A sub-100 w mics/ism band transmitter based\non injection-locking and frequency multiplication, Solid-State Circuits,\nIEEE Journal of, vol. 46, no. 5, pp. 10491058, May 2011.\n[5] J. Ma, T. Zhang, and M. Dong, A novel ecg data compression method\nusing adaptive fourier decomposition with security guarantee in e-health\napplications, IEEE Journal of Biomedical and Health Informatics,\nvol. 19, no. 3, pp. 986994, May 2015.\n[6] T. Marisa, T. Niederhauser, A. Haeberlin, R. A. Wildhaber, R. Vogel,\nM. Jacomet, and J. Goette, Bufferless compression of asynchronously\nsampled ecg signals in cubic hermitian vector space, IEEE Transactions\non Biomedical Engineering, vol. 62, no. 12, pp. 28782887, Dec 2015.\n[7] C. Deepu and Y. Lian, A joint QRS detection and data compression\nscheme for wearable sensors, Biomedical Engineering, IEEE Transac-\ntions on, vol. 62, no. 1, pp. 165175, Jan 2015.\n[8] A. Bendifallah, R. Benzid, and M. Boulemden, Improved ECG com-\npression method using discrete cosine transform, Electronics Letters,\nvol. 47, no. 2, pp. 8789, January 2011.\n[9] A. Bilgin, M. Marcellin, and M. Altbach, Compression of electro-\ncardiogram signals using JPEG2000, Consumer Electronics, IEEE\nTransactions on, vol. 49, no. 4, pp. 833840, Nov 2003.\n[10] Z. Lu, D. Y. Kim, and W. Pearlman, Wavelet compression of ECG sig-\nnals by the set partitioning in hierarchical trees algorithm, Biomedical\nEngineering, IEEE Transactions on, vol. 47, no. 7, pp. 849856, July\n2000.\n[11] Y. Zou, J. Han, S. Xuan, S. Huang, X. Weng, D. Fang, and X. Zeng, An\nenergy-efcient design for ECG recording and R-peak detection based\non wavelet transform, Circuits and Systems II: Express Briefs, IEEE\nTransactions on, vol. 62, no. 2, pp. 119123, Feb 2015.\n\nFig. 15. Bit rate in ECG compression on double sensors\n\nfrom the two sensors.\n\nC. Simulation in Joint Bit Rate Optimization\nUntil now, the simulations are performed in the precon-\ndition that the sampling period is xed. In this subsection,\nwe investigate the performance of joint bit rate optimization\nover quantization bit number and transmission period. As a\ncomparison, quantization results are transmitted at a period of\n1/360s which is the same with the sampling period for the\nrecords in MIT-BIH database. Via interpolation, the equivalent\nsampling at 360H z is also performed on the data records for\nEuropean ST-T database.\n\nFig. 16.\n\nJoint bit rate optimization\n\nFig. 16 plots MSE-bit rate curves with and without the\njoint optimization. The two dimensional optimization is con-\nstrained by the bounds both in sampling period and quantiza-\ntion bits per sample. These bounds, which are calculated in\nSection IV-A, enable us guarantee key features of ECG signal\ncan be retained in the compression. From Fig. 16, advantage\n\n\f[12] L. Polania, R. Carrillo, M. Blanco-Velasco, and K. Barner, Exploit-\ning prior knowledge in compressed sensing wireless ECG systems,\nBiomedical and Health Informatics, IEEE Journal of, vol. 19, no. 2,\npp. 508519, March 2015.\n[13] V. Cambareri, M. Mangia, F. Pareschi, R. Rovatti, and G. Setti, A\ncase study in low-complexity ecg signal encoding: How compressing is\ncompressed sensing? IEEE Signal Processing Letters, vol. 22, no. 10,\npp. 17431747, Oct 2015.\n[14] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst, Com-\npressed sensing for real-time energy-efcient ecg compression on wire-\nless body sensor nodes, IEEE Transactions on Biomedical Engineering,\nvol. 58, no. 9, pp. 24562466, Sept 2011.\n[15] U. E. Ruttimann and H. V. Pipberger, Compression of the ECG by pre-\ndiction or interpolation and entropy encoding, Biomedical Engineering,\nIEEE Transactions on, vol. BME-26, no. 11, pp. 613623, Nov 1979.\n[16] C.-C. Sun and S.-C. Tai, Beat-based ECG compression using gain-\nshape vector quantization, Biomedical Engineering, IEEE Transactions\non, vol. 52, no. 11, pp. 18821888, Nov 2005.\n[17] S.-L. Chen and J.-G. Wang, VLSI implementation of low-power cost-\nefcient lossless ECG encoder design for wireless healthcare monitoring\napplication, Electronics Letters, vol. 49, no. 2, pp. 9193, January 2013.\n[18] G. Einarsson, An improved implementation of predictive coding com-\npression, Communications, IEEE Transactions on, vol. 39, no. 2, pp.\n169171, Feb 1991.\n[19] S. Lloyd, Least squares quantization in PCM, Information Theory,\nIEEE Transactions on, vol. 28, no. 2, pp. 129137, Mar 1982.\n[20] P. Huang and D. Rajan, Estimation of centralized spectrum sensing\noverhead for cognitive radio networks, in 2014 IEEE 25th Annual\nInternational Symposium on Personal, Indoor, and Mobile Radio Com-\nmunication (PIMRC), Sept 2014, pp. 659663.\n[21] , Bounds on the overhead of spectrum sensing in cognitive radio,\nin 2014 IEEE Global Communications Conference, Dec 2014, pp. 846\n850.\n[22] P. Huang, Y. Du, and Y. Li, Stability analysis and hardware resource\noptimization in channel emulator design, IEEE Transactions on Circuits\nand Systems I: Regular Papers, vol. 63, no. 7, pp. 10891100, July 2016.\n[23] P. Huang, W. Wang, and Y. Pi, Estimation on channel state feedback\noverhead lower bound with consideration in compression scheme and\nfeedback period, IEEE Transactions on Communications, vol. 65, no. 3,\npp. 12191233, March 2017.\n[24] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. C.\nIvanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and\nH. E. Stanley, PhysioBank, PhysioToolkit, and PhysioNet: Components\nof a new research resource for complex physiologic signals, Circu-\nlation, vol. 101, no. 23, pp. e215e220, 2000 (June 13), circulation\nElectronic Pages: http://circ.ahajournals.org/cgi/content/full/101/23/e215\nPMID:1085218; doi: 10.1161/01.CIR.101.23.e215.\n[25] , Physiobank, physiotoolkit, and physionet, Circulation, vol.\n101, no. 23, pp.\ne215e220, 2000.\n[Online]. Available: http:\n//circ.ahajournals.org/content/101/23/e215\n\n\f", 
        "tag": "Information Theory", 
        "link": "https://arxiv.org/list/cs.IT/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nG\nL\n.\ns\nc\n[\n \n \n1\nv\n4\n7\n5\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nBag-of-Words Method Applied to Accelerometer\nMeasurements for the Purpose of Classication\nand Energy Estimation\n\nKevin M. Amaral\nPhD. Student\nComputer Science\nUniversity of Massachusetts Boston\n\nPing Chen, PhD.\nAssociate Professor\nComputer Science and Engineering\n\nUniversity of Massachusetts Boston\n\nScott Crouter, PhD.\nAssistant Professor\nExercise Physiology\nUniversity of Tennessee-Knoxville\n\nWei Ding, PhD.\nAssociate Professor\nComputer Science\nUniversity of Massachusetts Boston\n\nApril 2017\n\n1 Abstract\n\nAccelerometer measurements are the prime type of sensor information most\nthink of when seeking to measure physical activity. On the market, there are\nmany tness measuring devices which aim to track calories burned and steps\ncounted through the use of accelerometers. These measurements, though good\nenough for the average consumer, are noisy and unreliable in terms of the pre-\ncision of measurement needed in a scientic setting. The contribution of this\npaper is an innovative and highly accurate regression method which uses an in-\ntermediary two-stage classication step to better direct the regression of energy\nexpenditure values from accelerometer counts.\nWe show that through an additional unsupervised layer of intermediate fea-\nture construction, we can leverage latent patterns within accelerometer counts to\nprovide better grounds for activity classication than expert-constructed time-\nseries features. For this, our approach utilizes a mathematical model originat-\ning in natural language processing, the bag-of-words model, that has in the\npast years been appearing in diverse disciplines outside of the natural language\nprocessing eld such as image processing. Further emphasizing the natural lan-\nguage connection to stochastics, we use a gaussian mixture model to learn the\ndictionary upon which the bag-of-words model is built. Moreover, we show that\nwith the addition of these features, were able to improve regression root mean-\n\n1\n\n\fsquared error of energy expenditure by approximately 1.4 units over existing\nstate-of-the-art methods.\n\n2\n\nIntroduction\n\n2.1 Background and Related Work\n\nIn 2005, Crouter et. al.\nintroduce the two-regression model which alternates\nbetween a quadratic regression model and a linear regression model based on\nthe coecients of variations of each bout.\n[3] This novel approach broke the\noverall problem ob jective into two key parts: rst, separating instances by their\nvariability into two groupings based on their coecients of variation; second,\napplying to each grouping a regression model which is more appropriate for\ninstances of that variability.\nIn 2012, Trost et. al. was able to improve physical activity classication\naccuracy as well as low root mean squared-error (RMSE) in energy expenditure\nestimation with an Articial Neural Network (ANN) model. [7]\nIn that same year, Mu et. al., revisited the two-regression model of Crouter\net. al. and extended it to a number of regression models, one per each activ-\nity type.\n[5] The data used in this study including each activity bout therein\nwas structured rather variably, which made it analogous to a free-living data\ncollection. This method utilized distance metric learning methods to learn the\nunderlying block structure of variable-length activity bouts.\nIn 2014, Staudenmayer et. al. expanded on the eld with another ANN\nmodel which they applied to their own dataset. [6] However, their classication\nprocedure was targeting learned activity types, as opposed to expert-dened\ntypes. They produced these types through clustering based on their signal\nactivity levels.\nIn 2015, Montoye et. al. did an analysis of accelerometer placement for the\npurpose of energy expenditure estimation and found in their results that the\nthigh-mounted accelerometer produced the most accurate measurements of all\nconsidered mount-points. [4]\nBastion et. al. published an evaluation of cutting-edge methods outside of\nthe rigid laboratory setting and conrmed the activity classication commu-\nnitys suspicions that existing methods would not perform well in the free-living\nsetting. [1]\n\n3 Methods\n\nFor our experiments, we utilized a subset of the dataset used in [5] whose activ-\nities most-closely resembled those of [7]. In total, one hundred and eighty-four\n(184) child participants data were used. For each of these participants, one\nbout of lying resting for up to thirty (30) minutes with a median time of sev-\nenteen (17) minutes. All other activities were performed for up to twelve (12)\nminutes with a median time of four (4) minutes.\n\n2\n\n\fSedentary\n\nLight Household and Games\n\nModerate-Vigorous Household and Sports\n\nWalk\n\nRun\n\nLying Rest\nPlaying Computer Games\nReading\nLight Cleaning\nSweeping\nWorkout Video\nWall Ball\nPlaying Catch\nBrisk Track Walking\nSlow Track Walking\nWalking Course\nTrack Running\n\nTable 1: Activity Classes and the types of activities performed within them.\n\nIn Table 1, we list the types of activities which have been included in our\nexperiment.\nIn the left column, we have the activity classes of which each\nactivity bout in the dataset only corresponds to one. On the right, are the more\nspecic activity types that each class consists of.\nIn Table 2, the number of bouts associated with each activity class is listed,\nas well as the total number of intervals in each class.\n\nSed. LHH MtV Walk Run\n\nBouts\n\n259\n\n116\n\n79\n\nIntervals\n\n16475\n\n2505\n\n1570\n\n150\n\n3775\n\n23\n\n485\n\nTable 2: Dataset Summary\n\n3.1 Classication Model\n\nArticial neural networks are the state of the art method for activity classi-\ncation. Trost et. al. used in their work a feed-forward neural network with a\nsingle hidden layer to predict MET values directly [7]. Staudenmayer et. al.\nalso use an articial neural network as their model [6], in the rst step to predict\nthe physical activity type and then afterwards separately to predict the MET\nvalues. Our method improves on the ideas of the two-regression framework by\nexpanding the number of regression models to one per each activity type and\nleverages the model from Staudenmayers and Trosts works as our frameworks\nnal classication component.\nOur classication model is a three-stage framework which consists of a clus-\ntering phase over the activity windows, a bag-of-words construction phase for\neach unique activity bout, and a neural network classication phase over the\nnew bag of words features.\n\n3\n\n\fThe Bag-of-Words structure within the framework brings with it its repre-\nsentation power from the eld of Natural Language Processing, and integrating\nit into our activity classication and energy expenditure estimation framework\nbridges the gap between these two disciplines.\n\nFigure 1: Classication Framework Diagram\n\n3.1.1 Time Series Features - Atom Construction\n\nThe standard feature construction seen in the state of the art for physical ac-\ntivity time series has been to use percentile features. These features represent\nthe time series signal by their moments, more specically their 10th, 25th, 50th,\n75th, and 90th percentiles. [6] [7]. We will start with these features and include\nlag-k autocorrelation features in our initial feature construction phase.\nFor the instance construction, we favor segmentation of the signal into 12-\nsecond blocks. This allows us to best manage the inconsistent bout lengths in\nour dataset.\nIn the work of Staudenmayer et. al., their activity bouts were\nall of a xed-length of 10 minutes. Trost et. al.s dataset considered for xed-\nlength bouts of 2 minutes, however they opted to increase their data resolution\nby further partitioning their bouts into 10, 15, 20, 30, and 60 seconds for their\nexperiments. In line with Trost et. al.s work, our narrower windows allow us\n\n4\n\n\fto tightly capture the volatile patterns of child physical activity, whereas wider\nwindows would be too long.\nIn either case, those works have a data setting which we do not have: xed-\nlength activity bouts. In later subsections of Section 3, we show how were able\nto overcome this challenge within our dataset through our intermediate feature\nconstruction phase to produce a xed-length data instances.\n\n3.1.2 Clustering Phase - Atom Classing\n\nIn Staudenmayer et. al., they cluster activity instances based on their signals to\nproduce their activity classes. [6] However, we do not use the clustering phase\nalone to determine physical activity classes. We reject the idea that classifying\nthe signal alone will give us the true activity class. Each performance of an\nactivity diers and any one moment spent idle or performing the activity in a\nnon-standard way will greatly aect the signal as a whole.\nAgain, we consider brief windows of 12-seconds to be characteristic of atomic\nmicro-performances within an activity. This is in line with previous work in Mu\net. al. [5] in which we considered the block structure of the timeseries signal. By\ncontrast, however, we are considering much smaller blocks which are expected\nto lose their homogeneity with the rest of the signal, as opposed to the 1-minute\nwindows used in that paper. One minute of an activity may look like any other\nminute of the same activity but as we choose this ner resolution, each 12-second\nwindow will be more distinguished from other block units in the same activity.\nBy considering these very brief local acts, we can better classify the activity\nas a whole. Henceforth, we will call these micro-performances atoms as they\nrepresent our smallest considered unit of activity.\nThese atoms types must be learned latently. While we have some high-level\nidea of what types of atoms we should be able to nd within the accelerometer\nmeasurements, such as jumping, taking a step, climbing a stair, etc., it is not\nclear which atoms best describe the space of accelerometer counts over the\ntypes of activities were considering.\nIts expected that the truly descriptive\natom classes are abstract spatial patterns which we can not dene empirically.\nAs such, we seek to identify these atom classes through clustering. The\nmodel weve chosen to represent our clusters is a Gaussian Mixture model. In\nstatistics, mixture models best model a distribution for which there are dis-\ntinct subdistributions which constitute the whole probability space [2].\nIn a\nGaussian mixture, we assume that each and every subpopulation in our sample\ndistribution is modelled at least approximately by a multivariate Gaussian.\nUsing variational Bayesian methods to approximate the Guassian mixture\nover our sample, we learn the most-likely set of subpopulations or clusters that\nour 12-second windows fall into. The associated distributions of these subpop-\nulations are each associated with a unique atom class.\n\n5\n\n\f\n\nzn\n\nxn\n\nN\n\n\n\n\n\nFigure 2: Graphical representation of the Bayesian mixture of Gaussians model\n\n3.1.3 Bag of Words Phase - Latent Feature Construction\n\nA bag is a mathematical structure likened to that of the mathematical set except\nthat it allows duplicate elements. It has a higher representation power than the\nset in that each element in the bag has an associated count or frequency. When\nused as a collection of words, it represents word frequency within a sentence or\ndocument in natural language processing and has found use in other elds of\nmachine learning. We seek to apply the bag-of-words model to our context in\nthe following way.\nWith each 12-second window of an activity bout assigned to a unique cluster,\nwe now have a basis for constructing an activity signature for the entire bout.\nIf we take each atom class to be a word, each bout can be seen as a sentence\ncomposed of words. Intuitively, the idea follows from the concept of the atoms\nin and of themselves, which are short meaningful chunks of the whole activity.\nLooking at them independent from the activity doesnt give us any indication of\nwhat activity is being performed. This is the same relationship between words\nand sentences.\nFor the bag-of-words construction, we must select a dictionary. The dic-\ntionary denes which words may appear in the bag structure. We must also\ndetermine if we are going to use word counts or word frequency. Not all activ-\nities are of the same length in the same way that not all sentences are of the\nsame length. As such, we will use a bag-of-words model over term frequency.\nThe Guassian mixture acts as our dictionary where each cluster is a word.\nFor any one activity bout, the frequency associated with each word in its bag-\nof-words is the ratio of how many atoms in the bout belong to that words\nassociated cluster.\nIn the bags representation, we include words with zero\nfrequency. This along with the xed-length dictionary allows us to build xed-\nlength vector representations of the bag-of-words model.\nThis allows us to leverage technologies we werent able to before. Before\nthis step, we had many activity bouts of various lengths; each instance in our\ndata would have variably many observations and would not reside in a xed-\ndimensional vector space. As a result, we were not able to use classication\nmodels which depended on those conditions directly.\nHowever, now we have produced exactly those conditions. At the end of\nthis phase, we have converted our data to a xed-dimensional vector space with\n\n6\n\n\fexactly one bag-of-word vector associated with each bout.\n\n3.1.4 Classication on Latent Features Phase\n\nWe apply the same feed-forward neural network model used in previous works\nto the newly constructed bag-of-word features.\nThe neural network is applied to the bag-of-word vector with each of its\ncomponents being nodes in the input layer. For the hidden layer, we use an\nane layer of 25 nodes. This layer includes a bias vector (cid:126)b whose components\nbi function as the bias terms for each node in the layer. The layer also includes\na weight matrix W whose components wi,j function as the weights associated\nwith the edges in the neural network. For the hidden layers activation function,\nthe hyperbolic tangent function was used.\nFor the output layer, we use softmax to give us a categorical output value\nfrom the neural network. This output is our models class prediction.\nThe model was trained using the MATLAB Neural Network Toolbox which\nuses the Levenberg-Marquardt algorithm for training neural networks.\n\n3.2 Regression Model\n\nIn our regression stage, we learn a least-squares linear regression model. As\ninputs to the model, we use the atoms with their original time-series features,\nas well as the activity class prediction of the bout it belongs to from the Classi-\ncation Stage. This is a carry-over from previous work in which we justied that\nincluding the class prediction of an activity increases the accuracy of estimation\nof energy expenditure. We also include as input the Bag-of-Words features as-\nsociated with the bout each atom belongs to. The reasoning behind this is that\nBag-of-Words features carry higher-level knowledge of the moments surrounding\neach atom which thereby improves the energy expenditure estimation for each\natom in the bout.\nAs output from this model, we get a low-error estimation of MET. We can\nthen aggregate the MET predictions over all atoms in a bout to get the energy\nexpenditure of the bout as a whole.\n\n4 Results\n\nAs can be seen in Table 4, during the classication stage we manage to get com-\npetitive accuracies with our competing methods. However, the ma jor success\nin our model comes from our acceptable classication misses: run-class atoms\nare never classied as sedentary-class atoms. Table 3 shows that relatively few\nmisclassications happen between distant classes when they happen at all. This\nimplies that the only misclassications that occur are happening at boundary\nor outlier cases.\nAs for our regression results, we provide the standard least-squares linear\nregression model on the data by itself as a baseline. This emphasizes the eect\n\n7\n\n\fSed.\n16455\n90\n20\n25\n0\n\nLHH MtV Walk Run\n0\n0\n0\n20\n0\n20\n310\n2085\n240\n1290\n20\n0\n40\n3685\n25\n0\n0\n0\n135\n350\n\nSed.\nLHH\nMtV\nWalk\nRun\n\nTable 3: Confusion Matrix as atoms for Classication Phase\n\nSed.\nSed.\n99.88\nLHH 3.59\n1.27\nMtV\n0.66\nWalk\nRun\n0.00\n\nLHH MtV Walk Run\n0.00\n0.00\n0.00\n0.12\n83.23\n12.38\n0.80\n0.00\n0.00\n1.27\n82.17\n15.29\n1.06\n97.62\n0.66\n0.00\n0.00\n0.00\n27.84\n72.16\n\nTable 4: Confusion Matrix as percentages for Classication Phase\n\nof each aspect of our regression stage. Table 5 shows that each additional el-\nement to the model has a signicant impact on overall RMSE. In fact, simply\nby including the activity class prediction, we beat the state of the arts regres-\nsion model in RMSE by nearly 0.5 units. Including our Bag-of-Word features\nincreased accuracy by another 0.1 units.\nFrom Table 6, we can see that our model has an in-class RMSE advantage\nover all other methods. We do not simply beat the methods overall, but in re-\ngressing any type of activity, our model performs with the least error. Especially\nsignicant is the 1.1 unit decrease in RMSE from the ANN in the Run-class and\nthe 1.2 unit decrease in RMSE from the ANN in the Moderate-to-Vigorous-\nclass. These activity classes contain the highest error rates for all models as\nthey are the most dicult to estimate. Our model improves greatly on these\ndicult classes.\n\nLinear Regression on Raw Features\n\nLinear Regression w/ Class Prediction\n\nLinear Regression w/ Class Prediction and BoW Features\n\nArticial Neural Network\n\nRMSE\n2.3690\n0.9548\n0.8502\n1.4402\n\nTable 5: Root Mean Squared Error for MET estimates of each Regression Model\n\n8\n\n\fLR-RF\nLR+CP\n\nLR+CP+BoWF\n\nANN\n\nSed.\n2.0105\n0.2284\n0.1798\n0.3999\n\nLHH MtV Walk Run\n4.1593\n2.6670\n3.3990\n2.7549\n2.5591\n1.4612\n1.8695\n1.4094\n1.3477\n1.5964\n1.3494\n2.0146\n3.6695\n2.1308\n2.7789\n2.2715\n\nTable 6: RMSE for MET estimates of each Regression Model by Activity Class\n\n5 Conclusion\n\nWe presented a Classication-Regression framework for predicting activity classes\nand estimating energy expenditure from time-series data collected from hip\nmounted accelerometers. Our approach of utilizing an unsupervised intermedi-\nate feature construction layer has been shown to generate meaningful and useful\nknowledge that contributes to high classication accuracy and lower regression\nerror. Integrating the Bag-of-Words model into our representation was shown\nto have a signicant impact on our results over those of other methods. We\nfurther show that our results exceed that of the state-of-the-art method.\n\nReferences\n\n[1] Thomas Bastian, Aurelia Maire, Julien Dugas, Abbas Ataya, Clement Vil-\nlars, Florence Gris, Emilie Perrin, Yanis Caritu, Maeva Doron, Stephane\nBlanc, Pierre Jallon, and Chantal Simon. Automatic identication of phys-\nical activity types and sedentary behaviors from triaxial accelerometer:\nlaboratory-based calibrations are not enough. Journal of Applied Physi-\nology, 118(6):716722, 2015.\n\n[2] Christopher M. Bishop.\nSpringer, 2006.\n\nPattern Recognition and Machine Learning.\n\n[3] Scott E. Crouter, Kurt G. Clowers, and David R. Bassett. A novel method\nfor using accelerometer data to predict energy expenditure. Journal of Ap-\nplied Physiology, 100(4):13241331, 2006.\n\n[4] AH Montoye, Lanay M Mudd, Subir Biswas, and Karin A Pfeier. Energy\nexpenditure prediction using raw accelerometer data in simulated free-living.\nMedicine & Science in Sports & Exercise, 47(8):17351746, 2015.\n\n[5] Y. Mu, H. Z. Lo, W. Ding, K. Amaral, and S. E. Crouter. Bipart: Learning\nblock structure for activity detection. IEEE Transactions on Know ledge and\nData Engineering, 26(10):23972409, Oct 2014.\n\n[6] J. Staudenmayer, D. Pober, S. Crouter, D. Bassett, and P. Freedson. An\narticial neural network to estimate physical activity energy expenditure\n\n9\n\n\fand identify physical activity type from an accelerometer. J. Appl. Physiol.,\n107(4):13001307, Oct 2009.\n\n[7] S. G. Trost, W. K. Wong, K. A. Pfeier, and Y. Zheng. Articial neural\nnetworks to predict activity type and energy expenditure in youth. Med Sci\nSports Exerc, 44(9):18011809, Sep 2012.\n\n10\n\n\f", 
        "tag": "Learning", 
        "link": "https://arxiv.org/list/cs.LG/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nO\nL\n.\ns\nc\n[\n \n \n1\nv\n6\n3\n7\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nGeneralized Satisability Problems\nvia Operator Assignments\n\nAlbert Atserias1\n\nPhokion G. Kolaitis2\n\nSimone Severini3\n\n1 Universitat Polit`ecnica de Catalunya\n2 University of California Santa Cruz and IBM ResearchAlmaden\n3 University College London and Shanghai Jiao Tong University\n\nApril 7, 2017\n\nAbstract\n\nSchaefer introduced a framework for generalized satisability problems on the Boolean\ndomain and characterized the computational complexity of such problems. We investi-\ngate an algebraization of Schaefers framework in which the Fourier transform is used\nto represent constraints by multilinear polynomials in a unique way. The polynomial\nrepresentation of constraints gives rise to a relaxation of the notion of satisability\nin which the values to variables are linear operators on some Hilbert space. For the\ncase of constraints given by a system of linear equations over the two-element eld,\nthis relaxation has received considerable attention in the foundations of quantum me-\nchanics, where such constructions as the Mermin-Peres magic square show that there\nare systems that have no solutions in the Boolean domain, but have solutions via op-\nerator assignments on some nite-dimensional Hilbert space. We obtain a complete\ncharacterization of the classes of Boolean relations for which there is a gap between\nsatisability in the Boolean domain and the relaxation of satisability via operator\nassignments. To establish our main result, we adapt the notion of primitive-positive\ndenability (pp-denability) to our setting, a notion that has been used extensively\nin the study of constraint satisfaction problems. Here, we show that pp-denability\ngives rise to gadget reductions that preserve satisability gaps. We also present several\nadditional applications of this method. In particular and perhaps surprisingly, we show\nthat the relaxed notion of pp-denability in which the quantied variables are allowed\nto range over operator assignments gives no additional expressive power in dening\nBoolean relations.\n\n1\n\n\f1\n\nIntroduction and Summary of Results\n\nIn 1978, Schaefer [22] classied the computational complexity of generalized satisability\nproblems. Each class A of Boolean relations gives rise to the generalized satisability problem\nSAT(A). An instance of SAT(A) is a conjunction of relations from A such that each conjunct\nhas a tuple of variables as arguments; the question is whether or not there is an assignment\nof Boolean values to the variables, so that, for each conjunct, the resulting tuple of Boolean\nvalues belongs to the underlying relation. Schaefers main result is a dichotomy theorem\nfor the computational complexity of SAT(A), namely, depending on A, either SAT(A)\nis NP-complete or SAT(A) is solvable in polynomial time. Schaefers dichotomy theorem\nprovided a unifying explanation for the NP-completeness of many well-known variants of\nBoolean satisability, such as POSITIVE 1-IN-3 SAT and MONOTONE 3SAT; moreover,\nit became the catalyst for numerous subsequent investigations, including the pursuit of a\ndichotomy theorem for constraints satisfaction problems, a pursuit that became known as\nthe Feder-Vardi Conjecture [9].\nEvery Boolean relation can be identied with its characteristic function, which, via the\nFourier transform, can be represented as a multilinear polynomial (i.e., a polynomial in\nwhich each variable has degree at most one) in a unique way. Moreover, in carrying out\nthis transformation, the truth values false and true are typically represented by +1 and 1,\ninstead of 0 and 1. For example, it is easy to see that the multilinear polynomial representing\n2 (1 + x + y  xy). The multilinear polynomial\nthe conjunction x  y of two variables x and y is 1\nrepresentation of Boolean relations makes it possible to consider relaxations of satisability\nin which the variables take values in some suitable space, instead of the two-element Boolean\nalgebra. Such relaxations have been considered in the foundations of physics several decades\nago, where they have played a role in singling out the dierences between classical theory\nand quantum theory.\nIn particular, it has been shown that there is a system of linear\nequations over the two-element eld that has no solutions over {+1, 1}, but the system\nof the associated multilinear polynomials has a solution in which the variables are assigned\nlinear operators on a Hilbert space of dimension four. The Mermin-Peres magic square\n[16, 17, 20] is the most well known example of such a system. These constructions give\nsmal l proofs of the celebrated Kochen-Specker Theorem [8] on the impossibility to explain\nquantum mechanics via hidden-variables [2]. More recently, systems of linear equations with\nthis relaxed notion of solvability have been studied under the name of binary constraint\nsystems, and tight connections have been established between solvability and the existence\nof perfect strategies in non-local games that make use of entanglement [6, 7].\nA Boolean relation is ane if it is the set of solutions of a system of linear equations\nover the two-element eld. The collection LIN of all ane relations is prominent in Schae-\nfers dichotomy theorem, as it is one of the main classes A of Boolean relations for which\nSAT(A) is solvable in polynomial time. The discussion in the preceding paragraph shows\nthat SAT(LIN) has instances that are unsatisable in the Boolean domain, but are satisable\nwhen linear operators on a Hilbert space are assigned to variables (for simplicity, from now\non we will use the term operator assignments for such assignments). Which other classes\nof Boolean relations exhibit such a gap between satisability in the Boolean domain and\n\n2\n\n\fthe relaxation of satisability via operator assignments? As a matter of fact, this question\nbifurcates into two separate questions, depending on whether the relaxation allows linear op-\nerators on Hilbert spaces of arbitrary (nite or innite) dimension or only on Hilbert spaces\nof nite dimension.\nIn a recent breakthrough paper, Slofstra [24] showed that these two\nquestions are dierent for LIN by establishing the existence of systems of linear equations\nthat are satisable by operator assignments on some innite-dimensional Hilbert space, but\nare not satisable by operator assignments on any nite-dimensional Hilbert space.\nIn a\nrelated vein, Ji [15] showed that a 2CNF-formula is satisable in the Boolean domain if and\nonly if it is satisable by an operator assignment in some nite-dimensional Hilbert space.\nMoreover, Ji showed that the same holds true for Horn formulas. Note that 2SAT, HORN\nSAT, and DUAL HORN SAT also feature prominently in Schaefers dichotomy theorem as,\ntogether with SAT(LIN), which from now on we will denote by LIN SAT, they constitute the\nmain tractable cases of generalized satisability problems (the other tractable cases are the\ntrivial cases of SAT(A), where A is a class of 0-valid relations or a class of 1-valid relations,\ni.e., Boolean relations that contain the tuple consisting entirely of 0s or, respectively, the\ntuple consisting entirely of 1s).\nIn this paper, we completely characterize the classes A of Boolean relations for which\nSAT(A) exhibits a gap between satisability in the Boolean domain and satisability via\noperator assignments. Clearly, if every relation in A is 0-valid or every relation in A is\n1-valid, then there is no gap, as every constraint is satised by assigning to every variable\nthe identity operator or its negation, respectively. Beyond this, we rst generalize and ex-\ntend Jis results [15] by showing that if  is a class of Boolean relations such that every\nrelation in A is bijunctive1 , or every relation in A is Horn, or every relation in A is dual\nHorn2 , then there is no gap whatsoever; this means that an instance of SAT(A) is satisable\nin the Boolean domain if and only if it is satisable by an operator assignment on some\nnite-dimensional Hilbert space if and only if is satisable by an operator assignment on\nsome arbitrary Hilbert space. In contrast, we show that for all other classes A of Boolean\nrelations, SAT(A) exhibits a two-level gap: there are instances of SAT(A) that are not\nsatisable in the Boolean domain, but are satisable by an operator assignment on some\nnite-dimensional Hilbert space; moreover, there are instances of SAT(A) that are not satis-\nable by an operator assignment on any nite-dimensional Hilbert space, but are satisable\nby an operator assignment on some (innite-dimensional) Hilbert space.\nThe proof of this result uses several dierent ingredients. First, we use the substitution\nmethod [7] to show that there is no satisability gap for classes of relations that are bijunc-\ntive, Horn, and dual Horn. This gives a dierent proof of Jis results [15], which were for\nnite-dimensional Hilbert spaces, but also shows that, for such classes of relations, there is\nno dierence between satisability by linear operators on nite-dimensional Hilbert spaces\nand satisability by linear operators on arbitrary Hilbert spaces. The main tool for proving\nthe existence of a two-level gap for the remaining classes of Boolean relations is the notion\n\n1A Boolean relation is bijunctive if it is the set of satisfying assignments of a 2CNF-formula.\n2A Boolean relation is Horn (dual Horn ) if it is the set of satisfying assignments of a Horn (dual Horn)\nformula.\n\n3\n\n\fof pp-denability, that is, denability via primitive-positive formulas, which are existential\nrst-order formulas having a conjunction of (positive) atoms as their quantier-free part.\nIn the past, primitive-positive formulas have been used to design polynomial-time reduc-\ntions between decision problems; in fact, this is one of the main techniques in the proof of\nSchaefers dichotomy theorem. Here, we show that primitive-positive formulas can also be\nused to design gap-preserving reductions, that is, reductions that preserve the gap between\nsatisability on the Boolean domain and satisability by operator assignments. To prove the\nexistence of a two-level gap for classes of Boolean relations we combine gap-preserving re-\nductions with the two-level gap for LIN discussed earlier (i.e., the results of Mermin [16, 17],\nPeres [16], and Slofstra [24]) and with results about Posts lattice of clones on the Boolean\ndomain [21].\nWe also give two additional applications of pp-denability. First, we consider an extension\nof pp-denability in which the existential quantiers may range over linear operators on some\nnite-dimensional Hilbert space. At rst sight, it appears that new Boolean relations may\nbe pp-denable in the extended sense from a given set of Boolean relations. We show,\nhowever, that this is not the case. Specically, by analyzing closure operations on sets of\nlinear operators, we show that if a Boolean relation is pp-denable in the extended sense from\nother Boolean relations, then it is also pp-denable from the same relations. In other words,\nfor Boolean relations, this extension of pp-denability is not more powerful than standard\npp-denability. Second, we apply pp-denability to the problem of quantum realizability\nof contextuality scenarios. Recently, Fritz [12] used Slofstras results [24] to resolve two\nproblems raised by Acin et al. in [1]. Using pp-denability and Slofstras results, we obtain\nnew proofs of Fritzs results that have the additional feature that the parameters involved\nare optimal.\n\n2 Denitions and Technical Background\n\n2.1 Notation\nFor an integer n, we write [n] for the set {1, . . . , n}. We use mainly the +1, 1 representation\nof the Boolean domain (+1 for false and 1 for true). We write {1} for the set\n{+1, 1}. If a denotes a tuple of length r we write a1 , . . . , ar to denote its r components. If\na is such a tuple and f is a function that has a1 , . . . , ar in its domain, we write f (a) to denote\nthe tuple (f (a1 ), . . . , f (ar )). We write T and F for the full and empty Boolean relations,\nrespectively. The letters stand for true and false. Their arity is unspecied by the notation\nand will be made clear by the context.\n\n2.2 Linear Operators and Polynomials Thereof\n\nLet V be a complex vector space. A linear operator on V is a linear map from V to V .\nThe linear operator that is the identity on V is denoted by I , and the linear operator\nthat is identically 0 is denoted by 0. The pointwise addition of two linear operators A\n\n4\n\n\fand B is denoted by A + B , the composition of two linear operators A and B is denoted\nby AB , and the pointwise scaling of a linear operator A by a scalar c  C is denoted\nby cA. All these are linear operators. As a result, if C(cid:104)X1 , . . . , Xn (cid:105) denotes the ring of\npolynomials with complex coecients and non-commuting variables in X1 , . . . , Xn , then for\na polynomial P (X1 , . . . , Xn ) in C(cid:104)X1 , . . . , Xn (cid:105) and linear operators A1 , . . . , An on V , the\nnotation P (A1 , . . . , An ) is explained. If A1 , . . . , An pairwise commute, i.e., AiAj = Aj Ai for\nall i, j  {1, . . . , n}, then the notation is explained even for a polynomial in C[X1 , . . . , Xn ],\nthe ring of polynomials with commuting variables in X1 , . . . , Xn .\nLet V and W be complex vector spaces. Let A be a linear operator on V and let B be\na linear operator on W . We say that A and B are similar if there exists an invertible linear\nmap C : V  W such that A = CBC 1 . Let A1 , . . . , An and B1 , . . . , Bn be linear operators\non V and W , respectively. We say that A1 , . . . , An and B1 , . . . , Bn are simultaneously similar\nif there exists an invertible linear map C : V  W such that Ai = CBiC 1 holds for all\ni  [n]. The following simple fact with an equally simple proof will be used multiple times.\n\nLemma 1. Let V and W be complex vector spaces, and let P (X1 , . . . , Xn ) be a polynomial\nin C(cid:104)X1 , . . . , Xn (cid:105). If A1 , . . . , An and B1 , . . . , Bn are simultaneously similar linear operators\non V and W , respectively, then so are P (A1 , . . . , An ) and P (B1 , . . . , Bn ).\nlength of the sequence . Let P (X1 , . . . , Xn ) = (cid:80)\n(cid:81)||\nProof. We write [n] for the set of nite sequences with components in [n], and || for the\ni=1 Xi , where only nitely\n[n] c\nmany of the coecients c are non-zero. Let C : V  W be an invertible linear map\nholds for every j  [n]. Note that for every   [n] of length (cid:96) we have (cid:81)(cid:96)\nwitnessing that A1 , . . . , An and B1 , . . . , Bn are simultaneously similar; thus Aj = CBj C 1\n(cid:1)C 1 , and linearity\n(cid:1)C 1 . It follows that P (A1 , . . . , An ) = (cid:80)\n[n] cC (cid:0)(cid:81)||\nC (cid:0)(cid:81)(cid:96)\ni=1 (CBi C 1 ) =\ni=1 Bi\ni=1 Bi\ngives P (A1 , . . . , An ) = C P (B1 , . . . , Bn )C 1 .\n\n2.3 Unique Multilinear Polynomial Representations\n\nA polynomial P (X1 , . . . , Xn ) is called multilinear if it has individual degree at most one on\neach variable. Each function f : {1}n  C has a unique representation as a multilin-\n(cid:89)\n(cid:88)\near polynomial in C[X1 , . . . , Xn ] given by the Fourier or Walsh-Hadamard transform [18].\nExplicitly:\nf (S )\nPf (X1 , . . . , Xn ) =\n(cid:89)\n(cid:88)\niS\nS[n]\n1\n2n\niS\na{1}n\nThe polynomial represents f in the sense that Pf (a) = f (a) holds for every a  {1}n . If the\nrange of f is a subset of R, then each f (S ) is indeed a real number. The Convolution Formula\ndescribes the Fourier coecients of pointwise products f g of functions f , g : {1}n  C. It\n\nf (S ) =\n\nwhere\n\nf (a)\n\nai .\n\nXi ,\n\n(1)\n\n(2)\n\n5\n\n\fstates that\n\n(3)\n\nf (S )g(ST )\n\n(cid:88)\n(cid:99)f g(S ) =\nT [n]\nfor every S  [n], where ST denotes symmetric dierence; i.e. ST = (S \\ T )  (T \\ S ).\nWe give an example of use of the uniqueness of the Fourier transform that will be useful\nlater on. We begin by recalling some notation and terminology. A literal\nis a Boolean\nvariable x or its negation x. The literals x and x are said to be complementary of each\nother, and x is their underlying variable. If (cid:96) is a literal, then (cid:96) denotes its complementary\nliteral. The sign sg((cid:96)) of (cid:96) is dened as follows: sg((cid:96)) = 1 if (cid:96) = x, and sg((cid:96)) = 1 if (cid:96) = x,\nwhere x is its underlying variable. Clearly, sg((cid:96)) = sg((cid:96)).\nA clause is a disjunction of literals. Let C = ((cid:96)1      (cid:96)r ) be a clause. In the 1 represen-\ntation of Boolean values, the clause C represents the relation {1}r \\ {(sg((cid:96)1 ), . . . , sg((cid:96)r ))},\nwhich will be denoted by RC . The indicator function of the clause C = ((cid:96)1      (cid:96)r ) is\nthe Boolean function from {1}r  {1} that maps the tuple (sg((cid:96)1 ), . . . , sg((cid:96)r )) to +1\nand every other tuple to 1. We write PC (X1 , . . . , Xr ) to denote the unique multilinear\npolynomial representation of the indicator function of the clause C .\nLemma 2. Let C = ((cid:96)1      (cid:96)r ) be a clause on r dierent variables. Then, over the ring\nof polynomials C[X1 , . . . , Xr ], the fol lowing identity holds.\n(cid:16)\n(cid:17)  1.\nr(cid:89)\ni=1\nProof. Let RC = {1}r \\ {(sg((cid:96)1 ), . . . , sg((cid:96)r ))} be the Boolean relation represented by C .\nSince the right-hand side of equation (4) is a multilinear polynomial and its left-hand side\nis the unique multilinear polynomial that agrees with the indicator function of RC on {1},\nit suces to check that the right-hand side also agrees with the indicator function of RC on\n{1}r . In other words, we claim that for every (a1 , . . . , ar )  {1}r , the right-hand side\nevaluates to 1 if the truth-assignment (a1 , . . . , ar ) satises the clause C , and it evaluates\nthat aj = sg((cid:96)j ). It follows that 1 + sg((cid:96)j )aj = 0 and so (cid:81)r\nto 1, otherwise.\nAssume that (a1 , . . . , ar ) satises the clause C . Then there is some j  {1, . . . , r} such\ni=1 (1 + sg((cid:96)i )ai ) = 0, which,\nin turn, implies that PC (a1 , . . . , ar ) = 1. Assume that (a1 , . . . , ar ) does not satisfy the\ni  {1, . . . , r}, we have that 1 + sg((cid:96)i )ai = 2 and so (cid:81)r\nclause C . Then, for every i  {1, . . . , r}, we have that ai = sg((cid:96)i ). Consequently, for every\ni=1 (1 + sg((cid:96)i )ai ) = 2r , which, in turn,\nimplies that PC (a1 , . . . , ar ) = 1. This completes the proof of the claim.\n\nPC (X1 , . . . , Xr ) = 21r\n\n1 + sg((cid:96)i )Xi\n\n(4)\n\n2.4 Hilbert Space\n\nA Hilbert space is a complex vector space with an inner product whose norm induces a\ncomplete metric. All Hilbert spaces of nite dimension d are isomorphic to Cd with the\nstandard complex inner product. In particular, this means that after the choice of a basis,\nwe can identify the linear operators on a d-dimensional Hilbert space with the matrices\n\n6\n\n\fin Cdd . Composition of operators becomes matrix multiplication. A matrix A is Hermitian\nif it is equal to its conjugate transpose A . A diagonal matrix is one all whose o-diagonal\nentries are 0. A matrix A in unitary if AA = AA = I , where I is the identity matrix. Two\nmatrices A and B commute if AB = BA, and a collection of matrices A1 , . . . , Ar pairwise\ncommute if AiAj = Aj Ai for all i, j  [r].\nFor the basics of general Hilbert spaces and their linear operators we refer the reader\nto Halmos monograph [14]. We need from it the concepts of bounded linear operator and\nof adjoint A of a densely dened linear operator A. Two operators A and B commute\nif AB = BA. A sequence of operators A1 , . . . , Ar pairwise commute if AiAj = Aj Ai for\nall i, j  [r]. A linear operator A is called normal if it commutes with its adjoint A ; i.e.,\nAA = AA. A linear operator is called self-adjoint if A = A. A linear map from a Hilbert\nspace H1 to another Hilbert space H2 is called unitary if it preserves norms.\nWe also make elementary use of general L2 - and L -spaces. Let (, M, ) be a measure\nspace. Then L2 (, ) denotes the collection of square integrable measurable functions, up\nto almost everywhere equality. Also L (, ) denotes the collection of essentially bounded\nmeasurable functions, up to almost everywhere equality. All measure-theoretic terms in\nthese denitions refer to . See [11] for denitions.\n\n2.5 Constraint Languages, Instances, Value and Satisability\nA Boolean constraint language A is a collection of relations over the Boolean domain {1}.\nLet V = {X1 , . . . , Xn} be a set of variables. An instance I on the variables V over the\nconstraint language A is a nite collection of pairs\nI = ((Z1 , R1 ), . . . , (Zm , Rm ))\nwhere each Ri is a relation from A and Zi = (Zi,1 , . . . , Zi,ri ) is a tuple of variables from V\nor constants from {1}, where ri is the arity of Ri . Each pair (Zi , Ri ) is called a constraint,\nand each Zi is called its constraint-scope. A Boolean assignment is a mapping f assigning\na Boolean value ai  {1} to each variable Xi , and assigning 1 and +1 to the constants\n1 and +1, respectively. We say that the assignment satises the i-th constraint if the\ntuple f (Zi ) = (f (Zi,1 ), . . . , f (Zi,ri )) belongs to Ri . The value of f on I is the fraction of\nconstraints that are satised by f . The value of I , denoted by  (I ), is the maximum value\nover all Boolean assignments. We say that I is satisable in the Boolean domain if there is\na Boolean assignment that satises all constraints; equivalently, if  (I ) = 1.\n\n(5)\n\n2.6 Operator Assignments and Satisability via Operators\nLet X1 , . . . , Xn be n variables, and let H be a Hilbert space. An operator assignment for\nX1 , . . . , Xn over H is an assignment of a bounded linear operator on H to each variable,\nf : X1 , . . . , Xn (cid:55) A1 , . . . , An , such that the following conditions hold:\n1. Aj is self-adjoint for every j  [n],\nj = I for every j  [n].\n2. A2\n\n7\n\n\fIf S is a subset of {X1 , . . . , Xn}, we say that the operator assignment A1 , . . . , An pairwise\ncommutes on S if in addition it satises Aj Ak = AkAj for every Xj and Xk in the set S .\nIf it pairwise commutes on the whole set {X1 , . . . , Xn}, we say that the assignment ful ly\ncommutes.\nLet A be a Boolean constraint language, let I be an instance over A, with n variables\nX1 , . . . , Xn as in (5), and let H be a Hilbert space. An operator assignment for I over H\nis an operator assignment f : X1 , . . . , Xn (cid:55) A1 , . . . , An for the variables X1 , . . . , Xn that\npairwise commutes on the set of variables of each constraint scope Zi in I ; explicitly\nfor every Xj and Xk in Zi , for every i  {1, . . . , m}.\n(6)\nAj Ak = AkAj\nWe also require that f maps the constant 1 and +1 to I and I , respectively, where I is\nthe identity operator on H. We say that the assignment f satises the i-th constraint if\nPRi (f (Zi )) = PRi (f (Zi,1 ), . . . , f (Zi,ri )) = I ,\nwhere PRi denotes the unique multilinear polynomial representation of indicator function\nof the relation Ri , i.e., the function that maps each tuple in Ri to 1, and each tuple\nin its complement {1}ri \\ Ri to +1. Note that since f (Zi,1 ), . . . , f (Zi,ri ) are required to\ncommute by denition, this notation is unambiguous despite the fact that PRi is dened as\na polynomial in commuting variables. The value of f on I is the fraction of constraints\nthat are satised by f ; note that this quantity takes one of a nite set of values in the set\n{0, 1/m, 2/m, . . . , (m  1)/m, 1}. The value of I over H is the maximum value over all\noperator assignments for I over H. We say that f satises I if it satises all constraints.\nIn that case we also say that f is a satisfying operator assignment for I over H.\nThe nite-dimensional value of I , denoted by   (I ), is the maximum of its value over all\nnite-dimensional Hilbert spaces. The value of I , denoted by   (I ), is the maximum of its\nvalue over all Hilbert spaces. We say that an instance I is satisable via nite-dimensional\noperator assignments, or satisable via fd-operators for short, if   (I ) = 1. We say that I\nis satisable via operator assignments, or satisable via operators for short, if   (I ) = 1.\n\n(7)\n\n3 The Strong Spectral Theorem\n\nThe Spectral Theorem plays an important role in linear algebra and functional analysis.\nIt has also been used in the foundations of quantum mechanics (for some recent uses see\n[7, 15]). We will make a similar use of it, but we will also need the version of this theorem\nfor innite-dimensional Hilbert spaces. In this section we discuss the statement, both for\nnite- and innite-dimensional Hilbert spaces, as well as one of its important applications\nthat we encapsulate in a lemma for later reuse.\n\n3.1 Statement\n\nIn its most basic form, the Spectral Theorem for complex matrices states that every Hermi-\ntian matrix is unitarily equivalent to a diagonal matrix. Explicitly: if A is a d  d Hermitian\n\n8\n\n\fmatrix, then there exist a unitary matrix U and a diagonal matrix E such that A = U 1EU .\nIn its strong form, the Strong Spectral Theorem (SST) applies to sets of pairwise commuting\nHermitian matrices and is stated as follows.\nTheorem 1 (Strong Spectral Theorem; nite-dimensional case). Let A1 , . . . , Ar be d  d\nHermitian matrices, for some positive integer d. If A1 , . . . , Ar pairwise commute, then there\nexists a unitary matrix U and diagonal matrices E1 , . . . , Er such that Ai = U 1EiU for every\ni  [r].\n\nThis form of the SST will be enough to discuss satisability via fd-operators. For operator\nassignments over arbitrary Hilbert spaces, we need to appeal to the most general form of\nthe SST in which the role of diagonal matrices is played by multiplication operators on an\nL2 (, )-space. These are dened as follows.\nLet V be a complex function space; a complex vector space of functions mapping indices\nfrom an index set X to C. A multiplication operator of V is a linear operator whose value\nat a function f : X  C in V is given by pointwise multiplication by a xed function\na : X  C. In symbols, the multiplication operator given by a is\nfor each x  X.\n\n(Ta (f ))(x) = a(x)f (x)\n\n(8)\n\nIn its weak form, the general Spectral Theorem states that any normal bounded linear\noperator on a Hilbert space is unitarily equivalent to a multiplication operator on an L2 -\nspace. We need the following strong version of the Spectral Theorem that states that the\nsame is true for a collection of such operators, simultaneously through the same unitary\ntransformation, provided they commute. The statement we use is a direct consequence of\nTheorem 1.47 in Follands monograph [10].\n\nTheorem 2 (Strong Spectral Theorem; general case). Let A1 , . . . , An be normal bounded\nlinear operators on a Hilbert space H. If A1 , . . . , Ar pairwise commute, then there exist a\nmeasure space (, M, ), a unitary map U : H  L2 (, ), and functions a1 , . . . , ar \nL (, ) such that Ai = U 1Tai U for every i  [r].\nThe special case in which H has nite dimension d, the measure space is actually a nite\nset of cardinality d with the counting measure, and thus L2 (, ) is isomorphic to Cd with\nthe usual complex inner product.\n\n3.2 An Oft-Used Application\n\nThe following lemma encapsulates a frequently used application of the Strong Spectral The-\norem.\nIt states that whenever a set of polynomial equations entail another polynomial\nequation over the Boolean domain, then the entailment holds as well for fully commuting\noperator assignments.\nLemma 3. Let X1 , . . . , Xr be variables, let Q1 , . . . , Qm , Q be polynomials in C[X1 , . . . , Xr ],\nand let H be a Hilbert space.\nIf every Boolean assignment that satises the equations\n\n9\n\n\fQ1 =    = Qm = 0 also satises the equation Q = 0, then every ful ly commuting oper-\nator assignment over H that satises the equations Q1 =    = Qm = 0 also satises the\nequation Q = 0.\n\nAlthough the same proof applies to all Hilbert spaces, the proof of the nite-dimensional\ncase can be made more elementary. Since for certain applications only the nite-dimensional\ncase of the lemma is relevant, we split the proof accordingly into cases.\nProof of Lemma 3; nite-dimensional case. Assume H has nite dimension d. Since all\nHilbert spaces of dimension d are isometrically isomorphic to Cd , let us assume without\nloss of generality that H = Cd . In such a case, a self-adjoint bounded linear operator is just\na Hermitian d  d matrix, and the composition of linear operators is matrix multiplication.\nAssume the hypotheses of the lemma and let A1 , . . . , Ar be Hermitian d  d matrices.\nAssume that A1 , . . . , Ar make a fully commuting operator assignment for X1 , . . . , Xr such\nthat the equations Q1 =    = Qm = 0 are satised. The matrices A1 , . . . , Ar pairwise\ncommute, so the Strong Spectral Theorem (i.e. Theorem 1) applies to them. Thus, there\nexist a unitary matrix U and diagonal d  d matrices E1 , . . . , Em such that Ai = U 1EiU for\nevery i  [r]. Equivalently, U AiU 1 = Ei . From A2\ni = I we conclude E 2\ni = I . Hence, if ai (j )\ndenotes the j -th diagonal entry of Ei , then ai (j )2 = 1 for all j  [d]. Thus ai (j )  {1}\nfor all j  [d]. The conditions of Lemma 1 apply, so Qk (A1 , . . . , Ar ) and Qk (E1 , . . . , Er ) are\nsimilar matrices for each k  [m]. Since Qk (A1 , . . . , Ar ) = 0 and the unique matrix that\nis similar to the null matrix is the null matrix itself, we conclude that Qk (E1 , . . . , Er ) = 0.\nNow, Ei is the diagonal matrix that has the vector (ai (1), . . . , ai (d)) in the diagonal, so\nQk (a1 (j ), . . . , ar (j )) = 0 for all j  [d]. Since ai (j ) is in {1} for each i  [r] and j  [d],\nthe hypothesis of the lemma says that also Q(a1 (j ), . . . , ar (j )) = 0 for all j  [d]. Thus\nQ(E1 , . . . , Er ) = 0, and another application of Lemma 1 shows that Q(A1 , . . . , Ar ) = 0, as\nwas to be proved.\n\nThe proof for the general case follows the same structure as the proof of the nite-\ndimensional case, using Theorem 2 in place of Theorem 1. Other than taking care of nul l\nsets of exceptions, there are no further dierences in the two proofs. At a later stage we\nwill nd an application of the SST whose proof for the innite-dimensional case does require\nsome new ingredients. For now, let us ll in the details of the null-set-of-exceptions argument\nas a warm-up.\n\nProof of Lemma 3; general case. Assume the hypotheses of the lemma and let A1 , . . . , Ar be\nbounded self-adjoint linear operators on H. Suppose that A1 , . . . , Ar make a fully commuting\noperator assignment for X1 , . . . , Xr such that the equations Q1 =    = Qm = 0 are satised.\nThe operators A1 , . . . , Ar pairwise commute, and since they are self-adjoint they are also\nnormal, so the Strong Spectral Theorem (i.e. Theorem 2) applies to them. Thus, there exist\na measure space (, M, ), a unitary map U : H  L2 (, ) and functions a1 , . . . , ar \nL (, ) such that, for the multiplication operators Ei = Tai of L2 (, ), the relations\nAi = U 1EiU hold for every i  [r]. Equivalently, U AiU 1 = Ei . From A2\ni = I we conclude\ni = I . Hence, ai ()2 = 1 for almost all   ; i.e. formally, ({   : ai ()2 (cid:54)=\nE 2\n\n10\n\n\f1}) = 0. Thus ai ()  {1} for almost all   . The conditions of Lemma 1 apply,\nthus Qk (A1 , . . . , Ar ) and Qk (E1 , . . . , Er ) are similar linear operators for each k  [m]. Since\nQk (A1 , . . . , Ar ) = 0 and the unique linear operator that is similar to the null operator is\nthe null operator itself, we conclude that Qk (E1 , . . . , Er ) = 0. Now, Ei is the multiplication\noperator given by the function ai , so Qk (a1 (), . . . , ar ()) = 0 for almost all   . Since\nfor almost all    the component ai () is in {1} for each i  [r], the hypothesis of the\nlemma says that also Q(a1 (), . . . , ar ()) = 0 for almost all   . Thus Q(E1 , . . . , Er ) = 0,\nand another application of Lemma 1 shows that Q(A1 , . . . , Ar ) = 0, as was to be proved.\n\n4 Reductions through Primitive Positive Formulas\n\nLet A be a Boolean constraint language, let r be a positive integer, and let x1 , . . . , xr be\nvariables ranging over the Boolean domain {1}. A primitive positive formula, or pp-formula\nfor short, is a formula of the form\n(x1 , . . . , xr ) = y1    ys (R1 (z1 )      Rm (zm ))\n\n(9)\n\nwhere each Ri is a relation in A and each zi is an ri -tuple of variables or constants from\n{x1 , . . . , xr }  {y1 , . . . , ys}  {1}, where ri is the arity of Ri . A relation R  {1}r is\npp-denable from A if there exists a pp-formula (x1 , . . . , xr ) such that\nR = {(a1 , . . . , ar )  {1}r : (x1/a1 , . . . , xr /ar ) is true in A}.\n\n(10)\n\nA Boolean constraint language A is pp-denable from another Boolean constraint language\nB if every relation in A is pp-denable from B . Whenever the constants +1 and 1 do not\nappear in the pp-formulas, we speak of pp-formulas and pp-denability without constants\nor, also, without parameters.\nIn the following we show that if A is pp-denable from B , then every instance I over\nA can be translated into an instance J over B in such a way that the satisfying operator\nassignments for I lift to satisfying operator assignments for J . We make this precise.\n\n4.1 The Basic Construction\n\nLet A and B be two Boolean constraint languages and assume that every relation in A is\npp-denable from B . For R in A, let\nR (x1 , . . . , xr ) = y1    yt (S1 (w1 )      Sm (wm ))\n\n(11)\n\nbe the pp-formula that denes R from B , where S1 , . . . , Sm are relations from B , and\nw1 , . . . , wm are tuples of variables or constants in {x1 , . . . , xr }  {y1 , . . . , yt}  {1} of ap-\npropriate lengths. For every instance I of A we construct an instance J of B as follows.\nConsider a constraint (Z, R) in I , where Z = (Z1 , . . . , Zr ) is a tuple of variables of I\nor constants in {1}. In addition to the variables in Z , in J we add new fresh variables\nY1 , . . . , Yt for the quantied variables y1 , . . . , yt in R . We also add one constraint (Wj , Sj ) for\n\n11\n\n\feach j  [m], where Wj is the tuple of variables and constants obtained from wj by replacing\nthe variables in x1 , . . . , xr by the corresponding components Z1 , . . . , Zr of Z , replacing any\nyi -variable by the corresponding Yi , and leaving all constants untouched. We do this for\neach constraint in I one by one. The collection of variables Z1 , . . . , Zr , Y1 , . . . , Yt that are\nintroduced by the constraint (Z, R) of I is referred to as the block of (Z, R) in J . Note that\ntwo blocks of dierent constraints may intersect, but only on the variables of I .\nThis construction is referred to as a gadget reduction in the literature. Its main property\nfor satisability in the Boolean domain is the following straightforward fact:\nLemma 4. I is satisable in the Boolean domain if and only if J is.\n\nWe ommit its very easy proof. Our goal in the rest of this section is to show that one direction\nof this basic property of gadget reductions is also true for satisability via operators, for both\nnite- and innite-dimensional Hilbert spaces, and that the other direction is almost true in\na sense we will make precise in due time.\n\n4.2 Correctness: Operator Solutions Lift\n\nThe following lemma shows that the left-to-right direction in Lemma 4 also holds for satisa-\nbility via operators: satisfying operator assignments for I can be lifted to satisfying operator\nassignments for J , over the same Hilbert space.\nLemma 5. Let I and J be as above and let H be a Hilbert space. For every f that is a\nsatisfying operator assignment for I over H, there exists g that extends f and is a satisfying\noperator assignment for J over H. Moreover, g is pairwise commuting on each block of J .\n\nAs in the proof of Lemma 3 we split into cases.\n\nProof of Lemma 5, nite-dimensional case. As in the proof of the nite-dimensional case of\nLemma 3, we may assume that H = Cd for some positive integer d, and that A1 , . . . , An are\nHermitian d  d matrices that make a satisfying operator assignment f for I . We need to\ndene Hermitian matrices for the new variables of J that were introduced by its construction.\nWe dene these matrices simultaneously for all variables Y1 , . . . , Yt that come from the same\nconstraint (Z, R) of I .\nBy renaming the entries in Z if necessary, let us assume without loss of generality that\nthe variables in Z are X1 , . . . , Xr , where r is the arity of R. By the commutativity condition\nof satisfying operator assignments, the matrices A1 , . . . , Ar pairwise commute. As each Ai is\nHermitian, the Strong Spectral Theorem applies to them. Thus, there exist a unitary matrix\nU and diagonal d  d matrices E1 , . . . , Er such that the relations Ai = U 1EiU hold for each\ni  [r]. Equivalently, U AiU 1 = Ei . From A2\ni = I we conclude E 2\ni = I . Hence, if ai (j )\ndenotes the j -th diagonal entry of Ei , then ai (j )2 = 1 for all j  [d]. Thus ai (j )  {1} for\nall j  [d]. The conditions of Lemma 1 apply, thus PR (A1 , . . . , Ar ) and PR (E1 , . . . , Er ) are\nsimilar matrices. Since PR (A1 , . . . , Ar ) = I and the unique matrix that is similar to I\nis I itself, we conclude that PR (E1 , . . . , Er ) = I . Now, Ei is the diagonal matrix that\nhas the vector (ai (1), . . . , ai (d)) in the diagonal, so PR (a1 (j ), . . . , ar (j )) = 1 for all j  [d].\n\n12\n\n\fThus the tuple a(j ) = (a1 (j ), . . . , ar (j )) belongs to the relation R for all j  [d]. Now we are\nready to dene the matrices for the variables Y1 , . . . , Yt .\nFor each j  [d], let b(j ) = (b1 (j ), . . . , bt (j ))  {1}t be a tuple of witnesses to the\nexistentially quantied variables in R (x1/a1 (j ), . . . , xr /ar (j )); such a vector of witnesses\nmust exist since the tuple a(j ) belongs to R and R denes R. Let Fk be the diagonal\nmatrix that has the vector (bk (1), . . . , bk (d)) in the diagonal, and let Yk be assigned the\nmatrix Bk = U 1FkU . Since U is unitary, each such matrix is Hermitian and squares\nto the identity since bk (j )  {1} for all j  [d]. Moreover, E1 , . . . , Er , F1 , . . . , Ft pairwise\ncommute since they are diagonal matrices; thus A1 , . . . , Ar , B1 , . . . , Bt also pairwise commute\nsince they are simultaneously similar via U . Moreover, as each atomic formula in the matrix\nof R is satised by the mapping sending xi (cid:55) ai (j ) and yi (cid:55) bi (j ) for all j  [d], another\napplication of Lemma 1 shows that the matrices that are assigned to the variables of this\natomic formula make the corresponding indicator polynomial evaluate to I . This means\nthat the assignment to the X and Y -variables makes a satisfying operator assignment for the\nconstraints of J that come from the constraint (Z, R) of I . As dierent constraints from I\nproduce their own sets of Y -variables, these denitions of assignments do not conict with\none another, and the proof of the lemma is complete.\n\nThe proof for the general case requires some new ingredients. Besides the need to take\ncare of null sets of exceptions as in the proof of Lemma 3, a new complication arises from\nthe need to build the operators for the new variables that are introduced by the reduction.\nConcretely we need to make sure that the functions of witnesses, in contraposition to the\nnite tuples of witnesses in the nite-dimensional case, are bounded and measurable. We go\ncarefully through the argument.\n\nProof of Lemma 5, general case. Assume that A1 , . . . , An are bounded self-adjoint linear op-\nerators on H for the variables of I . Suppose that the operators A1 , . . . , Ar make a valid satis-\nfying operator assignment for I . We need to dene bounded self-adjoint linear operators for\nthe new variables of J that were introduced by the construction. We dene these operators\nsimultaneously for all variables Y1 , . . . , Yt that come from the same constraint (Z, R) of I .\nBy renaming the components of Z if necessary, assume without loss of generality that\nthe variables in Z are X1 , . . . , Xr , where r is the arity of R. By the commutativity condition\nof satisfying operator assignments, the operators A1 , . . . , Ar pairwise commute. As each\nAi is self-adjoint, it is also normal, and the Strong Spectral Theorem (c.f. Theorem 2)\napplies. Thus, there exist a measure space (, M, ), a unitary map U : H  L2 (, )\nand functions a1 , . . . , ar  L (, ) such that, for the multiplication operators Ei = Tai of\nL2 (, ), the relations Ai = U 1EiU hold for each i  [r]. Equivalently, U AiU 1 = Ei .\ni = I . Hence, ai ()2 = 1 for almost all   ; i.e., formally\nFrom A2\ni = I we conclude E 2\n({   : ai ()2 (cid:54)= 1}) = 0. Thus, ai ()  {1} for almost all   . The conditions of\nLemma 1 apply, thus PR (A1 , . . . , Ar ) and PR (E1 , . . . , Er ) are similar linear operators. Since\nPR (A1 , . . . , Ar ) = I and the unique linear operator that is similar to I is I itself, we\nconclude that PR (E1 , . . . , Er ) = I . Now, Ei is the multiplication operator given by ai , and\nai ()  {1} for almost all   , so PR (a1 (), . . . , ar ()) = 1 for almost all   . Thus\n\n13\n\n\fQ =\n\n,\n\nQi,ai\n\nthe tuple a() = (a1 (), . . . , ar ()) belongs to the relation R for almost all   . Now we\nare ready to dene the operators for the variables Y1 , . . . , Yt .\nFor each    for which the tuple a() belongs to R, let b() = (b1 (), . . . , bt ()) \n{1}t be the lexicographical ly smal lest tuple of witnesses to the existentially quantied vari-\nables in R (x1/a1 (), . . . , xr /ar ()); such a vector of witnesses must exist since R denes R,\nand the lexicographically smallest exists because R is nite. For every other   , dene\nb() = (b1 (), . . . , bt ()) = (0, . . . , 0).\nNote that each function bk :   C is bounded since its range is in {1, 0, 1}. We claim\nthat such functions of witnesses bk are also measurable functions of (, M, ). This will follow\nfrom the fact that a1 , . . . , ar are measurable functions themselves, the fact that R is a nite\nrelation, and the choice of a denite tuple of witnesses of each   ; the lexicographically\nsmallest if a() is in R, or the all-zero tuple otherwise. We discuss the details.\nSince R is nite, the event Q = {   : bk () = }, for xed   {+1, 0, 1}, can be\nexpressed as a nite Boolean combination of events of the form Qi, = {   : ai () =  },\n(cid:17)\n(cid:16) (cid:92)\n(cid:91)\nwhere i  [r] and   {1}. Here is how: If  (cid:54)= 0, then\naR:\ni[r ]\nb(a)k=\nwhere b(a) denotes the lexicographically smallest tuple of witnesses in {1}t for the quanti-\ned variables in R (x1/a1 , . . . , xr /ar ). If  = 0, then Q is the complement of this set. Each\nQi, is a measurable set in the measure space (, M, ) since ai is a measurable function and\nQi, = a1\ni (B1/4 ( )), where B1/4 ( ) denotes the complex open ball of radius 1/4 centered at\n , which is a Borel set in the standard topology of C. Since the range of bk is in the nite set\n{1, 0, 1}, the preimage b1\nk (S ) of each Borel subset S of C is expressed as a nite Boolean\ncombination of measurable sets, and is thus measurable in (, M, ).\nWe just proved that each bk is bounded and measurable, so its equivalence class under\nalmost everywhere equality is represented in L (, ). We may assume without loss of\ngenerality that bk is its own representative; else modify it on a set of measure zero in order\nto achieve so. Let Fk = Tbk be the multiplication operator given by bk and let Yk be assigned\nthe linear operator Bk = U 1FkU , which is bounded because bk is bounded and U is unitary.\nAlso because U is unitary, each such operator is self-adjoint and squares to the identity since\nbk ()  {1} for almost all   . Moreover, E1 , . . . , Er , F1 , . . . , Ft pairwise commute since\nthey are multiplication operators; thus A1 , . . . , Ar , B1 , . . . , Bt pairwise commute since they\nare simultaneously similar via U . Moreover, as each atomic formula in the matrix of R is\nsatised by the mapping sending xi (cid:55) ai () and yi (cid:55) bi () for almost all   , another\napplication of Lemma 1 shows that the operators that are assigned to the variables of this\natomic formula make the corresponding indicator polynomial evaluate to I . This means\nthat the assignment to the X and Y -variables makes a satisfying operator assigment for the\nconstraints of J that come from the constraint (Z, R) in I . As dierent constraints from\nI produce their own sets of Y -variables, these denitions of assignments are not in conict\nwith each other, and the proof of the lemma is complete.\n\n(12)\n\n14\n\n\f4.3 The Extended Construction\nWe proved so far that satisfying operator assignments for I lift to satisfying operator as-\nsignments for J . We do not know if the converse is true. One could try to just take the\nrestriction of the satisfying assignment for J to the variables of I , but there is little chance\nthat this will work because there is no guarantee that the operators that are assigned to any\ntwo variables that appear together in a constraint of I will commute. Instead of trying to\nmodify the assignment, we modify the instance J . Let us discuss a slightly modied version\nof J , over a very minor extension of the constraint language B , that still allows lifting of\nsolutions, and for which the naif pro jection works for the backward direction. Let us stress\nnow that we plan to use this modied construction over a minor extension of the constraint\nlanguage merely as a technical device to get other results.\nIn the following, let T denote the full binary Boolean relation; i.e., T = {1}2 . Observe\nthat the indicator polynomial PT (X1 , X2 ) of the relation T is just the constant 1; the\nletter T stands for true.\nLet A and B be the constraint languages such that A is pp-denable from B . Let I and\nJ be the instances over A and B as dened above. The modied version of J will be an\ninstance over the expanded constraint language B  {T}. We denote it J and it is dened\nas follows: the variables and the constraints of J are dened as in J , but we also add all\nthe binary constraints of the form ((Xi , Xj ), T), ((Xi , Yk ), T) or ((Yk , Y(cid:96) ), T), for every four\ndierent variables Xi , Xj , Yk and Y(cid:96) that come from the same block in J .\n\n4.4 Correctness: Operator Solutions Lift and also Pro ject\nWe argue that in this new construction, satisfying assignments not only lift from I to J ,\nbut also pro ject from J to I .\nLemma 6. Let I and J be as above and let H be a Hilbert space. Then the fol lowing\nassertions are true.\n1. For every f that is a satisfying operator assignment for I over H, there exists g that\nextends f and is a satisfying operator assignment for J over H,\n2. For every g that is a satisfying operator assignment for J over H, the restriction f of\ng to the variables of I is a satisfying operator assignment for I over H.\n\nProof. Statement 1 follows from Lemma 5: Fix f that is a satisfying operator assignment\nfor I and let g be given by Lemma 5. This is also an assignment for the variables of J .\nThe constraints of J that are already in J are of course satised by g . Next consider an\nadditional constraint of the form ((Xi , Xj ), T), ((Xi , Yk ), T) or ((Yk , Y(cid:96) ), T), for variables Xi ,\nXj , Yk and Y(cid:96) coming from the same block in J . By the moreover clause in Lemma 5,\nthe operators Ai , Aj , Bk and B(cid:96) associated to Xi , Xj , Yk and Y(cid:96) by g pairwise commute.\nMoreover, the associated polynomial constraints PT (Ai , Aj ) = I , PT (Ai , Bk ) = I and\nPT (Bk , B(cid:96) ) = I are trivial (i.e., void) since the indicator polynomial PT (X1 , X2 ) of T is\njust the constant 1.\n\n15\n\n\fFor statement 2, x g that is a satisfying operator assignment for J over H, and let f\nbe the restriction of g to the variables of I . Since g satises J , for every two variables Xi\nand Xj that appear together in a constraint (Z, R) of I , the associated operators g(Xi ) and\ng(Xj ) commute since Xi and Xj appear in the same block of J . Hence f (Xi ) and f (Xj )\ncommute. We still need to show that the polynomial constraint PR (f (Z )) = I is satised\nfor every constraint (Z, R) of I . To do so, we use Lemma 3 on an appropriately dened\nsystem of polynomial equations.\nLet r be the arity of R and let R be the pp-formula as in (11) that denes R from B . The\npolynomials we dene have variables X1 , . . . , Xr , Y1 , . . . , Yt , Z1 , Z+1 that correspond to the\nvariables and constants in (11). For every k  [m], let Qk be the polynomial PSk (Wk ) + 1, so\nthat the equation Qk = 0 ensures PSk (Wk ) = 1, where PSk is the characteristic polynomial\nof Sk , and Wk is the tuple of components from X1 , . . . , Xr , Y1 , . . . , Ys , Z1 , Z+1 that appear\nin the atom Sk (wk ) of (11). Here we use Xi and Yj in place of xi and yj , respectively, and\nZ1 and Z+1 in place of the constants 1 and +1, respectively. Let also Qm+1 and Qm+2\nbe the polynomials Z1 + 1 and Z+1  1, so that the equations Qm+1 = Qm+2 = 0 ensure\nthat Z1 = 1 and Z+1 = +1. Finally, let Q be the polynomial PR (X1 , . . . , Xr ) + 1, where\nPR is the characteristic polynomial of R. It follows from the denitions that every Boolean\nassignment that satises all equations Q1 =    = Qm+2 = 0 also satises Q = 0. Thus\nLemma 3 applies, and since g extended to g(Z1 ) = I and g(Z+1 ) = I satises all equations\nQ1 =    = Qm+2 = 0, it also satises Q = 0. It follows that PR (f (Z )) = PR (g(Z )) = I ,\nas was to be proved.\n\n5 Satisability Gaps via Operator Assignments\nLet A be a Boolean constraint language and let I be an instance over A. It is easy to see\nthat the following inequalities hold:\n (I )    (I )    (I ).\n(13)\nIndeed, the rst inequality holds because if we interpret the eld of complex numbers C as\na 1-dimensional Hilbert space, then the only solutions to the equation X 2 = 1 are X = 1\nand X = +1. The second inequality is a direct consequence of the denitions. For the same\nreason, if I is satisable in the Boolean domain, then it is satisable via fd-operators, and\nif it is satisable via fd-operators, then it is satisable via operators. The converses are, in\ngeneral, not true; however, nding counterexamples is a non-trivial task. For the Boolean\nconstraint language LIN of ane relations, counterexamples are given by Mermins magic\nsquare [16, 17] for the rst case, and by Slofstras recent construction [24] for the second\ncase. These will be discussed at some length in due time. In the rest of this section, we\ncharacterize the Boolean constraint languages that exhibit such gaps.\nWe distinguish three types of gaps. Specically, we say that an instance I witnesses\n1. a satisability gap of the rst kind if  (I ) < 1 and   (I ) = 1;\n2. a satisability gap of the second kind if  (I ) < 1 and   (I ) = 1;\n\n16\n\n\f3. a satisability gap of the third kind if   (I ) < 1 and   (I ) = 1.\nAs a mnemonic rule, count the number of stars  that appear in the dening inequalities in\n1, 2 or 3 to recall what kind the gap is.\nWe say that a Boolean constraint language A has a satisability gap of the i-th kind,\ni = 1, 2, 3, if there is at least one instance I over A that witnesses such a gap. Clearly, a gap\nof the rst kind or a gap of the third kind implies a gap of the second kind. In other words,\nif A has no gap of the second kind, then A has no gap of the rst kind and no gap of the\nthird kind. A priori no other relationships seem to hold. We show that, in a precise sense,\neither A has no gaps of any kind or A has a gap of every kind. Recall from Section 4 that\nT denotes the full binary Boolean relation; i.e. T = {1}2 . We are now ready to state and\nprove the main result of this section.\n\nTheorem 3. Let A be a Boolean constraint language. Then the fol lowing statements are\nequivalent.\n\n1. A does not have a satisability gap of the rst kind.\n\n2. A does not have a satisability gap of the second kind.\n3. A  {T} does not have a satisability gap of the third kind,\n4. A is 0-valid, or A is 1-valid, or A is bijunctive, or A is Horn, or A is dual Horn.\n\nThe proof of Theorem 3 has two main parts. In the rst part, we show that if A satises\nat least one of the conditions in the fourth statement, then A has no satisability gaps of the\nrst kind or the second kind, and A  {T} has no satisability gaps of the third kind. In the\nsecond part, we show that, in all other cases, A has satisability gaps of the rst kind and\nthe second kind, and A  {T} has satisability gaps of the third kind. The ingredients in the\nproof of the second part are the existence of gaps of all three kinds for LIN, results about\nPosts lattice [21], and gap-preserving reductions that use the results about pp-denability\nestablished in Section 4.\n\n5.1 No Gaps of Any Kind\n\nAssume that A satises at least one of the conditions in the fourth statement in Theorem 3.\nFirst, we observe that the full relation T is 0-valid, 1-valid, bijunctive, Horn, and dual\nHorn.\nIndeed, T is obviously 0-valid and 1-valid. Moreover, it is bijunctive, Horn, and\ndual Horn because it is equal to the set of satisfying assignments of the Boolean formula\n(x  x)  (y  y), which is bijunctive, Horn, and dual Horn. Therefore, to prove that the\nfourth statement in Theorem 3 implies the other three statement, it suces to prove that if\nA satises at least one of the conditions in the fourth statement, then A has no gaps of any\nkind. Towards this goal, we argue by cases.\nWe start with the trivial cases in which A is 0-valid or 1-valid. If an instance I of A\ncontains a constraint of the form (Z, F), where F is an empty relation (of some arity), then\nI is not satisable by any operator assignment. Otherwise, I is satisable in the Boolean\n\n17\n\n\fdomain, hence it is satisable by assigning the identity operator I to every variable, if A is\n0-valid, or by assigning the operator I to every variable, if A is 1-valid.\nNext, we have to show that if A is bijunctive or Horn or dual Horn, then A has no gaps\nof any kind. As discussed earlier, it suces to show that A does not have a gap of the second\nkind (since a gap of the rst kind or a gap of the third kind implies a gap of the second\nkind).\nJi [15] proved that if I is a 2SAT instance or a HORN SAT instance that is satisable via\nfd-operators, then I is also satisable in the Boolean domain. In other words, Ji showed that\n2SAT and HORN SAT have no gaps of the rst kind. This is quite close to what we have\nto prove, but there are two dierences. First, a constraint language A of Boolean relations\nis bijunctive if every relation in A is the set of satisfying assignments of a 2CNF-formula,\nbut this formula need not be a 2-clause. Similarly, A is Horn (dual Horn) if every relation\nin A is the set of satisfying assignments of a Horn (dual Horn) formula, but this formula\nneed not be a Horn (dual Horn) clause. This, however, is a minor complication that can\nbe handled with some additional arguments, the details of which will be provided later on.\nSecond, at rst glance, Jis proof for 2SAT and HORN SAT does not seem to extend to\noperator assignments of arbitrary (nite or innite) dimension. The reason for this is that\nJis argument relies on the existence of eigenvalues and associated orthogonal eigenspaces\nfor the linear operators, which are not guaranteed to exist in the innite-dimensional case,\neven for self-adjoint bounded linear operators. Note however that in our case we have the\nadditional requirement that the operators satisfy A2 = I , and in such a case their eigenvalues\nand associated eigenspaces can be reinstated. This observation could perhaps be used to give\na proof along the lines of Jis that 2SAT and HORN SAT have no gaps of the second kind.\nHowever, we prefer to give an alternative and more direct proof that does not rely at all on\nthe existence of eigenvalues. Our proof is based on the manipulation of non-commutative\npolynomial identities, a method that has been called the substitution method (see, e.g., [7]).\nLemma 7. Let I be a 2SAT instance or a HORN SAT instance or a DUAL HORN SAT\ninstance. Then the fol lowing statements are equivalent.\n1. I is satisable in the Boolean domain;\n2. I is satisable via fd-operators;\n3. I is satisable via operators.\n\nWe split the proof into two: one for 2SAT and another one for HORN SAT; the proof for\nDUAL HORN SAT is analogous to the proof for HORN SAT, and it is omitted.\nProof of Lemma 7 for 2SAT. Let I be a 2CNF-formula. The implications 1 = 2 and\n2 = 3 follow from the denitions. To prove the implication 3 = 1, assume that f is a\nsatisfying operator assigment for I over a (nite-dimensional or innite-dimensional) Hilbert\nspace H, and, towards a contradiction, assume that I is unsatisable in the Boolean domain.\nWe will make use of the well-known characterization of unsatisable in the Boolean domain\n2SAT instances in terms of a reachability property of their associated implication graph.\nFor I , the implication graph is the directed graph G that has one vertex for each literal x\n\n18\n\n\for x of every variable x in I , and two directed edges for each clause ((cid:96)1  (cid:96)2 ) of I , one\nedge from (cid:96)1 to (cid:96)2 , and another one from (cid:96)2 to (cid:96)1 . The well-known characterization states\nthat I is unsatisable in the Boolean domain if and only if there exists a variable x and\ntwo directed paths in G, one from the variable x to the literal x, and another one from\nthe literal x to the variable x (see, e.g., [19]). Accordingly, let (cid:96)1 , . . . , (cid:96)r and m1 , . . . , ms be\nliterals such that x, (cid:96)1 , . . . , (cid:96)r , x and x, m1 , . . . , ms , x are the vertices in the paths from x\nto x and from x to x, respectively, in the order they are traversed.\nThe existence of the path x, (cid:96)1 , . . . , (cid:96)r , x from the variable x to the literal x in the\nimplication graph G means that the clauses\n(x  (cid:96)1 ), ((cid:96)1  (cid:96)2 ), . . . , ((cid:96)r1  (cid:96)r ), ((cid:96)r  x)\n(14)\nare clauses of the instance I . Symmetrically, the existence of the path x, m1 , . . . , ms , x\nfrom the literal x to the variable x in the implication graph G means that the clauses\n(x  m1 ), (m1  m2 ), . . . , (ms1  ms ), (ms  x)\nare clauses of the instance I .\nIn the case of satisability in the Boolean domain, one reasons that the instance I\nis unsatisable, because if it were satisable by some truth assignment, then the path of\nimplications from x to x forces x to be set to false, while the path of implications from x\nto x forces x to be set to true. In what follows, we will show that, with some care, essentially\nthe same reasoning can be carried out for operator assignments that satisfy the instance I .\nExtend the operator assignment f to all literals by setting f ((cid:96)) = sg((cid:96))f (x), where x the\nvariable underlying (cid:96). Since f is a quantum satisfying assignment for I , Lemma 2 implies\nthat\n\n(15)\n\n(I  f (x))(I + f ((cid:96)1 )) = 0\n(I  f ((cid:96)i ))(I + f ((cid:96)i+1 )) = 0,\n(I  f ((cid:96)r ))(I  f (x)) = 0\n\n1  i  r  1.\n\nWe now claim that\n\n(I  f (x))(I + f ((cid:96)i )) = 0,\n\n1  i  r.\n\n(16)\n(17)\n(18)\n\n(19)\n\nWe prove the claim by induction on i. For i = 1, what we need is just equation (16). By\ninduction, assume now that\n\n(I  f (x))(I + f ((cid:96)i1 )) = 0\nholds for some i with 2  i  r  1. By (17), we have that\n(I  f ((cid:96)i1 ))(I + f ((cid:96)i )) = 0.\n\nholds. First, by multiplying equation (20) from the right by (I + f ((cid:96)i )), we get\n(I  f (x))(1 + f ((cid:96)i1 ))(1 + f ((cid:96)i )) = 0\n\n(20)\n\n(21)\n\n(22)\n\n19\n\n\f(25)\n\n(26)\n\n(23)\n\n(24)\n\nSecond, by multiplying equation (21) from the left by (I  f (x)), we get\n(I  f (x))(1  f ((cid:96)i1 ))(1 + f ((cid:96)i )) = 0\nBy adding equations (22) and (23), we obtain\n(I  f (x))(I + f ((cid:96)i )) = 0,\nas desired. In particular, by considering the case i = r, we get\n(I  f (x))(I + f ((cid:96)r )) = 0,\nwhich, after multiplying out the left-hand side, becomes\nI + f ((cid:96)r )  f (x)  f (x)f ((cid:96)r ) = 0.\nFurthermore, by multiplying out the left-hand side of equation (18), we get\nI  f (x)  f ((cid:96)r ) + f ((cid:96)r )f (x) = 0.\n(27)\nSince the variable x and the literal (cid:96)r appear in the same clause of the instance I , namely, the\nclause ((cid:96)r  x), we have that f (x)f ((cid:96)r ) = f ((cid:96)r )f (x). Therefore, by adding equations (26)\nand (27), we get that 2I  2f (x) = 0, which implies that f (x) = I .\nAn entirely symmetric argument using the path from x to x, instead of the path from\nx to x, gives f (x) = I , which contradicts the previous nding that f (x) = I .\nProof of Lemma 7 for HORN SAT. Let I be a Horn formula. As with the proof for 2SAT,\nthe only non-trivial direction is 3 = 1. To prove the implication 3 = 1, assume that\nf is a satisfying operator assigment for I over a (nite-dimensional or innite-dimensional)\nHilbert space H, and, towards a contradiction, assume that I is unsatisable in the Boolean\ndomain. As in the proof for 2SAT, let f be extended to all literals by f ((cid:96)) = sg((cid:96))f (x), where\nx is the variable underlying x. We will make use of the characterization of unsatisable in\nthe Boolean domain Horn instances in terms of unit resolution. For this, we need to rst\nintroduce some terminology and notation. If C and C (cid:48) are two clauses such that C contains\na literal (cid:96) and C (cid:48) contains the complementary literal (cid:96) of (cid:96), then the resolution rule produces\nin one step the resolvent clause D that is the disjunction of all literals in the premises C\nand C (cid:48) other than (cid:96) and (cid:96). The unit resolution rule is the special case of the resolution rule\nin which (at least) one of the clauses C and C (cid:48) is a single literal. It is well known (see, e.g.,\n[23]) that a Horn formula I is unsatisable if and only if there is a unit resolution derivation\nof the empty clause from the clauses of I , i.e., there is a sequence C1 , . . . , Cm of clauses such\nthat, for each i  {1, . . . , m}, we have that Ci is one of the clauses of I or Ci is obtained\nfrom earlier clauses Cj and Ck in the sequence via the unit resolution rule. Clearly, in a\nunit resolution derivation of the empty clause, the last application of the unit resolution rule\ninvolves two clauses each of which is the complementary literal of the other.\nIn what follows, we will show that a unit resolution derivation can be simulated by a\nsequence of equations involving operator assignments. We begin by formulating and proving\nthe following claim.\n\n20\n\n\fClaim 1: Let ((cid:96)1      (cid:96)r ) be clause and let (cid:96)j be the complementary literal of some literal\n(cid:96)j in that clause. If f satises both the clause ((cid:96)1      (cid:96)r ) and the literal (cid:96)j , then f also\nsatises the resolvent ((cid:96)1      (cid:96)j1  (cid:96)j+1      (cid:96)r ) of ((cid:96)1      (cid:96)r ) and (cid:96)j ; equivalently,\nthe operators {f ((cid:96)i ) : i (cid:54)= j } pairwise commute and\nj1(cid:89)\nr(cid:89)\n(I + f ((cid:96)i ))\n(I + f ((cid:96)i )) = 0.\ni=1\ni=j+1\n\n(28)\n\nObserve that for the unit resolution rule, as is the case here, the resolvent is always\na subclause of one of the premises.\nIn particular, since f satises both premises, all the\noperators involved in the premises commute, and so do the ones involved in the resolvent\nclause. To complete the proof of the claim observe that, since f satises both the clause\n((cid:96)1      (cid:96)r ) and the literal (cid:96)j , the corresponding operators commute, and the identity of\nr(cid:89)\npolynomials in commuting variables of Lemma 2 implies that\n(I + f ((cid:96)i )) = 0\ni=1 (I + f ((cid:96)i )) from the left, and by (cid:81)r\nBy multiplying equation (30) by (cid:81)j1\ni=1\n(I  f ((cid:96)j )) = 0.\ni=j+1 (I + f ((cid:96)i ))\nfrom the right, we get(cid:32)j1(cid:89)\n(cid:32) r(cid:89)\n(cid:33)\n(cid:33)\n(I + f ((cid:96)i ))\n(I + f ((cid:96)i ))\ni=1\ni=j+1\n\n(I  f ((cid:96)j ))\n\n= 0.\n\n(29)\n\n(30)\n\n(31)\n\nBy adding equations (29) and (31), we get (28), which completes the proof of Claim 1.\nConsider now a unit resolution derivation C1 , . . . , Cm of the empty clause from the clauses\nof I . Since the operator assignment f satises every clause of I , we can apply Claim 1\nrepeatedly and, by induction, show that f satises each clause in this derivation. Since Cm\nis the empty clause, it must have been derived via the unit resolution rule from two earlier\nclauses each of which is the complementary literal of the other, say, (cid:96) and (cid:96). So, we must\nhave f ((cid:96)) = I and f ((cid:96)) = I , which is a contradiction since f ((cid:96)) = f ((cid:96)).\n\nIn what follows, we will use Lemma 7 to show that if A is bijunctive or Horn or dual\nHorn, then A has no gaps of any kind.\nAssume that A is bijunctive. Note that we cannot apply Lemma 7 directly to conclude\nthat A has no gaps of any kind, because the relations in the constraint-language A are\ndened by conjunctions of 2-clauses, but need not be dened by individual 2-clauses.\nIn\norder to be able to apply Lemma 7, we rst need to verify the following claim. Assume that\n(Z, R) is a constraint in which R is a relation in A dened by a conjunction C1      Cm ,\nwhere each Ci is a 2-clause on the variables in Z . Then a satisfying operator assignment\nfor the instance consisting of the single constraint (Z, R) will also satisfy each of the 2-\nclause constraints (W1 , C1 ), . . . , (Wr , Cr ) individually, where Wi = (Zci , Zdi ) is the tuple of\n\n21\n\n\fcomponents of Z = (Z1 , . . . , Zr ) that appear in Ci . To prove this claim, rst note that\nthe commutativity condition on the operators assigned to the variables in Wi is guaranteed\nby the commutativity condition on the variables in Z . Thus, we just need to check that\nthe characteristic polynomial of Ci evaluates to I , and to do so we use Lemma 3 for an\nappropriately dened system of polynomial equations. In the remaining, x i  [m].\nOur polynomials have variables X1 , . . . , Xr . Let Q1 be the polynomial PR (X1 , . . . , Xr )+1,\nso that the equation Q1 = 0 ensures PR (X1 , . . . , Xr ) = 1, where PR is the characteristic\npolynomial of R. Let Q be the polynomial PCi (Xci , Xdi ) + 1, so that the equation Q = 0\nensures PCi (Xci , Xdi ) = 1, where PCi is the characteristic polynomial of Ci , and ci and di are\nthe indices of the components of Z in Wi . Then, every Boolean assignment that satises the\nequation Q1 = 0 belongs to R, from which it follows that the Boolean assignment satises\nthe conjunct Ci in the bijunctive denition of R, and hence it also satises the equation\nQ = 0. Thus, Lemma 3 applies and every operator assignment that satises PR (Z ) = I\nalso satises PCi (W ) = I , as was to be proved.\nWe are now ready to complete the proof that if A is bijunctive, then A has no gaps. Let\nI be an instance over A that is satisable via operators. The preceding paragraph shows\nthat the 2SAT instance that results from replacing each constraint in the instance I by its\ndening conjunction of 2-clauses is also satisable via operators. By Lemma 7, this 2SAT\ninstance is also satisable in the Boolean domain. But then I itself is satisable in the\nBoolean domain, as was to be shown.\nIf A is Horn or dual Horn, then the proof is entirely analogous.\n\n5.2 Background on Posts Lattice\n\nBefore we start with the second part in the proof of Theorem 3, we need to introduce some\nbasic terminology and basic results from universal algebra; we devote this section to that.\nLet R  {1}r be a Boolean relation of arity r and let f : {1}m  {1} be a Boolean\noperation of arity m. The relation R is invariant under f if, for all sequences of m many\nr-tuples (a1,1 , . . . , a1,r ), . . . , (am,1 , . . . , am,r ) in {1}r , the following holds:\n\nif (a1,1 , . . . , a1,r ), . . . , (am,1 , . . . , am,r ) are tuples in R,\nthen (f (a1,1 , . . . , am,1 ), . . . , f (a1,r , . . . , am,r )) is also a tuple in R.\n\n(32)\n\nNote that the tuple in the second line is obtained by applying the m-ary operation f to the\nm many tuples in the rst line componentwise. If A is a Boolean constraint language, we\nsay that A is invariant under f if every relation in A is invariant under f . Whenever A is\ninvariant under f we also say that f is a closure operation of A.\nThe importance of the closure operations of a constraint language stems from the fact\nthat they completely determine the relations that are pp-denable from it. This semantic\ncharacterization of the syntactic notion of pp-denability was discovered by Geiger [13] and,\nindependently, Bodnarchuk et al. [3], for all constraint languages of arbitrary but nite\ndomain. Here we state the special case of this characterization for the Boolean domain,\nsince only this special case is needed in our applications.\n\n22\n\n\fTheorem 4 ([13, 3]). Let A be a Boolean constraint language and let R be a Boolean relation.\nThe fol lowing statements are equivalent:\n\n1. R is pp-denable from A by a pp-formula without constants,\n\n2. R is invariant under al l Boolean closure operations of A.\n\nIn the following we refer to Theorem 4 as Geigers Theorem.\nRecall from Section 4 that a pp-formula without constants is one in which the constants\n+1 and 1 do not appear in its quantier-free part of the formula. Although it will not\nbe used until a later section, it is worth pointing out here that a similar characterization\nof pp-denability with constants exists.\nIndeed, it is easy to see that Geigers Theorem\nimplies that a Boolean relation R is pp-denable from the Boolean constraint language A\nby a pp-formula with constants if and only if R is invariant under all idempotent Boolean\nclosure operations of A, or equivalently, invariant under all Boolean closure operations of the\nBoolean constraint language A+ that is obtained from A by adding the two unary singleton\nrelations {+1} and {1}; i.e., A+ = A  {{+1}, {1}}. We return to the issue of denability\nwith constants in Section 7.\nFor every set F of Boolean operations, let Inv(F ) denote the set of all Boolean relations\nthat are invariant under all operations in F . Conversely, for every set of Boolean relations A,\nlet Pol(A) denote the set of all Boolean operations under which all relations in A are invariant.\nGeigers Theorem implies that the mappings A (cid:55) Pol(A) and F (cid:55) Inv(F ) are the lower and\nupper adjoints of a Galois connection [8] between the partially ordered set of sets of Boolean\nrelations ordered by inclusion, and the partially ordered set of sets of Boolean operations,\nalso ordered by inclusion.\nNote that for every constraint language A, the set Pol(A) contains all projection oper-\nations : all operations f : {1}r  {1} for which there exists an index i  [r] such that\nf (x1 , . . . , xr ) = xi for all (x1 , . . . , xr )  {1}r . Also, Pol(A) is closed under compositions :\nif f : {1}s  {1} and g1 , . . . , gs : {1}r  {1} are operations from Pol(A), then the\noperation h = f  (g1 , . . . , gs ) dened by h(x1 , . . . , xr ) = f (g1 (x1 , . . . , xr ), . . . , gs (x1 , . . . , xr ))\nfor all (x1 , . . . , xr )  {1}r is also in Pol(A). Any set of relations that contains all pro jection\noperations and that is closed under compositions is called a clone.\nPost [21] analyzed the collection of all clones of Boolean operations and completely de-\ntermined the inclusions between them. In particular, he showed that this collection forms a\nlattice under inclusion, which is known as Posts lattice. In denoting clones in Posts lattice,\nwe will follow the notation and terminology used by Bohler et al. [4]. The lattice is repre-\nsented by the diagram in Figure 1, which is also borrowed from [4] (we thank Steen Reith\nfor allowing us to reproduce the diagram here).\nEach circle in the diagram of Figure 1 represents a clone of Boolean operations, and a\nline between two circles denotes inclusion of the clone of the lower circle into the clone of\nthe upper circle. Post showed that every clone of Boolean operations is represented in the\ndiagram. Post also identied a nite basis of operations for each clone, which means that\nthe clone is the smallest class of operations that contains the operations in the basis and all\nthe pro jections operations, and that is closed under composition. For our application, we\n\n23\n\n\fFigure 1: Graph of all Boolean clones (diagram by Steen Reith).\n\n24\n\nR1R0BFR2MM1M0M2S20S30S0S202S302S02S201S301S01S200S300S00S21S31S1S212S312S12S211S311S11S210S310S10DD1D2LL1L0L2L3VV1V0V2EE0E1E2II1I0I2N2N\f\n{false}\n{true}\n{(x  y)  (x  z )  (y  z )}\n\nI2\nI0\nI1\nD2\n\n{x  y}\n{x  y}\n{x  y  z}\n{x}\n\nE2\nV2\nL2\nN2\n\nFigure 2: Bases of some selected clones from Figure 1. Here , ,  and  denote Boolean\nconjunction, Boolean disjunction, Boolean negation, and Boolean exclusive or, respectively.\n\nneed only the bases for the eight clones called I2 , I0 , I1 , D2 , E2 , V2 , L2 and N2 . These are\nlisted in the table in Figure 2.\nThe nal ingredient we need from Posts lattice is a characterization of the tractable\nBoolean constraint languages from Schaefers Theorem in terms of their closure operations.\n\nTheorem 5 (see Section 1.1 in [5]). Let A be a Boolean constraint language. The fol lowing\nstatements hold.\n\n1. A is 0-valid if and only if A is invariant under the constant false operation.\n\n2. A is 1-valid if and only if A is invariant under the constant true operation.\n3. A is bijunctive if and only if A is invariant under (x  y)  (x  z )  (y  z ).\n4. A is Horn if and only if A is invariant under x  y .\n5. A is dual Horn if and only if A is invariant under x  y .\n6. A is ane if and only if A is invariant under x  y  z .\n\nFor the connection with Posts lattice, note that, by Figure 2, the six conditions listed on\nthe right of the entries 1 through 6 in Theorem 5 are equivalent to Pol(A) containing the\nclones I0 , I1 , D2 , E2 , V2 and E2 , respectively.\n\n5.3 Gaps of Every Kind\n\nWe are ready to proceed with the second part in the proof of Theorem 3. Assume that A\nsatises none of the conditions in the fourth statement in Theorem 3, i.e., A is not 0-valid,\nA is not 1-valid, A is not bijunctive, A is not Horn, and A is not dual Horn. We will show\nthat A has a satisability gap of the rst kind (hence, A also has a satisability gap of the\nsecond kind) and A  {T} has a satisabiilty gap of the third kind.\nAs a stepping stone, we will use the known fact that LIN has gaps of every kind. We\nnow discuss the proof of this fact and give the appropriate references to the literature.\nevery such equation is a parity equation of the form (cid:81)r\nRecall that LIN is the class of all ane relations, i.e., Boolean relations that are the set of\nsolutions of a system of linear equations over the two-element eld. In the 1-representation,\ni=1 xi = y , where y  {1}.\n\n25\n\n\fMermin [16, 17] considered the following system M of parity equations:\n\nX11X12X13 = 1\nX21X22X23 = 1\nX31X32X33 = 1\n\nX11X21X31 = 1\nX12X22X32 = 1\nX13X23X33 = 1.\n\n(33)\n\nGraphically, this system of equations can be represented by a square, where each equation\non the left of (33) comes from a row, and each equation on the right of (33) comes from a\ncolumn.\n\nX11 X12 X13\n\n+1\n\nX21 X22 X23\n\n+1\n\nX31 X32 X33\n+1 +1 1\n\n+1\n\nIt is easy to see that this system of equations has no solutions in the Boolean domain.\nIndeed, by multiplying the left-hand sides of all equations, we get 1 because every variable\nXij occurs twice in the system and X 2\nij = 1. At the same time, by multiplying the right-hand\nsides of all equations, we get 1, hence the system has no solutions in the Boolean domain.\nObserve, however, that this argument used the assumption that variables commute pairwise,\neven if they do not appear in the same equation. Thus, this argument does not go through if\none assumes only that variables occurring in the same equation commute pairwise. Mermin\n[16, 17] showed that the system M has a solution consisting of linear operators on a Hilbert\nspace of dimension four. Thus, in our terminology, Mermin established the following result.\nTheorem 6 ([16, 17]). M witnesses a satisability gap of the rst kind for LIN.\n\nCleve and Mittal [7, Theorem 1] have shown that a system of parity equations has a\nsolution consisting of linear operators on a nite-dimensional Hilbert space if and only if\nthere is a perfect strategy in a certain non-local game in the tensor-product model. Cleve,\nLiu, and Slofstra [6, Theorem 4] have shown that a system of parity equations has a solution\nconsisting of linear operators on a (nite-dimensional or innite-dimensional) Hilbert space if\nand only if there is a perfect strategy in a certain non-local game in the commuting-operator\nmodel. Slofstra [24] obtained a breakthrough result that has numerous consequences about\nthese models.\nIn particular, Corollary 3.2 in Slofstras paper [24] asserts that there is a\nsystem S of parity equations whose associated non-local game has a perfect strategy in\nthe commuting-operator model, but not in the tensor-product model. Thus, by combining\nTheorem 1 in [7], Theorem 4 in [6], and Corollary 3.2 in [24], we obtain the following result.\nTheorem 7 ([6, 7, 24]). S witnesses a satisability gap of the third kind for LIN.\n\n26\n\n\fLIN has a rather special place among all classes of Boolean relations that are not 0-valid,\nare not 1-valid, are not bijunctive, are not Horn, and are not dual Horn. This special role\nis captured by the next lemma, which follows from Posts analysis of the lattice of clones of\nBoolean functions from Section 5.2.\n\nLemma 8. Let A be a Boolean constraint language. If A is not 0-valid, not 1-valid, not\nbijunctive, not Horn, and not dual Horn, then LIN is pp-denable from A.\n\nProof. Assume that A is a Boolean constraint language satisfying the hypothesis of Lemma 8.\nWe consider the clone Pol(A) and distinguish several cases using Posts lattice.\nIf Pol(A) is the smallest clone I2 in Posts lattice, then Pol(A) contains only the pro jection\nfunctions; hence, every Boolean relation is closed under every function in Pol(A). Geigers\nTheorem implies that every Boolean relation and, in particular, every relation in LIN, is\npp-denable from A (and, in fact, it is pp-denable without using constants).\nIf Pol(A) is not the smallest clone I2 in Posts lattice, then it must contain one of the\nseven minimal clones I0 , I1 , D2 , E2 , V2 , L2 , N2 that contain I2 . Recall that these clones have\nbases of operations as described in Figure 2. Since A is not i-valid, where i = 0, 1, and since\nthe clone Ii is generated by the constant function ci (x) = i, it must be the case that Pol(A)\ndoes not contain the clone I0 or the clone I1 . Since A is not bijunctive, there is a relation in\nA that is not closed under the majority function ma j(x, y , z ) = (x  y)  (x  z )  (y  z ).\nSince the clone D2 is generated by the function ma j(x, y , z ), it must be the case that that\nPol(A) does not contain the clone D2 . Since A is not Horn, there is a relation in A that\nis not closed under the function and(x, y) = x  y . Since the clone E2 is generated by the\nfunction and(x, y), it must be the case that Pol(A) does not contain the clone E2 . Since A is\nnot dual Horn, there is a relation in A that is not closed under the function or(x, y) = x  y .\nSince the clone V2 is generated by the function or(x, y), it must be the case that that Pol(A)\ndoes not contain the clone V2 .\nThe preceding analysis shows that there are just two possibilities: Pol(A) contains the\nclone L2 or Pol(A) contains the clone N2 . Assume rst that Pol(A) contains the clone L2 .\nSince L2 is generated by the exclusive or function (x, y , z ) = x  y  z and since a relation is\nane if and only if it is closed under the function , Geigers Theorem implies that a relation\nis pp-denable without constants from A if and only if it is an ane relation. Thus, LIN\nis pp-denable from A (and, in fact, it is pp-denable without constants). Finally, assume\nthat Pol(A) contains the clone N2 . Since Pol(A) is generated by the function not(x) = x,\nGeigers Theorem implies that a relation is pp-denable without constants from A if and\nonly if it is it is closed under the function not(x). In particular, for every n  1 and for\ni = 0, 1, the ane relation that is the set of solutions of the equation x1 +    + x2n = i mod 2\nis pp-denable without constants from A. By using the constant 0 in these equations, we\nhave that for every n  1 and for every i = 0, 1, the ane relation that is the set of solutions\nof the equation x1 +    + x2n1 = i mod 2 is pp-denable from A (recall that pp-denitions\nallow constants). It follows that LIN is pp-denable from A.\n\nThe nal lemma in this section asserts that reductions based on pp-denitions preserve\nsatisability gaps upwards.\n\n27\n\n\fLemma 9. Let B and C be Boolean constraint languages such that B is pp-denable from C .\n\n1. If B has a satisability gap of the rst kind, then so does C .\n2. If B has a satisability gap of the third kind, then so does C  {T}.\nProof. For the rst part, assume that B is pp-denable from C and that I is an instance that\nwitnesses a satisability gap of the rst kind for B . Thus, I is satisable via fd-operators,\nbut is not satisable in the Boolean domain. Let J be the instance over C as dened in\nSection 4.1. On the one hand, by Lemma 5, the instance J is also satisable via fd-operator.\nOn the other hand, by Lemma 4, the instance J is also not satisable in the Boolean domain.\nThus, J witnesses a satisability gap of the rst kind for C .\nFor the second part, assume that B is pp-denable from C and that I is an instance that\nwitnesses a satisability gap of the third kind for B . Thus, I is satisable via operators,\nbut it is not satisable via fd-operators. Let J be the instance over C  {T} as dened in\nSection 4.3. By Lemma 6, the instance J is satisable via operators, but it is not satisable\nvia fd-operators. Thus, J witnesses a satisability gap of the third kind for C  {T}.\n\nWe now have all the machinery needed to put everything together.\nLet A be a Boolean constraint language that is not 0-valid, not 1-valid, not bijunctive,\nnot Horn, and not dual Horn. By Lemma 8, we have that LIN is pp-denable from A. Since,\nby Theorem 6, LIN has a satisability gap of the rst kind, the rst part of Lemma 9 implies\nthat A has a satisability gap of the rst kind. Since, by Theorem 7, LIN has a satisability\ngap of the third kind, the second part of Lemma 9 implies that A has a satisability gap of\nthe third kind. The proof of Theorem 3 is now complete.\n\n6 Further Applications\n\nIn this section we discuss two applications of the results from Sections 4 and 5. The rst\napplication is about classication theorems in the style of Schaefer. The second application\nbuilds on Slofstras results to answer some open questions from [1] on the quantum realiz-\nability of contextuality scenarios. While these open questions were solved earlier by Fritz\nalso using Slofstras results (see [12]), our alternative perspective may still add some value\nsince, as we will see, we obtain improved, and indeed optimal, parameters.\n\n6.1 Dichotomy Theorems\n\nFor a Boolean constraint language A, let SAT(A) denote the following decision problem:\nGiven an instance I over A, is I satisable in the Boolean domain?\nSimilarly, let SAT (A) and SAT (A) be the versions of the problem in which the questions\nare whether I is satisable via an operator assignment on a nite-dimensional Hilbert space,\nor on an arbitrary Hilbert space, respectively. We say that a problem poly-m-reduces to\n\n28\n\n\fanother if there is a polynomial-time computable function that transforms instances of the\nrst problem into instances of the second in such a way that the answer is preserved.\nRecall that T denotes the full binary Boolean relation {1}2 . The construction in Sec-\ntion 4.3 and Lemma 6 give the following:\nLemma 10. Let A and B be Boolean constraint languages and let A(cid:48) = A  {T} and\nB (cid:48) = B  {T}. If A is pp-denable from B , then\n1. SAT(A(cid:48) ) poly-m-reduces to SAT(B (cid:48) ), SAT (B (cid:48) ), and SAT (B (cid:48) ),\n2. SAT (A(cid:48) ) poly-m-reduces to SAT (B (cid:48) ).\n3. SAT (A(cid:48) ) poly-m-reduces to SAT (B (cid:48) ).\nSlofstras Corollary 3.3 in [24] in combination with Theorem 4 in [6] gives the undecid-\nability of SAT (LIN) which, from now on we denote by LIN SAT .\nTheorem 8 ([24],[6]). LIN SAT is undecidable.\nIn combination with Lemmas 7, 10, and 8, we get the following dichotomy theorem:\nTheorem 9. Let A be a Boolean constraint language and let A(cid:48) = A  {T}. Then, exactly\none of the fol lowing holds:\n1. SAT (A(cid:48) ) is decidable in polynomial time,\n2. SAT (A(cid:48) ) is undecidable.\nMoreover, the rst case holds if and only if A is 1-valid, or A is 0-valid, or A is bijunctive,\nor A is Horn, or A is dual Horn.\nProof. If A is 1-valid, 0-valid, bijunctive, Horn, or dual Horn, then A(cid:48) is also of the same\ntype; indeed T is both 1-valid and 0-valid, and it is also bijunctive, Horn and dual Horn\nsince it is dened by the empty conjunction of any kind of clauses. Thus SAT (A(cid:48) ) is the\nsame problem as SAT(A(cid:48) ) by Lemma 7, which is solvable in polynomial time.\nIf on the contrary A is neither 1-valid, nor 0-valid, nor bijunctive, nor Horn, nor dual\nHorn, then Lemma 8 applies and LIN has a pp-denition from A. In such a case Lemma 10\napplies and SAT (LIN(cid:48) ) reduces to SAT (A(cid:48) ), where LIN(cid:48) denotes LIN  {T}. Since every\ninstance of LIN SAT is also an instance of SAT (LIN(cid:48) ), the undecidability of SAT (A(cid:48) )\nfollows from Theorem 8.\nNote that, in case 2, Theorem 9 states that SAT (A(cid:48) ) is undecidable but it says nothing\nabout SAT (A). Luckily, in most cases it is possible to infer the undecidability of SAT (A)\nfrom the undecidability of SAT (A(cid:48) ). This is the case, for example, for both\n3SAT = {{1}3 \\ {(a1 , a2 , a3 )} : a1 , a2 , a3  {1}},\n3LIN = {{(a1 , a2 , a3 )  {1}3 : a1a2a3 = b} : b  {1}}.\nIn the following we write 3LIN SAT and 3LIN SAT to denote the problems SAT (A)\nand SAT (A) for A = 3LIN. Similarly, we use 3SAT and 3SAT to denote SAT (A) and\nSAT (A) for A = 3SAT.\n\n29\n\n\fTheorem 10. 3LIN SAT and 3SAT are both undecidable.\n\nProof. Let A be the Boolean constraint language of 3LIN or 3SAT. It follows from Theorem 9\nthat SAT (A(cid:48) ) is undecidable. Now we reduce this problem to SAT (A). Take any instance\nI over A(cid:48) and replace each constraint of the type ((Z1 , Z2 ), T) by an equation Z1Z2Y = 1\nin the case of 3LIN, and a clause Z1  Z2  Y in the case of 3SAT, where Y is a fresh\nvariable not used anywhere else in the instance. Let J be the resulting instance. If f is\na satisfying operator assignment for I , then we claim that an appropriate extension g of f\nis a satisfying operator assignment for J . For 3LIN, set g(Y ) = f (Z2 )f (Z1 ). For 3SAT,\nset g(Y ) = I . To see that this works, rst note that g(Z1 ) = f (Z1 ) and g(Z2 ) = f (Z2 )\ncommute since they appear together in a constraint of I . Thus, in both cases g(Z1 ), g(Z2 ) and\ng(Y ) pairwise commute. Moreover, in the 3LIN case the assignment g(Y ) = f (Z2 )f (Z1 )\nis chosen so that the equation g(Z1 )g(Z2 )g(Y ) = I is satised; to check this, multiply\ng(Y ) = f (Z2 )f (Z1 ) by g(Z1 )g(Z2 ) from the right and use g(Z2 )f (Z2 ) = f (Z2 )2 = I and\ng(Z1 )f (Z1 ) = f (Z1 )2 = I . Also, in the 3SAT case the assignment g(Y ) = I annihilates\nthe product in the expression of the characteristic polynomial of the clause Z1  Z2  Y in\nsee Lemma 2, which makes the characteristic polynomial evaluate to I regardless of what\ng(Z1 ) and g(Z2 ) are. Thus, the new constraints are satised by g and the claim is proved.\nConversely, if g is a satisfying operator assignment for J , then the restriction of g to\nthe variables of I is a satisfying operator assignment for I , just because the commutativity\nof f (Z1 ) and f (Z2 ) is enforced by the fact that they appear together in the constraint\nZ1Z2Y = I or Z1  Z2  Y of J , and because the characteristic polynomial of T is the\nconstant 1.\n\nThe same construction and argument that we used in Theorem 10 starting at a gap\ninstance over the constraint language 3SAT  {T} gives a gap instance over 3SAT that will\nbe useful later on.\n\nCorollary 1. There is an instance over the Boolean constraint language 3SAT that witnesses\na satisability gap of the third kind; it is satisable via operator assignments over some Hilbert\nspace but not over a nite-dimensional Hilbert space.\nFor the problems SAT (A(cid:48) ), a trichotomy theorem can be proved: 1) polynomial-time\nsolvable vs 2) polynomial-time equivalent to SAT (LIN(cid:48) ) vs 3) both SAT (LIN(cid:48) )-hard and\nNP-hard. Unfortunately, whether SAT (LIN(cid:48) ) or SAT (LIN) are polynomial-time solvable,\nNP-hard or undecidable is an open problem.\n\n6.2 Quantum Realizability of Contextuality Scenarios\nthat (cid:83)\nWe follow the terminology in the paper by Acn, Fritz, Leverrier and Sainz [1]. A contextuality\nscenario is a hypergraph H with set V (H ) of vertices and set E (H )  2V (H ) of edges such\neE (H ) e = V (H ). Given a contextuality scenario H , a quantum model for it is,\ninformally, an assignment of probabilities to the vertices of H that are reproduced as the\nobservation probabilities of a collection of pro jective measurements associated to the edges\n\n30\n\n\fof H , when the measurements are applied to some quantum state. When a contextuality\nscenario has at least one quantum model, one says that H allows quantum models. As argued\nin [1], this can be equivalently stated formally, without any reference to measurements or\nquantum states, as follows.\nWe say that a contextuality scenario H al lows a quantum model, or is quantum realizable,\nif there exists a Hilbert space H and an assignment of bounded linear operators Pv on H to\neach vertex v in V (H ) in such a way that:\n1. Pv is self-adjoint,\n3. (cid:80)\nv = Pv for each v  V (H ),\n2. P 2\nve Pv = I for each e  E (H ).\nNote that 1 and 2 together say that each Pv is an orthogonal pro jection operator3 , and 3 says\nthat the pro jection operators associated to the vertices of each edge resolve the identity. In [1]\nthe question was raised whether there exist contextuality scenarios that are quantum realiz-\nable but only over innite-dimensional Hilbert spaces (see Problem 8.3.2 in [1]). A related\ncomputational question was also raised: Is it decidable whether a contextuality scenario\ngiven as input allows a quantum state? (see Conjecture 8.3.3 in [1]). Following the notation\nin [1], this problem is called ALLOWS-QUANTUM. The restriction of the problem in which\nthe input hypergraph has edges of cardinality at most k we call k -ALLOWS-QUANTUM.\nSee [1] for a discussion on why these problems are important, and their relationship to Connes\nEmbedding Conjecture in functional analysis.\nSoon after Slofstra published his results, both questions raised in [1] were answered\nby Fritz by reduction from Slofstras Theorems 7 and 8 (see [12]).\nIn particular, Fritz\nproved that ALLOWS-QUANTUM is undecidable. In the following we illustrate the methods\ndevelopped in the previous sections to give alternative proofs of these results. As a bonus,\nour proof also gives optimal parameters; we get hypergraphs with edges of size at most 3 that\nseparate innite-dimensional realizability from nite-dimensional realizability, and we show\nthat already 3-ALLOWS-QUANTUM is undecidable. In contrast, Fritz reduction incurs\nan exponential loss in the size of the edges of the hypergraphs with respect to the arity\nof the constraints in Slofstras result, which is a priori not bounded, and the best it can\nachieve is size 4 anyway. Moreover, as we will see, our 3 in the maximum size of the edges is\noptimal since it turns out that 2-ALLOWS-QUANTUM is decidable (and even solvable in\npolynomial time).\nNext we show how our methods can be used to answer these questions. First, notice that\nthere is a clear similarity between the requirements 1, 2 and 3 in the denition of quantum\nrealization of H and the requirements that an operator assignment for a collection of variables\n{Xv : v  V (H )} associated to the vertices of H must satisfy. For one thing, if we dene\nAv = I  2Pv for every v  V (H ), then each Av is a bounded self-adjoint linear operator\n3Acn et al. refer to orthogonal pro jection operators as pro jections, and so we will to avoid confusion with\nthe fact that two orthogonal pro jection operators P and Q could fail to satisfy P Q (cid:54)= 0. It may also be worth\npointing out that linear-algebraic pro jection operators of this section are unrelated to the universal-algebraic\npro jection operations from Section 5.2.\n\n31\n\n\fsuch that A2\nv = I . Moreover, the fact that the pro jections associated to an edge of H resolve\nthe identity implies that they pairwise commute. Thus, the operators Av associated to the\nvertices of e also pairwise commute for every edge e of H . This means that the assignment\nXv (cid:55) Av thus dened is a valid operator assignment to any instance with constraint scopes\nHowever, the condition (cid:80)\ngiven by the hyperedges of H .\n2 (Av  I ) = I implied by condition 3 through the inverse\n1\nve\n2 (Av  I ) does not correspond directly to a constraint of the form\ntransformation Pv = 1\nPR (Av : v  e) = I for any Boolean relation R. This means that we cannot interpret the\nquantum realizability problem directly as an instance of a satisability problem via operator\nassignments over a Boolean constraint language. However, as it turns out, the problem that\nwe called 3-ALLOWS-QUANTUM is literally the same as the arbitrary Hilbert space version\n1-IN-3 SAT of the problem called 1-IN-3 SAT by Ji4 . Ji proved that 3SAT reduces to\n1-IN-3 SAT , and in view of Theorem 10, the question arises whether 3SAT also reduces\nto 1-IN-3 SAT , or to 3-ALLOWS-QUANTUM, which is the same. We show that it does.\nBefore we can do it, though, we need the following lemma that Ji proved for nite-\ndimensional Hilbert spaces (see Lemma 5 in [15]), and that we prove for all Hilbert spaces:\nLemma 11. Let H a Hilbert space. For every two projection operators P1 and P2 of H that\ncommute, there exist projection operators Q1 , Q2 , Q3 and Q4 of H such that\n\nP1 + Q1 + Q4 = I\nP2 + Q2 + Q4 = I\nQ1 + Q2 + Q3 = I .\nConversely, if P1 , P2 , Q1 , Q2 , Q3 , Q4 are projection operators of H that satisfy these equations,\nthen P1 and P2 commute.\n\nProof. To prove the rst claim, consider the pp-formula\n(Z1 , Z2 ) = U1U2U3U4 (R1/3 (Z1 , U1 , U4 )  R1/3 (Z2 , U2 , U4 )  R1/3 (U1 , U2 , U3 )),\n(34)\nwhere R1/3 = {(1, +1, +1), (+1, 1, +1), (+1, +1  1)}. It is straightforward to check that\nthis formula denes the full binary Boolean relation T = {1}2 . Now, let I be the instance\n((Z1 , Z2 ), T) and let J be the instance obtained from I as in Section 4.1. Let f be dened\nby f (Z1 ) = 1  2P1 and f (Z2 ) = 1  2P2 . Since P1 and P2 commute and the characteristic\npolynomial of T is the constant 1, the assignment f is a satisfying operator assigment for\nthe instance ((Z1 , Z2 ), T). By Lemma 5, there exists g that extends f and is a satisfying\n4There is an unfortunate clash in notation in that the problem 1-IN-3 SAT studied by Ji [15] is not\nthe same as the problem that we would call SAT (1-IN-3 SAT), where 1-IN-3 SAT is the Boolean relation\n{{(1, +1, +1), (+1, 1, +1), (+1, +1, 1)}}. Note that P1-IN-3 SAT (X1 , X2 , X3 ) = 3\n4 X1X2X3 + 1\n4 X1X2 +\n4 X1  1\n4 X2  1\n4 X1X3  1\n1\n4 X3 + 1\n4 X2X3 + 1\n4 , so the dierence is that, even though the characteristic polynomial\nequation P1-IN-3 SAT (X1 , X2 , X3 ) = I is satised by an operator assignment if and only if the resolution of\n2 (1  X1 ) + 1\n2 (1  X2 ) + 1\n2 (1  X3 ) = I is satised by the same operator assignment,\nthe identity equation 1\n2 (1X1 )+ 1\n2 (1X2 )+ 1\n2 (1X3 ) are by no means the same.\nthe two polynomials P1-IN-3 SAT (X1 , X2 , X3 ) and 1\n\n32\n\n\foperator assignment for J over H. Moreover, g is pairwise commuting on each block of\nJ . Take Qi = (1  g(Ui )/2 for i = 1, 2, 3, 4. Then Q1 , . . . , Q4 are pro jection operators,\nand P1 , P2 , Q1 , . . . , Q4 pairwise commute. We claim that they satisfy the equations in the\nlemma. To see this we apply Lemma 3. Since the equation PR1/3 (Z1 , U1 , U4 ) = 1 entails\nthe equation (1  Z1 )/2 + (1  U1 )/2 + (1  U4 )/2 = 1 over the Boolean domain {1}, and\nat the same time P1 , Q1 , Q4 pairwise commute, the equation PR1/3 (g(Z1 ), g(U1 ), g(U4 )) = I\nimplies P1 + Q1 + Q4 = I by Lemma 3. For the other two equations, the argument is the\nsame.\nFor the converse, we use the following easy to verify identities discovered via a computer\nsearch by Ji (see the proof of Lemma 5 in [15]):\n[P1 + Q1 + Q4  I , P1 + Q1 + Q3 ] = [P1 , Q3 ] + [Q4 , Q3 ]\n[P2 + Q2 + Q4  I , P1 ] = [P1 , P2 ] + [P1 , Q2 ]\n[Q1 + Q2 + Q3  I , P1 + Q4 ] = [Q2 , P1 ] + [Q3 , P1 ] + [Q3 , Q4 ],\nwhere [X, Y ] denotes the commutator polynomial X Y  Y X . Note that the equations in\nthe lemma imply that the left-hand sides are all 0. On the other hand, using the identity\n[X, Y ] + [Y , X ] = 0, the sum of the right-hand sides is [P1 , P2 ]. This gives [P1 , P2 ] = 0 and\nthus P1 and P2 commute.\nLemma 12. 3SAT poly-m-reduces to 3-ALLOWS-QUANTUM.\n\nProof. Schaefer proved that 3SAT is pp-denable from the constraint language given by\nthe single relation R1/3 = {(1, +1, +1), (+1, 1, +1), (+1, +1, 1)}. If in addition to R1/3\nwe allow also the relations R1/2 = {(1, +1), (+1, 1)} and R1/1 = {1}, then the pp-\ndenition can be assumed to have the property that each atom involves dierent variables\nand no constants. For example, an atom of the form R1/3 (X, X, Z ) can be replaced by\nR1/3 (X, Y , Z )  R1/2 (X, Y (cid:48) )  R1/2 (Y (cid:48) , Y ), where Y and Y (cid:48) are fresh quantied variables that\ndo not appear anywhere else in the formula.\nWe use this for the construction in Section 4.3. Let I be a 3SAT instance and let J\nbe the instance over the Boolean constraint language A = {R1/3 , R1/2 , R1/1 , T} given by the\nconstruction in Section 4.3, using the pp-denition of 3SAT from A. Starting at J we produce\nan instance of 3-ALLOWS-QUANTUM as follows: Each variable in J becomes a vertex\nin the hypergraph. Each constraint of the type ((Z1 , Z2 , Z3 ), R1/3 ) becomes a hyperedge\n{Z1 , Z2 , Z3}, each constraint of the type ((Z1 , Z2 ), R1/2 ) becomes a hyperedge {Z1 , Z2}, each\nconstraint of the type (Z, R1/1 ) becomes a singleton hyperedge {Z }, and each constraint\nof the type ((Z1 , Z2 ), T) introduces four fresh vertices U1 , U2 , U3 , U4 and three hyperedges\n{Z1 , U1 , U4}, {Z2 , U2 , U4} and {U1 , U2 , U3} in correspondance with the equations of Lemma 11\nwith Z1 , Z2 playing the role of P1 , P2 , and U1 , U2 , U3 , U4 playing the role of Q1 , Q2 , Q3 , Q4 .\nLet H be the hypergraph that results from this construction. We claim that for every\nHilbert space H, the instance I is satisable via operator assignments over H if and only if\nthe hypergraph H is quantum realizable over H.\nIn the forward direction, let f be a satisfying operator assignment for I over H. By\nLemma 6, there is a g that extends f and is a satisfying operator assignment for J over H.\n\n33\n\n\fRecall now that each vertex of H is indeed a variable of J , or an additional vertex of the\ntype U1 , U2 , U3 , U4 introduced by a constraint of the form ((Z1 , Z2 ), T). For each v of the rst\ntype, let Pv be the pro jection operator given by (1  g(v))/2. For each v of the second type,\nlet Pv be the pro jection given by Lemma 11 for the pro jection assignment P1 = PZ1 and\nP2 = PZ2 with U1 , U2 , U3 , U4 corresponding to Q1 , Q2 , Q3 , Q4 . Note that P1 and P2 commute,\nsince Z1 and Z2 appear together in ((Z1 , Z2 ), T) and hence g(Z1 ) and g(Z2 ) commute, so the\nlemma applies. We claim that this assignment of operators does the job.\nWe just need to check that the pro jection operators resolve the identity on every edge\nof H . For edges of the type {Z1 , Z2 , Z3} introduced by a constraint ((Z1 , Z2 , Z3 ), R1/3 ) we\nshow this with an application of Lemma 3: the equation PR1/3 (Z1 , Z2 , Z3 ) = 1 entails the\nequation (1Z1 )/2+(1Z2 )/2+(1Z3 )/2 = 1 over the Boolean domain {1}, and therefore,\nsince g(Z1 ), g(Z2 ), g(Z3 ) pairwise commute, the equation PR1/3 (g(Z1 ), g(Z2 ), g(Z3 )) = I\nimplies PZ1 + PZ2 + PZ3 = I by Lemma 3. For edges of the types {Z1 , Z2} or {Z } introduced\nby constraints of the types ((Z1 , Z2 ), R1/2 ) or (Z, R1/2 ), respectively, the argument is the\nsame. Finally, for the three edges that come from a constraint of the form ((Z1 , Z2 ), T), the\nclaim follows from Lemma 11. This completes one direction of the reduction.\nFor the other direction, let v (cid:55) Pv be an assignment of pro jection operators of H that\nwitnesses that H is quantum realizable. Recall again that each vertex v of H is a variable\nof J , or an additional vertex of the type U1 , U2 , U3 , U4 coming from a T-constraint. For\neach v of the rst type, let Av = I  2Pv . Each Av is a self-adjoint bounded linear operator\nthat squares to the identity. Moreover, any two variables of J that appear together in a\nconstraint that is not a T-constraint appear together as vertices in some edge of H . Therefore\nthe corresponding operators belong to the resolution of the identity of that edge, and a set of\npro jection operators that resolve the identity are pairwise orthogonal and hence commute.\nAlso, for any two variables of J that appear together in a constraint of the form ((Z1 , Z2 ), T),\nthe corresponding operators commute thanks to the conversely clause in Lemma 11. Thus,\nthe only thing left to do is checking that each constraint of J is satised.\nFor constraints of the type ((Z1 , Z2 , Z3 ), R1/3 ) this follows also from an application of\nLemma 3: the equation (1  Z1 )/2 + (I  Z2 )/2 + (I  Z3 )/2 = 1 entails the equation\nPR1/3 (Z1 , Z2 , Z3 ) = 1 over the Boolean domain {1}, and since AZ1 , AZ2 , AZ3 pairwise\ncommute, the equation PZ1 + PZ2 + PZ3 = I implies PR1/3 (AZ1 , AZ2 , AZ3 ) = I by Lemma 3.\nFor constraints of the type ((Z1 , Z2 ), R1/2 ) and (Z, R1/1 ) the argument is the same.\n\nIn combination with Theorem 10 we get the following.\n\nCorollary 2. 3-ALLOWS-QUANTUM and ALLOWS-QUANTUM are undecidable.\n\nThe same construction as in Lemma 12 starting from Corollary 1 gives the next.\n\nCorollary 3. There exists a hypergraph with edges of size at most three that is quantum\nrealizable on some Hilbert space, but not on a nite-dimensional Hilbert space.\n\nIt was mentioned earlier that 2-ALLOWS-QUANTUM is decidable in polynomial time.\nOne way to see this is by arguing that a hypergraph with edges of size two (i.e. a graph) is\nquantum realizable if and only if it is bipartite. Another is by reduction to 2SAT , which is\n\n34\n\n\fdecidable in polynomial time by Theorem 9. A close look reveals that, indeed, both proofs\nare the same.\n\nTheorem 11. 2-ALLOWS-QUANTUM is decidable in polynomial time.\nProof. We reduce to 2SAT . Given a hypergraph H , build the 2SAT instance that has\none variable Xv for each vertex in V (H ), two clauses Xu  Xv and Xu  Xv for every\nedge {u, v}  E (H ), and one unit clause Xu for each singleton edge {u} in E (H ).\nIt\nis straightforward to check that this reduction works through the usual conversion from\npro jection operators to involutions Pv (cid:55) 1  2Pv , and the usual conversion from involutions\nto pro jection operators Av (cid:55) (1  Av )/2.\n\n7 Closure Operations\n\nIn this section we develop a generalization of the concept of closure operation from Section 5.2\nfor sets of operator assignments. For every Boolean r-ary relation R, let R denote the set\nof fully commuting r-variable operator assignments over nite-dimensional Hilbert spaces\nthat satisfy the equation PR (X1 , . . . , Xr ) = I . We show that every closure operation for R\ngives a suitable closure operation for R . As an application, we show that the set of Boolean\nrelations that are pp-denable from a Boolean constraint language is not enlarged when we\nallow the existential quantiers to range over operator assignments.\n7.1 Closure Operations and pp-Denitions\nLet A be a Boolean constraint language and let R be a Boolean relation of arity r. Let\n = R1 (z1 )      Rm (zm ) be a conjunction of atoms with relations from A; i.e. each Ri\nis a relation from A, and each zi denotes a tuple of the appropriate arity made of rst-\norder variables or constants in {1}. Each such formula can be thought of as an instance\nover A. Concretely, it can be thought of as the instance I = ((Z1 , R1 ), . . . , (Zm , Rm )), where\neach Zi is obtained from the corresponding zi by replacing each rst-order variable x by a\ncorreponding variable X , and leaving all constants untouched.\nLet H be a nite-dimensional Hilbert space. We say that R is pp -denable from A over H\nif there is a pp-formula (x1 , . . . , xr ) = y1    ys ((x1 , . . . , xr , y1 , . . . , ys )) over A, where \nis a conjunction as above, such that, for every a1 , . . . , ar  {1}, the tuple (a1 , . . . , ar ) is in\nR if and only if the instance\n\n(35)\n(x1/a1 , . . . , xr /ar , y1/Y1 , . . . , ys/Ys )\nis satisable via operator assignments over H. We say that R is pp -denable from A if it is\npp -denable from A over a nite-dimensional Hilbert space. One of the goals of this section\nis to prove the following conservativity theorem:\n\nTheorem 12. Let A be a Boolean constraint language and let R be a Boolean relation. If\nR is pp -denable from A, then R is pp-denable from A.\n\n35\n\n\fIn order to prove this we need to develop the concept of closure operation for sets of oper-\nator assignments. Let r be a positive integer. A relation of operator assignments of arity r is\na set of fully commuting operator assignments for a xed set of r variables X1 , . . . , Xr . Note\nthat we do not require that all operator assignments come from the same Hilbert space. The\nrelation is called Boolean if all assignments in it come from a Hilbert space of dimension 1;\ni.e., from C. If H is a Hilbert space and R  {1}r is a Boolean relation of arity r, we write\nRH for the set of fully commuting operator assignments for X1 , . . . , Xr over H that satisfy\nthe polynomial equation PR (X1 , . . . , Xr ) = I , where PR is the characteristic polynomial\nof R. We write R for the union of RH over all nite-dimensional Hilbert spaces. If A is a\nset of Boolean relations, dene A = {R : R  A}.\nLet H1 , . . . , Hm and H be Hilbert spaces, and let f be a function that takes as inputs m\nmany linear operators, one on each Hi , and produces as output a linear operator on H. We\nsay that f is an operation if the following conditions are satised.\n1. If A1 , . . . , Am are 1-variable operator assignments over H1 , . . . , Hm , then f (A1 , . . . , Am )\nis a one-variable operator assignment over H.\n2. If (A1,1 , A1,2 ), . . . , (Am,1 , Am,2 ) are commuting 2-variable operator assignments over\nH1 , . . . , Hm , then (f (A1,1 , . . . , Am,1 ), f (A1,2 , . . . , Am,2 )) is a commuting two-variable\noperator assignment over H.\n\nLet R be a relation of operator assignments of arity r and let F be a collection of operations\nas above. We say that R is invariant under F if for each f  F the following additional\ncondition is also satised.\n\n3. If (A1,1 , . . . , A1,r ), . . . , (Am,1 , . . . , Am,r ) are fully commuting r-variable operator assign-\nments over H1 , . . . , Hm , respectively, and (Ai,1 , . . . , Ai,r ) belongs to R for every i  [m],\nthen (f (A1,1 , . . . , Am,1 ), . . . , f (A1,r , . . . , Am,r )) is a fully commuting r-variable operator\nassignment over H and belongs to R.\n\nIf A is a set of relations of operator assignments, we say that A is invariant under F if every\nrelation in A is invariant under F . We also say that F is a closure operation of A. A Boolean\nclosure operation of A is one in which the dimensions of all Hilbert spaces involved are 1;\ni.e., they are C. Before we prove the main technical result of this section, we work out a\nmotivating example.\n\n7.2 Example: LIN\nIn this section we study whether R for R = LIN has some closure operation. In the 0-1-\nrepresentation of Boolean values, the function (X1 , X2 , X3 ) (cid:55) X1  X2  X3 is a Boolean\nclosure operation of LIN. In the 1-representation of Boolean values, this is (X1 , X2 , X3 ) (cid:55)\nX1X2X3 . It is tempting to think that the map (X1 , X2 , X3 ) (cid:55) X1X2X3 applied to linear\noperators on a Hilbert space could already be a closure operation for LIN . However, the\nsolution to the Mermin-Peres magic square equations (33) is a counterexample: each row\nequation is a parity equation with even right-hand side that is satised, but the composition\n\n36\n\n\f(36)\n\nof columns by the operation X1X2X3 gives an operator assignment that satises a parity\nequation with odd right-hand side.\nIt turns out that the correct way of generalizing the Boolean closure operation is not\nby taking ordinary products, but Kronecker products. Let F be the function that takes\nany three linear operators X1 , X2 , X3 over the same nite-dimensional Hilbert space and is\ndened by\nF (X1 , X2 , X3 ) = X1  X2  X3 .\nNow let (A1 , . . . , Ar ), (B1 , . . . , Br ) and (C1 , . . . , Cr ) be three fully commuting r-variable op-\nerator assignments over a nite-dimensional Hilbert space, say Cd . We think of all operators\nas matrices. Take Di = F (Ai , Bi , Ci ) for i = 1, . . . , r . These are Hermitian matrices since\nthe operations of conjugate transposition and Kronecker product commute. Also\nDiDj = (AiAj )  (BiBj )  (CiCj ) = (Aj Ai )  (Bj Bi )  (Cj Ci ) = DjDi\n(37)\ni )  (C 2\ni )  (B 2\nso D1 , D2 , D3 pairwise commute. Equation (37) also gives D2\ni = (A2\ni ) =\nI  I  I = I , so (D1 , . . . , Dr ) is a fully commuting r-variable operator assignment. Next\nwe consider a relation in LIN, say R = {(a1 , . . . , ar )  {1}r : a1    ar = b}, with b  {1}.\nNote that its characteristic polynomial is PR (X1 , . . . , Xr ) = b  X1    Xr . We show that if\nPR (A1 , . . . , Ar ) = PR (B1 , . . . , Br ) = PR (C1 , . . . , Cr ) = I , then also PR (D1 , . . . , Dr ) = I .\n(cid:33)\n(cid:33)\n(cid:33)\n(cid:32) r(cid:89)\n(cid:32) r(cid:89)\n(cid:32) r(cid:89)\nr(cid:89)\nWe have\ni=1\ni=1\ni=1\ni=1\nHence PR (D1 , . . . , Dr ) = b2I = I . This shows that F is a closure operation of LIN .\nOne consequence of the existence of F as a closure operation of LIN is that the binary\nOR relation OR2 = {1}2 \\ {(+1, +1)} is not pp -denable from LIN.\nTheorem 13. OR2 is not pp -denable from LIN.\nNote that this follows from the more general statement in Theorem 12 since it is known\nthat the Boolean relation OR2 is not pp-denable from LIN. Indeed, OR2 is not closed under\nthe (idempotent) Boolean closure operation (X1 , X2 , X3 ) (cid:55) X1X2X3 of LIN, since (1, 1),\n(+1, 1) and (1, +1) are all three in the relation OR2 , but (+1, +1) is not in OR2 . The\nundenability of OR2 from LIN by a pp-formula (with or without constants) follows from\nthe easy direction in Geigers Theorem 4. Since we prove Theorem 12 below, we omit a proof\nof Theorem 13 at this point.\n\n= (bI )  (bI )  (bI ) = b3I = bI .\n\nDi =\n\n\n\nAi\n\n\n\nBi\n\nCi\n\n(38)\n\n7.3 Generalization\n\nWe show that every Boolean closure operation gives a closure operation for relations of\noperator assignments over nite-dimensional Hilbert spaces.\nIn the following, if Xi is a\nlinear operator on a Hilbert space, X 0\ni and X 1\ni are to be interpreted as the identity operator\nand Xi itself, respectively. If S is a set, we write S (i) for the 0-1-indicator of the fact that i\nis in S ; i.e. S (i) = 1 if i is in S , and S (i) = 0 if i is not in S .\n\n37\n\n\fTheorem 14. Let A be a Boolean constraint language and let f : {1}m  {1} be a\nBoolean closure operation of A. Then the function on linear operators on nite-dimensional\n(cid:88)\n(cid:79)\nHilbert spaces dened by\ni[m]\nS[m]\nis a closure operation of A . Moreover, F (a1I , . . . , amI ) = f (a1 , . . . , am )I holds for every\n(a1 , . . . , am )  {1}m .\n\nF (X1 , . . . , Xm ) =\n\nX S (i)\ni\n\nf (S )\n\n(39)\n\ni.e., it satises conditions 1 and 2 in the\nProof. First we show that F is an operation;\ndeniton of operation. Let X1 , . . . , Xm be 1-variable operator assignments over H1 , . . . , Hm .\n(cid:32)(cid:79)\n(cid:33)\nIn particular, X1 , . . . , Xm are all self-adjoint linear operators. Thus, for S  [m] we have\n(cid:79)\n(cid:79)\ni[m]\ni[m]\ni[m]\n\n(X \ni )S (i) =\n\n(Xi )S (i) .\n\n(Xi )S (i)\n\n(40)\n\n=\n\nX S (i)\ni\n\n,\n\n(41)\n\nX T (i)\ni\n\n=\n\nX U (i)\ni\n\nX U (i)\ni\n\n(X 2\ni )V (i)\n\nF (X1 , . . . , Xm )2 =\n\nFrom this it follows that F (X1 , . . . , Xm ) is self-adjoint since each f (S ) is a real number. Next\nwe want to show that F (X1 , . . . , Xm )2 = I . First note that for S, T  [m], their symmetric\n(cid:32)(cid:79)\n(cid:33)(cid:32)(cid:79)\n(cid:33)\n(cid:32)(cid:79)\n(cid:33)(cid:32)(cid:79)\n(cid:33)\n(cid:32)(cid:79)\n(cid:33)\ndierence U = ST and their intersection V = S  T , we have\ni[m]\ni[m]\ni[m]\ni[m]\ni[m]\ni = I for all i  [m]. Now we can expand\nwhere the last equality follows from the fact that X 2\n(cid:33)\n(cid:33)(cid:32)(cid:79)\n(cid:32)(cid:79)\n(cid:88)\n(cid:88)\nF (X1 , . . . , Xm )2 as follows\n(cid:32)(cid:79)\n(cid:33)\nf (S ) f (T )\nX S (i)\n(cid:88)\n(cid:88)\ni\nT [m]\nS[m]\ni[m]\ni[m]\n(cid:33)\n(cid:32)(cid:79)\nf (S ) f (SU )\n(cid:88)\n(cid:88)\ni[m]\nS[m]\nU [m]\n(cid:33) (cid:88)\n(cid:32)(cid:79)\nf (S ) f (SU )\n(cid:88)\nU [m]\nS[m]\ni[m]\nX U (i)\n=\ni\nU [m]\ni[m]\nS[m]\nBy the Convolution Formula (3) we have(cid:88)\nf (S ) f (SU ) = (cid:98)f 2 (U ).\nS[m]\n\nf (S ) f (SU ).\n\nX U (i)\ni\n\nX U (i)\ni\n\nX T (i)\ni\n\n=\n\n=\n\n(46)\n\n=\n\n(42)\n\n(43)\n\n(44)\n\n(45)\n\n38\n\n\f(cid:26) 1\nSince the range of f is {1}, the function f 2 is identically 1, from which it follows that\n(cid:98)f 2 (U ) =\nif U = \nif U (cid:54)= \n0\n(cid:32)(cid:79)\n(cid:33)\nby the uniqueness of the Fourier transform. Back into (45), this gives\ni[m]\nas was to be proved. Finally, if S  [m] and (X1 , Y1 ), . . . , (Xm , Ym ) are such that Xi and Yi\n(cid:33)\n(cid:33)(cid:32)(cid:79)\n(cid:32)(cid:79)\ncommute for every i  [m], then\n(cid:79)\n(cid:79)\ni[m]\ni[m]\ni[m]\ni[m]\n\n(cid:33)(cid:32)(cid:79)\ni[m]\n\n(cid:32)(cid:79)\ni[m]\n\nF (X1 , . . . , Xm )2 =\n\n(XiYi )S (i) =\n\n(YiXi )S (i) =\n\nX S (i)\ni\n\n.\n\nY S (i)\ni\n\n=\n\n(i)\ni\n\nX\n\n= I\n\n(49)\n\n(50)\n\n(47)\n\n(48)\n\nX S (i)\ni\n\n=\n\nY S (i)\ni\n\n(cid:33)\n\nIt follows that F (X1 , . . . , Xm ) and F (Y1 , . . . , Ym ) commute. This completes the proof that\nF is an operation.\nNext we show that for every relation R in A, the operator assignment relation R is\ninvariant under F . Let r be the arity of R and let PR (X1 , . . . , Xr ) be the characteristic\npolynomial of R. Let (A1,1 , . . . , A1,r ), . . . , (Am,1 , . . . , Am,r ) be r-variable operator assignments\nover nite-dimensional Hilbert spaces H1 , . . . , Hm . We may assume that Hi = Cdi where di\nis the dimension of Hi . From now on we switch to the language of matrices.\nAssume that all the assignments (A1,1 , . . . , A1,r ), . . . , (Am,1 , . . . , Am,r ) are in R . In par-\nticular, each sequence Ai,1 , . . . , Ai,r is a fully commuting assignment of Hermitian matrices\nand PR (Ai,1 , . . . , Ai,r ) = I . The Strong Spectral Theorem (i.e. Theorem 1) applies, so\nAi,1 , . . . , Ai,r simultaneously diagonalize. Let Ui be a unitary matrix of Hi that achieves\nthat, and let Di,j = U Ai,j U  for j  [r] be the resulting diagonal matrices. From A2\ni,j = I\nand U U = U U  = I we conclude that D2\ni,j = I and hence each entry in the diagonal of Di,j\nis +1 or 1. For c  [di ], let Di,j (c) denote the entry in position c of the diagonal of Di,j . The\nhypotheses of Lemma 1 apply to the pairs (Ai,1 , Di,1 ), . . . , (Ai,r , Di,r ), so PR (Ai,1 , . . . , Ai,r )\nand PR (Di,j , . . . , Di,r ) are similar matrices. As PR (Ai,1 , . . . , Ai,r ) = I , and the only matrix\nthat is similar to I is I itself, we get PR (Di,1 , . . . , Di,r ) = I . In particular\nPR (Di,1 (c), . . . , Di,r (c)) = 1\nfor every c  [di ]. This will be of use later.\ndo so by showing that (cid:88)\n(cid:89)\nOur next goal is to show that PR (F (A1,1 , . . . , Am,1 ), . . . , F (A1,r , . . . , Am,r )) = I and we\nF (A1,j , . . . , Am,j ) = I .\njT\nT [r ]\n\nR(T )\n\n(51)\n\n(52)\n\n39\n\n\f=\n\nUi\n\n.\n\n(54)\n\nFor xed T  [r], let AT = (cid:81)\njT F (A1,j , . . . , Am,j ) be the matrix product appearing in the\nleft-hand side of (52). Let t = |T |. By rst expanding on the denition of F and then\n(cid:89)\n(cid:88)\n(cid:79)\n(cid:88)\n(cid:89)\n(cid:89)\n(cid:79)\ndistributing the product over the sum we get\nf (S )\nf (S (t))\n(Ai,j )S (i) =\n(Ai,j )S (t)(i) .\nAT =\n(53)\n(cid:78)\nFor xed T  [r] and S : T  2[m] , let BT ,S = (cid:81)\njT\njT\njT\ni[m]\nS[m]\ni[m]\nS :T 2[m]\nappearing in the right-hand side of (53). By distributing (cid:81) over (cid:78) and applying Ai,j =\ni (Ai,j )S (t)(i) be the matrix product\nj\n(cid:32)(cid:89)\n(cid:33)S (t)(i)\n(cid:33)S (t)(i)\n(cid:32)(cid:89)\n(cid:32)\n(cid:33)\nU \ni Di,j Ui in (53) we get\n(cid:79)\n(cid:79)\nU \n(U \ni Di,j Ui )\nDi,j\ni\njT\njT\ni[m]\ni[m]\n(cid:32) (cid:88)\n(cid:33)S (t)(i)(cid:33)\n(cid:32)(cid:89)\n(cid:79)\n(cid:89)\nHence\nAT = U \nf (S (j ))\nfor U = (cid:78)\nDi,j\nU,\njT\njT\ni[m]\nDi,j is a di  di diagonal matrix, M is a d  d diagonal matrix with d = (cid:81)\nS :T 2[m]\ni[m] Ui . Let M denote the matrix sitting within U  and U in line (55). As each\ni[m] di . We think of\nthe entries in the diagonal of M as indexed by tuples c = (c1 , . . . , cm ) from [d1 ]      [dm ].\n(cid:88)\n(cid:89)\n(cid:89)\n(cid:89)\nLet M (c) denote the entry in position c of the diagonal of M . Then\nM (c) =\njT\njT\ni[m]\nS :T 2[m]\n(cid:89)\n(cid:89)\n(cid:89)\n(cid:88)\nFactoring back the product over j  T , the right-hand side in (56) reads\nf (S )\n(Di,j (ci ))S (i) =\njT\njT\ni[m]\nS[m]\nequations (56) and (57) give M (c) = (cid:81)\nFor xed j  [r] and c  [d1 ]      [dm ], let Xj,c = f (D1,j (c1 ), . . . , Dm,j (cm )) so that\njT Xj,c . From (51) and the fact that f is a Boolean\n(cid:88)\n(cid:89)\n(cid:88)\nclosure operator of R, the tuple (X1,c , . . . , Xr,c ) belongs to the relation R. Thus\nXj,c = PR (X1,c , . . . , Xr,c ) = 1.\nR(T )\nR(T )M (c) =\nSince this holds for every diagonal entry of M , we get (cid:80)\njT\nT [r ]\nT [r ]\n(cid:32) (cid:88)\n(cid:33)\n(cid:88)\ntogether, the left-hand side of our goal (52) evaluates to\nT [r ]\nT [r ]\n\nR(T )M = I . Putting it all\n\nU = U  (I )U = I .\n\nR(T )U M U = U \n\n(Di,j (ci ))S (t)(i) .\n\nf (D1,j (c1 ), . . . , Dm,j (cm )).\n\n(57)\n\nf (S (t))\n\n(58)\n\n(59)\n\n(55)\n\n(56)\n\nBT ,S =\n\nT [r]\n\nR(T )M\n\n40\n\n\fThis gives (52) as desired.\n(cid:32)(cid:89)\n(cid:33)\nIn order to prove the moreover clause of the theorem, observe that if S  [m] and\n(a1 , . . . , am )  {1}m , then (cid:79)\n(aiI )S (i) =\nai\nI ,\nand in the right-hand side the identity matrix has dimension d  d for d = (cid:81)\niS\ni[m]\nwhere in the left hand side the identity matrices have dimensions d1 , . . . , dm , respectively,\n(cid:33)\n(cid:32) (cid:88)\n(cid:89)\ni[m] di . It\nfollows that\nf (S )\nI = f (a1 , . . . , am )I .\niS\nS[m]\n\nF (a1I , . . . , amI ) =\n\nai\n\n(60)\n\n(61)\n\nThis completes the proof of the theorem.\n\n7.4 Finale\n\nBefore we prove Theorem 12, we need the following straightforward fact about the role of\nconstants in pp-denitions.\n\nLemma 13. Let A be a Boolean constraint language, let R be Boolean a relation, and let\nA+ = A  {{+1}, {1}}. The fol lowing two statements hold.\n1. R is pp-denable from A if and only if it is pp-denable without constants from A+ .\n2. R is pp -denable from A if and only if it is pp -denable without constants from A+ .\n\nProof. In both cases, for the only if  part it suces to replace each occurrence of a constant\nin the quantier-free part of the pp-formula by a new existentially quantied variable Z , and\nforce it to belong to the corresponding new unary relation in A+ by an additional conjunct: if\nZ replaces the constant 1, we force Z to belong {1} by a new conjunct, and it Z replaces\nthe constant +1, we force it to belong {+1} by a new conjunct. In both cases too, the if \npart follows from the reverse construction: replace each occurrence of a variable that appears\nwithin the scope of one of the new unary relations in A+ by the corresponding constant, and\nremove the conjuncts that involve the new unary relations. That these transformations are\ncorrect follows directly from the denitions and the fact that both I and I commute with\nany operator.\n\nWe are ready to prove Theorem 12.\nProof of Theorem 12. Assume R is pp -denable from A. By Lemma 13, the relation R is\nalso pp -denable without constants from A+ = A  {{+1}, {1}}. Let r be the arity of R\nand let (x1 , . . . , xr ) be the pp-formula without constants that pp -denes R from A+ . By\nGeigers Theorem 4 and Lemma 13 it suces to show that R is invariant under all Boolean\nclosure operations of A+ .\nLet f : {1}m  {1} be a Boolean closure operation of A+ . By Theorem 14, the\nfunction F is a closure operation of A+ \n. Let (a1,1 , . . . , a1,r ), . . . , (am,1 , . . . , am,r ) be tuples in\n\n41\n\n\fR and let aj = f (a1,j , . . . , am,j ) for every j  [m]. We need to show that (a1 , . . . , ar ) is also\nin R. Let (x1 , . . . , xr , y1 , . . . , ys ) be the quantier-free part of  and consider the instance\nover A+ that is given by\n\n(x1/ai,1 , . . . , xr /ai,r , y1/Y1 , . . . , ys/Ys )\n\n(62)\n\nas described in the begining of this section. Since the tuple (ai,1 , . . . , ai,r ) is in R and\n pp -denes R, the instance in (62) is satisable via operator assignments over a nite-\ndimensional Hilbert space for every i  [m]. Let Bi,1 , . . . , Bi,s be such a satisfying operator\nassignment for every i  [m]. Since I and I commute with any operator, this means that\nai,1I , . . . , ai,r I , Bi,1 , . . . , Bi,s is a satisfying operator assignment of\n\n(63)\n(x1/X1 , . . . , xr /Xr , y1/Y1 , . . . , ys/Ys )\nfor every i  [m]. Let Aj = F (a1,j I , . . . , am,j I ) and Bj = F (B1,j , . . . , Bm,j ). As F is a closure\noperation of A+ \n, the tuple A1 , . . . , Ar , B1 , . . . , Bs is a satisfying operator assignment for (63).\nMoreover, from the moreover clause in Theorem 14 we know that Aj = f (a1,j , . . . , am,j )I =\naj I for every j  [m]. Thus, the instance\n\n(x1/a1 , . . . , xr /ar , y1/Y1 , . . . , ys/Ys )\n\n(64)\n\nis satisable via operator assignments over a nite-dimensional Hilbert space; the nite-\ndimensional operator assignment B1 , . . . , Bs satises it. As  pp -denes R, it follows that\n(a1 , . . . , ar ) is in R, as was to be shown.\n\nAcknowledgments. We are grateful to Heribert Vollmer for sharing with us Steen Reiths\ndiagram of Posts lattice (Figure 1). This work was initiated and part of the research was\ncarried out while all three authors were in residence at the Simons Institute for the Theory\nof Computing during the fall of 2016, where they participated in the program on Logical\nStructures in Computation. The research of Albert Atserias was partially funded by the\nEuropean Research Council (ERC) under the European Unions Horizon 2020 research and\ninnovation programme, grant agreement ERC-2014-CoG 648276 (AUTAR), and by MINECO\nthrough TIN2013-48031-C4-1-P (TASSAT2); the research of Simone Severini was partially\nfunded by The Royal Society, Engineering and Physical Sciences Research Council (EPSRC),\nand the National Natural Science Foundation of China (NSFC).\n\nReferences\n\n[1] Antonio Acn, Tobias Fritz, Anthony Leverrier, and Ana Belen Sainz. A combinatorial\napproach to nonlocality and contextuality. Communications in Mathematical Physics,\n2(334):533628, 2015.\n\n42\n\n\f[2] John S Bell. On the problem of hidden variables in quantum mechanics. Reviews of\nModern Physics, 38(3):447, 1966.\n\n[3] V. G. Bodnarchuk, L. A. Kaluzhnin, V. N. Kotov, and B. A. Romov. Galois theory for\nPost algebras. I. Cybernetics, 5(3):243252, 1969.\n\n[4] Elmar Bohler, Nadia Creignou, Steen Reith, and Heribert Vollmer. Playing with\nboolean blocks, part I: Posts lattice with applications to complexity theory. In ACM\nSIGACT-Newsletter, 2003.\n\n[5] Elmar Bohler, Nadia Creignou, Steen Reith, and Heribert Vollmer. Playing with\nboolean blocks, part II: Constraint satisfaction problems. In ACM SIGACT-Newsletter,\n2004.\n\n[6] Richard Cleve, Li Liu, and William Slofstra. Perfect commuting-operator strategies for\nlinear system games. arXiv preprint arXiv:1606.02278, 2016.\n\n[7] Richard Cleve and Ra jat Mittal. Characterization of binary constraint system games. In\nAutomata, Languages, and Programming - 41st International Col loquium, ICALP 2014,\nCopenhagen, Denmark, July 8-11, 2014, Proceedings, Part I, pages 320331, 2014.\n\n[8] Brian A. Davey and Hilary A. Priestley. Introduction to lattices and Order. Cambridge\nUniversity Press, 2002.\n\n[9] Tomas Feder and Moshe Y Vardi. The computational structure of monotone monadic\nSNP and constraint satisfaction: A study through datalog and group theory. SIAM\nJournal on Computing, 28(1):57104, 1998.\n\n[10] Gerald B. Folland. A Course in Abstract Harmonic Analysis. Studies in Advanced\nMathematics. Taylor & Francis, 1994.\n\n[11] Gerald B. Folland. Real Analysis: Modern Techniques and Their Applications. Pure\nand Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts. Wiley,\n2013.\n\n[12] Tobias Fritz. Quantum logic is undecidable. arXiv preprint arXiv:1607.05870, 2016.\n\n[13] David Geiger. Closed systems of functions and predicates. Pacic journal of mathe-\nmatics, 27(1):95100, 1968.\n\n[14] Paul R. Halmos. Introduction to Hilbert Space and the Theory of Spectral Multiplicity.\nBenediction Classics, 2016.\n\n[15] Zhengfeng Ji. Binary constraint system games and locally commutative reductions.\narXiv preprint arXiv:1310.3794, 2013.\n\n[16] N. David Mermin. Simple unied form for the ma jor no-hidden-variables theorems.\nPhysical Review Letters, 65(27):3373, 1990.\n\n43\n\n\f[17] N. David Mermin. Hidden variables and the two theorems of John Bell. Reviews of\nModern Physics, 65(3):803, 1993.\n\n[18] Ryan ODonnell. Analysis of Boolean Functions. Cambridge University Press, 2014.\n\n[19] Christos H Papadimitriou. Computational complexity. Addison Wesley, 1994.\n\n[20] Asher Peres. Incompatible results of quantum measurements. Physics Letters A, 151(3-\n4):107108, 1990.\n\n[21] Emil L Post. The Two-Valued Iterative Systems of Mathematical Logic, volume 5 of\nAnnals of Mathematical Studies. Princeton University Press, 1941.\n\n[22] Thomas J. Schaefer. The complexity of satisability problems. In Proceedings of the\n10th Annual ACM Symposium on Theory of Computing, May 1-3, 1978, San Diego,\nCalifornia, USA, pages 216226, 1978.\n\n[23] Uwe Schoning. Logic for computer scientists. Springer Science & Business Media, 2008.\n\n[24] William Slofstra. Tsirelsons problem and an embedding theorem for groups arising\nfrom non-local games. arXiv preprint arXiv:1606.03140, 2016.\n\n44\n\n\f", 
        "tag": "Logic in Computer Science", 
        "link": "https://arxiv.org/list/cs.LO/new"
    }, 
    {
        "text": " \n\n  \n\nAbstract  -  This  paper  describes  the  design  and \nsimulation of an 8-bit dedicated processor for calculating the \nSine  and  Cosine  of  an  Angle  using  CORDIC  Algorithm \n(COordinate  Rotation  DIgital  Computer),  a  simple  and \nefficient algorithm to calculate hyperbolic and trigonometric \nfunctions.  We  have  proposed  a  dedicated  processor  system, \nmodeled  by  writing  appropriate  programs  in  VHDL,  for \ncalculating  the  Sine  and  Cosine  of  an  angle.  System \nsimulation  was  carried  out  using  ModelSim  6.3f  and  Xilinx \nISE Design Suite 12.3. A maximum frequency of 81.353 MHz \nwas  reached  with  a  minimum  period  of  12.292  ns.  126  (3%) \nslices  were  used.  This  paper  attempts  to  survey  the  existing \nCORDIC  algorithm  with  an  eye  towards  implementation  in \nField  Programmable  Gate  Arrays \n(FPGAs).  A  brief \ndescription  of  the  theory  behind  the  algorithm  and  the \nderivation  of  the  Sine  and  Cosine  of  an  angle  using  the \nCORDIC  algorithm  has  been  presented.  The  system  can  be \nimplemented  using  Spartan3  XC3S400  with  Xilinx  ISE  12.3 \nand VHDL.  \n \nKeywords  -  CORDIC,  VHDL,  dedicated  processor, \ndatapath, finite state machine. \n \n \n\nProceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)  \nIEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 \nDesign and Simulation of an 8-bit Dedicated Processor for calculating the Sine \nand Cosine of an Angle using the CORDIC Algorithm \n \n \nAman Chadha1,a, Divya Jyoti2,b and M. G. Bhatia3,c \n1,2 Thadomal Shahani Engineering College, Bandra (W), Mumbai, INDIA \n3 Ameya Centre for Robotics and Embedded Technology, Andheri (W), Mumbai, INDIA \na aman.x64@gmail.com, b dj.rajdev@gmail.com, c mgbhatia@acret.in \n \n \nthese technologies into a tool for exploring and evaluating \n \nmicro-architectural  designs \n[4].  Because  of \ntheir \n \nadvantage of real-time in-circuit reconfigurability, FPGAs \n \nbased  processors  are  flexible,  programmable  and  reliable \n[1].  Thus,  higher  speeds  can  be  achieved  by  these \ncustomized hardware  solutions at competitive costs. Also, \nvarious  simple  and  hardware-efficient  algorithms  exist \nwhich  map  well  onto  these  chips  and  can  be  used  to \nenhance  speed  and  flexibility  while  performing  the \ndesired signal processing tasks [1],[2],[3]. \n \nOne  such  simple  and  hardware-efficient  algorithm  is \nCOordinate  Rotation  DIgital  Computer  (CORDIC)  [5]. \nPrimarily  developed  for  real-time  airborne  computations, \nit  uses  a  unique  computing  technique  highly  suitable  for \nsolving  the  trigonometric  relationships  involved  in  plane \nco-ordinate  rotation  and  conversion  from  rectangular  to \npolar  form.  John  Walther  extended  the  basic  CORDIC \ntheory  to  provide  solution  to  and  implement  a  diverse \nrange  of  functions  [7].  It  comprises  a  special  serial \narithmetic  unit  having \nthree  shift \nregisters, \nthree \nadders/subtractors,  Look-Up  Table  (LUT)  and  special \ninterconnections.  Using  a  prescribed  sequence  of \nconditional  additions  or  subtractions, \nthe  CORDIC \narithmetic  unit  can  be  designed  to  solve  either  of  the \nfollowing equations: \n(\nY \n K Ycos \n \n=\n\n(\n K Xcos \n \nX \n=\n\nWhere, K is a constant.  \n \nBy making  slight  adjustments  to  the  initial  conditions \nand  the  LUT  values,  it  can  be  used  to  efficiently \nimplement \ntrigonometric, \nhyperbolic, \nexponential \nfunctions,  coordinate  transformations  etc.  using  the  same \nhardware.  Since  it  uses  only  shift-add  arithmetic,  the \nVLSI  implementation  of  such  an  algorithm  is  easily \nachievable [4]. \n \n\n \n \nOver  the  years,  the  field  of  Digital  Signal  Processing \n(DSP) \nhas \nbeen \nessentially \ndominated \nby \nMicroprocessors.  This  is  mainly  because  of  the  fact  that \nthey provide designers with the advantages of single cycle \nmultiply-accumulate \ninstruction  as  well  as  special \naddressing  modes  [4].  Although  these  processors  are \ncheap  and  flexible,  they  are  relatively  less  time-efficient \nwhen  it  comes  to  performing  certain  resource-intensive \nsignal  processing  tasks,  e.g.,  Image  Compression,  Digital \nCommunication  and  Video  Processing.  However  as  a \ndirect  consequence  of  rapid  advancements  in  the  field  of \nVLSI  and  IC  design,  special  purpose  processors  with \ncustom-architectures  are  designed  to  perform  certain \nspecific  tasks.  They  need  fewer  resources  and  are  less \ncomplex \nthan \ntheir  general  purpose  counterparts. \nInstructions  for  performing  a  task  are  hardwired  into  the \nprocessor  itself,  i.e.,  the  program  is  built  right  into  the \nmicroprocessor circuit itself [2]. Due to this, the execution \ntime  of  the  program  is  considerably  less  than  that  if  the \ninstructions  are  stored  in  memory.  Emerging  high  level \nhardware  description  and  synthesis \ntechnologies \nin \nconjunction  with  Field  Programmable  Gate  Arrays \n(FPGAs)  have  significantly  lowered  the  threshold  for \nhardware  development  as  opportunities  exist  to  integrate \n\n \n \nThe  CORDIC  algorithm  is  an  iterative  technique \nbased  on  the  rotation  of  a  vector  which  allows  many \ntranscendental  and \ntrigonometric \nfunctions \nto  be \ncalculated.  The  key  aspect  of  this  method  is  that  it  is \nachieved  using  only  shifts,  additions/subtractions  and \ntable  look-ups  which  map  well  into  hardware  and  are \nideal for FPGA implementation. The CORDIC algorithms \npresented in this paper are well known in the research and \nsuper-computing circles. \n \n\nII.  CORDIC ALGORITHM \n\nI.  INTRODUCTION \n\n+\n\n\n)\n Xsin \n\n)\n Ysin \n\n\n \n\n(1) \n\n \n\n1 \n\n\f \n\n-2i\n\nA  = \nn\n\nA.  Algorithm Fundamentals \n \n\nVector  rotation \nthe \nto  obtain \nthe  first  step \nis \ntrigonometric  functions.  It  can  also  be  used  for  polar  to \nrectangular  and  vice-versa  conversions, \nfor  vector \nmagnitude,  and  as  a  building  block  in  certain  transforms \nsuch  as  the  Discrete  Fourier  Transform  (DFT)  and \nDiscrete  Cosine  Transform  (DCT).  The  algorithm  is \nderived from Givens [6] rotation as follows: \n\nProceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) \nIEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 \n \nsystem or treated as part of a system processing gain. That \nproduct  approaches  0.6073  as  the  number  of  iterations \nreaches  infinity.  Therefore,  the  rotation  algorithm  has  a \ngain, An   1.65. The  exact  gain  depends  on  the  number  of \niterations, and follows the following equation: \n1 + 2\nn\nThe  angle  of  a  composite  rotation  is  realized  by  the \nsequence  of  the  directions  of  the  elementary  rotations. \nThat  sequence  can  be  represented  by  a  decision  vector. \nThe  set  of  all  possible  decision  vectors  is  an  angular \nmeasurement  system  based  on  binary  arctangents. \nConversions  between  this  angular  system  and  any  other \ncan  easily  be  accomplished  using  a  LUT.  A  better \nconversion  method  uses  an  additional  adder-subtractor \nthat  accumulates  the  elementary  rotation  angles  post \niteration.  The  angle  accumulator  adds  a  third  difference \nequation to the CORDIC algorithm: \n(\n)\n1\n2\n z  \n d tan\n\n(6) \ni + 1\ni\ni\nAs  discussed  above,  when  the  angle  is  in  the \narctangent  base,  this  extra  element  is  not  needed.  The \nCORDIC  rotator  is  normally  operated  in  one  of  two \nmodes, i.e., the Rotation mode and the Vectoring mode.  \n  \nB.  Rotation Mode \n \n\n(5) \n\n\n\n=\n\nz\n\n \n\n \n\n\n\ni\n\nThe first mode of operation, called rotation by Volder \n[5],[4], rotates the input vector by a specified angle (given \nas an argument). Here, the angle accumulator is initialized \nwith  the  desired  rotation  angle.  The  rotation  decision \nbased on the sign of the residual angle is made to diminish \nthe  magnitude  of \nthe  residual  angle \nin \nthe  angle \naccumulator.  If  the  input angle  is already expressed  in  the \nbinary  arctangent  base,  the  angle  accumulator  is  not \nneeded [4],[1]. The equations for this are: \n\nx  \ni 1\n+\ny  \ni 1\n+\nz\n \n\ni + 1\n\n=\n=\n=\n\n x  \ni\n y  \ni\n z  \ni\n\n\n+\n\n\n-i\n\n-i\n\n y d 2\n\n\ni\ni\n x d 2\n\n\ni\ni\n1\n\n d tan\n\ni\n\n \n\n(\n\n\n\ni\n\n2\n\n)\n\n(7) \n\n(2) \n\n \n \nFig. 1.  Illustration of the CORDIC algorithm \nIn Fig. 1, the diagonal blue line is at an angle \n1  above \nthe  horizontal.  The  diagonal  red  line  is  actually  the  blue \nline  rotated  anti-clockwise  by  an  angle. The  new X  and \nY values are related to the old X and Y values as follows: \nx\nx\ny\n  cos  \n'  \n  sin\n\n\n\n=\n \ny\nx\ny\n'  \n  cos  \n  sin\n\n\n=\n+\nFor CORDIC, the  final angle \n2  the angle whose sine \nor cosine we want to calculate and initial angle \n1  is set to \na convenient value such as 0. Rather than rotating from \n1  \nto \n2  in  one  full  sweep,  we  move  in  steps  with  careful \nchoice of step values. Rearranging (2) gives us: \n[\n]\nx'\n cos\nx \n y tan\n\n=\n\n\n\n]\n[\n x tan\ny' =  cos\ny \n+\n\n\n\nRestricting  the  rotation  angles  such  that  tan =    2-i, \ntransforms  the  multiplication  by  the  tangent  term  to  a \nsimple  shift  operation  [1]. Arbitrary  angles  of  rotation  are \nobtained  by  successively  performing  smaller  elementary \nrotations.  If  i,  the  decision  at  each  iteration,  is  which \ndirection  to  rotate  rather  than  whether  to  rotate  or  not, \nthen  cos(i)  is  constant  as  cos(i)  =  cos(-i).  Then  the \niterative rotation can be expressed as: \n\n(3) \n\n \n\nx  \ni 1\n+\ny  \ni 1\n+\n\n=\n\n=\n\n\n K x  \n\ni\ni\n\n K y  \n\ni\ni\n\n\n\n+\n\n y d 2\n\n\ni\ni\n x d 2\n\n\ni\ni\n\n-i\n\n-i\n\n \n\n\n\n\n\n\n(4) \n\nWhere, \n\n1\n-i\n\nK cos(tan 2 )  = \n=\ni\n\n1\n1+2\n\n-i\n\n(\n\n=\n\n \n\n-i\n\n1+2\n\n) 1\n\n\n \n\nid  =  1  \n \nRemoving \niterative \nthe \nthe  scale  constant  from \nequations  yields  a  shift-add  algorithm  for  vector  rotation. \nThe  product  of  the  Ki's  can  be  applied  elsewhere  in  the \n\n \n\n2 \n\nWhere, \n\ni\n\n \n\n \n\nd\n\n1   if z  < 0\n\ni\n \n= \n+1 otherwise\n\ny sin z ]\n A [x cos z\nx  \n\n=\n0\nn\n0\n0\nn\nx sin z ]\n A [y cos z\ny  \n=\n+\n0\n0\n0\nn\nn\nz  = 0\nn\n\nA  = \nn\nn\n\n1 + 2\n\n-2i\n\n0\n\n0\n\n \nC.  Vectoring Mode \n \n\n \n\n(8) \n\nIn the vectoring mode, the CORDIC rotator rotates the \ninput  vector  through  whatever  angle  is  necessary  to  align \nthe  result  vector  with  the  x  axis.  The  result  of  the \n\n\fProceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) \nIEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 \n \n\nvectoring  operation  is  a  rotation  angle  and  the  scaled \nmagnitude i.e. the x component of the original vector. The \nvectoring  function  works  by  seeking  to  minimize  the  y \ncomponent  of  the  residual  vector  at  each  rotation.  The \nsign  of  the  residual  y  component  is  used  to  determine \nwhich direction  to  rotate next. When  initialized with zero, \naccumulator  contains  the  traversed  angle  at  the  end  of  the \niterations [4]. The equations in this mode are: \n\nx  \ni 1\n+\ny  \ni 1\n+\nz\n \n\ni + 1\n\n=\n=\n=\n\n x  \ni\n y  \ni\n z  \ni\n\n\n+\n\n\n-i\n\n-i\n\n y d 2\n\n\ni\ni\n x d 2\n\n\ni\ni\n1\n\n d tan\n\ni\n\n \n\n(\n\n\n\ni\n\n2\n\n)\n\n(9) \n\nWhere, \n\nd\n\ni\n\n \n\n\n \n= \n\n\n+1   if y  < 0\ni\n-1 otherwise\n\n \n\nThen: \n\nx  \nn\ny  \nn\n\n=\n=\n\n2\n A x  + y\nn\n0\n 0\n\n2\n0\n\nz  = z  + tan\nn\n0\n\n-1\n\n\n\n\n1 + 2\n\ny\nx\n-2i\n\n \n\n\n\n\n\n0\n\n0\n\n(10) \n\nA  = \nn\n\n\nn\nThe  CORDIC  rotation  and  vectoring  algorithms  as \nstated  are  limited  to  rotation  angles  between  -/2  and  /2. \nFor  composite  rotation  angles \nlarger \nthan  /2,  an \nadditional  rotation  is  required  [1]. Volder  [4] describes an \ninitial rotation of  /2. This gives the correction iteration: \n  d y\nx'\n=  \ny' =  d x\n\n\n \n\n(11) \n\n\n\n \n\n \n\ni\n\nd\n\nWhere, \n\nz' = z + d\n\n\n2\n+1   if y < 0\n\n \n= \n-1 otherwise\n\nThere \nrotation. \ninitial \nthis \nfor \nis  no  growth \nAlternatively,  an  initial  rotation  of  either    or  0  can  be \nmade,  avoiding \nthe  reassignment  of \nthe  x  and  y \ncomponents  to  the  rotator  elements.  Again,  there  is  no \ngrowth due to the initial rotation: \nx\nx'\n d\n\n=\ny\ny' =  d\n\nz           if d = 1\n\n\nz -    if d =  1\n\n\n-1    if x < 0\n\n \n= \n+1 otherwise\n\nBoth  reduction  forms  assume  a modulo  2  representation \nof  the  input  angle.  The  second  reduction  may  be  more \nconvenient  when  wiring  is  restricted,  as  is  often  the  case \nwith FPGAs. \n\nWhere, \n\n(12) \n\nz' = \n\nd\n\n \n\n \n\n \n\n \n\ni\n\nD.  Evaluation of Sine and Cosine using CORDIC \n \n\nIn  rotational  mode  the  sine  and  cosine  of  the  input \nangle  can  be  computed  simultaneously.  Setting  the  y \ncomponent of the  input vector to zero reduces the rotation \nmode result to: \n\n0\n\n \n\n(13) \n\n A x cos z\nx  \n=\n\nn\n0\nn\n A x sin z\ny  \n=\n\n0\n0\nn\nn\nIf  x0  is  equal  to  1/An,  the  rotation  produces  the \nunscaled  sine  and  cosine  of  the  angle  argument,  z0.  Very \noften,  the  sine  and  cosine  values  modulate  a  magnitude \nvalue. Using other techniques (e.g., a LUT) requires a pair \nof  multipliers  to  obtain  the  required  modulation.  The \nalgorithm  performs  the  multiply  as  part  of  the  rotation \noperation,  and  therefore  eliminates  the  need  for  a  pair  of \nexplicit  multipliers.  The  output  of  the  CORDIC  rotator  is \nscaled  by  the  rotator  gain.  If  the  gain  is  not  acceptable,  a \nsingle  multiply  by  the  reciprocal  of  the  gain  constant \nplaced  before  the  CORDIC  rotator  will  yield  unscaled \nresults [1].  \n \nE.  Advantages \n \n\n  Number \nhardware \nin \nrequired \ngates \nof \nimplementation  on  an  FPGA,  are minimum.  Thus, \nhardware  complexity  is  greatly  reduced  compared \nto  other  processors  such  as  DSP  multipliers. \nHence, it is relatively simple in design.  \n  Due  to  reduced  hardware  requirement,  cost  of  a \nCORDIC  hardware  implementation  is  less  as  only \nshift registers, adders and look-up table (ROM) are \nrequired. \n  Delay  involved during processing  is comparable  to \nthat of a division or square-rooting operation.  \n  No  multiplication  and  only  addition,  subtraction \nand  bit-shifting  operation  ensures  simple  VLSI \nimplementation.  \n  Either  if  there  is  an  absence  of  a  hardware \nmultiplier (e.g. microcontroller, microprocessor) or \nthere is a necessity to optimize the number of logic \ngates  (e.g.  FPGA),  CORDIC  is  the  preferred \nchoice [4]. \n\n \nF.  Applications \n \n\n  The  algorithm  was  basically  developed  to  offer \ndigital  solutions  to  the  problems  of  real-time \nnavigation in B-58 bomber [5].  \n  This  algorithm \nin  8087  Math \nfinds  use \ncoprocessor,  the  HP-35  calculator  [8],  radar  signal \nprocessors [8] and robotics.  \n  CORDIC algorithm has also been described for the \ncalculation  of  DFT,  DHT,  Chirp  Z-transforms, \nfiltering, Singular value decomposition and solving \nlinear systems [4].  \n\n \n\n3 \n\n\fProceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) \nIEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 \n \n\n  Most  calculators,  especially  the  ones  built  by \nInstruments  and  Hewlett-Packard  use \nTexas \nCORDIC \nalgorithm \nfor \ncalculation \nof \ntranscendental functions.  \n\nIII.  SYSTEM ARCHITECTURE \n\n \n\n \n\nIn  this  paper,  the  FPGA  implementation  of  simple  8-\nbit dedicated processor  for calculating  the  sine and cosine \nof  an  angle  using  CORDIC  Algorithm  is  presented.  The \nprocessor  was  implemented  by  using  Xilinx  ISE  Design \nSuite  12.3  and  VHDL.  Fig.  2  shows  functional  block \ndiagram of our 8-bit processor.  It mainly consists of an 8-\nbit  multiplexers,  registers,  arithmetic  logic  unit  (ALU), \ntri-state  buffer,  comparator,  and  control  unit.  The  logic \ncircuit  for  dedicated  microprocessor  is  divided  into  two \nparts: the datapath unit and control unit [9].  \nInput  of  two  registers  can  be  either  from  an  external \ndata  input  or  from  the  output  of  ALU  unit.  Two  control \nsignals  ln_X  and  ln_Y  select  which  of  two  sources  are  to \nbe  loaded  into  registers.  Two  control  signals  XLoad  and \nYLoad  load  a  value  into  respective  registers.  Bottom \nmultiplier determines the source of two operands of ALU. \nThis  allows  the  selection  of  one  of  the  two  subtraction \noperations X-Y  or Y-X.  A  comparator  unit  is  used  to  test \ncondition  of  equal  to  or  greater  than  and  it  accordingly \ngenerates  status  signals.  Tristate  buffer  is  used  for \noutputting result from register X. \n\n \nFig. 3.  A simple, general datapath circuit for the dedicated \nmicroprocessor \n\nB.  Control Unit \n \n\nFig.  4  shows  the  block  diagram  of  control  unit  and \nFig. 5 shows the corresponding state diagram.  \n\n \nFig. 3.  Block diagram of the control unit \n\n \n\nFig. 2.  Functional block diagram of the 8-bit processor \n\n \nA.  Datapath Unit \n \n\nDatapath  is  responsible  for  the  actual  execution  of  all \ndata  operations  performed  by  the  dedicated  processor  [9]. \nFig.  3  shows  the  datapath  unit  for  the  8-bit  dedicated \nprocessor. \n\n  \nFig. 4.  State diagram of the control unit \nControl  signals  are  generated  by  the  control  unit \nwhich  is  modelled  as  a  finite  state  machine  with  6  states \nsay  S0-S5.  There  are  9  control  signals  which  form  control \nword  and  control  the  operation  of  datapath,  as  per  the \nfollowing table: \n\n \n\n4 \n\n\fProceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) \nIEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 \n \n\n  TABLE I \nCONTROL SIGNAL STATUS DURING DIFFERENT STATES \n\nFig. 7.  Datapath unit simulation \n\n \n\n \n\n \n\n \n\n \n\nThe  following  table  shows  the  synthesis  report  of  the \ndatapath unit: \n\n \n\n \n\n \n\nln_Y \n1 \n0 \n0 \n0 \n0 \n1 \n\nln_X \n1 \n0 \n0 \n0 \n0 \n1 \n\n \n\nXLoad \n\n1 \n0 \n1 \n0 \n0 \n0 \n\n \n\nYLoad \n1 \n0 \n0 \n1 \n0 \n0 \n\n \n\n \n \n\nXY \n0 \n0 \n1 \n0 \n0 \n0 \n\n \n\nClear \n0 \n0 \n0 \n0 \n0 \n1 \n \n\n \n\n \n\n \n\nln_X \n1 \n0 \n0 \n0 \n0 \n1 \n\nOE \n0 \n0 \n0 \n0 \n1 \n0 \n\nDone \n0 \n0 \n0 \n0 \n1 \n0 \n\nState \nln_Y  ALU(0,1,2) \nS0 \n1 \n101 \nS1 \n101 \n0 \nS2 \n101 \n0 \nS3 \n101 \n0 \nS4 \n0 \n101 \nS5 \n101 \n1 \n \n \n \n \n \n \n \n \n \n \n \n \nIf  Reset  =  1  then  state  S5  occurs.  In  this  state \n \n \nregisters are initialized to 0 by asserting the Clear signal. \nIf  Reset  =  0  then  at  rising  edge  of  clock,  the  state  is \nupgraded  from  S5  to  state  S0.  During  the  state  S0  two \ninputs  are  loaded  in  two  registers.  After  completion  of \nstate S0, state S1 is reached. In this state output of registers \nis  checked  in  comparator  for  equality  and  greater  than \nconditions. If both values are same, state S4 occurs else S2 \nor  S3  will  continue  depending  on  status  of  signal  neq1. \nState S1 is repeated again. Default state is S5. \n \n\nIV.  IMPLEMENTATION AND VERIFICATION \n\nAll  the  units  in  dedicated  processor  were  designed. \nThese  units  were  described  in  VHDL-modules  and \nsynthesized  using  ISE  Design  Suite  12.3.  ModelSim \nsimulator  was  used  to  verify  the  functionalities  of  each \nunit.  Finally  all  the  units  were  combined  together  and \nonce  again  tested  by  using  ModelSim  simulator.  Fig.  6 \nshows  the  RTL  schematic  of  the  CORDIC  processor \ngenerated from Xilinx ISE. \n\nFig. 6.  RTL schematic of the CORDIC processor \n\n \n\n \nA.  Datapath Unit \n \n\nSimulation result of datapath is shown in Fig. 7.  \n\n \n\n \n\n5 \n\n  TABLE II \nSYNTHESIS REPORT OF THE DATAPATH UNIT \n\n \n\nNumber of Slices \nMaximum Frequency \nMinimum Period \n \n \n\n61 (31%) \n114.05 MHz \n8.76 ns \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\nSimulation result of the control unit is shown in Fig. 8. \n\n \n \n \n \nB.  Control Unit \n \n \n\nFig. 8.  Control unit simulation \n\n \n\nThe  following  table  shows  the  synthesis  report  of  the \ncontrol unit: \n\n  TABLE III \nSYNTHESIS REPORT OF THE CONTROL UNIT \n\n \n\nNumber of Slices \nMaximum Frequency \nMinimum Period \n \n \n \n \n \n \n \nC.  Dedicated CORDIC Processor \n \n\n4 (2%) \n264.34 MHz \n3.78 ns \n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\nOnce \nthe  datapath  unit  and  control  unit  were \nsimulated,  they were  combined  and  a  dedicated  processor \nwas  constructed.  Simulation  shows \nthe  CORDIC \ncalculation  operation  of  the  Sine  and  Cosine  of  an  angle. \nSimulation  result  for  the  dedicated  processor  is  as  shown \nin Fig. 9. \n\nFig. 9.  Test-Bench waveforms indicating the Sine and Cosine output, \nobtained after the CORDIC Core Simulation \n\n \n\n\fProceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) \nIEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 \n \n\nThe  following  table  shows  the  synthesis  report  of  the \n8-bit dedicated processor: \n\n  TABLE IV \nSYNTHESIS REPORT OF THE 8-BIT DEDICATED PROCESSOR \n\n \n\nLogic Utilization \nNumber of Slices \nNumber of Slice Flip Flops \nNumber of 4 input LUTs \nNumber of bonded IOBs \nNumber of GCLKs \n\nUsed  Available  Utilization \n3% \n3584 \n126 \n0% \n7168 \n58 \n3% \n7168 \n238 \n28 \n141 \n19% \n12% \n8 \n1 \n\n \n\n \nFor a Speed Grade of -5, the minimum period required \nis 12.292 ns, which corresponds  to a maximum  frequency \nof  81.353  MHz.  The  minimum  input  arrival  time  before \nclock  and  maximum  output  required  time  after  clock  are \n9.657 ns and 8.133 ns respectively. \nThe  following  table  shows  the  comparison  between \nthe  actual  Sine  and  Cosine  values  and  the  ones  obtained \nfrom Test-Bench analysis in VHDL. \n\n  TABLE V \nCOMPARISON OF SUCCESSIVE ANGLE ROTATION VALUES \n\n \nAngle (A)  Rotation \n0.000000 \n5 \n10 \n0.000000 \n15 \n0.000000 \n20 \n0.000000 \n5 \n0.523599 \n10 \n0.523599 \n0.523599 \n15 \n20 \n0.523599 \n5 \n1.000000 \n10 \n1.000000 \n15 \n1.000000 \n1.000000 \n20 \n5 \n3.141593 \n10 \n3.141593 \n15 \n3.141593 \n3.141593 \n20 \n \n \nAngle (A)  Rotation \n5 \n0.000000 \n0.000000 \n10 \n15 \n0.000000 \n20 \n0.000000 \n5 \n0.523599 \n10 \n0.523599 \n0.523599 \n15 \n20 \n0.523599 \n5 \n1.000000 \n10 \n1.000000 \n15 \n1.000000 \n1.000000 \n20 \n5 \n3.141593 \n10 \n3.141593 \n15 \n3.141593 \n3.141593 \n20 \n \n\nsin(A) \n(Actual) \n0.00000000 \n0.00000000 \n0.00000000 \n0.00000000 \n0.50000000 \n0.50000000 \n0.50000000 \n0.50000000 \n0.84147098 \n0.84147098 \n0.84147098 \n0.84147098 \n0.00000000 \n0.00000000 \n0.00000000 \n0.00000000 \n\ncos(A) \n(Actual) \n1.00000000 \n1.00000000 \n1.00000000 \n1.00000000 \n0.86602540 \n0.86602540 \n0.86602540 \n0.86602540 \n0.54030231 \n0.54030231 \n0.54030231 \n0.54030231 \n-1.0000000 \n-1.0000000 \n-1.0000000 \n-1.00000000 \n\nsin(A) \n(Test-Bench) \n0.01483516 \n0.00117259 \n0.00001292 \n-0.00000043 \n0.48362630 \n0.49892865 \n0.50003905 \n0.50000106 \n0.80881306 \n0.84080033 \n0.84149350 \n0.84147186 \n-0.01483516 \n-0.00117259 \n-0.00001292 \n0.00000043 \n\ncos(A) \n(Test-Bench) \n0.99988995 \n0.99999931 \n1.00000000 \n1.00000000 \n0.87527459 \n0.86664307 \n0.86600286 \n0.86602479 \n0.58806584 \n0.54134537 \n0.54026724 \n0.54030094 \n-0.99988995 \n-0.99999931 \n-1.00000000 \n-1.00000000 \n\nError \n-1.4835e-002 \n-1.1725e-003 \n-1.2922e-005 \n4.2874e-007 \n1.6373e-002 \n1.0713e-003 \n-3.9047e-005 \n-1.0561e-006 \n3.2657e-002 \n6.7065e-004 \n-2.2515e-005 \n-8.7478e-007 \n1.4835e-002 \n1.1725e-003 \n1.2922e-005 \n-4.2874e-007 \n\nError \n1.1004e-004 \n6.8748e-007 \n8.3498e-011 \n9.2037e-014 \n-9.2491e-003 \n-6.1766e-004 \n2.2545e-005 \n6.0975e-007 \n-4.7763e-002 \n-1.0430e-003 \n3.5067e-005 \n1.3623e-006 \n-1.1004e-004 \n-6.8748e-007 \n-8.3498e-011 \n-9.2037e-014 \n\n \n\nV.  CONCLUSION \n\n \n  We  have  successfully  simulated  an  8-bit  dedicated \nprocessor  for calculating  the Sine and Cosine of an angle, \non  ModelSim  simulator  using  the  VHDL  language.  Our \nprocessor  has  six  main  components  namely,  control  unit, \nmultiplexer  unit,  ALU  unit,  register  unit,  tristate  buffer \nunit  and  comparator.  Our  dedicated  processor  has  a \nmaximum  frequency  of  81.353  MHz  was  reached  with  a \nminimum period of 12.292 ns. 126 (3%) slices were used. \nOur  System  can  be  implemented  on  Xilinx  Spartan  3 \nXC3S400  using  ISE  Design  Suite  12.3  and  VHDL \nlanguage.  Our  dedicated  processor  has  a  distinct \nadvantage  over  a  general  purpose  processor,  since  it \nrepeatedly  performs  same  task  its  design  is more  efficient \nand consumes less resources and is less time intensive.  \n \nREFERENCES \n\n \n[1]  R.  Andraka,  A  survey  of  CORDIC  algorithms  for  FPGA \nbased  computers,  Proceedings  of  the  1998  ACM/SIGDA \nsixth  international symposium on Field programmable gate \narrays, pp. 191  200. \n[2]  V.  Sharma,  FPGA  Implementation  of  EEAS  CORDIC \nbased  Sine  and  Cosine  Generator,  M.  Tech  Thesis,  Dept. \nof  Electronics  and  Communication  Engineering,  Thapar \nUniversity, Patiala, 2009. \n[3]  S.  Panda,  Performance  Analysis  and  Design  of  a  Discrete \nCosine Transform Processor using CORDIC Algorithm, M. \nTech  Thesis,  Dept.  of  Electronics  and  Communication \nEngineering, NIT Rourkela, Rourkela, Orissa, 2010. \n[4]  R.  K.  Jain,  B.  Tech  Thesis,  Design  and  FPGA \nImplementation  of  CORDIC-based  8-point  1D  DCT \nProcessor, NIT Rourkela, Rourkela, Orissa, 2011. \n[5]  J.  Volder,  \"The  CORDIC  Trigonometric  Computing \nTechnique,\"  IRE  Transactions  on  Electronic  Computing, \nVol EC-8, Sept 1959, pp. 330-334. \n[6]  F.  Ling,  Givens  rotation  based  least  squares  lattice  and \nIEEE  Transactions  on  Signal \nrelated  algorithms, \nProcessing, Jul 1991, pp. 1541  1551. \n[7]  J.  S.  Walther,  \"A  unified  algorithm  for  elementary \nfunctions,\"  Proceedings  of  the  Spring  Joint  Computer \nConference, 1971, pp. 379-385. \n[8]  R.  Andraka.  \"Building  a  High  Performance  Bit-Serial \nProcessor  in  an  FPGA,\"  Proceedings  of  Design  SuperCon, \nJan 1996, pp. 1-2. \nDigital  Logic  and  Microprocessor \n[9]  E. O. Hwang,  \nDesign  with  VHDL,  Thomson/Nelson,  2006,  pp.  379-413, \npp. 290-311. \n\n \n\n \n\n6 \n\n\f", 
        "tag": "Mathematical Software", 
        "link": "https://arxiv.org/list/cs.MS/new"
    }, 
    {
        "text": "Paper submitted to the ICDL-Epirob 2017 conference. Pre-review version. \n\nEmbodied Artificial Intelligence through \nDistributed Adaptive Control: An Integrated \nFramework \n\nClment Moulin-Frier \nSPECS Lab \nUniversitat Pompeu Fabra \nBarcelona, Spain \nEmail: \nclement.moulinfrier@gmail.com \n \n\nMart Sanchez-Fibla \nSPECS Lab \nUniversitat Pompeu Fabra \nBarcelona, Spain \nEmail: santmarti@gmail.com \n\n \n\nJordi-Ysard Puigb \nSPECS Lab \nUniversitat Pompeu Fabra \nBarcelona, Spain \nEmail:  \njordiysard.puigbo@upf.edu \n\nXerxes D. Arsiwalla \nSPECS Lab \nUniversitat Pompeu Fabra \nBarcelona, Spain \nEmail: \nx.d.arsiwalla@gmail.com \n \nPaul FMJ Verschure \nSPECS Lab \nUniversitat Pompeu Fabra & \nICREA \nBarcelona, Spain 96678-2391 \nEmail: \npaul.verschure@upf.edu \n\n \n\n \n\nAbstractIn  this  paper,  we  argue  that  the  future  of  Artificial \nIntelligence  research  resides  in  two  keywords:  integration  and \nembodiment.  We  support  this  claim  by  analyzing  the  recent \nadvances  of  the  field.  Regarding  integration,  we  note  that  the \nmost  impactful  recent  contributions  have  been  made  possible \nthrough  the  integration  of  recent  Machine  Learning  methods \n(based  in  particular  on  Deep  Learning  and  Recurrent  Neural \nNetworks)  with  more  traditional  ones  (e.g.  Monte-Carlo  tree \nsearch,  goal  babbling  exploration  or  addressable  memory \nsystems).  Regarding  embodiment,  we  note  that  the  traditional \nbenchmark  tasks  (e.g.  visual  classification    or  board  games)  are \nbecoming  obsolete  as  state-of-the-art \nlearning  algorithms \napproach  or  even  surpass  human  performance  in  most  of  them, \nhaving  recently  encouraged  the  development  of  first-person  3D \ngame  platforms  embedding  realistic  physics.  Building  upon  this \nanalysis,  we  first  propose  an  embodied  cognitive  architecture \nintegrating heterogenous sub-fields of Artificial Intelligence into a \nunified  framework.  We  demonstrate  the  utility  of  our  approach \nby showing how major contributions of the field can be expressed \nwithin \nthe  proposed \nframework.  We \nthen  claim \nthat \nbenchmarking  environments  need  to  reproduce  ecologically-valid \nconditions  for  bootstrapping  the  acquisition  of  increasingly \ncomplex  cognitive  skills  through  the  concept  of  a  cognitive  arms \nrace between embodied agents.  \n \n\nIndex  TermsCognitive  Architectures,  Embodied  Artificial \nIntelligence,  Evolutionary  Arms  Race,  Unified  Theories  of \nCognition.  \n\nI.  INTRODUCTION \n\nIn  recent  years,  research  in  Artificial  Intelligence  has  been \nprimarily  dominated  by  impressive  advances  in  Machine \nLearning,  with  a  strong  emphasis  on  the  so-called  Deep \nLearning framework. It has allowed considerable achievements \nsuch  as  human-level  performance  in  visual  classification  [1] \nand  description  [2],  in  Atari  video  games  [3]  and  even  in  the \nhighly  complex  game  of Go  [4]. The Deep Learning  approach \nis characterized by supposing very minimal prior on the task to \nbe  solved,  compensating  this  lack  of  prior  knowledge  by \nfeeding  the  learning  algorithm  with  an  extremely  high  amount \nof training data, while hiding the intermediary representations.   \n \nHowever,  it  is  important  noting  that  the  most  important \ncontributions of Deep Learning for  Artificial Intelligence often \nowe  their  success  in  part  to  their  integration  with  other  types \nof  learning  algorithms.  For  example,  the  AlphaGo  program \nwhich  defeated  the  world  champions  in  the  famously  complex \ngame  of  Go  [4],  is  based  on  the  integration  of  Deep \nReinforcement  Learning  with  a  Monte-Carlo  tree  search \nalgorithm.  Without  the  tree  search  addition,  AlphaGo  still \noutperforms  previous  machine  performances  but  is  unable  to \nbeat  high-level  human  players. Another  example  can be  found \nin  the  original Deep Q-Learning  algorithm  (DQN, Mnih  et  al., \n2015),  achieving  very  poor  performance  in  some  Atari  games \nwhere  the  reward  is  considerably  sparse  and  delayed  (e.g. \nMontezuma  Revenge).  Solving  such  tasks  has  required  the \n\n\fPaper submitted to the ICDL-Epirob 2017 conference. Pre-review version. \n\n \n\nintegration  of  DQN  with  intrinsically-motivated  learning \nalgorithms for novelty detection [5], or goal babbling [6].  \n \nA drastically different approach has also received considerable \nattention,  arguing  that  deep  learning  systems  are  not  able  to \nsolve  key  aspects of  human  cognition  [7]. The approach states \nthat  human  cognition  relies  on  building  causal  models  of  the \nworld  through  combinatorial  processes  to  rapidly  acquire \nknowledge  and  generalize  it  to  new  tasks  and  situations.  This \nhas  led  to  important  contributions  through  model-based \nBayesian  learning  algorithms,  which  surpass  deep  learning \napproaches  in  visual  classification  tasks,  while  displaying \npowerful  generalization  abilities  in  one-shot  training  [8].  This \nsolution  however,  comes  at  a  cost:  the  underlying  algorithm \nrequires  a  priori  knowledge  about  the  primitives  to  learn  from \nand  about  how  to  compose  them  to  build  increasingly  abstract \ncategories.  An  assumption  of  such  models  is  that  learning \nshould  be  grounded  in  intuitive  theories  of  physics  and \npsychology, supporting and enriching acquired knowledge  [7], \nas supported by infant behavioral data [9]. \n \nConsidering \nintuitive  physics  and \nthe  pre-existence  of \npsychology  engines  as an  inductive bias  for Machine Learning \nis far from being a  trivial assumption. It immediately raises the \nquestion: where does such knowledge come from and how is it \nshaped  through  evolutionary,  developmental  and  cultural \nprocesses? All  the  aforementioned  approaches  are  lacking  this \nfundamental  component  shaping  intelligence  in  the  biological \nworld,  namely  embodiment.  Playing  Atari  video  games, \ncomplex  board  games  or  classifying  visual  images  at  a  human \nlevel  are  considerable  milestones  of  Artificial  Intelligence \nresearch.  Yet,  in  contrast,  biological  cognitive  systems  are \nintrinsically  shaped  by \ntheir  physical  nature.  They  are \nembodied  within  a  dynamical  environment  and  strongly \ncoupled  with  other  physical  and  cognitive  systems  through \ncomplex feedback loops operating at different scales: physical, \nsensorimotor,  cognitive,  social,  cultural  and  evolutionary. \nNevertheless,  many  recent  Artificial  Intelligence  benchmarks \nhave  focused  on  solving  video  games  or  board  games, \nadopting  a  third-person  view  and  relying  on  a  discrete  set  of \nactions  with  no  or  poor  environmental  dynamics.  A  few \ninteresting  software  tools have however  recently been  released \nto provide more realistic benchmarking environments. This for \nexample,  is  the  case  of Project Malmo  [10] which provides an \nAPI  to  control  characters  in  the  MineCraft  video  game,  an \nopen-ended  environment  with  complex  physical  and \nenvironmental  dynamics;  or  Deepmind  Lab  [11],  allowing  the \ncreation  of  rich  3D  environments  with  similar  features. \nAnother  example  is  OpenAI  Gym  [12],  providing  access  to  a \nvariety  of  simulation  environments  for  the  benchmarking  of \nlearning  algorithms,  specially  reinforcement  learning  based. \nSuch  complex  environments  are  becoming  necessary  to \nvalidate  the  full  potential  of  modern  Artificial  Intelligence \nresearch,  in  an  era  where  human  performance  is  being \nachieved  on  an  increasing  number  of  traditional  benchmarks. \nThere  is also a  renewed  interest  for multi-agent benchmarks  in \nlight  of  the  recent  advances  in  the  field,  solving  social  tasks \n\nsuch as  the prisoner dilemma  [13] and studying  the emergence \nof cooperation and competition among agents [14].  \n \nThe  above  examples  emphasize  two  important  challenges  in \nmodern  Artificial  Intelligence.  Firstly,  there  is  a  need  for  a \nframework  providing  a  principled \nunified \nintegrative \nmethodology  for  organizing \nthe \ninteractions  of  various \nsubfields  (e.g.  planning  and  decision  making,  abstraction, \nclassification,  reinforcement  learning,  sensorimotor  control  or \nexploration).  Secondly,  Artificial  Intelligence  is  arriving  at  a \nlevel  of  maturation  where  more  realistic  benchmarking \nenvironments  are  required,  for  two  reasons:  validating  the  full \npotential  of  the  state-of-the-art  artificial  cognitive  systems,  as \nwell  as  understanding  the  role  of  environmental  complexity  in \nthe shaping of cognitive complexity.  \n \nIn  this  paper,  we  first  propose  an  embodied  cognitive \narchitecture  structuring  the  main  sub-fields  of  Artificial \nIntelligence  research  into  an  integrated  framework.  We \ndemonstrate  the utility of our approach by showing how major \ncontributions of the field can be expressed within the proposed \nframework,  providing  a  powerful  tool  for  their  conceptual \ndescription  and  comparison.  Then  we  argue \nthat \nthe \ncomplexity  of  a  cognitive  agent  strongly  depends  on  the \ncomplexity  of  the  environment  it  lives  in.  We  propose  the \nconcept  of  a  cognitive  arms  race,  where  an  ecology  of \nembodied  cognitive  agents  interact  in  a  dynamic  environment \nreproducing  ecologically-valid  conditions  and  driving  them  to \nacquire  increasingly  complex  cognitive  abilities  in  a  positive \nfeedback loop. \n\nII.  AN INTEGRATED COGNITIVE ARCHITECTURE FOR EMBODIED \nARTIFICIAL INTELLIGENCE \n\nConsidering  an \nto \nintegrative  and  embodied  approach \nArtificial  Intelligence  requires  dealing  with  heterogeneous \naspects  of  cognition,  where  low-level  interaction  with  the \nenvironment  interacts bidirectionally with high-level reasoning \nabilities.  This  reflects  an  historical  challenge  in  formalizing \nhow  cognitive  functions  arise  in  an  individual  agent  from  the \ninteraction  of  interconnected  information  processing  modules \nstructured  in  a  cognitive  architecture  [15],  [16].  On  one  hand, \ntop-down  approaches  mostly  rely  on  methods  from  Symbolic \nArtificial  Intelligence  (from  the  General  Problem  Solver   [17] \nto  Soar  [18]  or  ACT-R  [19]  and  their  follow-up),  where  a \ncomplex  representation  of  a  task  is  recursively  decomposed \nthe  other  hand,  bottom-up \ninto  simpler  elements.  On \napproaches \ninstead  emphasize \nlower-level  sensory-motor \ncontrol  loops  as  a  starting  point  of  behavioral  complexity, \nwhich  can  be  further  extended  by  combining  multiple  control \nloops  together,  as  implemented  in  behavior-based  robotics \nintelligence  without \n[20] \n(sometimes \nreferred \nas \nrepresentation  [21]).  These  two  approaches  thus  reflect \ndifferent  aspects  of  cognition:  high-level  symbolic  reasoning \nfor the former and low-level embodied behaviors for the latter. \nHowever,  both  aspects  are  of  equal  importance when  it  comes \nto defining a unified theory of cognition. It is therefore a major \nchallenge  of  cognitive  science  to  unify  both  approaches  into  a \nsingle  theory, where  (a)  reactive  control  allows  an  initial  level \n\n\fPaper submitted to the ICDL-Epirob 2017 conference. Pre-review version. \n\n \n\nof  complexity  in  the  interaction  between  an  embodied  agent \nand  its  environment  and  (b)  this  interaction  provides  the  basis \nfor  learning  higher-level  representations  and  for  sequencing \nthem in a causal way for top-down goal-oriented control. \nFor  this  aim,  we  adopt  the  principles  of  the  Distributed \nAdaptive  Control  (DAC)  theory  of  the  mind  and  brain  [22], \n[23].  Besides  its  biological  grounding,  DAC  is  an  adequate \nmodeling framework for integrating heterogeneous concepts of \nArtificial  Intelligence  and  Machine  Learning  into  a  coherent \ncognitive  architecture,  for  two  reasons:  (a)  it  integrates  the \nprinciples of both the aforementioned bottom-up and top-down \napproaches  into  a  coherent  information  processing  circuit;  (b) \nit  is  agnostic  to  the  actual  implementation  of  each  of  its \nfunctional modules. Over  the  last  fifteen  years, DAC  has been \napplied  to  a  variety  of  complex  and  embodied  benchmark \ntasks,  for  example  foraging  [22],  [24]  or  social  humanoid \nrobot control [16], [25].  \n\nA.  The  DAC-EAI  cognitive  architecture:  Distributed \nAdaptive Control for Embodied Artificial Intelligence \n\nDAC  posits  that  cognition  is  based  on  the  interaction  of \ninterconnected  control  loops  operating  at  different  levels  of \nabstraction  (Figure  1).  The  functional  modules  constituting \nthe  architecture  are  usually  described \nin  biological  or \npsychological terms (see e.g. [26]). Here we propose instead to \ndescribe  them  in  purely  computational  term,  with  the  aim  of \nfacilitating  the  description  of  existing  Artificial  Intelligence \nsystems  within \nthis  unified \nframework.  We  call \nthis \nthe  architecture  DAC-EAI:  Distributive \ninstantiation  of \nAdaptive Control for Embodied Artificial Intelligence.  \n\n \nFigure  1:  The  DAC-EAI  architecture  allows  a  coherent \norganization  of  heterogenous  subfields  of  Artificial  Intelligence. \nDAC-EAI  stands  for Distributed Adaptive Control  for Embodied \n\nMachine  Learning.  It  is  composed  of  three  layers  operating  in \nparallel  and  at  different  levels  of  abstraction.  See  text  for  detail, \nwhere each module name is referred with italics. \n\nThe  first  level,  called  the  Somatic  layer,  corresponds  to  the \nembodiment  of  the  agent  within  its  environment.  It  includes \nthe  sensors  and  actuators,  as  well  internal  variables  to  be \nregulated  (e.g.  energy  or  safety  levels).  The  self-regulation  of \nthese  internal  variables  occurs  in  the  Reactive  layer  and \nextends  the  aforementioned  behavior-based  approaches  (e.g. \nthe  Subsumption  architecture  [20])  with  drive  reduction \nmechanisms  through  predefined  sensorimotor  control  loops \n(i.e.  reflexes).  In  Figure  1,  this  corresponds  to  the  mapping \nfrom  the  Sensing  to  the  Motor  Control  module  through  Self \nRegulation. The Reactive layer offers several advantages when \nanalyzed  from  the  embodied  artificial  intelligence  perspective \nof  this  paper.  First,  reward  is  traditionally  considered  in \nMachine  Learning  as  a  scalar  value  associated  with  external \nstates of  the environment. DAC proposes instead that it should \nderive  from  the \ninternal  dynamics  of  multiple  internal \nvariables  modulated  by \nthe  body-environment  real-time \ninteraction,  providing  an  embodied  notion  of  reward  in \ncognitive  agents.    Second,  the  Reactive  layer  generates  a  first \nlevel  of  behavioral  complexity  through  the  interaction  of \npredefined  sensorimotor  control  loops  for self-regulation. This \nprovides a notion of embodied  inductive bias bootstrapping  an \nstructuring  learning  processes  in  the  upper  levels  of  the \narchitecture.  This  is  a  departure  from  the  model-based \napproaches mentioned  in  the  introduction  [7], where  inductive \nbiases  are  instead  considered  as  intuitive  core  knowledge  on \nthe form of a pre-existent physics and psychology engine. \nBehavior  generated  in  the  Reactive  layer  bootstraps  learning \nprocesses  for  acquiring  a  state  space  of  the  agent-environment \ninteraction in the Adaptive layer. The Representation Learning \nmodule  receives  input  from  Sensing  to  form  increasingly \nabstract  representations.  For  example,  unsupervised  learning \nmethods  such  as  deep  autoencoders  [27]  could  be  a  possible \nimplementation  of  this module. The  resulting abstract states of \nthe  world  are  mapped  to  their  associated  values  through  the \nValue Prediction module, informed by the internal states of the \nagent from Self Regulation. This allows the inference of action \npolicies  maximizing  value  through  Action  Selection,  a  typical \nreinforcement  learning  problem  [28].  An  interesting  point  is \nthat  the  self-regulation  of  multiple  internal  variables  in  the \nReactive  layer  requires  the  agent  to  switch  between  different \naction  policies  (differentiating  e.g.  between  situation  of  low \nenergy vs. low safety). In the current framework, such a switch \nis  controlled  higher  up  the  architecture  through  the  Goal \nSelection and Planning module (see below).   \nThe  state  space  acquired  in  the  Adaptive  layer  then  supports \nthe  acquisition  of  higher-level  cognitive  abilities  such  as  goal \nselection,  memory  and  planning  in  the  Contextual  layer.  The \nabstract  representations  acquired  in  Representation  Learning \nthrough  Relational  Learning.  The \nare \nlinked \ntogether \navailability  of  abstract  representations  in  possibly  multiple \nmodalities  provides  the  substrate  for  causal  and  compositional \nlinking.  Several  state-of-the-art  methods  are  of  interest  for \nlearning  such  relations,  such  as Bayesian program  learning  [8] \nor  Long  Short  Term  Memory  neural  network  (LSTM,  [29]). \n\n\fPaper submitted to the ICDL-Epirob 2017 conference. Pre-review version. \n\n \n\nBased  on  these  higher-level  representations,  Goal  Selection \nforms the basis of goal-oriented behavior by selecting valuable \nstates  to  be  reached,  where  value  is  provided  by  the  Value \nPrediction  module. \nIntrinsically-motivated  methods \nmaximizing  learning  progress  can  be  applied   here  for  an \nefficient  exploration  of  the  environment  [30].  The  selected \ngoals  are  reached  through  Planning,  where  any  adaptive \nmethod  of  this  field  can  be  applied  [31].  The  resulting  action \nplans,  learned  from  action-state-value  tuples  generated  by  the \nAdaptive  layer,  propagate  down  the  architecture  to  modulate \nbehavior.  Finally,  an  addressable memory  system  registers  the \nactivity of the Contextual layer, allowing the persistence of the \nagent  experience  over  the  long  term  for  lifelong  learning \nabilities  [32].  In  psychological  terms,  this  memory  system  is \nanalog to an autobiographical memory. \nThese  high-level  cognitive  processes \nin \nturn  modulate \nbehavior  at  lower  levels  via  top-down  pathways  shaped  by \nbehavioral  feedback.  The  control  flow  is  therefore  distributed \nwithin  the  architecture,  both  from  bottom-up  and  top-down \ninteractions between layers, as well as from lateral information \nprocessing into the subsequent layers. \n\nB.  Expressing  existing  Machine  Learning  systems  within \nthe DAC-EAI framework \n\nWe  now  demonstrate  the  generality  of  the  proposed  DAC-\nEML  architecture  by  describing  how  well-known  Artificial \nIntelligence  systems  can  be  conceptually  described  as  sub-\nparts of the DAC-EAI architecture (Figure 2).  \nWe start with behavior-based robotics [20], implementing a set \nof  reactive  controllers  through  low-level  coupling  between \nsensors to effectors . Within the proposed framework, there are \ndescribed  as  the  lower  part  of  the  architecture,   spanning  the \nSomatic  and  Reactive  layers  (Figure  2B).  However,  those \napproaches  are  not  considering  the  self  regulation  of  internal \nvariables  but  instead  of  exteroceptive  variables,  such  as  light \nquantity for example.  \nIn  contrast, \ntop-down  robotic  planning  algorithms  [33] \ncorrespond  to  the  right  column  (Action)  of  the  DAC-EAI \narchitecture:  spanning  from  Planning  to  Action  Selection  and \nMotor  Control,  where  the  current  state  of  the  system  is \ntypically \nprovided \nby \npre-processed \nsensory-related \ninformation  along  the  Reactive  or  Adaptive  layers  (Figure \n2C).    More  recent  Deep  Reinforcement  Learning  methods, \nsuch  as  the  original  Deep  Q-Learning  algorithm  (DQN,  [3]) \ntypically  span  over  all \nthe  Adaptive \nlayer,  They  use \nconvolutional  deep  networks  learning  abstract  representation \nfrom  pixel-level  sensing  of  video game  frames, Q-learning  for \npredicting  the  cumulated  value  of  the  resulting  states  and \ncompetition  among  discrete  actions  as  an  action  selection \n\nprocess  (Figure  2D).  Still,  there  is  no  real  motor  control  in \nthis system, given that most available benchmarks operate on a \nlimited  set  of  discrete  (up-down-left-right)  or  continuous \n(forward speed,  rotation speed) actions. Not shown on   Figure \n2,  classical  reinforcement  learning  [28]  relies  on  the  same \narchitecture  as  Figure  2D,  however  not  addressing  the \nrepresentation  learning  problem,  since  the  state  space  is \nusually  pre-defined  in  these  studies  (often  considering  a  grid \nworld). \nSeveral  extensions  based  on  the  DQN  algorithm  exist.  For \nexample,  intrinsically-motivated  deep  reinforcement  learning \n[6]  extends  it  with  a  goal  selection  mechanism  (Figure  2E). \nThis  extension  allows  solving  tasks  with  delayed  and  sparse \nreward (e.g. Montezuma Revenge) by encouraging exploratory \nbehaviors.  AlphaGo  also  relies  on  a  Deep  Reinforcement \nLearning method  (hence  spanning  the Adaptive  layer  as  in  the \nlast  examples),  coupled  with  a  Monte-Carlo  tree  search \nalgorithm  which  can  be  conceived  as  a  planning  process  (see \nalso  [34]), as represented in Figure 2F.  \nAnother  recent work,  adopting  a  drastically opposite approach \nas  compared  to  end-to-end  deep  learning,  addresses  the \nproblem  of  learning  highly  abstract  concepts  from  the \nperspective  of  the  human  ability  to  perform  one-shot  learning. \nThe  resulting  model,  called  Bayesian  Program  Learning  [8], \nrelies on a priori knowledge about  the primitives  to  learn  from \nand  about  how  to  compose  them  to  build  increasingly  abstract \ncategories.  In  this  sense,  it  is  described  within  the  DAC-EAI \nframework as addressing  the pattern  recognition problem  from \nthe  perspective  of  relational  learning,  where  primitives  are \ncausally  linked  for  composing  increasingly  abstract  categories \n(Figure 2G).  \nFinally,  the Differentiable Neural Computer  [35],  successor of \nthe  Neural  Turing  Machine  [36],  couples  a  neural  controller \n(e.g.  based  on  a  LSTM)  with  a  content-addressable  memory. \nThe  whole  system  is  fully  differentiable  and  is  consequently \noptimizable  through  gradient  descent.  It  can  solve  problems \nrequiring  some  levels  of  sequential  reasoning  such  has  path \nplanning  in  a  subway  network  or  performing  inferences  in  a \nfamily  tree.  In  DAC-EAI  terms,  we  describe  it  as  an \nimplementation  of  the  higher  part  of  the  architecture,  where \ncausal  relations  are  learned  from  experience  and  selectively \nstored  in  an  addressable  memory,  which  can  further  by \naccessed for reasoning or planning operations (Figure 2H).   \n \nAn  interesting  challenge  with  such  an  integrative  approach  is \ntherefore  to express a wide range of Artificial systems within a \nunified \nframework, \nfacilitating \ntheir  description \nand \ncomparison in conceptual terms.  \n\n\f \n\nPaper submitted to the ICDL-Epirob 2017 conference. Pre-review version. \n\n \nFigure  2:  The  DAC-EAI  architecture  allows  a  conceptual  description  of  many  Artificial  Intelligence  systems  within  a  unified \nframework.  A)  The  complete  DAC-EAI  architecture  (see  Figure  1  for  a  larger  version).  The  other  subfigures  (B  to  E)  show  conceptual \ndescriptions  of  different  Artificial  Intelligence  systems  within  the  DAC-EAI  framework.  B):  Behavior-based  Robotics  [20].  C)  Top-down \nrobotic  planning  [33].  D)  Deep  Q-Learning  [3].  E)  Intrinsically-Motivated  Deep  Reinforcement  Learning    [6].  F)  AlphaGo  [4].  G)  Bayesian \nProgram Learning [8]. H) Differentiable Neural Computer [35]. \n\nIII.  THE COGNITIVE ARMS RACE: REPRODUCING \nECOLOGICALLY-VALID CONDITIONS FOR DEVELOPING \nCOGNITIVE COMPLEXITY  \n\nA  general-purpose  cognitive  architecture \nfor  Artificial \nIntelligence,  as  the  one  proposed  in  the  previous  section,  \ntackles  the  challenge  of  general-purpose  intelligence  with  the \naim  of  addressing  any  kind  of  task.  Traditional  benchmarks, \nmostly  based  on  datasets  or  on  idealized  reinforcement \nlearning  tasks,  are  progressively  becoming  obsolete  in  this \nrespect.  There  are  two  reasons  for  this.  The  first  one  is  that \nstate-of-the-art  learning  algorithms  are  now  achieving  human \nperformance  in  an  increasing  number  of  these  traditional \nbenchmarks  (e.g.  visual  classification,  video  or  board  games). \nThe  second  reason  is  that  the  development  of  complex \ncognitive  systems  is  likely  to  depend  on  the  complexity  of  the \nenvironment  they  evolve  in1.  For  these  two  reasons,  Machine \nLearning  benchmarks  have  recently  evolved  toward  first-\n\n \n\n1  See  also  https://deepmind.com/blog/open-sourcing-deepmind-lab/:  It  is \npossible  that  a  large  fraction  of  animal  and  human  intelligence  is  a  direct \nconsequence  of  the  richness  of  our  environment,  and  unlikely  to  arise \nwithout it. \n\nperson  3D  game  platforms  embedding  realistic  physics  [10], \n[11] and likely to become the new standards in the field. \n \nIt  is  therefore  fundamental  to  figure  out what  properties of  the \nenvironment  act  as  driving  forces  for  the  development  of \ncomplex cognitive abilities in embodied agents. We propose in \nthis  paper  the  concept  of  a  cognitive  arms  race  as  a \nfundamental  driving  force  catalyzing  the  development  of \ncognitive  complexity.  The  aim  is  to  reproduce  ecologically-\nvalid  conditions  among  embodied  agents  forcing  them  to \ncontinuously  improve  their  cognitive  abilities  in  a  dynamic \nmulti-agent  environment.  In  natural  science,  the  concept  of  an \nevolutionary  arms  race  has  been  defined  as  follows:  an \nadaptation  in  one  lineage  (e.g.  predators)  may  change  the \nselection  pressure  on  another  lineage  (e.g.  prey),  giving  rise \nto  a  counter-adaptation  [37].  This  process  produces  the \nconditions  of  a  positive  feedback  loop  where  one  lineage \npushes  the  other  to  better  adapt  and  vice  versa.  We  propose \nthat  such  a  positive  feedback  loop  is  a  key  driving  force  for \nachieving  an  important  step  towards  the  development  of \nmachine general intelligence.  \n \nA  first  step  for  achieving  this  objective  is  the  computational \nmodeling  of  two  populations  of  embodied  cognitive  agents, \n\n\f \n\npreys  and  predators,  each  agent  being  driven  by  the  cognitive \narchitecture  proposed  in  the  previous  section.  Basic  survival \nbehaviors  are  implemented  as  sensorimotor  control  loops \noperating  in  the  Reactive  layer,  where  predators  hunt  preys, \nwhile  preys  escape  predators  and  are  attracted  to  other  food \nsources.  Since  these  agents  adapt  to  environmental  constraints \nthrough  learning processes occurring  in  the upper  levels of  the \narchitecture,  they  will  reciprocally  adapt  to  each  other.  A \ncognitive  adaptation  (in  term  of  learning)  of  members  of  one \npopulation  will  perturb  the  equilibrium  attained  by  the  others \nfor self-regulating  their own internal variables, forcing them to \nre-adapt  in  consequence.  This  will  provide  an  adequate  setup \nfor studying the conditions of entering in a cognitive arms race \nbetween  populations,  where  both  reciprocally  improve  their \ncognitive abilities against each other. \nIt  is  interesting  to  note  that  there  exist  precursors  of  this \nconcept  in  the  recent  literature  under  a  quite  different  angle. \nAn  interesting  example  is  a  Generative  Adversarial  Network \n[38],  where  a  pattern  generator  and  a  pattern  discriminator \ncompete  and  adapt  against  each  other. Another  example  is  the \nAlphaGo  program  [4]  which  was  partly  trained  by  playing \ngames  against  itself,  consequently  improving  its  performance \nin  an  iterative  way.  Both  these  systems  owe  their  success  in \npart  to  their  ability  to  enter  in  a  positive  feedback  loop  of \nperformance improvement.  \n\nIV.  CONCLUSION \n\nBuilding  upon  recent  advances  in  Artificial  Intelligence    and \nMachine Learning, we  have proposed  in  this paper a cognitive \narchitecture,  called  DAC-EAI,  allowing \nthe  conceptual \ndescription  of  many  Artificial  Intelligence  systems  within  a \nunified  framework.  Then  we  have  proposed  the  concept  of  a \ncognitive  arms  between  embodied  agent  population  as  a \npotentially  powerful  driving  force  for  the  development  of \ncognitive complexity. \nWe  believe  that  these  two  research  directions,  summarized  by \nthe  keywords  integration  and  embodiment,  are  key  challenges \nfor  leveraging  the  recent  advances  of  the  field  toward  the \nachievement  of  General  Artificial  Intelligence.  This  ambitious \nobjective  requires  integrating  all  aspects  of  cognition  (from \nlow-level  sensorimotor  coupling \nto  high-level  symbolic \nreasoning)  within  an  embodied  cognitive  architecture,  as  well \nas  to  consider  the  strongly  non-stationary  nature  of  ecological \nconditions  shaping  the  cognitive  development  of  biological \norganisms.  \nThe main lesson of our integrative effort at the cognitive  level, \nas  summarized  in  Figure  2,  is  that  powerful  algorithms  and \ncontrol systems are existing which,  taken  together, span all the \nrelevant  aspects  of  cognition  required  to  solve  the  problem  of \nGeneral  Artificial  Intelligence2.  We  see  however  that  there  is \nstill  a  considerable  amount  of  work  to  be  done  in  order  to \nintegrate  all  the  existing  sub-parts  into  a  coherent  and \ncomplete  cognitive  system.  This  effort  is  central  to  the \nresearch  program  of  our  group  and  we  have  already \ndemonstrated our  ability  to  implement complete version of  the \narchitecture (see [16], [24] for our most recent contributions). \n\n \n\n2 But this does not mean those aspects are sufficient to solve the problem.  \n\nAs  we  already  noted  in  previous  publications  [15],  [26],  [39], \n[40],  there  is  however  a  missing  ingredient  in  these  systems \npreventing  them  to  being  considered  at  the  same  level  as \nanimal  intelligence:  they  are  not  facing  the  constraint  of  the \nmassively  multi-agent  world  in  which  biological  systems \nevolve.  We  propose  here  that  a  key  constraint  imposed  by  a \nmulti-agent world  is  the  emergence of positive  feedback  loops \nbetween  competing  agent  populations,  forcing \nto \nthem \ncontinuously adapt against each other.  \nOur  approach  facing  several  important  challenges.  The  first \none  is  to  leverage  the  recent advances  in  robotics and machine \nlearning \ntoward \nthe  achievement  of  general  artificial \nintelligence, based on  the principled methodology provided by \nthe  DAC  framework.  The  second  one  is  to  provide  a  unified \ntheory  of  cognition  [41]  able  to  bridge  the  gap  between \ncomputational  and  biological  science.  The  third  one  is  to \nunderstand  the  emergence  of  general  intelligence  within  its \necological  substrate,  i.e.  the  dynamical  aspect  of  coupled \nphysical and cognitive systems. \n\n ACKNOWLEDGMENT \n\nWork  supported  by  ERCs  CDAC  project:  \"Role  of \nin  Adaptive  Behavior\" \nConsciousness \n(ERC-2013-ADG \n341196)  &  EU \nproject  Socialising  Sensori-Motor \nContingencies \n(socSMC-641321H2020-FETPROACT-\n2014). \n\n[1] \n\n[2] \n\n[3] \n\n[4] \n\n[5] \n\n[6] \n\n[7] \n\n[8] \n\nV.  REFERENCES \n\nO.  Russakovsky,  J.  Deng,  H.  Su,  J.  Krause,  S.  Satheesh,  S. Ma,  Z. \nHuang,  A.  Karpathy,  A.  Khosla,  M.  Bernstein,  A.  C.  Berg,  and  L. \nFei-Fei, ImageNet Large Scale Visual Recognition Challenge,  Int. \nJ. Comput. Vis., vol. 115, no. 3, pp. 211252, 2015. \nA. Karpathy  and L. Fei-Fei, Deep Visual-Semantic Alignments for \nGenerating  Image  Descriptions,  in  Proceedings  of  the  IEEE \nConference  on  Computer  Vision  and  Pattern  Recognition ,  2015, \npp. 31283137. \nV. Mnih, K. Kavukcuoglu, D.  Silver, A. A. Rusu,  J. Veness, M. G. \nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, \nS.  Petersen,  C.  Beattie,  A.  Sadik,  I.  Antonoglou,  H.  King,  D. \nKumaran, D. Wierstra, S. Legg, D. Hassabis, others, S. Petersen, C. \nBeattie,  A.  Sadik,  I.  Antonoglou,  H.  King,  D.  Kumaran,  D. \nWierstra,  S.  Legg,  and  D.  Hassabis,  Human -level  control  through \ndeep  reinforcement  learning,  Nature,  vol. 518, no. 7540, pp. 529\n533, Feb. 2015. \nD.  Silver, A. Huang, C.  J. Maddison, A. Guez, L. Sifre, G. van den \nDriessche,  J.  Schrittwieser,  I.  Antonoglou,  V.  Panneershelvam,  M. \nLanctot,  S.  Dieleman,  D.  Grewe,  J.  Nham,  N.  Kalchbrenner,  I. \nSutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and \nD. Hassabis,  Mastering  the game of Go with deep neural networks \nand  tree  search,  Nature,  vol.  529,  no.  7587,  pp.  484489,  Jan. \n2016. \nM.  Bellemare,  S.  Srinivasan,  G.  Ostrovski,  T.  Schaul,  D.  Saxton, \nand  R.  Munos,  Unifying  count-based  exploration  and  intrinsic \nmotivation,  in  Advances  in  Neural  Information  Processing \nSystems, 2016, pp. 14711479. \nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, \nHierarchical  deep  reinforcement  learning:  Integrating  temporal \narXiv \nPrepr. \nabstraction \nand \nintrinsic  motivation, \narXiv1604.06057, 2016. \nB.  M.  Lake,  T.  D.  Ullman,  J.  B.  Tenenbaum,  and  S.  J.  Gershman, \nBuilding  Machines  That  Learn  and  Think   Like  People,  Behav. \nBrain Sci., Nov. 2017. \nB. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, Human -level \nconcept  learning  through  probabilistic program  induction,  Science \n(80-. )., vol. 350, no. 6266, pp. 13321338, Dec. 2015. \n\n\fS. M. LaValle, Planning Algorithms. 2006. \nZ.  Chen  and  B.  Liu,  Lifelong  Machine  Learning .  Morgan  & \nClaypool Publishers, 2016. \nJ.-C.  Latombe,  Robot  motion  planning ,  vol.  124.  Springer  Science \n& Business Media, 2012. \nX.  Guo,  S.  Singh,  H.  Lee,  R.  L.  Lewis,  and  X.  Wang,   Deep \nlearning  for  real-time  Atari  game  play  using  offline  Monte-Carlo \nin  Advances \nin  neural \ninformation \ntree  search  planning, \nprocessing systems, 2014, pp. 33383346. \nA.  Graves,  G.  Wayne,  M.  Reynolds,  T.  Harley,  I.  Danihelka,  A. \nGrabska-Barwiska,  S.  G.  Colmenarejo,  E.  Grefenstette,  T. \nRamalho,  and  others,  Hybrid  computing  using  a  neural  network \nwith dynamic external memory, Nature, pp. 14764687, 2016. \nA. Graves, G. Wayne, and I. Danihelka, Neural Turing Machines, \nArxiv, vol. abs/1410.5, 2014. \nR.  Dawkins  and  J.  R.  Krebs,  Arms  races  between  and  within \nspecies.,  Proc.  R.  Soc.  Lond.  B.  Biol.  Sci. ,  vol.  205,  no.  1161,  pp. \n489511, 1979. \nI.  Goodfellow,  J.  Pouget-Abadie,  M.  Mirza,  B.  Xu,  D.  Warde-\nFarley,  S.  Ozair,  A.  Courville,  and  Y.   Bengio,  Generative \nadversarial  nets,  in  Advances  in  neural  information  processing \nsystems, 2014, pp. 26722680. \nX. D. Arsiwalla, I. Herreros, C. Moulin-Frier, M. Sanchez, and P. F. \nM.  J.  Verschure,  Is  Consciousness  a  Control  Process?,  in \nInternational  Conference  of  the  Catalan  Association  for  Artificial \nIntelligence, 2016, pp. 233238. \nC.  Moulin-Frier  and  P.  F.  M.  J.  Verschure,  Two  possible  driving \nthe  evolution  of  animal  communication. \nforces  supporting \nComment \non \nTowards \na  Computational  Comparative \nNeuroprimatology:  Framing  the  language-ready  brain  by  Michael \nA. Arbib, Phys. Life Rev., 2016. \nA. Newell, Unified  theories of  cognition . Harvard University Press, \n1990. \n\n[31] \n[32] \n\n[33] \n\n[34] \n\n[35] \n\n[36] \n\n[37] \n\n[38] \n\n[39] \n\n[40] \n\n[41] \n\n \n\n \n\n[9] \n\n[10] \n\n[11] \n\n[12] \n\n[13] \n\n[14] \n\n[15] \n\n[16] \n\n[17] \n\n[18] \n\n[19] \n\n[20] \n\n[21] \n\n[22] \n\n[23] \n\n[24] \n\n[25] \n\n[26] \n\n[27] \n\n[28] \n\n[29] \n\n[30] \n\nA.  E.  Stahl  and  L.  Feigenson,  Observing  the  unexpected  enhances \ninfants  learning  and  exploration,  Science  (80-.  ).,  vol.  348,  no. \n6230, pp. 9194, 2015. \nM.  Johnson,  K.  Hofmann,  T.  Hutton,  and  D.  Bignell,  The Malmo \nPlatform  for  Artificial  Intelligence  Experimentat ion,  Int.  Jt.  Conf. \nArtif. Intell., pp. 42464247, 2016. \nC. Beattie,  J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. \nKttler,  A.  Lefrancq,  S.  Green,  V.  Valds,  A.  Sadik,  and  others, \nDeepMind Lab, arXiv Prepr. arXiv1612.03801 , 2016. \nG. Brockman, V. Cheung, L. Pettersson,  J. Schneider, J. Schulman, \nJ.  Tang,  and  W.  Zaremba,  OpenAI  Gym,  in  arXiv  preprint \narXiv:1606.01540, 2016. \nJ.  Z.  Leibo,  V.  Zambaldi,  M.  Lanctot,  J.  Marecki,  and  T.  Graepel, \nMulti-agent  Reinforcement  Learning \nin  Sequential  Social \nDilemmas, Feb. 2017. \nA.  Tampuu,  T.  Matiisen,  D.  Kodelja,  I.  Kuzovkin,  K.  Korjus,  J. \nAru,  J.  Aru,  and  R.  Vicente,  Multiagent  Cooperation  and \nCompetition with Deep Reinforcement Learning, Nov. 2015.  \nC.  Moulin-Frier,  X.  D.  Arsiwalla,  J.-Y.  Puigb,  M.  Snchez-Fibla, \nA.  Duff,  and  P.  F.  M.  J.  Verschure,  Top-Down  and  Bottom-Up \nInteractions  between  Low-Level  Reactive  Control  and  Symbolic \nRule  Learning  in  Embodied  Agents,  in  Proceedings  of  the \nWorkshop  on  Cognitive  Computation:  Integrat ing  neural  and \nsymbolic  approaches.  30th  Annual  Conference  on  Neural \nInformation Processing Systems (NIPS 2016) , 2016. \nC. Moulin-Frier, T. Fischer, M. Petit, G. Pointeau,  J.-Y. Puigbo, U. \nPattacini,  S.  C.  Low,  D. Camilleri,  P. Nguyen, M. Hoffmann, H.  J. \nChang,  M.  Zambelli,  A.-L.  Mealier,  A.  Damianou,  G.  Metta,  T. \nPrescott,  Y.  Demiris,  P.-F.  Dominey,  and  P.  Verschure,  DAC-h3: \nA  Proactive  Robot  Cognitive  Architecture  to  Acquire  and  Express \nKnowledge About  the World and the Self, Submitt. to IEEE Trans. \nCogn. Dev. Syst., 2017. \nA.  Newell,  J.  C.  Shaw,  and  H.  A.  Simon,  Report  on  a  general \nproblem-solving program, IFIP Congr., pp. 256264, 1959. \nJ.  E.  Laird,  A.  Newell,  and  P.  S.  Rosenbloom,  SOAR:  An \narchitecture  for  general  intelligence,  Artificial  Intelligence,  vol. \n33, no. 1. pp. 164, 1987. \nJ.  R.  Anderson,  The  Architecture  of Cognition . Harvard University \nPress, 1983. \nR.  Brooks,  A  robust  layered  control  system  for  a  mobile  robot, \nIEEE J. Robot. Autom., vol. 2, no. 1, pp. 1423, 1986. \nR.  A.  Brooks,  Intelligence  without  representation,  Artif.  Intell., \nvol. 47, no. 13, pp. 139159, 1991. \nP.  F.  M.  J.  Verschure,  T.  Voegtlin,  and  R.  J.  Douglas, \nEnvironmentally  mediated  synergy  between  perception  and \nbehaviour  in  mobile  robots,  Nature,  vol.  425,  no.  6958,  pp.  620\n624, 2003. \nP.  F.  M.  J.  Verschure,  C.  M.  A.  Pennartz,  and  G.  Pezzulo,  The \nwhy, what, where, when  and  how  of  goal-directed  choice: neuronal \nand  computational  principles,  Philos.  Trans.  R.  Soc.  B  Biol.  Sci. , \nvol. 369, no. 1655, p. 20130483, 2014.  \nG. Maffei,  D.  Santos-Pata,  E. Marcos, M.  Snchez-Fibla,  and  P.  F. \nM.  J.  Verschure,  An  embodied  biologically  constrained  model  of \nforaging:  From  classical  and  operant  conditioning  to  adaptive  real-\nworld behavior in DAC-X, Neural Networks, 2015. \nV.  Vouloutsi,  M.  Blancas,  R.  Zucca,  P.  Omedas,  D.  Reidsma,  D. \nDavison,  and  others,  Towards  a  Synthetic  Tutor  Assistant:  The \nEASEL  Project  and  its  Architecture,  in  International  Conference \non Living Machines, 2016, pp. 353364. \nP.  F.  M.  J.  Verschure,  Synthetic  consciousness:  the  distributed \nadaptive  control  perspective,  Philos.  Trans. R. Soc. Lond. B. Biol. \nSci., vol. 371, no. 1701, pp. 263275, 2016. \nG.  E.  Hinton  and  R.  R.  Salakhutdinov,  Reducing \nthe \nDimensionality  of  Data  with  Neural  Networks,  Science  (80-.  )., \nvol. 313, no. 5786, 2006. \nR.  S.  Sutton  and  A.  G.  Barto,  Reinforcement  Learning:  an \nintroduction. The MIT Press, 1998. \nS.  Hochreiter  and  J.  Schmidhuber,  Long  Short -Term  Memory, \nNeural Comput., vol. 9, no. 8, pp. 17351780, Nov. 1997. \nA. Baranes  and P.-Y. Oudeyer, Active Learning of Inverse Models \nwith  Intrinsically  Motivated  Goal  Exploration  in  Robots,  Rob. \nAuton. Syst., vol. 61, no. 1, pp. 4973, 2013. \n\n\f", 
        "tag": "Multiagent Systems", 
        "link": "https://arxiv.org/list/cs.MA/new"
    }, 
    {
        "text": "Rotation, Scaling and Translation Analysis of Biometric Signature Templates \nAman Chadha, Divya Jyoti, M. Mani Roja  \nThadomal Shahani Engineering College, Mumbai, India  \naman.x64@gmail.com \n\n \n  \n\nAbstract \n \nBiometric  authentication  systems  that  make  use  of \nsignature  verification  methods  often  render  optimum \nperformance  only  under \nlimited  and \nrestricted \nconditions.  Such  methods  utilize  several \ntraining \nsamples  so  as  to  achieve  high  accuracy.  Moreover,  \nseveral constraints are  imposed on  the end-user  so  that \nthe  system  may  work  optimally,  and  as  expected.  For \nexample,  the user  is made  to sign within a small box,  in \norder  to  limit  their  signature  to  a  predefined  set  of \ndimensions,  thus  eliminating  scaling.  Moreover,  the \nangular  rotation  with  respect \nto \nthe  referenced \nsignature  that  will  be  inadvertently  introduced  as \nhuman  error,  hampers  performance  of  biometric \nsignature  verification  systems.  To  eliminate \nthis, \ntraditionally, a user  is asked  to sign  exactly on  top of a \nreference  line.  In  this  paper,  we  propose  a  robust \nsystem  that  optimizes  the  signature  obtained  from  the \nuser  for a  large range of variation  in Rotation-Scaling-\nTranslation  (RST)  and  resolves  these  error  parameters \nin  the  user  signature  according  to  the  reference \nsignature stored in the database. \n \nKeywords:  rotation;  scaling; \ntranslation;  RST; \nimage registration; signature verification. \n \n1.  Introduction  \n \n\nThe  aim  o f  a  biometric  verificat ion  system  is   to \ndetermine  if  a  person  is  who  he/she  purports  to  be, \nbased  on  one  or  more  intrinsic,  physical  or  behaviora l \nattributes.  This  trait  or  biometric  attribute  can  be  the \nsignature,  voice,  iris,  face,  fingerpr int,  hand  geometry \netc.  \nA  simple  biometric  system  has  a  sensor  module,  a \nfeature  extraction  module,  a  matching  module  and  a \ndecision  making  module.  The  sensor  module  acquires \nthe  biometric  data  of  an  individual.  In  this  case,  the \ndigital  pen  tablet  functions  as  the  sensor.  In  the  feature \nextraction  module,  the  acquired  biometric  data  is \nprocessed to extract a feature set that represents the data. \nFor  example,  the  position  and  orientation  of  certain \nspecific  points  in  a  signature  image  are  extracted  in  the \nfeature  extraction  module  o f  a  signature  authentication \nsystem.  In  the  matching  module,  the  extracted  feature  \n\n \n \nset  is  compared  against  that  of  the  template  by \ngenerating a matching score. In this module, the number  \nof  matching  points  between  the  acquired  and  reference \nsignatures  are  determined,  and  a  matching  score  is \nobtained.  Decision-making  involves  either  verification \nor  identification.  In  the  decision-making  module,  the \nuser's  claimed  identity  is  either  accepted  or  rejected \nbased  on \nthe  matching  score, \ni.e.,  verification. \nAlternately,  the system may  identify a user based on  the \nmatching scores, i.e., identification [1],[11]. \nSignature  recogni tion  is  one  of  the  oldest  biometric \nauthentication  methods,  with  wide-spread \nlegal \nacceptance.  Handwritten  signatures  are  commonly  used \nto  approbate  the  contents  of  a  document  or  to \nauthenticate a  financial  transaction [1]. A  trivial method \nof  signature  verificat ion  is  visual  inspection.  A  manual \ncomparison  of  the  two  signatures  is  done  and  the  given \nsignature  is  accepted  if  it  is  sufficient ly  similar  to  the \nreference  signature,  for  example,  on  a  credit-card.   In \nmost  scenarios,  where  a  signature  is  used  as  the  means \nof authentication, no verificat ion takes place at all due to \nthe  entire  process  being  excessively  t ime  intensive  and \ndemanding. An automated signature verification process \nwill  help  improve  the  current  situation  and  thus, \neliminate  fraud.  Well-known biometric methods  include \niris,  retina,  face  and  fingerprint  based  identification  and \nverification.  Even  though  human  features  such  as  iris, \nretina and  fingerprints do not change over time and have \nlow  intra-class  variation,  i.e.,  the  variations  in  the \nrespective  biometric  attribute  are  low,  special  and \nrelatively  expensive  hardware  is  needed  for  data \nacquisit ion  in  such  systems. An  important  advantage  of \nsignatures \nas \nthe  human \ntrait \nfor  biometric \nauthentication over other attributes is their  long standing \ntradition  in  many  commonly  encountered  verification \ntasks.  In  other  words,  signature  verificat ion  is  already \naccepted  by  the  general  public.  In  addi tion,  it  is  also \nrelatively  less  expensive  than  the  other  biometric \nmethods [1],[2]. \nThe  difficult ies  associated  with  signature  verification \nsystems due to the extensive  intra-class variations, make \nsignature  verification  a  difficult  pattern  recognit ion \nproblem.  Examples  of  the  various  alterations  observed \nin  the  signature of  an  individua l have been  illustrated  in \nFig.  1. \n\nAman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1419ISSN:2229-6093\f \n\n \n\n \n\n \nFigure 1: Intra-class variations, i.e. variations \nin the signature of an individual \nDepending  on \nthe  data  acquisit ion  method, \nautomatic  signature verificat ion  can be  divided  into  two \nmain  types:  off-line  and  on-line  signature  verification. \nThe  most  accurate  systems  almost  always \ntake \nadvantage  of  dynamic \nfeatures \nlike  acceleration, \nvelocity  and  the  difference  between  up  and  down \nstrokes  [3].  This  class  of  so lut ions  is  called  on-line \nsignature  verification.  However  in  the  most  common \nreal-world  scenarios,  because  such  systems  require  the \nobservation  and  recording  off  the  signing  process,  this \ninformation  is  not  readily  available.  This  is  the  main \nreason,  why  static  signature  analysis  is  still  in  focus  of \nmany  researchers.  On-line  signature  verificat ion  uses \nspecial  hardware,  such  as  a  digit izing  tablet  or  a \npressure  sensitive  pen,  to  record  the  pen  movements \nduring  writ ing.  In  addition  to  shape,  the  dynamics  of \nwriting  are  also  captured  in  on-line  signatures, which  is \nnot   present  in  the  2-D  representation  of  the  signature \nand  hence  it  is  difficult  to  forge.  Off-line  methods   do \nnot  require special acquisit ion hardware, just a pen and a \npaper,  and  are  therefore  less  invasive  and  more  user \nfriendly.  In  the  past  decade  a  bunch  of  solutions   has \nbeen  introduced,  to  overcome  the  limitations  of  off-line  \nsignature  verification  and  to  compensate  for  the  loss  of \naccuracy  [2],[3].  In  off-line  signature  verification,  the \nsignature is available on a document which is scanned to \nobtain  its  digital  image.  In  all  applications  where \nhandwritten  signatures  currently  serve  as  means  o f \nauthentication,  automatic  signature  verification  can  be \nused  such  as  cashing  a  check,  signing  a  credit  card \ntransaction  or  authenticating  a \nlegal  document. \nBasically,  any  system  that  uses  a  password  can  instead \nuse  an  on-line  signature  for  access.  The  advantages  are \nsuch  systems  are  obvious   a  signature  is more difficult \nto  steal  or  guess  than  a  password  and  is  also  easier  to \nremember for the user.  \nHowever,  the  high  level  of  intra-class  variations  in \nsignatures,  as  shown  in  Fig.  1,  hinder  the  performance \nof  signature  verification  systems  and  thus  minimize  the \n\naccuracy  of  such  systems.  Hence,  to  reduce  errors  and \nthe inefficiency problems associated with these systems, \nthe  intra-class  variat ions  in  the  signatures  need  to  be \nminimized.   This  involves  eliminating  or  reducing  the \nrotation,  scaling  and  translation  factors  between  the \nreference and the test signature images. Fig. 2 shows the \ndiagram  o f  a  typical  signature  verification  system  with \nrotation, scaling and  translation (RST) cancellation. The \nreference  image within  the  database  and  the  user  image \nact  as  inputs  to  the  system.  Feature  extraction  is  done \nfrom  the  reference  signature  which  describes  certain \ncharacteristics of  the  signature  and  stored  as  a  template. \nFor  verification,  the  same  features  are  extracted  from \nthe test signature and compared to the template. \n \n\n \n \n\nFigure 2: A typical signature verification \nsystem with RST  cancellation \nIt  should  be  noted  that  a  dist inct  advantage  of  the \nproposed  system,  illustrated  in  Fig. 2,  is  that  it  does not \nrequire mult iple signature reference samples  for training \nin  order  to  achieve  high  levels  of  accuracy.  Previous \nwork  by  researchers  has  witnessed  the  use  of  affine  \ntransformation  for  calculating  the  angular  rotation \nbetween two images, the scaling and translation [4]-[7].  \nIn  this  paper,  we  propose  the  use  of  the  concept  of \ncorrelation to identify the rotation and a simple cropping \nmethod  to  eliminate  scaling  and  translation,  thereby \ncreating  an  optimum  template  after  subjecting  the  user \nimage to RST correction.  \n \n2. Idea of the proposed solution \n  \n\nThe  foremost  concern  is  fetching  the  angle  of \nrotation  between  the  user  and  the  reference  images.  In \norder  to  achieve  this,  the  concept   of  correlation  is \ndeployed.  The \nterm  correlat ion \nis  a  statistica l \nmeasure,  which  refers  to  a  process  for  establishing \nwhether  or  not   relat ionships  exist  between \ntwo \nvariables  [8] .  The  maximum  value   of  cross-correlation \nbetween  the  original,  i.e.,  the  reference  image  and  the \nuser  image  is  found  by  means  of  repetit ive  iterations \ninvo lving \nthe  cross-correlation \nthe  calculation  of \nbetween the two  images  in quest ion.   \nThe  proposed  algorithm  essent ially  finds  the  cross-\ncorrelat ion  between  origina l  image  and  the  user  image.  \nIf  X(m,  n)  is  reference  image  and  Y(m,  n)  is  the  user \n\nAman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1420ISSN:2229-6093\fmn\n\n0\n\n\n\n)\n\n \n\nY\n0\n\n(1) \n\nimage  then  the  cross-correlat ion  r  between  X  and  Y  is \ngiven by the fo llowing equat ion: \n \n(\n) (\nY\nr\nX\nX\n=\n\nmn\nm n\nMinimum  value  o f  r  ind icates  dissimilarity  of \nimages  and  for  the  same  image  (autocorrelat ion)  it will \nhave  a  peak  value  so  as  to  indicate  maximum \ncorrelat ion. X0  and Y0  represent mean of  Image X  and \nY respectively.  \nWe  use  normalized  cross-correlat ion  to  simplify \nanalys is  and  comparisons  of  coefficient  values \ncorresponding  to  the  respective  angular  values.  Min-\nmax  normalizat ion  is  the  procedure  used  to  obtain \nnormalized \n[9].  Min-max \ncross-correlat ion \nnormalizat ion  preserves  the  relat ionships  among  the \noriginal  data  values.  The  normalizat ion  operation \ntransforms  the  data  into  a  new  range,  generally  [0,  1].  \nGiven  a  data  set  xi,  such  that  i  =  1,  2,  .  .  ,  n,  the \nnormalized value x  is given by the fo llowing equation: \nx\nx\nmin(\n)\n\ni\nx\nx\n) min(\n)\nmax(\n\ni\ni\nThe  second  aim  is  to  deal  with  the  translat ion \nassociated  with  the  images.  This  is  achieved  by  a \nsimple  cropping  technique.  Init ially,  we  calculate  the \nnumber  o f  rows  and  columns  bordering  the  signature \npixels  within  the  image.  The  image  devo id  of  these \nrows  and  co lumns  is  extracted.  The  result  is  an  image \nconsist ing  of  only  the  signature  pixels.  Add it iona l \nbackground surrounding the  image  is thus eliminated. \nThird  factor  is  the  scaling  between  the  two  images. \nFor  calculat ion  of  the  scaling  factor,  the  cropped \nimages  obtained  dur ing  translat ion  are  utilized.  The \nsize  o f  the  reference  image  divided  by  the  size  of  the \nuser  image gives the scaling ratio. \nThe proposed solut ion  is  illustrated by Fig. 3. \n\n(2) \n\nx\n\n'\n\n=\n\n \n\n \n \n\nFigure 3: Schematic block diagram of the \nproposed system \n3. Implementation steps \n \n3.1. Image acquisition and pre-processing  \n \n\nOur  image   acqu isit ion  is  inherent ly  simple  and does \nnot  employ  any  special  illuminat ion.  The  system \nimplemented  here  uses  a  digital  pen  tablet,  namely,  \nWACOM  Bamboo  [10],  as  the  data-capturing  device.  \nThe pen has  a  touch  sensit ive  switch  in  its  t ip  such  that \n\nonly  pen-down  samples  (i.e., when  the  pen  touches  the \npaper)  are  recorded.  The  database  consists  of  a  set  of \nsignature  samples  of  90  people.  For  each  person,  there \nare  9  test  images  and  1  training  or  reference  image  in \nthe  database.  Upon  signature  acqu isit ion,  the  next   step \nis  co lour  normalizat ion  and  binarizat ion.  Colour \nnormalizat ion  is  the  conversion  o f  the  image  from  the \nRGB  form  to  the  corresponding  Grayscale  image.  \nBinarizat ion  is  the  conversion  of  this  grayscale  image \nto  an  image  cons ist ing  of  two  luminance  elements, \nnamely,  black  and  white.  On  complet ion  o f  the  image \nacquisit ion  and  pre-processing  stage,  the  resultant \nimage  thus  obtained,  becomes  ready  for  the  corrective \nphases: rotation, scaling and translat ion cancellat ion.  \n \n\n \n Figure 4: Wacom Bamboo Digital Pen Tablet \n\n  \n3.2. Rotation correction  \n \nWhile  co llect ing  signature  samples,  it was observed \nthat  users  gave  consecut ive  samples  having  angu lar  \nvariat ions  approximately  from  60  to  +60.  Hence, \nbefore  feature  extraction,  the  user  image  should  be \naligned  with  the  reference  image.  For  simplicity  in \ncomputing  rotation  angle,  we  choose  to  align  the \nreference  image  with  the  trial  image  fetched  from  the \nuser, i.e., the user  image.  \nThe  preprocessed  reference  image  is  cropped  in \norder  to  extract  only  the  signature  pixels  without  any \naddit ional  background  and  used \nfor  all \nfurther \ncomputations.  In  order  to  make  the  program  time \nefficient  and  less  resource  intensive,  two  stages  of \nrotation  correction  are  applied.  The  first  stage  is \ndesigned  to  offer  a  relat ively  lower  reso lut ion  o f  5  so  \nas  to  offer  an  approximate  value  of  the  angle  o f \nrotation.  In  contrast,  the  second  stage  is  designed  for  a \ncomparat ively  higher  resolut ion. Within  a  range  of  +3 \nto  3  of  the  approximate  value,  a  reso lut ion  of  1  is \nselected for a more precise value of the rotation angle.  \nAfter  pre-processing,  the  user  image  is  then  rotated \nby  5  within  the  range  o f  60  to  +60  in  successive \niterat ions.  Cross-correlat ion  values  between \nthe \nreference  image  and  the  user  image  are  recorded  on \ncomplet ion  o f  each  iteration  of  the  rotation  process. \nThe  maximum  cross-correlat ion  value  refers  to  the \ncorrect angle o f rotation within a 5 range, further, after \nthe  approximate  angle  value  is  obtained,   +3  or  3  o f \n\nAman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1421ISSN:2229-6093\fthis  angle  can  be  inspected  for  maximum  correlation \nvalue  which  corresponds  to  angle  of  rotation  accurate \nto up to 1. The user  image  is rotated by the negat ive o f \nthe  angle  thus  obtained,  and  then  subjected  to  feature \nextraction. Thus, rotation cancellat ion is achieved.  \nThe steps  invo lved  in the rotation correction process \ncan be summarized as fo llows: \n1)  Obtain user image and the reference image. \n2)  Carry  out  pre-processing  by  converting  both \nimages \nperforming \nand \ngrayscale \nto \nnormalization. \n3)  Trim  the  reference  signature  to  remove  any \nexcess background; this will act as the template. \n4)  Starting  with  the angle as  60,  in  increments of  \n5, \nrecord  normalized  correlation  values \nbetween pre-processed reference  image and user \nimage. \n5) \nIf angle is less than or equal to 60, go to step 4. \n6)  Maximum  correlation  value  corresponds \nto \nangle  within  a  5  range.  Let  this  angular  value \nbe x. \n7)  Starting with  the angle as  (x  3),  in  increments \nof  1,  record  normalized  correlation  values \nbetween  the  preprocessed  reference  image  and  \nthe user image. \nIf  angle  is  less  than  or  equal  to  (x  +  3),  go  to \nstep 7. \n9)  Correct  the  user  image  by  the  obtained  angle \nand proceed for further correction, if required. \nFig.   5 \nshows  a \nreference \nimage  and \nthe \ncorresponding  image rotated by 20.9. \n \n\n8) \n\n \nFigure 5: Reference image and the \ncorreponding rotated image \n \n\n \n\n3.3. Scaling correction  \n \n\nAn  end  user  o ften  modifies  his/her  signature \naccording  to  the  size  of  signing  box.  For  smaller  \nspaces,  the  signature  may  be  compressed,  for  no  space \nlimitation,  the  sign  may  be  enlarged.  Thus,  before \nextraction  o f  feature  points,  it  is  essent ial  that  any \nscaling,  if present  in  the  test sample,  be removed. Upon \ntrimming  both  images,  the  ratio  of  height  gives  Y \n\nscaling and ratio o f width gives X scaling. However,  to \nresize  the  user  image  and  make  it  the  same  size  as  the \nregistered  image,  either  of  the  scaling  ratios  can  be \nused.  For  a  rotation  range  of  60  to  +60,  height  was \nobserved  to  vary  significant ly  as  compared  to  the \nlength.  Hence,  Y  scaling  was  chosen  as  the  scaling \nratio.  To  account  for  scaling,  the  above  mentioned \ncropping technique  is applied to both the user as well as \nreference  image.  Scaling  ratio  is  calculated  by  the \nfo llowing equat ion: \nScaling ratio =  Size of the reference image\nSize of the test image\n\n(3) \n\n \n\nThe  user  image  is  resized  as  per  the  obtained \nscaling  rat io  and  then  sent  to  the  feature  extraction \nsegment.  Fig.  6  shows  a  reference  image  and  the \ncorresponding  image  down  scaled  by  a  scaling  ratio  of \n1.4045. \n \n\n \nFigure 6: Reference image and the \ncorresponding down-scaled image  \n\n \n\n \n3.4. Translation correction  \n \n\nOn the apparatus used for taking signature  input, the \nuser  is  free  to  sign  without  using  any  fixed  starting \npo int.  This  may  introduce  translat ion  in  X  and/or  Y \ndirect ion,  having  a  maximum  value  equal  to  the  width \nor  height  of  the  signature  canvas  respect ively.   The \nboundary  condit ions  for  translation  error  are  computed \nassuming that the user starts to sign from the edge.  \nThis  problem  is  overcome  by  cropping  the  pre-\nprocessed  reference  image  so  as  to  extract  only  the \nsignature  pixels  without  any  addit ional  background. \nThis  cropping  process  truncates  the  extra  background \nregion by  trimming  the  image  canvas. Thus,  translation \nis  removed  completely.  For  representational  purposes, \nbottom  left corner of test image  is assumed  to be or igin.   \nThe  number  of  co lumns  from  left  and  number  of \nrows  from  the  bottom,  which  contain  no  black  pixels \ncorresponding  to  the  actual  signature,  i.e.,  which \nconsist solely o f  image-background, are counted. These \nvalues  give  X \ntranslat ion \nand  Y \ntranslation \nrespectively.  Fig.  7  shows  a  reference  image  and  the \n\nAman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1422ISSN:2229-6093\fcorresponding  image  translated  by  35px  along  X-Axis \nand 9px along Y-Axis. \n \n\n \n\n \n\n \nFigure 7: Reference image and the \ncorresponding translated image \n\n  \n3.5. Combined RotationScalingTranslation \n \n\nIt  is  easy  to  manipulate  the  samples  to  get  pure \nrotation,  translat ion  and  scaling,  however,  for  actual \nsignatures,  all  the  above  ment ioned  factors  are  altered \nsimultaneous ly. Hence,  rotation,  translat ion  and  scaling  \ncorrections are applied  in the same order.  \nRotation  correction  precedes  translat ion  correction \nas  the  assumed  origin  at  bottom  left  corner  also  gets \nrotated  and  translation  effects  cannot  be  eliminated \nunless  the origin  is returned  to bottom  left as accurately \nas  possible.  Therefore,  rotation  correction  needs  to  be \nperformed  first  as  the  scaling  ratio  calculated  by  the \npure  scaling  method  is  not  consistent with  scaling  ratio \nof the rotated image, as shown  in Fig. 8.  \nConseque nt ly,  \nscaling \nof \nthe \neffect iveness \ncorrection  depends,  to  a  large  extent, on  the percentage \nerror obtained dur ing rotation correction.  \n \n\n \n\n \n\n \n\n \n\n \n\nFigure 8: Change in the width and height of the \nimage before and after rotation correction \nAfter correcting the angle of rotation, the user  image \npre-processed  copy  is  cropped  to  eliminate  translation \nand  the  image  so  obtained  is  a  case  of  pure  scaling \nwhich has been discussed above.  \nThus,  rotationscalingtranslat ion  cancellat ion  is \nachieved. \n \n4. Results  \n \n\n4.1. Rotation \n \n\nResults  obtained  for  pur e  rotation  have  been \ntabulated as follows: \n\nTable 1: Results obtained for pure rotation  \n\nSignature \nSamples \nSample 1 \nSample 2 \nSample 3 \nSample 4 \nSample 5 \nSample 6 \nSample 7 \nSample 8 \nSample 9 \nSample 10  \n\nActual \nAngle \n-60 \n-48 \n-20 \n-6 \n0 \n4 \n13 \n27 \n37 \n59 \n\nDetected \nAngle \n-60 \n-48 \n-20 \n-6 \n0 \n4 \n13 \n27 \n37 \n60 \n\n% \nError \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n1.69 \n\n \nr\no\nr\nr\nE\n \n%\n\n \n\nFigure 9: Plot of % error values in case of pure \nrotation for various samples \n\n4.2. Scaling \n \nResults  obtained  for  pure  scaling  have  been \ntabulated as follows: \n\nTable 2: Results obtained for pure scaling  \n\nSignature \nSamples \nSample 1 \nSample 2 \nSample 3 \nSample 4 \n\nActual \nScaling Ratio \n7.69 \n5 \n4 \n2.17 \n\nDetected \nScaling Ratio \n10.55 \n5.70 \n4.22 \n2.27 \n\n% \nError \n37.2 \n14 \n5.5 \n4.6 \n\nAman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1423ISSN:2229-6093\fSignature \nSamples \nSample 5 \nSample 6 \nSample 7 \nSample 8 \nSample 9 \nSample 10  \n\nActual \nScaling Ratio \n1.28 \n1 \n0.63 \n0.54 \n0.48 \n0.31 \n\nDetected \nScaling Ratio \n1.34 \n1.05 \n0.66 \n0.57 \n0.49 \n0.33 \n\n% \nError \n4.7 \n5 \n4.8 \n5.6 \n2.1 \n6.5 \n\n \nr\no\nr\nr\nE\n \n%\n\n \nr\no\nr\nr\nE\n \n%\n\nFigure 10: Plot of % error values in case of \npure scaling for various samples \n\n \n\n4.3. Translation \n \nResults  obtained  for  pur e \ntabulated as follows: \n\ntranslat ion  have  been \n\nTable 3: Results obtained for pure translation  \n\nSignature \nSamples \nSample 1 \nSample 2 \nSample 3 \nSample 4 \nSample 5 \nSample 6 \nSample 7 \nSample 8 \nSample 9 \nSample 10  \n\nActual \nTranslation \n0,5 \n5,5 \n10,0 \n15,10 \n0,25 \n25,25 \n25,50 \n50,50 \n50,100 \n150,150 \n\nRecovered \nTranslation \n0,5 \n5,5 \n10,0 \n15,10 \n0,25 \n25,25 \n25,50 \n50,50 \n50,100 \n150,150 \n\n% \nError \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n0 \n\nSignature \nSamples \n\nSample 1 \nSample 2 \nSample 3 \nSample 4 \nSample 5 \n \n \n\n \n \n \n \n\n \n\n \n\nFigure 11: Plot of % error values in case of \npure translation \n\n \n4.4. Rotation-Scaling-Translation \n \nResults  obtained  upon  combining  rotation,  scaling \nand translat ion  have been tabulated as fo llows: \n\nTable 4: Results obtained on combining \nrotation, scaling and translation \nDetected \nActual \nParameters \nParameters \nRotation  Scaling  Rotation  Scaling \n1.90 \n52 \n1.67 \n50 \n1.39 \n10 \n1.33 \n12 \n1.25 \n34 \n1.11 \n31 \n-40 \n0.91 \n-42 \n0.94 \n0.82 \n-32 \n0.8 \n-30 \n\n \n\n \n \n\n \n\n \n\n \n\n \n\nFigure 12: User image before RST  correction \n(top); reference image (bottom-left) and the \nuser image after RST  Correction (bottom-right) \n\nAman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1424ISSN:2229-6093\f \n[5]  G.  Wo lberg  and  S.  Zokai,  Robust \nImage \nRegistration Using Log-Polar Transform, Proceedings \nof   the  IEEE  International  Conference  on  Image \nProcessing, Sep. 2000 pp. 1-2. \n \n[6]  Z.  Y.   Cohen,  Image  registration  and  object \nrecognit ion  using  affine  invariants  and  convex  hulls,  \nIEEE  Transactions  on  Image  Processing,  July  1999, \npp. 1-3. \n \n[7]  N.  Chumchob  and  K.  Chen,  A  Robust  Affine \nImage  Registration  Method,  International  Journal  Of \nNumerical  Analysis  And Modeling, Volume  6, Number  \n2, pp. 311-334. \n \n[8]  R. \nJ.  Rummel,  Understanding  Correlat ion, \nHonolulu:   Depa rtment  of  Po lit ical  Science,  University \nof Hawaii, 1976. \n \n[9]  J.  Han,  M.  Kamber,  Data  mining:  concepts  and \ntechniques, Morgan Kaufmann, 2006, pp. 70-72. \n \n[10]  WACOM  Bamboo  Digital \nwww.wacom.co.in/bamboo, June 2011. \n \n[11]  Henk  C.  A.  van  T ilborg,  Encyclopedia  of \ncryptography and security, Springer, 2005, pp. 34-36. \n\nTablet, \n\nPen \n\n5. Conclusion \n \n\nThe  system  has  been  designed  to  correct  variat ions \nin  angle  of  rotation,  in  the  range  o f  60  to  +60.  \nHowever,  it  can  be  extended  to  cover  the  ent ire  360 \nplanar rotation, to design a fool proof system capable o f \ncreating  an  optimum  template  after  RST  cancellation,  \neven  in  a  situat ion where  the  input   pad  may  have  been \ninverted. On similar  lines,  the reso lut ion of rotation can \nbe  improved  from  1  to  0.5  or  0.25  or  even  more, \nwith  the  trade-off  being  increased  program  execution \ntimes.  \nPure  scaling  and  pure  translat ion  can  be  detected \naccurately  as  long  as signature pixels do  not go beyond \nthe  defining  boundaries  o f  the  template.  For  the \nsignatures  used,  a  maximum  translation  o f  200  pixels \nwas  detected  along  X  and  Y  axes.  Maximum  scaling  \nratio  was  found  to  be  0.55.  However,  maximum \nvariance  of  both  translat ion  and  scaling   may  show \nslight variations from one signature to another. \nFor  combined  RST,  it was  experimentally  observed \nthat  the  correlat ion  approach  tends  to  be  less  reliable \nwith  significant  increase  or  decrease  in  the  scaling \nratio.  Signature  images  used  for  testing  gave  optimum \nresult  for  scaling  ratio,  i.e.,  within  0.67  to  1.33, \nhowever,  the  scaling  range giving  angle  and  translation \naccurately  may  increase  or  decrease  depending  on  the \nsignature sample under test. \nThus,  an  optimum  template  was  generated  by  the \nproposed system after subject ing the user  image to RST \ncorrection with respect to the reference  image.  \n \n6. References  \n \n[1]  A.  K.  Jain,  F.  D.  Griess  and  S.  D.  Connell,  On-\nverificat ion,  Elsevier, \nline \nPattern \nsignature \nRecognit ion 35, 2002, pp. 2963-2972. \n \n[2]  M.  Mani  Ro ja  and  S.  Sawarkar,  A  Hybr id \nApproach  using  Majority  Voting \nfor  Signature \nRecognit ion,  International  Conference  on  Electronics \nComputer Technology (ICECT) 2011, pp. 1-3. \n \n[3]  B.  Kovari,  I.  Albert  and  H.  Charaf,  A  Genera l \nRepresentation  for  Modeling  and  Benchmarking  Off-\nline  Signature  Verifiers,  BME  Publicat ions,   12th \nWSEAS  International  Conference  on  Computers,  2008, \np. 1 \n \n[4]  M.  Holia  and  V.  Thakar,  Image  registration  fo r \nrecovering  affine  transformat ion  using  Nelder  Mead \nInternational \nSimplex  method \nfor  optimization, \nJournal of Image Processing (IJIP), Volume 3, Issue 5, \n2009. pp. 218-221. \n\nAman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1425ISSN:2229-6093\f", 
        "tag": "Multimedia", 
        "link": "https://arxiv.org/list/cs.MM/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nI\nN\n.\ns\nc\n[\n \n \n1\nv\n3\n8\n8\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nSecrecy-Energy Efciency Performance of\n\nUAV-Enabled Communication Networks\n\nXiaohui Qi, Bin Li, Zheng Chu, Kaizhi Huang, and Hongbin Chen\n\n1\n\nAbstract\n\nRecent researches show that unmanned aerial vehicle (UAV) can offer an efcient solution to achieve\n\nwireless connectivity with high mobility and low cost. This letter investigates the secrecy energy efciency\n\n(SEE) in a multi-tier UAV-enabled communication network via a threshold-based access scheme and\n\nmulti-antenna technique, where the UAV-enabled transmitters, legitimate receivers and eavesdroppers are\n\ndeployed randomly. In particular, we rst exploit the association probability of a randomly located receiver\n\nand the activation probability of UAV-enabled transmitters. Then, we analyze the security, reliability, and\n\nSEE of the UAV-enabled networks. Simulation results are provided to show the effect of the predetermined\n\naccess threshold on the reliability as well as security performance, and determine the optimal design\n\nparameters for a given UAV-enabled network to maximize the SEE.\n\nI . INTRODUCT ION\n\nIn recent years, unmanned aerial vehicle (UAV) is emerging as a novel paradigm in civil and military\n\napplications, such as trafc monitoring, disaster rescue, and military reconnaissance [1]. In contrast to\n\na terrestrial transmitter, UAV, as a mobile transmitter, can provide a promising solution to complement\n\nthe capacity and coverage of terrestrial cellular systems, especially in extreme environments without\n\ninfrastructure [2]. On the other hand, information security is a critical issue facing national defense when\n\nX. Qi and K. Huang are with the National Digital Switching System Engineering & Technological Research Center, Zhengzhou\n\n450002, China (e-mails: seven66226067@163.com; huangkaizhi@tsinghua.org.cn).\n\nB. Li is with the School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China (e-mail:\n\nlibin sun@bit.edu.cn).\n\nZ. Chu is with the School of Science and Technology, Middlesex University, London, NW4 4BT, U.K. (e-mail:\n\nz.chu@mdx.ac.uk).\n\nH. Chen is with the Key Laboratory of Cognitive Radio and Information Processing, Guilin University of Electronic\n\nTechnology, Ministry of Education, 1 Jinji Road, Guilin 541004, China (e-mail: chbscut@guet.edu.cn).\n\nApril 7, 2017\n\nDRAFT\n\n\f2\n\npeople rely heavily on wireless network for transmitting private information [3], [4]. Toward this, the\n\nuse of UAV can offer new opportunities for security enhancement via a cooperative air-ground network.\n\nHowever, the performance and operation of a UAV-enabled communication network is constrained by\n\nthe limited onboard energy. Therefore, the joint performance analysis of security and energy efciency\n\nfor UAV-enabled networks is urgently needed and is the emphasis of this work.\n\nAn overview of UAV-enable wireless communications was provided in [2], where the basic network-\n\ning architecture and main channel characteristics were portrayed, and the key design challenges were\n\ndiscussed. Owing to high mobility, UAVs could also be deployed as mobile relays to provide wireless\n\nconnectivity between distant ground terminals whose direct links were severely blocked [5]. The authors\n\nof [6] proposed an algorithm to allocate the time to different ground receivers based on the ying UAVs\n\nposition to maximize the minimum throughput. [7] modeled the locations of the UAV base stations\n\nin a nite area as a uniform binomial point process and derived exact expression for the coverage\n\nprobability of a target receiver situated on the ground. The aforementioned works addressed the basic\n\nnetworking architecture and optimization problem of the throughput. However, the information security\n\nagainst eavesdropping attacks was not taken into account.\n\nA very recent effort [8] considered physical layer security in a UAV-enabled mobile relaying system\n\nwhere the air-to-ground link was established. Note that the authors of [8] focused on the optimization\n\nof transmit power, but not from the perspective of network analysis and deployment. They considered\n\nneither the multi-UAV multi-eavesdropper wiretap scenarios, nor the random spatial positions of network\n\nnodes. To the best of our knowledge, such work has not tried to design and analyze the secrecy energy\n\nefciency (SEE) performance in UAV-enable communication networks, which motivates this work.\n\nIn this letter, we focus on the SEE in downlink UAV-enabled communication networks. Main contri-\n\nbutions of this letter are summarized as follows. 1) Modeling multi-antenna UAV-enabled transmitters,\n\nreceivers, and eavesdroppers as independent homogeneous Poisson point processes (HPPPs). By using the\n\nthreshold-based access scheme, a fundamental analysis framework for evaluating the SEE performance in\n\nUAV-enabled communication networks is proposed; 2) The inuences on connection outage probability\n\n(COP) and secrecy outage probability (SOP), caused by the predetermined access threshold and the\n\nnumber of receivers served by each transmitter, are further analyzed in this scenario.\nNotations: Boldface lowercase letter denotes vector. () , k  k, P{}, and E() denote the conjugate\ntranspose, Euclidean norm, probability, and expectation operation. (a, b) is the Gamma distribution with\n\nshape parameter a and scale parameter b.\n\nApril 7, 2017\n\nDRAFT\n\n\f3\n\nFig. 1: A simplied system model for a 2-tier UAV-enabled communication network.\n\nA. Network Descriptions\n\nI I . DOWNL INK SY STEM MODEL\n\nWe consider a wireless system consisting of K -tier UAV-enabled transmitters, multiple legitimate\n\nreceivers, and multiple eavesdroppers, as shown in Fig. 1. Note that the UAVs of the kth tier are assumed\nto be at the same height Hk for simplicity of exposition. Let  , {1, 2, . . . , K } denote the set of UAVs. In\nthe kth tier, each UAV-enabled transmitter equipped with Mk antennas can collect and transmit information\n\nto the ground receivers. The number of receivers served in each transmitters resource block is k , and the\n\ntransmit power is Pk . The legitimate receivers and eavesdroppers are equipped with a single antenna. We\ndenote the set of UAVs in the kth tier, legitimate receivers, and eavesdroppers locations as k , u , and\n\nE , which follow independent HPPPs with densities k , u , and E , respectively. According to Slivnyaks\n\ntheorem [9], the analysis can be performed at a typical legitimate receiver located at the origin. Compared\n\nwith interference, noise almost has no effect for legitimate receivers in multi-tier wireless networks [10].\n\nHence, we assume that the noises received by legitimate receivers and eavesdroppers are neglected.\n\nIn this letter, the system model has other three restraints:\n\n All the channels undergo independent and identically distributed quasi-static Rayleigh fading;\n\n Perfect channel state information (CSI) is available at each UAV-enabled transmitter;\n All the transmitters use precoding w = h/khk, where h is the corresponding channel.\nIn UAV-enabled communication networks, the received signal-interference-plus-noise ratio (SINR) of\nthe typical receiver o served by the UAV-enabled transmitter xk  k in the kth tier is given by\nPk hxk ,oklxk ,ok\nSINRk\nu =\nI inter\no,k\n\n(1)\n\nApril 7, 2017\n\nDRAFT\n\n\f4\n\nwhere Pk hxk ,oklxk ,ok denotes the received power of the kth receiver, lxk ,o = qH 2\nk + r2\nxk ,o denotes the\ndistance between the serving transmitter xk and o, rxk ,o denotes the distance between xk and o (the center\nof the plane of k ), hxk ,o  (k , 1) stands for the array gain of the main channel, k = Mk  k + 1,\nPi go,iklyi ,ok represents receivers received\nand klxk ,ok is the path loss [11]. I inter\no,k = Pi Pyio\ni \\xk\ninterference from all the active transmitters, where lyi ,o = qH 2\ni + r2\nyi ,o denotes the distance between the\ntransmitter yi and o, ryi ,o denotes the distance between yi and o , and go,i  (i , 1) is the array gain\nof corresponding interference channel. The set of active transmitters in the ith tier is a thinning of i ,\nact i , where P i\ndenoted by o\ni with density o\ni = P i\nact denotes the activation probability of transmitters in\n\nthe ith tier.\n\nWe consider the non-colluding and passive eavesdropping scenario that each eavesdropper intercepts\n\nthe information signal of typical receiver independently without any attacks. In this case, we only pay\n\nour attention to the eavesdropper that has the largest received SINR, which was commonly assumed [11].\n\nSuch an eavesdropper e is considered as the most malicious one and its received SINR can be expressed\n\nas\n\nxeE ( Pk hxe ,k klxe ,k k\nxe ,k )\nSINRk\ne = max\nI intra\nxe ,k + I inter\nwhere hxe ,k  exp(1) denotes the equivalent small-scale fading channel power gain for the received\nSINR of eavesdropper xe  E , lxe ,k = qH 2\nk + r2\nxe ,k indicates the distance between the eavesdropper\nxe and its target transmitter xk , and rxe ,k is the eavesdroppers horizontal distance from xk . I intra\nxe ,k =\nPi gxe ,yi klxe ,yi k with gxe ,yi \nPk gxe ,k klxe ,k k with gxe ,k   (k  1, 1) and I inter\nxe ,k = Pi Pyio\ni \\xk\n (i , 1) are intra-cell and inter-cell interference, respectively [11]. lxe ,yi = qH 2\ni + r2\nis the distance\nxe ,yi\nbetween the eavesdropper xe and transmitter yi , and rxe ,yi is the eavesdroppers horizontal distance from\nyi .\n\n(2)\n\nB. Secrecy mobile association scheme\n\nIn this subsection, we assume open access, i.e., a legitimate receiver is permitted to access any tiers\n\nUAV-enabled transmitters. In addition, we consider a mobile association based on highest average received\n\nsignal power (ARSP), where a legitimate receiver is only allowed to associate with the UAV-enabled\n\ntransmitter providing the highest ARSP. For a legitimate receivers o, the ARSP related to the kth tier is\ndened as Pk = Pkk l\nxk ,o .\n\nFollowing the idea of [12], the secure mobile association scheme is designed for improving the\n\nsecurity/reliability of downlink transmission in UAV-enabled communication networks. For the secure\n\nApril 7, 2017\n\nDRAFT\n\n\fmobile association scheme, the served transmitter broadcasts data only when the truncated ARSP at\n\nreceiver is larger than a predetermined access threshold  , i.e. lxk ,o  Rk = (Pkk / )\ndenotes the radius of the serving region. The following lemma provides the association probability.\n\n1\n , where Rk\n\nLemma 1: The probability with which a typical legitimate receiver o associates with a transmitter in\n\n5\n\n(a)\n=\n\nthe kth tier is given as\nR2\nk H 2\nk +x2 )/2 > 1 |rxk )frxk\nP ( Pkk (cid:16)H 2\nxj (cid:17)/2\nj +r2\nk\nR0\nQj\\k\nSk =\nPj j (H 2\nexp\" Pj\nj (cid:17)#\nexp\" Pj\\k\nj (cid:16)( Pj,k j,k )2/\nj (cid:16)( Pj,k j,k )2/\nH 2\nk H 2\nj,k ( Pj,k j,k )2/\nj,k ( Pj,k j,k )2/\nPj\nPj\nwhere the step (a) can be easily recognized by the probability generating functional of HPPP [12],\n, rxi is the receivers horizontal distance from transmitter xi , Pi,j = Pi /Pj ,\n(x) = 2k xek x2\nfrxk\ni,j = i/j , and i,j = i/j .\nIt is worth noting that a transmitter in the kth tier may be active when existing an associated receiver,\n\nj (cid:17)#\nkH 2\nR2\n\n(x) dx\n\n\n\n(3)\n\n,\n\nand the activation probability of transmitter Bk is dened as [12]\n\nP k\nact = P (Bk associates with at least one receiver)\nP (xu is not associated with Bk )#.\n= 1  Eu \" Qxuu\nFrom (1) and (2), we know that the derivation for activation probability of transmitter is necessary, which\n\n(4)\n\nis given in Lemma 2.\n\nLemma 2: The activation probability of UAV-enabled transmitters in the kth tier is given by\nact = 1  Eu \" Qxuu\nxj ,xu (cid:27)#\nP (cid:26) Pkk\nP k\nPj j l\n< max\nl\nj\nxu ,Bk\nj (cid:17)xdx#\n= 1  exp \"2u\nj (cid:16)( Pj,k j,k )2/\nRk\nx2H 2\n Pj\n(b)\nRHk\ne\nj (R2\nj )\nj (H 2\n= 1  exp \" e\n# ,\nj,k )2/\nH 2\nk ( Pj,k\nk ( Pj,k\n P\n P\njK\njK\nj ( Pj,k j,k )2/\n1\n1\n Pj\n Pj\nwhere the step (b) is derived following the basic nature of PPP, lxu ,Bk is the distance between xu and\n\nj ( Pj,k j,k )2/\n\n e\n\nj,k )2/\n\nj )\nH 2\n\n(5)\n\nBk , and lxj ,xu is the distance between xu and xj .1\n\n1For convenience, we interchangeably use exp(x) and e\n\nx to denote the exponential function of x.\n\nApril 7, 2017\n\nDRAFT\n\n\f6\n\nI I I . PER FORMANCE ANALY S IS\n\nIn this section, we analyze the SEE in UAV-enabled communication networks. In an effort to assess\n\nthe SEE, we rst derive the COP and the SOP in UAV-enabled networks.\n\nWhen the legitimate receivers message cannot be decoded with error-free, the connection outage\n\noccurs. The expression of COP in the kth tier is given in Theorem 1.\n\nTheorem 1: When the typical receiver is associated with a UAV-enabled transmitter in the kth tier, its\n\nCOP can be expressed as\n\ncop (k ) = P (cid:0)SINRk\n(2k  1) ,\nP k\nu < 2k  1(cid:1) = FSINRk\nu\nwhere k is the target channel capacity. The cumulative distribution function (CDF) of SINRk\nu can be\n\n(6)\n\ngiven by\n\n2/\nCj,k ( Pj,k )\n\nPk (cid:17)n\nn! (cid:16) \nSk Pk1\n( ) = 1  k\n1\nP mM (n) C ( m)F ( m) (P ml + 1)\nFSINRk\nn=0\nu\n(cid:19)\n(cid:18)eQk ( )R2\n2(P mli)\n2(P mli)\neQk ( )H 2\nk R\nk H\nk\nk\nPP ml\n,\ni=0\nj,k )2/\nH 2\nj ( Pj,k\n P\nk\n[Qk ( )]i+1(P ml+1i)e\nj\nn\njmj = n}, uj,k = (cid:16)1 +  /( j,k Bj,k )(cid:17)1\nwhere M (n) = { m = (m1 , m2 ,    , mn )T :\nPj=1\n, C ( m) =\nj !ml\nQl=1   Pj\n2\nn\n\nDj (l)P\nj (cid:16) Pj,k j,k(cid:17)2/\nn!\no\n+ Pj\nQj (mj !(j !)mj ) , Qk ( ) = Pj\n, F ( m) =\nj\nk ) 2\n P ml+n\n(2 ) P ml\n(P 1\nPm=1 \nm \nj\nj\nCj,k = 2\n , uj,k (cid:1), Bi,j = Bi /Bj , Bi = pi/i , B  (a, b, z ) =\n , m  2\n B  (cid:0)j  m + 2\n\n\nB  (j + 2\n ,uj,k )\n ,l 2\no\nR 1\nz ta1 (1  t)b1 dt is the Beta function, and Dj (l) =\nj\n.\n((j +l1)!)1 (j 1)!\n\nProof : The CDF of SINRk\nu can be obtained as\nR2\nkH 2\nP (cid:18)hxk ,o >\nPkklxk ,ok |rxk ,o (cid:19) frxk ,o (x) dx\nk\n I inter\nR0\n( ) = 1 \nFSINRk\no,k\nu\nRk\nk1\ndnLI inter\n(s)\n2yk (n!)1 (s)nS1\nRHk\nPn=0\ndy ,\no,k\nk\ndsn\nj,k )2/ (y2 H 2\nk )\nj ( Pj,k\n P\nj\ni (cid:16)1  (1 + sPiz )1(cid:17) zdz(cid:21) is obtained by the basic\nexp (cid:20)R \n2o\n(s) = Qi\n1\n( Pi,k i,k )\n y\nj ( Pj,k j,k )2/\nx2\ndnLI inter\n Pj\n(s)\n, s = P 1\nnature of PPP, frxk ,o (x) = 2k x\nk y , and\ne\no,k\nis derived in\ndsn\nSk\n[12]. Substituting (8) into (6), we complete the proof.\nMathematically, the COP of a typical receiver can be given by Pcop = Pk\n\nSk P k\ncop (k ).\n\nwhere LI inter\no,k\n\n= 1 \n\ne\n\n(7)\n\n,\n\n(8)\n\nApril 7, 2017\n\nDRAFT\n\n\fAs such, when the eavesdroppers have a better channel than the access threshold, the secrecy outage\n\noccurs to ensure the secrecy of those messages. As an important indicator of security, the expression of\n\nSOP in the kth tier is given in Theorem 2.\n\nTheorem 2. When the typical receiver is associated with a UAV-enabled transmitter in the kth tier, its\n\n7\n\n(9)\n\nFSINRk\ne\n\nSOP can be given by\ns  1(cid:17) ,\ne (cid:16)2k  Rk\ns (cid:17) = P (cid:16)log2 (cid:0)1 + SINRk\ns (cid:17) = 1  FSINRk\nsop (cid:16) Rk\ne (cid:1) > k  Rk\nP k\ns (cid:17) and the CDF of SINRk\nsop (cid:16) Rk\nwhere Rk\ns is the target secrecy rate of P k\ne can be expressed as\ni (cid:18)1(cid:16)1+ Pi y\nPk z (cid:17)i (cid:19)zdz\n( ) = exp \"2E ( + 1)(k1)\nydy#\n\n Pi R \n2o\nHi\nRHk\ne\nProof : The CDF of SINRk\ne can be derived as follows:\nPk klxe ,k k (cid:19)#\n( ) = EE \" QxeE\nP (cid:18)hxe ,k \nxe ,k )\n (I intra\nxe ,k +I inter\nFSINRk\ne\n(c)\n(s) ydy(cid:17) ,\n= exp (cid:16)2E R \n(s) LI inter\nLI intra\nHk\nxe ,k\nxe ,k\nwhere the step (c) is achieved by the probability generating functional of HPPP E [12], the Laplace\ni R \nHi (1(1+sPi z )i )zdz\n2o\n Pi\nxe ,k and I intra\ntransform of I inter\nxe ,k are given by LI inter\n(s) = e\nand LI intra\nxe ,k\nxe ,k\n( + 1)(k 1) , respectively. Substituting (11) into (9), we can arrive at the nal result.\ns (cid:17).\nsop (cid:16) Rk\nSkP k\nMathematically, the SOP of a typical receiver can be given by Psop = Pk\nDue to the requirement of secure communication and the limitation of energy, SEE as an important\nmetric is used to evaluate the secrecy performance achieved with unit energy consumption. Similar to\n\n(s) =\n\n(11)\n\n(10)\n\n[13], SEE is dened as the ratio of the average secrecy rate at which the condential messages are\n\nreliably and securely transmitted from the UAV-enabled transmitters to the intended receivers over the\n\ntotal power consumption (bits/Joule). The following theorem provides the SEE achieved by the UAV-\n\nenabled communication network.\n\nwhere\n\nSk SEEk ,\n\nTheorem 3. The SEE of the UAV-enabled communication networks is given by\nSEE = Xk\nsop (cid:16) Rs,k(cid:17)(cid:17) Rs,k\ncop (k )(cid:1) (cid:16)1  P k\nk (cid:0)1  P k\nP total\nk\ndenotes the SEE for the kth tier. The total power consumption for a UAV-enabled transmitter in each\n3\nk (cid:16) k\nPt=1 (cid:16)t1\nt + Mkt(cid:17)(cid:17) [13]. P 0\nk + Pk\n= P 0\nk and k represent the static\nk\n\nchannel is given by P total\nk\n\nSEEk =\n\n(12)\n\n(13)\n\n+\n\nApril 7, 2017\n\nDRAFT\n\n\f8\n\n \n\nk=4,k=1(T)\nk=4,k=2(T)\nk=4,k=4(T)\nk=4,k=1(S)\nk=4,k=2(S)\nk=4,k=4(S)\n\n0.8\n\n0.7\n\n0.6\n\n0.5\n\nP\nO\nC\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n \n\nk =4,k=1(T)\nk =4,k=2(T)\nk =4,k=4(T)\nk =4,k=1(S)\nk =4,k=2(S)\nk =4,k=4(S)\n\n0.35\n\n0.3\n\n0.25\n\n0.2\n\nP\nO\nS\n\n0.15\n\n0.1\n\n0.05\n\n0\n\n \n\n-70\n\n-60\n-50\n (dBm)\n\n-40\n\n0\n\n \n\n-70\n\n-60\n-50\n (dBm)\n\n-40\n\n(a) COP versus \n\n(b) SOP versus \n\nFig. 2: The COP and SOP versus  with P1 = 15dBm and P2 = 5dBm. S denotes the simulation results\n\nand T denotes the theoretical results.\n\nhardware power consumption and the efciency of the power amplier for the kth tier, respectively. The\nparameters k\nt and t depends on the transceiver chains, coding and decoding, etc.\n\nIV. NUMER ICAL RE SULT S\n\nIn this section, numerical results are provided to examine the COP and SOP for a 2-tier UAV-enabled\n\nnetworks. In addition, the impact of  and P1 on the SEE are also investigated. The validity of the\n\ntheoretical derivations are veried by the Monte Carlo simulation results. In the following results, we\nassume  = 4, 1 = 1.5  105m2 , 2 = 3  105m2 , u = 6  106m2 , E = 2  105m2 , H1 = 13m,\n2 = 0, k\n1 = 4.8, k\nH2 = 5m, 1 = 2 = 0.38, k\n3 = 2.08k  108 , 1 = 1, 2 = 9.5  108 ,\n1 = 4W, and P 0\n3 = 6.25  108 , P 0\n2 = 13.6W. All the simulation results shown in this section are\naveraged over 100,000 Monte Carlo simulations.\n\nFig. 2 compares the SOP and COP in UAV-enabled networks with different k (k  {1, 2}). Intuitively,\nthe simulation results highly consistent with the theoretical results, which validates the accuracy of those\n\ntwo analytical expressions derived. It is also observed that the COP increases with k and the SOP\n\ndecreases with k , which is mainly due to the fact that k not only increases the interference received\n\nby eavesdroppers, but also increases the interference received by legitimate receivers. Furthermore, the\n\nApril 7, 2017\n\nDRAFT\n\n\f9\n\n)\ne\nl\nu\no\nJ\n/\ns\nt\ni\nb\n(\nE\nE\nS\n\n0.01\n\n0.008\n\n0.006\n\n0.004\n\n0.002\n\n0\n-90\n\nX: 21\nY: 66\nZ: 0.009853\n\n-80\n\n-70\n\n-60\n\n (dBm)\n\n0\n\n5\n\n10\n\n-50\n\n-40\n\n15\n\n20\n\n-30\n\n25\n\nP1 (dBm)\n\nFig. 3: SEE versus  and P1 , with 1 = 2 = 1, 1 = 4, 2 = 2 and P2 = 5dBm.\n\nCOP and SOP performances over different  are also shown in Fig. 2. Obviously, both SOP and COP\n\ndegrades with increasing  . This implies that the predetermined access threshold can affect both security\n\nand reliability. This is due to both the association probability and the activation probability of UAV-enabled\n\ntransmitter degrades with increasing  .\n\nFig. 3 shows the inuences on SEE caused by  and P1 . From (12), we note that the SEE is not a\n\nmonotonous function of  and P1 . Consequently, the optimal value of SEE can be obtained by properly\n\ndesigning  and P1 . In Fig. 3, the SEE reveals a maximum value for a given network with the optimal\npair of ( , P1 ) = (66, 21), which is marked in the gure.\n\nV. CONCLU S ION\n\nIn this letter, we studied the SEE of downlink UAV-enabled networks, where the locations of network\n\nnodes were characterized by independent HPPPs. To ensure the reliability and security of UAV-enabled\n\nnetworks, both the threshold-based access scheme and multi-antenna technology were employed. The\n\nsecurity, reliability, and SEE of UAV-enabled networks was analyzed. Simulation results have revealed\n\nthat the reliability and security of UAV-enabled networks can be improved by using the threshold-based\n\naccess scheme, and the optimal value of SEE can be achieved by designing the transmit powers and\n\npredetermined access threshold.\n\n[1] Y. Zhou, N. Cheng, N. Lu, and X. S. Shen, Multi-UAV-aided networks: Aerial-ground cooperative vehicular networking\n\narchitecture, IEEE Veh. Technol. Mag., vol. 10, no. 4, pp. 36-44, Dec. 2015.\n\nRE FERENCE S\n\nApril 7, 2017\n\nDRAFT\n\n\f10\n\n[2] Y. Zeng, R. Zhang, and T. J. Lim, Wireless communications with unmanned aerial vehicles: Opportunities and challenges,\n\nIEEE Commun. Mag., vol. 54, no. 5, pp. 36-42, May 2016.\n\n[3] Z. Chu, H. Xing, M. Johnston, and S. L. Goff, Secrecy rate optimizations for a MISO secrecy channel with multiple\n\nmultiantenna eavesdroppers, IEEE Trans. Wireless Commun., vol. 15, no. 1, pp. 283-297, Jan. 2016.\n\n[4] B. Li, Z. Fei, and H. Chen, Robust articial noise-aided secure beamforming in wireless-powered non-regenerative relay\n\nnetworks, IEEE Access, vol. 4, pp. 7921-7929, Nov. 2016.\n\n[5] J. Lyu, Y. Zeng, R. Zhang, and T. J. Lim, Placement optimization of uav-mounted mobile base stations, IEEE Commun.\n\nLett., vol. 21, no. 3, pp. 604-607, Mar. 2017.\n\n[6] J. Lyu, Y. Zeng, and R. Zhang, Cyclical multiple access in uav-aided communications: A throughput-delay tradeoff, IEEE\n\nWireless Commun. Lett., vol. 5, no. 6, pp. 600-603, Dec. 2016.\n\n[7] V. V. C. Ravi and H. S. Dhillon, Downlink coverage probability in a nite network of unmanned aerial vehicle (UAV) base\n\nstations, in Proc. IEEE SPAWC, Edinburgh, UK, Jul. 2016, pp. 1-5.\n\n[8] Q. Wang, Z. Chen, W. Mei, and J. Fang, Improving physical layer security using UAV-enabled mobile relaying, IEEE\n\nWireless Commun. Lett., published online.\n\n[9] D. B. Taylor, H. S. Dhillon, T. D. Novlan, and J. G. Andrews, Pairwise interaction processes for modeling cellular network\n\ntopology, in Proc. IEEE GLOBECOM, Anaheim, USA, Dec. 2012, pp. 4524-4529.\n\n[10] H. S. Dhillon, R. K. Ganti, F. Baccelli, and J. G. Andrews, Modeling and analysis of K-tier downlink heterogeneous\n\ncellular networks, IEEE J. Sel. Areas Commun., vol. 30, no. 3, pp. 550-560, Apr. 2012.\n\n[11] Y. Deng, L. Wang, K.-K. Wong, A. Nallanathan, M. Elkashlan, and S. Lambotharan, Safeguarding massive MIMO aided\n\nhetnets using physical layer security, in Proc. IEEE WCSP, Nanjing, China, Oct. 2015, pp. 1-5.\n\n[12] H.-M. Wang, T.-X. Zheng, J. Y. D. Towsley, and M. H. Lee, Physical layer security in heterogeneous cellular networks,\n\nIEEE Trans. Commun., vol. 64, no. 3, pp. 1204-1219, Mar. 2016.\n\n[13] J. Ouyang, M. Lin, Y. Zou, W.-P. Zhu, and D. Massicotte, Secrecy energy efciency maximization in cognitive radio\n\nnetworks, IEEE Access, vol. 5, pp. 2641-2650, Mar. 2017.\n\nApril 7, 2017\n\nDRAFT\n\n\f", 
        "tag": "Networking and Internet Architecture", 
        "link": "https://arxiv.org/list/cs.NI/new"
    }, 
    {
        "text": "2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nTackling Dynamic Vehicle Routing Problem with\nTime Windows by means of Ant Colony System\n\nRaluca Necula\nFaculty of Computer Science\nAlexandru Ioan Cuza University\nIasi, Romania\nEmail: raluca.necula@info.uaic.ro\n\nMihaela Breaban\nFaculty of Computer Science\nAlexandru Ioan Cuza University\nIasi, Romania\nEmail: pmihaela@info.uaic.ro\n\nMadalina Raschip\nFaculty of Computer Science\nAlexandru Ioan Cuza University\nIasi, Romania\nEmail: mionita@info.uaic.ro\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nE\nN\n.\ns\nc\n[\n \n \n1\nv\n9\n5\n8\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstractThe Dynamic Vehicle Routing Problem with Time\nWindows (DVRPTW) is an extension of the well-known Vehicle\nRouting Problem (VRP), which takes into account the dynamic\nnature of the problem. This aspect requires the vehicle routes\nto be updated in an ongoing manner as new customer requests\narrive in the system and must be incorporated into an evolving\nschedule during the working day. Besides the vehicle capacity\nconstraint involved in the classical VRP, DVRPTW considers in\naddition time windows, which are able to better capture real-\nworld situations. Despite this, so far, few studies have focused on\ntackling this problem of greater practical importance. To this end,\nthis study devises for the resolution of DVRPTW, an ant colony\noptimization based algorithm, which resorts to a joint solution\nconstruction mechanism, able to construct in parallel the vehicle\nroutes. This method is coupled with a local search procedure,\naimed to further improve the solutions built by ants, and with an\ninsertion heuristics, which tries to reduce the number of vehicles\nused to service the available customers. The experiments indicate\nthat the proposed algorithm is competitive and effective, and on\nDVRPTW instances with a higher dynamicity level, it is able to\nyield better results compared to existing ant-based approaches.\n\nI . INTRODUCT ION\n\nThe Vehicle Routing Problem plays a central part in cap-\nturing logistics and distribution operations, with an impact in\nthe economy, more prominent as the need for transportation\nis increasingly growing. Thus, this class of problems has a\npractical importance and has attracted a lot of research inter-\nest over the years. Recently, the technological advancements\nmade possible the use of mobile devices to enable the direct\ncommunication between the clients and the drivers, such that\na driver could dynamically change his plan while executing\nhis route. Also, the emergence of global positioning systems\nallows a dispatcher to know the current position of a driver\nand communicate him in a timely manner the next customer\nto visit on his route. This leads to a VRP variant known as\nDynamic Vehicle Routing Problem (DVRP), also referred to\nas online or real-time VRP.\nAs indicated in [1], where the authors perform a review\non VRP literature, more challenging problems such as DVRP\nbegan being studied more intensively only recently, due to\ntheir ability to capture real-world scenarios. Possible sources\nof dynamism in DVRP include: customers requests, travel time\nand demands, which can be dynamically revealed for a set\nof known customers. In this paper, we considered that the\n\ndynamic nature of DVRP is given by the customers requests,\nwhich is the most common source of dynamism, as pointed\nout in [2]. Therefore, DVRP allows customers requests to be\nserviced in a real-time manner, while the drivers have already\nstarted their routes, visiting in their way other customers. Thus,\nthe algorithm needs to adjust the ongoing vehicle routes, such\nas to take into account these dynamic customer requests, while\nmaintaining the feasibility of solution.\nThe aim of this study is to design and analyse ant colony\noptimization based algorithms, that are able to achieve good\nquality solutions for DVRPTW instances with a higher level of\ndynamicity. To this end, we propose in this paper an ant colony\nsystem (ACS) based algorithm, relying on a joint mechanism\nfor constructing the solutions, in which the vehicle and the next\ncustomer to be added in its tour are simultaneously selected\nduring the transition step. Our approach is hybridized with a\nlocal search procedure, consisting of two operators, relocate\nand exchange, that are applied in an iterative manner, until\nno further improvement is possible. Moreover, we integrated\nin our method an insertion heuristics to better incorporate\nunvisited customers in the existing tours, thus reducing the\nnumber of vehicles needed to service all the customers.\nThe remainder of this paper is structured as follows. Section\nII describes the static variant of the considered DVRPTW\nproblem. Section III presents a survey on existing literature\nrelated to DVRPTW. In Section IV the ant colony system\noptimization algorithm is summarized, as the underlying meta-\nheuristic for our approach that will be described in Section\nV. In Section VI the experimental study and the obtained\nresults will be presented. The paper concludes with Section\nVII, which draws the nal remarks and offers future research\ndirections.\n\nI I . THE VEH ICL E ROUT ING PROBL EM W ITH T IME\nW INDOW S\n\nIn this section, we describe the (static) vehicle routing\nproblem with time windows (VRPTW), which is the static\nvariant of our considered DVRPTW problem.\nAs in the classical VRP or capacitated VRP, VRPTW\nrequires to nd a set of routes for each vehicle, such that\neach customer, having a given demand, is visited exactly\nonce by a single vehicle and the total demand on the route\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\ndoes not exceed the vehicles capacity. The eet of vehicles\nis assumed to be homogeneous, all the vehicles are located\nat a single depot, where they start and end their route, and\neach vehicle has a limited capacity. Besides this, VRPTW\nbrings the additional constraint of time windows associated\nto a customer, such that a customer i must be serviced by a\nvehicle within a time interval, dened as [ei , li ], where ei is\nthe earliest arrival time, and li is the latest arrival time. The\nservice at customer i cannot start before its earliest arrival\ntime, and if the vehicle arrives at the customer earlier than\nei , waiting occurs. Also, the service at each customer takes\nan amount of time, denoted by si , required for the pickup\nand/or delivery of goods or services. In case of hard time\nwindows constraints, the vehicle must arrive at the customers\nnot later than the latest arrival time, constraint that must be\nsatised for each customer within a vehicles tour in order to\nmaintain the solution feasible. There is also the possibility of\napplying penalties if the services start after the allowed time\nwindows, which is the case of soft time windows. The depots\ntime window induces a constraint regarding the maximum total\nroute time, meaning that each vehicle route must start and end\nwithin the time window associated with the depot.\nIn this study, we consider hard time windows constraints\nand the existence of a homogeneous eet of vehicles, that are\nlocated at a single depot. Also, we assume that the primary\nobjective of VRPTW is to minimize the number of tours (vehi-\ncles), whilst a second objective is to minimize the total traveled\ndistance needed to supply all customers in their required\nhours. This means that the number of tours minimization\ntakes precedence over total distance minimization. In case of\nsolutions with the same number of vehicles, those solutions\nof lower distance are preferred.\nit can be noticed that VRPTW is\nBy its description,\nmore complicated than multiple traveling salesman problem\n(mTSP), problem that we have tackled in our previous studies\n[3], [4] with ant colony based methods, and for which we pro-\nposed a benchmark1. Thus, mTSP can be seen as a relaxation\nof VRPTW, if removing the time windows associated with the\ncustomers, and assuming that vehicles have unlimited capacity,\nwhich imposes no restriction on the number of customers to\nbe visited on a vehicles route.\n\nI I I . RE LAT ED WORK\n\nDuring the last decades, the VRP and his famous VRPTW\nextension have been an intensive research area. Heuristic and\nexact optimization approaches have been developed for the\nVRPTW problem. Surveys of proposed techniques can be\nfound in [5], [6]. As the VRP and VRPTW problems are both\nNP-hard and they generalize the traveling salesman problem\n(TSP), ant-based methods were proposed for solving them. A\nmulti-ant colony system consisting in two colonies, one for\noptimizing the number of vehicles and one for optimizing the\ntotal travel time, was proposed for the VRPTW problem in\n[7].\n\n1 http://profs.info.uaic.ro/mtsplib/\n\nBecause of the recent technological advances, the research\non dynamic routing increased. Techniques ranging from lin-\near programming to metaheuristics were designed to address\ndynamism and uncertainty. A comprehensive review of appli-\ncations and approaches for dynamic vehicle routing problems\nis given in [2]. A taxonomy of papers concerning DVRP\nproblems was developed in [8]. Also, the interested reader\nis referred to [9], for a survey of hybrid articial intelligence\nalgorithms, employed for solving DVRP problems.\nVery few previous studies on using metaheuristics on\nDVRPTW exist, that take into account both the dynamic nature\nof VRP and the time windows as additional constraint. A\nrst such study is due to [10], which presents a tabu search\nalgorithm that works on the DVRPTW, but with soft time\nwindows. A hybrid genetic approach based on two populations\nof individuals which evolve concurrently are used to minimize\nthe total traveled distance, and respectively, the number of\nunserviced customers, total lateness at customers locations\nand temporal constraint violations in [11]. A metaheuristic\nalgorithm is developed in [12] for tackling two variants of\nDVRPTW, which consider several real-world constraints: mul-\ntiple time windows, customers priorities and vehicle-customer\nconstraints. In [13], the DVRPTW problem is decomposed into\nseveral static VRPTW problems, which are then solved using\nan improved large neighborhood search algorithm.\nTo the best of our knowledge, the study from [14] is the only\nant colony based approach found in the literature, that tackles\nthe DVRPTW problem. To this end, the authors propose an\nACO based method, denoted as MACS-DVRPTW, by extend-\ning to the dynamical case the state-of-the-art ant algorithm\n[7] developed for VRPTW. This study was continued in [15],\nwhere MACS-DVRPTW was applied to schedule the routes of\na eet of cars for a surveillance company. Recently, MACS-\nDVRPTW was enhanced in [16] with two strategies for dealing\nwith DVRPTW problem in which customers have different\npriority levels.\n\nIV. THE ANT COLONY SY S T EM\n\nAnt Colony Optimization (ACO) is a nature-inspired meta-\nheuristic, which follows the metaphor of real ants that succeed\nin nding the shortest paths between their nest and food\nsources. In case of ACO, each articial ant, referred henceforth\nas ant, constructs solutions in a probabilistic manner by\niteratively adding components, until a complete solution to\nthe problem is obtained. When building a solution, an ant is\ninuenced by two factors: 1) pheromone trails, updated dy-\nnamically at runtime, that reect the gained search experience\nof ants, and 2) the heuristic information, which is a static\nvalue, dependent upon the problem.\nThe algorithm presented in this study is based on the Ant\nColony System (ACS) [17], which is a successor of the Ant\nSystem (AS) [18]. AS is the rst ACO algorithm proposed in\nthe literature, that was initially designed for solving instances\nof TSP. ACS is an improvement over AS and it brings the\nfollowing changes: 1) a modied state transition rule, which\nis more focused on exploitation, 2) the inclusion of a local\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\npheromone update, which reduces the amount of pheromone\non edges visited by ants, in order to increase exploration\nand prevent an early stagnation of search, and 3) the global\npheromone update is performed by a single ant, generally\nbeing the best so far ant, which produced the best solution.\n\nV. THE ALGOR ITHM INVE S T IGAT ED FOR DVRPTW\n\nthe\nThis section describes our approach designed for\nDVRPTW problem, which is based on the ant colony system\nmetaheuristic, outlined in Section IV. To tackle the dynamic\nnature of the problem, we adopted in our algorithm the\ntime slices and nodes commitment approach from the MACS-\nDVRPTW ant based solver, introduced in [14]. Accordingly,\na working day of twd seconds is equally split into nts time\nslices, the length of a time slice (tts ) being computed as:\ntts = twd /nts . Thus, the initial DVRPTW problem is divided\ninto nts static problems, that will be solved consecutively.\nIn this context, we introduce the concept of current problem\nconguration, which denotes one of these static problems,\ndened by the subset of available nodes (customer requests).\nWe will denote by S best the currently optimal solution, which\nis the best feasible solution including all the available customer\nrequests known so far.\nis\nOur approach, referred henceforth as DVRPTW-ACS,\ncomposed of two parts, that will be presented in the next\nsubsections, and each part is running in a separate thread. The\ncore part of the algorithm, briey denoted as the planner,\nis concerned with the time slices management and with the\ndynamic nature of the problem. The other part is responsible\nwith the optimization task, by running the ACS based algo-\nrithm on the current problem conguration.\n\nA. The planner\n\nBefore the beginning of the working day, an initial feasible\nsolution (with respect to capacity, time window and vehicle\narrival time at the depot constraints), is built using the time-\noriented nearest neighbour heuristic, introduced in [19]. The\ntime-oriented nearest neighbour heuristic is a sequential tour\nbuilding algorithm, that takes into account both geographical\nand temporal closeness of customers. This initial solution\ndenes the tentative tours for the vehicles, considering only a\npriori nodes, known in the system from the beginning.\nthen\nInitially, a vehicle starts its tour from the depot,\niteratively adds the closest (in terms of a metric m) unvisited\ncustomer, relative to the last customer of this tour, while\nkeeping feasible the tour under construction. The metric mij ,\nwhich denes the closeness between i, the last customer on\nthe current emerging tour, and j , that refers to any unrouted\ncustomer that could be visited next, is dened as:\n\nmij = 0.4  dij + 0.4  Tij + 0.2  uij\n\n(1)\n\nIn the previous equation, dij\nis the distance between the\ntwo customers, Tij means the time difference between the\nbeginning of service at customer j and the completion of\nservice at customer i, and is expressed as: Tij = bj  (bi + si ),\nand uij represents the urgency of delivery to customer j ,\n\ndened as: uij = lj  (bi + si + dij ), where bj denotes\nthe beginning of service at customer j , being computed as:\nbj = max(ej , bi + si + dij ), assuming that the vehicle travels\nto the next customer j , as soon as it has nished the service\nat the current customer i.\nEach time when there are still remaining unrouted cus-\ntomers, that cannot be feasibly appended to the current tour,\na new tour, that starts from the depot, is added to incorporate\nthem.\nThe obtained solution is used to initialize the best so\nfar solution, S best , and to compute the value of the initial\npheromone level, 0 , as:\n\n0 = (nav  LN N )1\n\n(2)\n\nwhere nav is the number of available nodes (customers) and\nLN N denotes the total traveled distance of this solution.\nWhen the working day begins, a timer is started which keeps\ntrack of the current time, so that when the working day is over,\nthe algorithm will stop its execution. Also, the planner starts\nthe ant colony thread, which will perform its optimization task,\nconsidering only the currently available customers.\nTaking into account the current time, the planner repeatedly\nveries whether a new time slice began. If this is the case, the\nplanner checks: 1) if there are new nodes that became available\nin the meantime, during the last time slice, and 2) if there\nare new nodes that must be committed. If one of these two\nconditions is met, the planner stops the execution of the ACS\nbased algorithm, since the problem denition has changed.\nThe next node i, after the last committed node from a tour\nof the best so far solution, S best , is marked as committed if\nthe following condition holds: bi  idxts  tts , where idxts is\nthe index of the current time slice, and the idxts  tts product\ndenotes the time when the current time slice ends. When a\nnode becomes committed, its location within the vehicles tour\ncannot be changed anymore and it will be found at this position\nin its tour in the nal solution to be returned by the algorithm\nat the end of the working day.\nAfter committing the necessary nodes from the tours of\nS best , if condition 1) holds, the list of available customer\nrequests is updated and the new available nodes are added in\nthe tours of S best by adopting the insertion technique from I1\ninsertion heuristic [19]. The insertion heuristic starts by com-\nputing for each unrouted node its best feasible insertion place\nin the tours of S best according to a criterion (c1 ). The score of\ninserting a customer u between two adjacent customers i and\nj on a route, is computed as: c1 (i, u, j ) = 0.1  c11 (i, u, j ) +\n0.9  c12 (i, u, j ), where c11 (i, u, j ) = diu + duj  dij and\nc12 (i, u, j ) = bju  bj , where bju denotes the new time of\nbeginning the service at customer j , after inserting customer\nu in the route. For a given node, its best insertion position is\nthe one that achieves the minimum score for the c1 criterion.\nBased on this, the best customer u to be inserted in the route\nis selected to be the one that obtains the minimum value for c2\ncriterion, dened as: c2 (i, u, j ) = 2.0  d0u  c1 (i, u, j ), where\nd0u denotes the distance between the depot and customer u.\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\nThe insertion heuristic is applied in an iterated manner, until\nno further feasible insertions are possible.\nIf there are still remaining available nodes which couldnt\nbe feasibly inserted in the existing tours of S best , a new\ntour is added, which starts from the depot. The unrouted\nnodes will be tried to be added in this tour by following\nthe same construction mechanism used in the time-oriented\nnearest neighbour heuristic. This process continues until all the\navailable customer requests are incorporated in S best , meaning\nthat a complete feasible solution was obtained.\nAfter the necessary commitments were made and the new\navailable nodes were added in the best so far solution, the\nant colony is restarted, performing its optimization task on\nthe new problem conguration. The algorithm stops after twd\nseconds of execution, which marks the ending of the working\nday. At this point in time, the best solution obtained so far is\nreturned as the output of DVRPTW-ACS, indicating the nal\ntours to be traveled by each vehicle.\n\nB. The ACS based algorithm\n\nOur DVRPTW-ACS approach relies on the ACS algorithm\nfor performing the optimization task, such that all the available\ncustomers are visited only once, while minimizing the number\nof vehicles and the total traveled distance. An ants solution is\ncomposed of a set of tours, each one designating the route to be\ntraveled by a vehicle. As mentioned in Section II, the formu-\nlation of the considered VRPTW problem assumes a hierarchy\nbetween these two objectives, namely the minimization of the\nnumber of vehicles takes precedence over the minimization\nof the traveled distance. This implies that a solution which\nuses less vehicles will be preferred over a solution with more\nvehicles, even if it has a smaller traveled distance.\nUnlike the MACS-DVRPTW solver which resorts to two\ncolonies, ACS-VEI and ACS-TIME, each one trying to op-\ntimize a different objective, our algorithm uses one single\ncolony, which simultaneously optimizes the two objectives,\nthe number of vehicles and the total traveled distance. Conse-\nquently, MACS-DVRPTW uses two pheromone trail matrices,\none for each colony, whereas our approach involves only one\npheromone trial matrix. Besides that, DVRPTW-ACS works\nonly with feasible solutions, in which all the known customers\nare incorporated, which is different from MACS-DVRPTW,\nwhere the ACS-VEI colony works also with unfeasible solu-\ntions.\nAt the rst run of the ACS based algorithm, the pheromone\ntrail matrix is initialized with 0 , computed as in equation (2).\nThen, at subsequent runs, when the ant colony is restarted,\n0 is computed like in equation (2), except for the fact that\nLN N is replaced with Lbest , which denotes the total distance\nof the tours from S best . In addition to this, we have adopted\nthe pheromone preservation strategy, employed also in one of\nthe variants of MACS-DVRPTW, which allows that a certain\namount of pheromone from the previous run of the ant colony\nto be preserved in the current execution.\nIn the implementation of the algorithm, we resorted to\ncandidate lists, as indicated in [20], when solving instances of\n\nTSP with ACS. A candidate list of size cl contains for a given\nnode, the most closest cl neighbours, arranged in increasing\norder according to their distance from the considered node. In\nour case, all the nodes part of the DVRPTW instance will be\nconsidered when computing the candidate list of a node. Then,\nin the transition step of ACS, when an ant located in node r\nhas to decide the next node to move to, it will rst examine\nthe candidate list of node r, choosing only those nodes that are\nrevealed. Only when all the available nodes from the candidate\nlist have already been visited, the search of the next customer\nis extended on the rest of available nodes.\nInitially, a vehicle is placed at the depot, where it starts\nand ends its tour. In an iteration of the ACS based algorithm,\nseveral ants build feasible solutions for the problem, that inte-\ngrate all the available customers and do not violate any of the\nVRPTW constraints: capacity, time window and vehicle arrival\ntime at the depot. Later on during the algorithm execution,\nwhen the planner starts to commit nodes, the committed parts\nof S best are used to initialize the ants solutions, being copied\nat exactly the same positions. Therefore, it may result that an\nants solution to be comprised from the beginning of more\nthan one tour and use more than one vehicle. In case there are\ncommitted nodes, the solution construction will continue from\nthese added portions of tours, which will be next extended by\nants during the transition step.\nUnlike the MACS-DVRPTW solver, in which the tours are\nconstructed sequentially and lled up with customers one by\none, in our approach the tours are built simultaneously and the\nvehicles compete towards extending their partially tours with\na new available customer. In addition to this, we employed a\njoint mechanism for building the vehicles tours, in which the\nvehicle and the customer to be added in its tour, are selected at\nthe same time, during the state transition rule from the standard\nACS. Both the vehicle (v ) and the next customer to visit (s)\nare chosen by iterating over the existing tours (vehicles) from\nthe set T and the candidate set C , comprised of available\ncustomers, that have not been visited and committed yet, and\ncan be appended to the tour of v vehicle, while keeping the\nsolution feasible:\n(v , s) = (arg max\nvT sC\nS,\nwhere r represents the ants current position, being the last\ncustomer on the tour denoted by v , rs is the pheromone trail\nassociated to edge rs, rs is the heuristic information corre-\nsponding to edge rs and is computed as: \nrs = (1/mrs) ,\nwhere mrs is dened as in equation (1). In addition,  and\n are parameters that reect the importance of pheromone,\nrespectively heuristic information, q0  [0, 1] is a parameter\nand S is a random variable selected according to the following\nprobability distribution:\nprs = (\n\nif rand(0, 1) < q0\n\nrs  \n( \nrs ),\n\notherwise\n\notherwise\n\n(3)\n\n\n \nrs\nrs\nPwT PuC\n0,\n\n,\n\nif s  C\n\n \nru\n\n\nru\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\nDuring the building process of an ants solution, when there\nare remaining unrouted available customers that cannot be\nfeasibly added in the existing tours and the number of these\ncustomers is less or equal to ten, we apply the same insertion\nheuristic used by the planner to update S best with the new\navailable nodes. Its aim is to insert in the existing tours the\nremaining available unvisited nodes. In this way, the insertion\nheuristic is applied in order to prevent the addition of new\ntours needed to incorporate these nodes, thus reducing the\nnumber of vehicles used to service all the known customers.\nAfter this step, if there are still remaining unvisited customers,\nthat are available, a new tour is added to incorporate them.\nThis process is repeated until the ants solution covers all the\navailable customers, meaning that a feasible complete solution\nfor the DVRPTW problem was obtained.\nOnce an ant moves from customer r to s, it updates the\npheromone level on the edge associated to it, by applying the\nlocal pheromone update rule, as in the standard ACS:\n\nrs = (1  )  rs +   rs\n\n(4)\n\nwhere   (0, 1) is a local pheromone decay parameter and\nfor the rs we have used the following setting, as indicated\nin [17]: rs = 0 , where 0 is the initial pheromone level,\ncomputed as in equation (2). The aim of the local pheromone\nupdate is to make visited edges less desirable for subsequent\nants, enforcing diversity within the same iteration and thus\navoiding the situation in which all the ants construct very\nsimilar tours.\nAfter all the ants nished to construct their solution, the\nbest iteration ant is selected as the one that built the best\nsolution, either regarding the number of vehicles or the total\ndistance objective. Since the number of vehicles objective has\npriority over the total distance objective, a solution which uses\nless vehicles is always preferred over a solution with more\nvehicles, even if it has a smaller total distance. The global best\nant, which corresponds to the best so far solution, is updated,\nif necessary, based on the best iteration ant. Then it follows\nthe global pheromone update and like in the standard ACS, it\nreinforces with additional pheromone the edges used by the\nglobal best ant:\n\nrs = (1  )  rs +   rs\n\n(5)\n\nif rs  best so far solution\notherwise\n\nwhere\nrs = (cid:26) (Lgb )1 ,\n0,\nwhere   (0, 1) is the pheromone decay parameter and Lgb\ndenotes the total traveled distance of the best so far solution,\nobtained by the global best ant.\nSince it is known from the literature that combining an ACO\nalgorithm with a local search phase can greatly improve its\nperformance, we integrated in DVRPTW-ACS a local search\nprocedure. This method consists of two multi-route operators,\nrelocate and exchange [21], which modify simultaneously\ntwo different tours. The relocate operator removes one node\nfrom a tour and reinserts it\ninto another tour, whilst\nthe\n\nexchange operator swaps the position of two nodes from\ntwo different tours. These two operators are applied in an\niterated manner, meaning they are called repeatedly, until no\nfurther improvement, regarding the number of vehicles or the\ntotal distance, can be obtained. We applied the local search\nprocedure to the initial solution, produced by the time-oriented\nnearest neighbour heuristic, and to the best solution obtained\nin the current iteration (best iteration ant) of the algorithm.\nAccording to the concept of commitment, which involves\nthat the position of committed nodes within a tour is xed, the\ninsertion heuristic and the local search operators are applied\nonly to positions that do not belong to the committed parts of\na tour. In case of insertion heuristic, this means that a node\ncannot be inserted in front of a node that is committed, whilst\nin case of the local search procedure, the positions eligible for\nrelocation and exchange will start from the rst uncommitted\nnode within a tour.\nThe ACS based algorithm iteratively performs its optimiza-\ntion task, trying to improve S best in any of the two objectives\n(number of vehicles and total distance), until it is stopped by\nthe planner, either because new nodes became available or\nbecause there are nodes that must be committed.\n\nV I . EX P ER IMENT S\n\nA. Problem instances\n\nThe experimental analysis in this paper is conducted on\nseveral DVRPTW instances, belonging to the benchmark in-\ntroduced in [14]. These problem instances were constructed\nstarting from the Solomons 100 customers static VRPTW\nbenchmark2, that was extended to the dynamical case, by spec-\nifying a dynamicity level of X %. To this end, the Solomons\nbenchmark was enhanced with an additional attribute, namely\nthe available time, which conveys the time when a customers\nrequest is revealed to the system. Starting with this time, the\ncustomer request is considered to be known and will be taken\ninto account by the algorithm.\nA dynamicity of X % means that a percentage of X %\ncustomer requests have a non-zero available time, these being\ndynamic requests which are revealed during the working day.\nThe a-priori nodes (customer requests), that are known from\nthe beginning of the algorithm execution, have their available\ntime equal to 0. The problem instances from the DVRPTW\nbenchmark have specied different values for the dynamicity\nlevel, ranging in the interval [0%..100%], with an increment\nof 10%. The instances with 0% dynamicity are static VRPTW\nproblems taken from the Solomons benchmark, whilst the\ninstances with 100% dynamicity are the most dynamic ones.\nThe DVRPTW benchmark encompasses euclidian test prob-\nlems, divided in six categories: R1, R2, C1, C2, RC1 and\nRC2. The test instances in R1 and R2 categories have nodes\nwith randomly generated coordinates, the nodes from C1 and\nC2 categories are clustered, whilst in RC1 and RC2 there is\na combination of randomly generated and clustered nodes.\nTest problems of type 1 have a short scheduling horizon,\n\n2 http://web.cba.neu.edu/ msolomon/problems.htm\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\nallowing only a few nodes to be visited on a vehicles route.\nOn the other hand, problems of type 2 have a long scheduling\nhorizon, such that more customers can be serviced by the same\nvehicle, and thus fewer vehicles are needed to visit all the\ncustomers. From each of these six categories, we have chosen\ntwo problem instances and for each of it we have selected\nthree different values for dynamicity: 10%, 50% and 100%,\nresulting in a total of 36 DVRPTW instances, that were used in\nour experimental study. We have chosen different dynamicity\ndegrees in order to analyze the impact of the dynamicity on the\nquality of solutions produced by the investigated algorithms. In\neach DVRPTW instance, the depot is considered to be the rst\nentry in the list of given customers. Also, the eet of vehicles\nis homogeneous and the travel time between two customers is\nassumed to be equal with the distance between them.\nThe VRPTW problem instances from the Solomons bench-\nmark dene a time window for each node, including the depot,\nnamely [e0 , l0 ], which denotes the scheduling horizon. At the\nsame time, the specications of the DVRPTW problem involve\na certain length of the working day, which establishes when\nthe algorithm will stop from its execution. To comply with\nthese two requirements, we scaled in our algorithm all time\nrelated values, as it was done in the MACS-DVRPTW solver.\nMore precisely, for each customer request from a particular\nDVRPTW instance,\nthe values for time windows, service\ntime, available time and distance between two customers, are\nmultiplied with the sv factor, computed as: sv = twd/(l0  e0).\nIn a similar manner, the results reported in Section VI-C for the\ntotal traveled distance (T D) are obtained as: T D = T D/sv .\nTo allow the reproduction of results, the DVRPTW test\ninstances, used in the experimental part of this study, and the\nJava source code of our DVRPTW-ACS algorithm are available\nonline3 .\n\nB. Parameters setup\n\nTo allow a fair comparison of the two investigated algo-\nrithms, DVRPTW-ACS and MACS-DVRPTW, we used the same\nsetting of the parameters, their values being taken from [14].\nFor the ACS based algorithms we have set: q0 = 0.9,  = 1.0,\n = 1.0,  = 0.9 and the number of ants was set to 10, whilst\nfor the dynamic scenario we have set: twd = 100 seconds\nand nts = 50. Also, we have employed in both algorithms\nthe pheromone preservation strategy, by specifying  = 0.3 in\nthe equation: ij = (1  )   old\nij +   0 . Besides that, the\nsize of the candidate list, used in our algorithm, was set to\ncl = 20. For both algorithms 30 runs were carried out on a\nLinux server with 6 GB RAM memory, processor Intel Core\n2 Duo P9xxx (Penryn Class Core 2) at 2.5 GHz.\n\nC. Results\n\nTo assess the performance of the proposed algorithm, we\ncompared it with another ACO based approach from the\nliterature, namely MACS-DVRPTW, described in [14]. More\nprecisely, among the four variants of ACO based algorithms\n\n3 http://profs.info.uaic.ro/mtsplib/vrp- extension/dvrptw/\n\nemployed for the MACS-DVRPTW solver, we have chosen\nthe ACS based algorithm with pheromone preservation (WPP)\nand which uses no improved initial solution (IIS), constructed\nbefore the start of the working day. We selected this variant\nof algorithm, denoted as DVRP, 0.3 wpp, no IIS in [14],\nsince according to the results the authors reported in their\nstudy, this variant achieves overall one of the best results on\nDVRPTW instances with 50% dynamicity. For performing the\ncomparison of the investigated approaches, we stored for each\nmethod the best so far solution and the number of feasible\nsolutions, computed during the working day.\nBased on the 30 best so far solutions achieved at\nthe\nend of each algorithm execution on all DVRPTW instances,\nwe report in Tables I to VI the average, minimum (best),\nmaximum (worst) and standard deviation values. These values\nare computed in terms of the two objectives: number of\nvehicles (abbreviated in tables as NV) and total\ntraveled\ndistance (abbreviated in tables as TD).\nIn addition to this, we also indicate the results obtained by\ntwo approaches after performing 30 runs on static VRPTW\ninstances. The static VRPTW instances correspond to the\nconsidered DVRPTW instances, in which all the customer\nrequests are known beforehand and there is no node to be\nrevealed dynamically during the working day. The instances\nfrom the DVRPTW benchmark were constructed such as to\nstill allow obtaining the optimal solution of the corresponding\nstatic VRPTW instance. Therefore, the solutions produced for\nthe static instances will act as a reference point when assessing\nthe quality of solutions found by the investigated approaches\non dynamic instances. In this sense, we have computed the\ndecline in the solution quality, relative to the solution ob-\ntained for the static instance, that will allow us to analyze\nhow much the solution deteriorates with the increase in the\ndynamicity level. The decline measure is indicated in tables\nas increase(%) and its value is computed for each of the two\nobjectives as: increasek = (minDk  minSk )/minSk  100,\nk = 1, 2, where minDk is the minimum value obtained on\nthe dynamic instance for the k-th objective, and minSk is the\nminimum value obtained on the static instance for the k-th\nobjective.\nIn the evaluation of the two algorithms, we take into account\nthe hierarchy between the two objectives, meaning that we aim\nwith priority minimum values for the number of vehicles, then\nsecondly we seek for smaller total distance values. In each\ntable, we highlighted with boldface the best (minimum) value\nfrom a row, except for the instances where the statistical test\ndid not show statistically signicant differences. The end part\nfrom the name of a DVRPTW instance reects its associated\ndynamicity, such that 0.0 denotes static instances having 0%\ndynamicity, 0.1 is for instances with a low dynamicity of\n10%, 0.5 is for instances with 50% dynamicity, and 1.0\ndenotes instances having the maximum dynamicity of 100%.\nAnalyzing the best values obtained for the two objectives,\nit can be stated that MACS-DVRPTW is a clear winner on\nmost of the static instances, except for C201-0.0 and C202-\n0.0 instances, where our algorithm has a better average per-\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\nformance. Actually, on these instances, the minimum values\nproduced on C201-0.0 by both algorithms, and on C202-0.0 by\nDVRPTW-ACS, coincide with the best known solution reported\nin the literature4 . Also, on most of DVRPTW instances with\na low dynamicity of 10% and in case of a few instances\nwith higher dynamicity, such as R103-0.5, R104-0.5 and R104-\n1.0, MACS-DVRPTW achieves better results than our approach\non both objectives. The advantage of our algorithm over\nMACS-DVRPTW becomes apparent as the dynamicity level of\ninstances increases. Thus, on almost all instances of 50% and\n100% dynamicity, DVRPTW-ACS manages to obtain superior\nsolutions with less vehicles and of lower total distance than\nMACS-DVRPTW. The better performance of our algorithm\nis more noticeable on C201-0.5, C202-0.5, C101-1.0, C102-\n1.0 and C201-1.0 instances, where the values attained by\nDVRPTW-ACS on both objectives are almost half the ones\nobtained by MACS-DVRPTW.\nThe values recorded by the two approaches for the increase\nmeasure reveal\nthe same good performance of DVRPTW-\nACS. More precisely,\nit can be noticed that\nthe solution\nquality on DVRPTW instances with higher dynamicity of\n50% and 100% is not inuenced so much by the increase\nin the dynamicity. Besides that, on instances with clustered\nnodes such as C101-0.5, C101-1.0, C201-0.5 and C201-1.0,\nDVRPTW-ACS is still able to attain the same good results as\nobtained for the corresponding static instances, as indicated\nby the zero values for the increase measure. In contrast to\nthis, MACS-DVRPTW yields on most instances higher values\nfor the increase measure. Also on some instances with higher\ndynamicity such as R201-1.0, C201-0.5, C201-1.0, C202-1.0\nand RC202-1.0, the value for increase measure in terms of\nthe number of vehicles is greater than or equal to 100, which\nshows that the decline in the solution quality is signicant.\nAlso, compared to the average number of vehicles required\non static instances, MACS-DVRPTW uses in addition up to 2\nvehicles on instances of 10% dynamicity, on instances of 50%\ndynamicity it uses at most extra 6 vehicles, and on instances\nof 100% dynamicity it uses at most extra 10 vehicles. On the\nother hand, our approach requires on average on all DVRPTW\ninstances, regardless of their dynamicity level, up to 2 extra\nvehicles. This aspect shows that our algorithm manages to\ncope better on instances of higher dynamicity, in which fewer\ncustomers are known in advance and more nodes are revealed\nin an ongoing fashion during the working day.\nFigure 1 illustrates the distribution of the results of the two\nalgorithms with respect to the total distance objective. Yel-\nlow boxplots correspond to the results obtained with MACS-\nDVRPTW, and green ones to DVRPTW-ACS. The results are\ngrouped based on the instance category and dynamicity level.\nAnalyzing the boxplots, it can be noticed that our algorithm\nwins on instances from C category for all the dynamicity\nlevels. Also, on R and RC instances with low dynamicity,\nDVRPTW-ACS is surpassed. However, on the same R and\nRC instances having 100% dynamicity, DVRPTW-ACS out-\n\n4 https://goo.gl/YbMGJx\n\nperforms MACS-DVRPTW. Besides that, the boxplots reveal\nthat the variance of DVRPTW-ACS is smaller, which indicates\na more stable algorithm.\nWe conducted two statistical\nthe Mann-Whitney-\ntests,\nWilcoxon nonparametric test and the Students t-test (we\nperformed 30 runs for each algorithm), to assess if these\ndifferences in the performance of the two approaches are statis-\ntically signicant or not. The results obtained for the dynamic\ninstances in terms of the total distance objective, indicate\nstatistically signicant differences on all DVRPTW instances,\nexcepting R202-1.0, RC102-0.5 and RC203-1.0. Also when\nconsidering the number of vehicles objective, statistically\nsignicant differences are recorded on all dynamic instances,\nexcept for R202-0.1 and RC102-0.5. On static instances, only\nfor C102-0.0 the two statistical tests do not show signicant\ndifferences, in terms of the total distance objective. For the\nnumber of vehicles objective, the results indicate statistically\nsignicant differences on all static instances.\nCorrelating the values obtained for the considered measures\nby the two investigated approaches, it can be stated that our\nalgorithm achieves the best overall performance on both objec-\ntives. Moreover, in some cases where DVRPTW-ACS does not\nhave such a good performance, the obtained values are close\nto the ones attained by MACS-DVRPTW. This indicates that\nour approach is competitive with the MACS-DVRPTW solver,\nand on higher dynamicity instances DVRPTW-ACS is able to\nyield better results.\nThe good performance of our approach can be ascribed\nto its underlying joint mechanism employed in the transition\nstep of the ACS based algorithm, in which the vehicles tours\nare constructed simultaneously. This offers better chances of\ncovering the available unvisited customers with the existing\nvehicles, without needing to resort to additional vehicles. In\ncontrast to this, in MACS-DVRPTW the tours are constructed\nsequentially and are lled up one by one with customers.\nThis aspect restricts the number of choices for adding an\nunrouted customer, which may lead to using more vehicles.\nBesides this, DVRPTW-ACS is coupled with a powerful inser-\ntion heuristic, being among the best heuristics proposed by\nSolomon for VRPTW. Our insertion procedure manages to\nbetter incorporate unvisited available customers in the tours\nof existing vehicles, by searching for each unvisited node the\nbest insertion position. In this way, all the unvisited nodes\nhave equal chances and compete for being inserted in best\nposition within a vehicles tour. In contrast to this, in MACS-\nDVRPTW the nodes are inserted sequentially into their best\nposition, according to a partial order of the customers, which\nare sorted in descending order based on their demand.\nIn terms of the computational cost, the comparison of the\nnumber of feasible solutions obtained by the two approaches,\nreveals that for most of the DVRPTW instances, MACS-\nDVRPTW computes more feasible solution than our algorithm.\nThis may be attributed to the fact that DVRPTW-ACS was\nimplemented in Java, whilst the MACS-DVRPTW solver was\ncoded in C, which may induce more rapid execution times.\nAlso, the relocation and exchange operators from our local\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nTABLE I\nT H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R\nNUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON R 1\nI N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY\n\nTABLE II\nT H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R\nNUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON R 2\nI N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY\n\ninstance\n\nmeasure\n\nR103-0.0\n\nR104-0.0\n\nR103-0.1\n\nR104-0.1\n\nR103-0.5\n\nR104-0.5\n\nR103-1.0\n\nR104-1.0\n\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\n\nMACS-DVRPTW\nTD\nNV\n1087.923\n10.933\n10\n1046.225\n1148.068\n11\n23.397\n0.254\n979.076\n10\n944.92\n10\n1007.948\n10\n0\n15.742\n1078.205\n10.367\n1026.682\n10\n1135.584\n11\n21.565\n0.49\n0\n-1.868\n969.424\n10\n934.079\n10\n996.333\n10\n16.031\n0\n0\n-1.147\n12.367\n1153.761\n1113.695\n12\n1199.753\n13\n20.675\n0.49\n6.449\n20\n976.452\n9.133\n9\n933.443\n1013.922\n10\n24.099\n0.346\n-1.215\n-10\n1327.262\n14.567\n14\n1279.144\n1390.228\n15\n29.899\n0.504\n22.263\n40\n1035.714\n9.8\n955.579\n9\n10\n1092.117\n0.407\n29.13\n-10\n1.128\n\nDVRPTW-ACS\nTD\nNV\n1267.525\n14\n14\n1246.521\n1287.189\n14\n10.78\n0\n1072.18\n10.8\n10\n1037.729\n1107.054\n11\n0.407\n16.243\n1272.788\n14\n1248.78\n14\n1302.275\n14\n14.11\n0\n0\n0.181\n1059.812\n10.533\n10\n1023.262\n1108.842\n11\n17.035\n0.507\n-1.394\n0\n14\n1301.596\n1245.081\n14\n1350.338\n14\n0\n22.676\n-0.116\n0\n1097.715\n10.567\n10\n1054.123\n1168.38\n11\n25.714\n0.504\n1.58\n0\n1301.524\n14\n14\n1267.567\n1355.115\n14\n18.167\n0\n1.688\n0\n1103.916\n10.533\n1054.419\n10\n11\n1161.368\n24.571\n0.507\n0\n1.608\n\ninstance\n\nmeasure\n\nR201-0.0\n\nR202-0.0\n\nR201-0.1\n\nR202-0.1\n\nR201-0.5\n\nR202-0.5\n\nR201-1.0\n\nR202-1.0\n\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\n\nMACS-DVRPTW\nNV\nTD\n977.568\n3.767\n3\n869.717\n1154.818\n4\n0.43\n77.168\n928.359\n3\n846.279\n3\n1019.08\n3\n0\n39.238\n1075.4\n4.9\n966.55\n4\n1143.901\n5\n43.654\n0.305\n33.333\n11.134\n949.171\n3.967\n858.555\n3\n1044.958\n4\n46.793\n0.183\n1.451\n0\n1262.015\n5.933\n1171.521\n5\n1341.553\n6\n45.987\n0.254\n34.701\n66.667\n1148.596\n5.133\n1046.799\n5\n1233.396\n6\n48.095\n0.346\n23.694\n66.667\n1811.948\n7.2\n6\n1625.431\n1995.438\n8\n86.74\n0.484\n86.892\n100\n1249.129\n5.433\n1129.508\n5\n6\n1415.06\n0.504\n54.7\n33.468\n66.667\n\nDVRPTW-ACS\nTD\nNV\n1307.182\n4.7\n4\n1239.081\n1414.325\n5\n47.917\n0.466\n1209.136\n4\n1157.297\n4\n1295.792\n4\n0\n27.184\n4.533\n1308.653\n4\n1218.004\n5\n1433.059\n56.646\n0.507\n0\n-1.701\n1196.264\n4\n1163.795\n4\n1255.849\n4\n22.327\n0\n0.561\n0\n4\n1365.705\n4\n1323.148\n4\n1433.617\n25.78\n0\n6.785\n0\n4\n1229.833\n4\n1166.377\n4\n1349.382\n34.816\n0\n0.785\n0\n1394.533\n4\n4\n1357.513\n1450.423\n4\n23.731\n0\n9.558\n0\n4\n1247.435\n4\n1195.321\n4\n1323.849\n0\n33.132\n0\n3.286\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\nsearch procedure are applied multiple times, and this involves\na higher computational cost, compared to the cross exchange\noperator from MACS-DVRPTW, which is called only once.\nThis aspect indicates that the better performance of our algo-\nrithm on DVRPTW instances with higher dynamicity is not\ndue to a greater number of feasible solutions that it computes.\n\nFig. 1. Distribution of total distance objective for dynamicity levels ranging\nin {0.1, 0.5, 1}; yellow is for MACS-DVRPTW, green is for DVRPTW-ACS.\nResults are reported for each of the three problem classes (C, R, RC)\n\nV I I . CONCLU S ION S\n\nIn this paper, we have proposed an ACS based algorithm\nto tackle the single-objective DVRPTW problem, which con-\nsiders a hierarchy among the two objectives, minimizing the\nnumber of vehicles having priority over minimizing the trav-\neled distance. Experiments conducted on several DVRPTW\ninstances with different dynamicity levels, showed that our\nalgorithm is efcient and competitive with another ACO\nbased approach from the literature. Besides that, on DVRPTW\ninstances with a higher dynamicity, DVRPTW-ACS was able to\nachieve better results than MACS-DVRPTW, in terms of both\nobjectives. Also, our algorithm proved to cope better on higher\ndynamicity instances, since the decline in the solution quality\nis not so signicant, compared to the solution obtained for\nthe corresponding static instances. This makes our approach\nmore suitable to be applied on DVRPTW instances with a\nhigher dynamicity, in which few customers requests are a\npriori known, case more likely to occur in real-world scenarios.\nPossible avenues for future work include solving this prob-\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nTABLE III\nT H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R\nNUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON C 1\nI N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY\n\nTABLE IV\nT H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R\nNUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON C 2\nI N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY\n\ninstance\n\nmeasure\n\nC101-0.0\n\nC102-0.0\n\nC101-0.1\n\nC102-0.1\n\nC101-0.5\n\nC102-0.5\n\nC101-1.0\n\nC102-1.0\n\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\n\nMACS-DVRPTW DVRPTW-ACS\nNV\nTD\nNV\nTD\n828.937\n10\n828.937\n10\n10\n828.937\n10\n828.937\n828.937\n10\n828.937\n10\n0\n0\n0\n0\n10\n10\n864.627\n857.087\n10\n10\n834.639\n829.133\n10\n10\n902.735\n899.302\n0\n0\n19.344\n19.69\n828.937\n10\n1003.766\n11\n828.937\n10\n1002.601\n11\n828.937\n10\n1003.904\n11\n0\n0\n0\n0.397\n0\n0\n10\n20.95\n845.559\n10\n974.14\n12\n830.814\n10\n916.369\n12\n890.275\n10\n1087.706\n12\n12.499\n0\n0\n38.088\n-0.458\n0\n10.521\n20\n10\n828.937\n16.667\n1392.599\n828.937\n10\n1293.749\n16\n828.937\n10\n1629.158\n18\n0\n0\n68.256\n0.661\n0\n0\n56.073\n60\n868.538\n10\n1170.861\n11.633\n10\n828.937\n11\n1040.89\n932.31\n10\n1273.721\n12\n25.149\n0\n61.062\n0.49\n-0.683\n0\n25.54\n10\n828.937\n10\n2016.359\n20.167\n10\n828.937\n19\n1879.521\n828.937\n10\n2185.256\n21\n0\n0\n69.944\n0.461\n0\n0\n126.739\n90\n881.348\n10\n1521.483\n16.833\n872.031\n10\n1457.996\n16\n10\n888.646\n17\n1596.63\n5.455\n0\n36.923\n0.379\n0\n4.48\n60\n75.846\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\ninstance\n\nmeasure\n\nC201-0.0\n\nC202-0.0\n\nC201-0.1\n\nC202-0.1\n\nC201-0.5\n\nC202-0.5\n\nC201-1.0\n\nC202-1.0\n\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\n\nMACS-DVRPTW\nNV\nTD\n619.148\n3.767\n3\n591.557\n675.307\n4\n0.43\n20.922\n670.613\n3.267\n3\n605.298\n734.512\n4\n0.45\n37.403\n626.003\n4\n622.535\n4\n650.237\n4\n0\n9.014\n33.333\n5.237\n4.033\n689.57\n626.809\n4\n797.32\n5\n49.843\n0.183\n3.554\n33.333\n6.133\n899.03\n814.258\n6\n1122.569\n7\n82.495\n0.346\n100\n37.647\n878.535\n5.933\n5\n767.05\n1008.27\n6\n75.38\n0.254\n66.667\n26.723\n1577.455\n7.233\n7\n1339.839\n1831.838\n8\n116.901\n0.43\n126.494\n133.333\n948.312\n6.067\n856.878\n6\n7\n1126.274\n77.829\n0.254\n100\n41.563\n\nDVRPTW-ACS\nTD\nNV\n591.557\n3\n3\n591.557\n591.557\n3\n0\n0\n600.733\n3\n591.557\n3\n634.5\n3\n0\n15.079\n591.557\n3\n591.557\n3\n591.557\n3\n0\n0\n0\n0\n594.185\n3\n591.557\n3\n623.531\n3\n7.969\n0\n0\n0\n3\n591.557\n591.557\n3\n591.557\n3\n0\n0\n0\n0\n606.853\n3\n3\n591.557\n636.705\n3\n17.391\n0\n0\n0\n591.557\n3\n3\n591.557\n591.557\n3\n0\n0\n0\n0\n593.19\n3\n591.557\n3\n3\n626.741\n6.422\n0\n0\n0\n\nlem from a multi-objective perspective and extending this\nresearch to tackle dynamic rich vehicle routing problems,\nthat can incorporate more complex constraints and objectives,\nfound in real-life VRPs.\n\nACKNOW L EDGEMENT S\n\nWe would like to thank to the authors of MACS-DVRPTW\nsolver [14]\nfor providing us their source code,\nthat al-\nlowed us to perform the comparison with our algorithm.\nThis work is partly funded from the European\nUnion's Horizon 2020 research and innovation\nprogramme under grant agreement No 692178.\n\nRE F ERENCE S\n\n[1] K. Braekers, K. Ramaekers, and I. V. Nieuwenhuyse, The vehicle\nrouting problem: State of the art classication and review, Computers\n& Industrial Engineering, vol. 99, pp. 300-313, 2016.\n[2] V. Pillac, M. Gendreau, C. Gu eret, and A. Medaglia, A review of\ndynamic vehicle routing problems, European Journal of Operational\nResearch, vol. 225(1), pp. 1-11, 2013.\n\n[3] R. Necula, M. Breaban, and M. Raschip, Performance evaluation of\nant colony systems for the single-depot multiple traveling salesman\nproblem, in Proceedings of the 10th International Conference on Hybrid\nArticial Intelligence Systems, vol. 9121, pp. 257-268, 2015.\n[4] R. Necula, M. Breaban, and M. Raschip, Tackling the Bi-criteria Facet\nof Multiple Traveling Salesman Problem with Ant Colony Systems, in\nProceedings of the 2015 IEEE 27th International Conference on Tools\nwith Articial Intelligence, pp. 873-880, 2015.\n[5] J. F. Cordeau, G. Desaulniers, J. Desrosiers, M. M. Solomon, and F.\nSoumis, The VRP with time windows, in P. Toth, D. Vigo (eds.) The\nVehicle Routing Problem, SIAM Monographs on Discrete Mathematics\nand Applications, pp. 157-194, 2001.\n[6] O. Br aysy, and M. Gendreau, Vehicle routing problem with time\nwindows, Part I: Route construction and local search algorithms,\nTransportation Science, vol. 39(1), pp. 104-118, 2005.\n[7] L. Gambardella, E. Taillard, and G. Agazzi, Macs-vrptw: a multiple\nant colony system for vehicle routing problems with time windows, in\nD. Corne, M. Dorigo, F. Glover, D. Dasgupta, P. Moscato, R. Poli, KV.\nPrice (eds.) New ideas in optimization, ch 5, McGraw-Hill, pp. 63-76,\n1999.\n[8] H. N. Psaraftis, M. Wen, and C. A. Kontovas, Dynamic vehicle routing\nproblems: Three decades and counting, Networks, vol. 67(1), pp. 3-31,\n2016.\n[9] V. Ilin, D. Simi c, J. Tepi c, G. Stoji c, and N. Sauli c, A Survey of\nHybrid Articial Intelligence Algorithms for Dynamic Vehicle Routing\n\n\f2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings\n\nTABLE V\nT H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R\nNUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON RC 1\nI N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY\n\nTABLE VI\nT H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R\nNUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON RC 2\nI N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY\n\ninstance\n\nmeasure\n\nRC101-0.0\n\nRC102-0.0\n\nRC101-0.1\n\nRC102-0.1\n\nRC101-0.5\n\nRC102-0.5\n\nRC101-1.0\n\nRC102-1.0\n\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\n\nMACS-DVRPTW\nTD\nNV\n1419.259\n13.567\n13\n1353.045\n1514.738\n14\n36.447\n0.504\n1316.636\n11.967\n1278.533\n11\n1383.166\n12\n0.183\n24.018\n1499.454\n14.9\n1438.998\n14\n1569.396\n15\n0.305\n33.063\n7.692\n6.353\n1361.773\n11.167\n1279.555\n11\n1460.245\n12\n0.379\n46.071\n0.08\n0\n18.733\n1947.191\n1772.705\n17\n2124.249\n20\n85.384\n0.828\n31.016\n30.769\n1566.836\n14\n13\n1471.19\n1693.733\n15\n54.201\n0.587\n15.069\n18.182\n2219.684\n20.867\n19\n2057.832\n2453.04\n23\n86.721\n0.819\n52.089\n46.154\n1714.181\n16.433\n1613.665\n15\n17\n1778.713\n44.416\n0.626\n36.364\n26.212\n\nDVRPTW-ACS\nTD\nNV\n1669.585\n15.1\n15\n1649.803\n1705.964\n16\n14.753\n0.305\n1538.879\n13.767\n1501.709\n13\n1589.491\n14\n21.2\n0.43\n1698.417\n15.267\n1666.823\n15\n1740.601\n16\n19.908\n0.45\n0\n1.032\n1563.956\n13.667\n1514.923\n13\n1629.863\n14\n22.929\n0.479\n0\n0.88\n15.133\n1733.718\n1669.847\n14\n1781.071\n16\n28.038\n0.434\n1.215\n-6.667\n1580.218\n13.933\n13\n1540.802\n1651.335\n15\n28.635\n0.365\n2.603\n0\n1720.723\n15.1\n15\n1669.856\n1781.512\n16\n26.007\n0.305\n1.215\n0\n1602.576\n13.9\n1551.439\n13\n15\n1704.936\n32.527\n0.481\n0\n3.312\n\ninstance\n\nmeasure\n\nRC202-0.0\n\nRC203-0.0\n\nRC202-0.1\n\nRC203-0.1\n\nRC202-0.5\n\nRC203-0.5\n\nRC202-1.0\n\nRC203-1.0\n\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\naverage\nmin\nmax\nstdev\nincrease(%)\n\nMACS-DVRPTW\nNV\nTD\n1004.369\n3\n3\n893.646\n1134.003\n3\n0\n51.035\n900.178\n3\n822.186\n3\n975.809\n3\n0\n35.982\n1044.917\n4\n4\n952.876\n1139.719\n4\n0\n48.588\n33\n6.628\n899.799\n3\n814.817\n3\n997.529\n3\n39.368\n0\n-0.896\n0\n1261.691\n4.833\n4\n1146.6\n1438.765\n5\n71.85\n0.379\n28.306\n33.333\n1123.38\n4.333\n1016.05\n4\n1235.297\n5\n50.062\n0.479\n33.333\n23.579\n1688.338\n6.7\n6\n1460.384\n1825.705\n7\n76.98\n0.466\n100\n63.419\n1231.181\n5.8\n1139.477\n5\n6\n1334.584\n41.599\n0.407\n66.667\n38.591\n\nDVRPTW-ACS\nTD\nNV\n1301.551\n4\n4\n1245.37\n1366.973\n4\n32.926\n0\n1046.39\n4\n1011.552\n4\n1112.606\n4\n0\n24.857\n4\n1291.909\n4\n1234.551\n4\n1343.794\n29.813\n0\n0\n-0.869\n1076.276\n3.967\n3\n1019.385\n1216.303\n4\n41.685\n0.183\n-25\n0.774\n4\n1340.959\n4\n1246.376\n4\n1440.575\n47.483\n0\n0.081\n0\n3.1\n1247.353\n3\n1122.897\n4\n1392.911\n0.305\n67.217\n11.007\n-25\n1364.969\n3.967\n3\n1277.912\n1544.412\n4\n60.915\n0.183\n2.613\n-25\n3.067\n1243.447\n3\n1097.193\n4\n1345.215\n0.254\n60.904\n-25\n8.466\n\nN\nO\nI\nS\nR\nE\nV\nR\nO\nH\nT\nU\nA\n\nProblem, in Proceedings of the 10th International Conference on Hybrid\nArticial Intelligence Systems, vol. 9121, pp. 644-655, 2015.\n[10] M. Gendreau, F. Guertin, JY. Potvin, and E. Taillard, Parallel tabu\nsearch for real-time vehicle routing and dispatching, Transportation\nScience, vol. 33(4), pp. 381-390, 1999.\n[11] G. B. Alvarenga, R. M. de Abreu Silva, and G. R. Mateus, A hybrid\napproach for the dynamic vehicle routing problem with time windows,\nin Fifth International Conference on Hybrid Intelligent Systems, pp. 61-\n67, 2005.\n[12] J. de Armas, and B. Meli an-Batista, Constrained dynamic vehicle\nrouting problems with time windows, Soft Computing, vol. 19(9), pp.\n2481-2498, 2015.\n[13] L. Hong, An improved LNS algorithm for real-time vehicle routing\nproblem with time windows, Computers & Operations Research, vol.\n39(2), pp. 151-163, 2012.\n[14] B. van Veen, M. Emmerich, Z. Yang, T. B ack, and J. Kok, Ant\ncolony algorithms for the dynamic vehicle routing problem with time\nwindows, in Natural and Articial Computation in Engineering and\nMedical Applications, vol. 7931, Springer, pp. 1-10, 2013.\n[15] Z. Yang, J. P. van Osta, B. van Veen, R. van Krevelen, R. van Klaveren,\nA. Stam, J. Kok, T. B ack, and M. Emmerich, Dynamic vehicle routing\nwith time windows in theory and practice, Natural Computing, pp. 1-16,\n2016.\n[16] Z. Yang, M. Emmerich, and T. B ack, Ant based solver for dynamic\nvehicle routing problem with time windows and multiple priorities, in\n\n2015 IEEE congress on evolutionary computation (CEC), pp. 2813-2819,\n2015.\n[17] M. Dorigo, and L. M. Gambardella, Ant Colony System: A Coop-\nerative Learning Approach to the Traveling Salesman Problem, IEEE\nTransactions on Evolutionary Computation, vol. 1(1), pp. 53-66, 1997.\n[18] M. Dorigo, V. Maniezzo, and A. Colorni, The Ant System: Opti-\nmization by a Colony of Cooperating Agents, IEEE Transactions on\nSystems, Man, and Cybernetics, Part B, vol. 26(1), pp. 29-41, 1996.\n[19] M. Solomon, Algorithms for the vehicle routing and scheduling prob-\nlems with time window constraints, Operations Research, vol. 35(2),\npp. 254-265, 1987.\n[20] L. M. Gambardella, and M. Dorigo, Solving symmetric and asymmetric\nTSPs by ant colonies, in Proceedings of the 1996 IEEE International\nConference on Evolutionary Computation, pp. 622-627, 1996.\n[21] M. W. P. Savelsbergh, The vehicle routing problem with time windows:\nminimizing route duration, Informs Journal on Computing, vol. 4(2),\npp. 146-154, 1992.\n\n\f", 
        "tag": "Neural and Evolutionary Computing", 
        "link": "https://arxiv.org/list/cs.NE/new"
    }, 
    {
        "text": "1\n\nVico-Greengard-Ferrando quadratures in the tensor solver for\nintegral equations\n\nV. Khrulkov1 , M. Rakhuba1 , I. Oseledets1,2\n\n1Skolkovo Institute of Science and Technology, Russia\n2 Institute of Numerical Mathematics, Russia\n\nAbstract Convolution with Greens function of a dierential operator appears in a lot of\napplications e.g. Lippmann-Schwinger integral equation. Algorithms for computing such are\nusually non-trivial and require non-uniform mesh. However, recently Vico, Greengard and Fer-\nrando developed method for computing convolution with smooth functions with compact support\nwith spectral accuracy, requiring nothing more than Fast Fourier Transform (FFT). Their ap-\nproach is very suitable for the low-rank tensor implementation which we develop using Quantized\nTensor Train (QTT) decomposition.\n\n1. INTRODUCTION\n\nh(r(cid:48) ) =\n\n(cid:90)\nIn this paper we propose an algorithm for computing an approximation to a convolution\ng(r  r(cid:48) )(r)dr,\nR3\nwhere G is a continuous Greens function of some PDE. It has been shown recently in [1] that\ndiscrete approximation to h yieding spectral accuracy for compactly supported smooth functions \ncan be constructed using relatively simple idea. Idea is based on the fact that continuous Greens\n (R3 ). Apply-\nfunction can be replaced by the truncated one, Fourier transform of which is in C\ning convolution theorem one gets answer as a Fourier type integral of a rapidly decreasing function\n (R3 ) which is then discretized on ner grid and evaluated using Discrete Fourier Transform\nfrom C\n(DFT). This scheme achieves spectral order of accuracy due to the superalgebraic approximation of\nthe continuous Fourier transform by DFT and has been shown to also give second order with small\nconstant for continuos functions. We implemented this scheme using data compression via Quan-\ntized Tensor Train (QTT). Approach is straightforward  we compute discrete truncated Greens\nfunction using formulas from [1] and convert it to QTT-format. Then we use standard algorithms\nin QTT which allow for logarithmic storage with respect to the grid size and accuracy of approxi-\nmation. Moreover, discrete convolution and solution of integral equations are then performed also\nwith logarithmic complexity by the algorithms described in [2],[3]. Paper is organized as follows:\n\n We briey describe the algorithm developed in Section 2\n We present necessary denitions and algorithms from TT theory in Section 3.1\n We describe TT implementation of the algorithm in Section 3.2\n We present numerical results in Section 4\nThe main reference for Vico-Greengard-Ferrando quadrature is the original paper [1], in the pre-\nsentation of the algorithm we follow [4].\n\n2. ALGORITHM\n\n7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nA\nN\n.\ns\nc\n[\n \n \n1\nv\n9\n6\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nWe wil present the scheme described in [1] in a way which is most suitable for transitioning to\n(cid:90)\nTT-format. Let (r) be a smooth function such that supp   D = [0, L1 ]  [0, L2 ]  [0, L3 ] and we\nare interested in computing\ng(r  r(cid:48) )(r(cid:48) )dr(cid:48) ,\nR3\n\nh(r) =\n\n(1)\n\n\f2\n\nR3\n\nL = (cid:112)L2\nwhere g(r) is a Greens function of some dierential operator. Let us assume that g(r) depends\nonly on r = |r|. If we seek restriction of the solution h(r) to D then |r  r (cid:48)\n| in (1) doesnt exceed\n(cid:90)\n(cid:90)\n3 . Thus if we replace g(r) by gL (r) = g(r) rect r\n1 + L2\n2 + L2\n2L ,\ng(r  r(cid:48) )(r(cid:48) )dr(cid:48) =\ngL (r  r(cid:48) )(r(cid:48) )dr(cid:48) .\nR3\nAdvantage of using gL (r) is that since it has compact support its Fourier transform GL (s) is in\nC (R3 ), and is straightforward to compute for many dierential operators. We mainly focus on\nHelmholtz dierential operator 2 + k2 for which the following formula holds:\n(cid:90)\n(cid:17)\n(cid:16) r\neikr\n,\ng(r) =\n4r\nGL (s) =\nR3\n2L\n= 1 + eiLk (cos Ls  i k\ns sin Ls)\n(k  s)(k + s)\n(r) by (cid:98)(s) we obtain the nal formula:\n(cid:17)3 (cid:90)\nIt is easy to check that GL (s) is indeed smooth and nonsingular. If we denote Fourier transform of\n(cid:16) 1\neisr (cid:98)(s)GL (s)ds.\n(3)\nh(r) =\nR3\n2\nslightly oscillatory behavior of (cid:99)GL (s) zero padding by a factor of at least 3 is required (we will use\nThis integral is then discretized using trapezoidal rule and computed using DFT. However to cancel\nfactor 4 to keep grid size being a power of 2 - for the analysis see [4]). Suppose that domain D is\ndiscretized using uniform grid with Ni nodes in corresponding dimensions and function  is sampled\non this grid yielding an array ij k i.e.\n\neisrdr\n\ng(r) rect\n\n(2)\n\n.\n\nij k := (ih1 , jh2 , kh3 ),\n\nhi =\n\nLi\nNi\n\n.\n\nAlgorithm then is summarized as following\n1: Zero pad ijk by a factor of 4 and compute 3d FFT dening (cid:98)(s) for s = \nAlgorithm 1 Basic Vico-Greengard-Ferrando quadrature\n2: Evaluate GL (s) for L = (cid:112)L2\n, s2\n, s3\n2 ( s1\n3 for s dened above, and multiply elementwise by (cid:98)(s).\nL3\nL2\nL1\n{2Ni , . . . , 2Ni  1}.\n1 + L2\n2 + L2\n3: Perform 3d IFFT on the array dened above and truncate the result keeping rst N1  N2  N3 entries,\nobtaining approximation to h(r) on the grid.\n\n), si \n\nhij k =\n\nwhere\n\n2.1. Convolution form of the algorithm\n2N11(cid:88)\n2N21(cid:88)\n2N31(cid:88)\nLet us write Algorithm 1 more explicitly. All steps together can be represented as follows\nGL (s) (cid:98)(s)e2 i s1 i\n1\n1\n1\n4N2 e2 i s3 k\n4N1 e2 i s2 j\n4N3 ,\n4N1\n4N2\n4N3\ns2=2N2\ns1=2N1\ns3=2N3\n4N11(cid:88)\n4N31(cid:88)\n4N21(cid:88)\n(cid:98)(s) =\ni(cid:48) j (cid:48) k(cid:48) e\nj (cid:48)=0\ni(cid:48)=0\nk(cid:48)=0\n4N11(cid:88)\n4N21(cid:88)\n4N31(cid:88)\nBy plugging (5) into (4) and by changing the order of summation it is easy to see that\nj (cid:48)=0\ni(cid:48)=0\nk(cid:48)=0\n\nGM\nii(cid:48) ,jj (cid:48) ,kk(cid:48) i(cid:48) j (cid:48) k(cid:48) ,\n\n2 i s2 j (cid:48)\n4N2 e\n\n2 i s1 i(cid:48)\n4N1 e\n\n2 i s3 k(cid:48)\n4N3 .\n\nhij k =\n\n(4)\n\n(5)\n\n(6)\n\n\fwhere\n\nGM\nii(cid:48) ,jj (cid:48) ,kk(cid:48) =\n\n2N31(cid:88)\ns3=2N3\n\n2N21(cid:88)\n2N11(cid:88)\n1\n1\n1\n4N1\n4N2\n4N3\ns2=2N2\ns1=2N1\n N2 , k (cid:48)\n N1 , j (cid:48)\nMoreover, since i(cid:48) j (cid:48) k(cid:48) is 0 for i(cid:48)\n N3 (see step 1 of Algorithm 1) and we truncate\nN31(cid:88)\nN21(cid:88)\nN11(cid:88)\nthe result, formula (6) simplies and nally:\nj (cid:48)=0\ni(cid:48)=0\nk(cid:48)=0\n\nGL (s)e2 i s1 (ii(cid:48) )\n4N2 e2 i s3 (kk(cid:48) )\n4N1 e2 i s2 (jj (cid:48) )\n4N3\n\nhij k =\n\nGM\nii(cid:48) ,jj (cid:48) ,kk(cid:48) i(cid:48) j (cid:48) k(cid:48) .\n\n3\n\n.\n\n(7)\n\n(8)\n\nWe see that (8) takes the form of a discrete aperiodic convolution with discrete Greens function\nGM (which we will call mol lied Greens function ). One can notice that to fully determine GM\nit is sucient to run Algorithm 1 once for a special right hand side ij k = i0 j 0 k0 . Formula (8)\nplays essential role in the further analysis. Multiplication by multilevel Toeplitz matrix generated\nby GM can be performed with logarithmic complexity in QTT format as described in [2], and we\ndiscuss neccessary denitions and algorithms in the next section.\n\n3. LOW-RANK TENSOR APPROACH\n\n3.1. TT and QTT formats\nTo understand the QTT format let us start with describing the TT-format, which is a nonlinear low-\nparamentric representation of multidimensional arrays, called tensors. Tensor X  Cn1n2nd is\nsaid to be in the TT-format if it represents as\n\nXi1 i2 ...id = X (1) (i1 )X (2) (i2 ) . . . X (d) (id ),\n(9)\nwhere X (k) (ik )  Crk1rk , r0 = rd = 1, ik = 1, . . . , nk . Matrices X (k) are called TT-cores and rk\nare called TT-ranks. Notice that if r = maxk rk is small, then there is a signicant compression to\nstore X . Indeed, initial tensor requires storing nd parameters, while to store its TT-representation\nonly O(dnr2 ) parameters are needed.\nIn fact, one could use TT representation to store and to work with arising in Algorithm 1 3-\ndimensional arrays. However we will use a more sophisticated approach called QTT format, which\nallows for additional storage reduction compared to TT. QTT format is the following modication\nof the TT format. First we assume that d = 3, ni = 2li , i = 1, 2, 3. Then each physical index i,\nj , k is represented in the binary format, i.e.\ni = i1 + 21 i2 +    + 2li1 id ,\nand we have initial tensor ij k encoded as a (l1 + l2 + l3 )-dimensional array :\n\nim = 0, 1, m = 1, . . . , d\n\n.\n\nij k  i1 ...il1 j1 ...jl2 k1 ...kl3\nTT decomposition of  is called the QTT decomposition. The storage of the QTT decomposition is\nO(r2 (l1 + l2 + l3 )) = O(r2 log n), so if r = maxi ri is bounded, the total storage scales logarithmically.\nIn practice tensors of exact low-rank rarely occur. Typically one xes accuracy \u0001 and tries to nd\nbest approximation with this accuracy. It has been shown that in some applications ranks grow as\nr = O(log \u00011 ),  > 1 [5, 6].\n3.2. Translation of the algorithm to the QTT format\nComputation of kernel GM . To use (8) we rst need to nd GM (7). For this purpose we run\nAlgorithm 1 for ij k = i0 j 0 k0 . Precomputations are done in the full format, in other words we\nform the whole dense tensor GM and utilize TT-SVD algorithm [7] to nd its QTT representation.\nTT-SVD algorithm is based on the computation of SVD decompositions of tensors reshaped into\nfull 2D matrices and therefore is quite expensive.\nIn principle one could use DFT in the QTT\nformat [8] to avoid forming full tensors. Unfortunately, we found that intermediate tensors arising\nin Algorithm 1 are of large rank. We will address this problem in our future work.\n\n\f4\n\nComputation of . Tensor  can be already given in the QTT representation. This can happen,\ne.g. if we are running a certain iterative process involving computation of convolution (1) and all\noperations in this process are done within the QTT format. Otherwise, ij k can be approximated\nwith logarithmic complexity by using the cross approximation method [9], which adaptively samples\nelements of a tensor. In this case we just need ij k be given as a function which returns value by\ngiven 3 indices i, j, k .\n\nComputation of convolution GM  . Next goal is to nd convolution of tensors GM and  (8).\nThe convolution can be considered as a multiplication of multilevel Toeplitz matrix generated by\nGM and vector . Matrices can also be represented in the TT and by analogy in the QTT format.\n(cid:88)\nThe denition is similar to that of TT-tensor: given matrix (operator) Ai1 ...id j1 ...jd , which acts on\nvector Xj1 ...jd such that\nAi1 ...id j1 ...jd Xj1 ...jd .\nYi1 ...id =\nj1 ...jd\n\nits TT-decomposition is dened as\nAi1 ...id j1 ...jd = A(1) (i1 , j1 )A(2) (i2 , j2 ) . . . A(d) (id , jd ),\nwhere A(k) (ik , jk )  CRk1Rk , R0 = Rd = 1, ik = 1, . . . , nk . QTT decomposition of 3D operator\nGM\nii(cid:48) ,jj (cid:48) ,kk(cid:48) is dened by analogy with the QTT decomposition of a tensor  we quantize indices\ni, j, k and i(cid:48) , j (cid:48) , k (cid:48) , group them pairwise and then compute TT decomposition:\n1 ) . . . G(l1+2) (il1+2 , i(cid:48)\nii(cid:48) ,jj (cid:48) ,kk(cid:48) =G(1) (i1 , i(cid:48)\nGM\nl1+2 )\n1 ) . . . G(l1+l2+4) (jl2+2 , j (cid:48)\nG(l1+3) (j1 , j (cid:48)\nl2+2 )\n1 ) . . . G(l1+l2+l3+6) (kl3+2 , i(cid:48)\nG(l1+l2+5) (k1 , k (cid:48)\nl3+2 ).\nWe use approach from [2] and analytically construct QTT representation of the induced multilevel\nToeplitz matrix GM\nii(cid:48) ,jj (cid:48) ,kk(cid:48) given QTT representation of GM\ni,j,k . Then matrix-vector product (8)\ncan be done in dierent ways. We used optimization procedure AMEn (alternating minimal energy\nmethod) [10, 11] which allows for rank adaptation compared to standard ALS (alternating least\nsquares) [12] optimization which works with the representation of a given size.\n\n4. NUMERICAL EXPERIMENTS\nApproximating GM using QTT. Firstly we show that using QTT representation greatly reduces\nnumber of degrees of freedom (DOF) of GM . Suppose that tensor X is given in the QTT format\nd(cid:88)\nwith ranks {r1 , . . . , rd}. Then it is easy to count total number of DOF of X :\ni=2\n\nDOF(X ) = 2(r1 + rd ) +\n\n2ri1 ri .\n\nBy applying this formula to GM computed as described in 3.2 for various values of k (while keeping\nL = 1) we obtained the following results (see Figure 1). This shows advantages of using QTT.\nSolving scattering problems. To further test our approach we solve the Lippmann-Schwinger\n(cid:90)\nequation which is used for solving scattering problems:\neik|rr(cid:48) |\n(r(cid:48) )dr(cid:48) = k2 q(r)inc ,\n|r  r(cid:48)\n(cid:90)\nR3\n|\neik|rr(cid:48) |\n(r(cid:48) )dr(cid:48) .\nscat =\n|r  r(cid:48)\nR3\n|\nWe used rounding by \u0001 = 107 in our computations. Firstly we xed k = 1 and L = 32 and took\nq(r) to be a 3D gaussian:\n|rr(cid:48) |2\n2a2\n\nand then we nd\n\n(r) + k2 q(r)\n\nq(r) = e\n\n,\n\n(10)\n\n(11)\n\n\f5\n\n(a)\n(b)\nFigure 1: DOF of GM in 2D (1a) and 3D (1b) for \u0001 = 107 and L = 1\nTable 1: Relative error (err) and eective rank (erank) for dierent grid sizes and rounding errors \u0001. Results\nare presented for two types of function q : Gaussian (11) and smoothed cube (12).\n\n3D grid size\n\n\u0001 = 103\n\u0001 = 105\n\u0001 = 107\n\u0001 = 103\n\u0001 = 105\n\u0001 = 107\n\nGaussian\n\nSmoothed cube\n\nwith r(cid:48) = (cid:0) L\n2 , L\n2 , L\n2\n\n(cid:1) , and a = L\n10 , and\n\n323\nerank\n14\n24\n30\n22\n29\n29\n\nerr\n8e-1\n8e-1\n8e-1\n8e-1\n8e-1\n8e-1\n\n643\nerank\n15\n25\n34\n35\n57\n73\n\nerr\n8e-2\n4e-4\n5e-6\n8e-2\n5e-4\n2e-5\n\n1283\nerank\n14\n26\n38\n34\n59\n80\n\nerr\n7e-2\n4e-4\n4e-6\n6e-2\n5e-4\n6e-6\n\nTo solve arising systems in the QTT format we used AMEn [10, 11] , which allows for rank\nadaptation. Taking the solution computed on a grid with size 2563 as a reference we measured\n\ninc (x, y , z ) = eix .\n\n(a) Section z = 16 .\n\n(b) Section x = 17.5 .\n\nFigure 2: Scattering on the Gaussian with standard deviation 3.2 in D = [0, 32 ]  [0, 32 ]  [0, 32 ] for\nk = 1. Eective rank of the solution is equal to 39.\n\nrelative error of the solutions computed on smaller grids for various rounding parameters \u0001. Results\nare given in the Table 1. We see that error is roughly equal to the \u0001 even for modest number of\ngrid nodes per wavelength.\n\n28292102112121Dgridsize104105106107DOFUncompressedk=2k=22k=42k=62k=82252627281Dgridsize105106107DOFUncompressedk=2k=22k=42k=62k=82020406080100x020406080100y6.604.953.301.650.001.653.304.956.60020406080100y020406080100z1.000.750.500.250.000.250.500.751.001.25\fAs a next experiment we performed the same computations for q(r) representing smoothed cube:\n(cid:16) |rr(cid:48) |\n(cid:17)8\n(cid:1) and a = L\nfor r(cid:48) = (cid:0) L\na\n2 , L\n2 , L\n4 . For the results see Figure 3.\n2\n\n0.5\nq(r) = e\n\n,\n\n6\n\n(12)\n\n(a) Section z = 16 .\n\n(b) Section x = 17.5 .\n\nFigure 3: Scattering on the smoothed cube of size 8 in D = [0, 32 ]  [0, 32 ]  [0, 32 ] for k = 1. Eective\nrank of the solution is equal to 81.\n\nScattering problems on quasiperiodic structures. As a nal test we took q(r) to be a periodic\ngrid of smoothed cubes (structures like this are extremely suitable for QTT computations). Namely,\ndenoting q(r) dened by formula (12) by qr(cid:48) ,a (r) we solve the equation (10) in the domain [0, 2] \n1(cid:88)\n3(cid:88)\n19(cid:88)\n[0, 1]  [0, 10] on the grid 64  64  1024 for q(r) dened as\nj=0\ni=1\nk=0\n\nq(0.25+0.5j,0.25+0.5k,0.5i),0.1 (r)\n\nq(r) =\n\nand for k = 4 . inc in this case is a plane wave propagating in z -direction\n\nFigure 4 demonstrates our results.\n\ninc (x, y , z ) = e4 iz .\n\nFigure 4: Scattering on the periodic cube structure in D = [0, 2]  [0, 1]  [0, 10] for k = 2 . Eective rank\nof the solution is equal to 67. Section x = 1.\n\n020406080100x020406080100y432101234020406080100y020406080100z2.752.201.651.100.550.000.551.101.6502468z0.00.5y2.41.81.20.60.00.61.21.82.4\f7\n\nAcknowledgements\n\nThis study was supported by the Ministry of Education and Science of the Russian Federation\n(grant 14.756.31.0001), by RFBR grants 16-31-60095-mol-a-dk, 16-31-00372-mol-a and by Skoltech\nNGP program.\n\nREFERENCES\n1. F. Vico, L. Greengard, and M. Ferrando, Fast convolution with free-space Greens functions,\nJournal of Computational Physics, vol. 323, pp. 191203, 2016.\n2. V. Kazeev, B. Khoromskij, and E. Tyrtyshnikov, Multilevel Toeplitz matrices generated by\ntensor-structured vectors and convolution with logarithmic complexity, SIAM J. Sci. Com-\nput., vol. 35, no. 3, pp. A1511A1536, 2013.\n3. I. V. Oseledets, Approximation of 2d  2d matrices using tensor decomposition, SIAM J.\nMatrix Anal. Appl., vol. 31, no. 4, pp. 21302145, 2010.\n4. L. a. Klinteberg, D. S. Shamshirgar, and A.-K. Tornberg, Fast Ewald summation for free-space\nStokes potentials, arXiv preprint arXiv:1607.04808, 2016.\n5. V. Kazeev and C. Schwab, Quantized tensor-structured nite elements for second-order ellip-\ntic pdes in two dimensions, tech. rep., SAM research report 2015-24, ETH Zurich, 2015.\n6. V. Kazeev, I. Oseledets, M. Rakhuba, and C. Schwab, QTT-nite-element approximation for\nmultiscale problems I: model problems in one dimension, Adv. Comp. Math., 2016.\n7. I. V. Oseledets, Tensor-train decomposition, SIAM J. Sci. Comput., vol. 33, no. 5, pp. 2295\n2317, 2011.\n8. S. V. Dolgov, B. N. Khoromskij, and D. V. Savostyanov, Superfast Fourier transform using\nQTT approximation, J. Fourier Anal. Appl., vol. 18, no. 5, pp. 915953, 2012.\n9. I. V. Oseledets and E. E. Tyrtyshnikov, TT-cross approximation for multidimensional arrays,\nLinear Algebra Appl., vol. 432, no. 1, pp. 7088, 2010.\n10. S. V. Dolgov and D. V. Savostyanov, Alternating minimal energy methods for linear systems\nin higher dimensions. Part I: SPD systems, arXiv preprint 1301.6068, 2013.\n11. S. V. Dolgov and D. V. Savostyanov, Alternating minimal energy methods for linear systems\nin higher dimensions. Part II: Faster algorithm and application to nonsymmetric systems,\narXiv preprint 1304.1222, 2013.\n12. S. Holtz, T. Rohwedder, and R. Schneider, The alternating linear scheme for tensor opti-\nmization in the tensor train format, SIAM J. Sci. Comput., vol. 34, no. 2, pp. A683A713,\n2012.\n\n\f", 
        "tag": "Numerical Analysis", 
        "link": "https://arxiv.org/list/cs.NA/new"
    }, 
    {
        "text": "Tackling Diversity and Heterogeneity by Vertical Memory Management* \n Lei Liu (SKL, ICT) \n  \n\n                              Abstract    \nExisting  memory  management  mechanisms  used  in  commodity \ncomputing  machines  typically  adopt  hardware  based  address \ninterleaving  and  OS  directed  random  memory  allocation  to \nservice  generic  application  requests.  These  conventional \nmemory  management  mechanisms  are  challenged  by  contention \nat  multiple  memory  levels,  a  daunting  variety  of  workload \nbehaviors,  and  an  increasingly  complicated  memory  hierarchy. \nOur  ISCA-41  paper  proposes  vertical  partitioning  to  eliminate \nshared  resource  contention  at  multiple  levels  in  the  memory \nhierarchy.  Combined  with  horizontal  memory  management \npolicies,  our  framework  supports  a  flexible  policy  space  for \ntackling  diverse  application  needs  in  production  environment \nand is suitable for future heterogeneous memory systems. \n1. Introduction \nEfficiently  utilizing  shared  resources  in  the  memory  hierarchy \nsuch as Last Level Cache (LLC) and main memory  is at  the core \nof  constructing  high  performance  multi-core  machines.  To  date, \nthe  most  common  mechanism  of  memory  and  cache  sharing \nused  in memory  controllers  in  commodity machines  is  based  on \ngeneric  address  interleaving,  where  the  physical  address  of  a \nmemory request determines which LLC set and DRAM bank  the \nrequest  is  serviced.  Previous  studies  [1,3,7,8]  indicate  that  this \nsimple \napproach \ncan \ncause \nsignificant  memory/cache \ninterference  as  multiple  threads  share  the  same  DRAM  banks \nand  cache  sets.  Additionally,  this  approach  is  entirely  oblivious \nto  application  and  architecture  characteristics,  thus  fail  to \nefficiently  use  memory  resource  on  modern  computing  systems \nwith increasing diversity and heterogeneity. \n1.1 Challenges for Existing Memory Management   \nExisting  memory  management  approaches  [6,8,9,11]  typically \nfocus  on  horizontally  optimizing  a  single  level  in  the  memory \nhierarchy and have the following drawbacks:   \n(1)  Contention  at  different  memory  hierarchy:  Shared \nresources  (i.e.,  LLC  and  DRAM)  by  multiple  threads  lead  to \ncontention  at  multiple  levels  in  the  memory  hierarchy.  Past \nefforts  [7,8,9]  focus  on  horizontally  partitioning  and  managing \nLLC  or  DRAM  banks  to  minimize  contention  at  a  single  level. \nHowever,  the  contention  problem  has  never  been  addressed  for \nall  levels  in  the  memory  hierarchy  (except  per-core  private \ncaches  that  do  not  suffer  from  inter-thread  interference)  at  the \nsame  time.  To  completely  eliminate  interference  in  the  memory \nhierarchy,  a  new  approach  is  needed  to  vertically  combine \ncontention elimination techniques on multiple levels. \n(2)  Single  policy  management:  Existing  memory  management \nin  operating  system  (OS)  is  largely  single  policy  based,  which \nfails  to  support  flexible  and  effective  memory  allocation  with \nrespect  to  different  applications  sharing  and  capacity  needs.  As \ndisparate  non-volatile  memory  technologies  are  emerging  and \nevolving  to  more  sophisticated  and  hybrid  memory  systems \n[10,17],  adaptive  and  reconfigurable  policies  are  needed  to \nmanage  the  heterogeneity  in  terms  of  retention,  access  speed, \nfault  tolerance,  and  energy  efficiency.  In  such  heterogeneous \nmemory  environments,  single  policy  management  can  result  in \nsignificant resource underutilization.  \n(3)  Application obliviousness:  Emerging   workloads   contain \n\n*  I  wish  to  extend  my  deep  thanks  to  Yong  Li  (Pitt  &  VMware  CA),  Prof. \nChen Ding (Rochester) and Prof. Xiaodong Zhang (Ohio) for their efforts. \nThe  first  step  work  is  published  in  ISCA-14  (Corresponding  author:  Lei  Liu). \nTitle:  Going  Vertical  in  Memory  Management:  Handling  Multiplicity  by \nMulti-Policy.  \n \n\n \nFigure  1.  Address  mapping  from  the  view  of  OS  and  three \ncategories of color bits on a typical multicore machine. \nnumerous  applications  and  exhibit  diverse  and  dynamic \nbehaviors. Our results demonstrate  that servicing all applications \nusing  one  simple  generic  policy  in  a  program-oblivious  way \noften  results  in  inter-program  perturbation,  resources  thrashing, \npoor  memory/cache  utilization,  and,  consequently,  degraded \nperformance.  Therefore,  an \nintelligent  system \nthat  can \nunderstand  application  behaviors  is  extremely  important, \nespecially in complicated computing environments such as cloud \nand  data  center,  where  a  large  number  of  applications  are \nrunning and sharing memory resources [1,6,16,18]. \n1.2 Going Vertical in Memory Management \nOur  ISCA-2014  paper  proposes  a  novel  solution  to  meet  the \naforementioned  challenges.  (1)  To  address  the  contention \nproblem  in  the  entire  memory  space,  we  expose  architecture \nfeatures  (i.e.  physical  address  mapping  for  cache  sets  and \nDRAM  banks)  to  OS  by  utilizing  different  types  of  addressing \nbits.  Based  on  these  architecture  features,  our  ISCA-2014  paper \nintroduces Vertical Partitioning  (VP)  [10]  into modern OS. VP \npartitions  the  memory  hierarchy  vertically  through  cache  and \nDRAM  simultaneously  to  completely  eliminate  the  contention \nissue  for  all  susceptible  levels  in  the  memory  hierarchy.  (2) \nCombined with horizontal partitioning mechanisms (i.e, bank- or \ncache-only  partitioning),  we  group  the  memory  partitioning \npolicies  into  several  categories  to  form  a  memory  management \npolicy  space  for  diverse workloads  to  choose  from. This  enables \nflexible  and  customized  resource  allocation  that  meets  each \nindividual applications memory and cache resource requirement. \n(3) By  dynamically  monitoring  applications  page-table  using  a \nlow-overhead  algorithm, we  equip  the OS with  the  capability  of \nunderstanding  applications  memory  behavior  on  the  fly. \nCombining  all  these  components  and  a  large  set  of  experimental \nresults  conducted  in  real  systems,  we  devise  an  intelligent \nmemory  management  mechanism  that  can  choose  appropriate \nallocation policy for workloads with arbitrary characteristics.    \n2. Solution \n2.1 Vertical and Horizontal Partitioning \nIn  memory  controllers  (MC),  address  mapping  policies  are \ndetermined by platform and configurations. Typically, beside the \nbits that only index DRAM banks (B-bits) and LLC sets (C-bits), \nthere  are  also  some  bits  that  denote  both  (noted  as  O-\nbits).   Figure  1  illustrates  the  three  categories  of  coloring  bits on \na  mainstream  machine  (Intel  i7-860  with  8GB  memory  and  64 \nbanks, B-bits: 21~22; C-bits: 16~18; O-bits: 14~15 [8,9,10]). \n   We  observe  that  by  considering  different  number  of  bits  of \nvarious  types  when  allocating  a  memory  page,  OS  can  exploit \ndiverse  memory allocation  approaches. As  shown  in  Table  1, \nbank-only  partitioning  and  interleaving  based  page  allocation \ncan  be  derived  by  considering  B-bits  when  allocating  a  page \nnumber  to  an  application. Particularly,  the O-bits  enable  vertical \npartitioning  that  partitions  both  LLC  sets  and  DRAM  banks \nvertically  through  the  memory  hierarchy.  We  further  derive \nseveral  sub-policies  (i.e. ,   A/B/C-VP)   by  choosing   different \n\n\f4/8-core \n\n4-core \n\n8-core \n\n8-core \n\n4/8-core \n\nB-VP \n\nC-VP \n\nA-VP \n\nO-bits {14~15} \n\nTarget Cores \n\nColoring Bits \nPolicy  \nInterleaving  B-bits {21~22} \nO-bits {14~15} \nBank-Only  B-bits {21~22} \nO-bits {15} \n\nDescription \n \n  Bank-Interleaving w/  \n random page allocation \nLLC  2 groups \n  Banks  8 groups \n     LLC  4 groups \n     Banks  4 groups \n     LLC  4 groups \nB-bits {22} + \n  Banks  8 groups \nO-bits {14~15} \nLLC  8 groups \nC-bits {16} +  \n  Banks  4 groups \nO-bits {14~15} \nTable 1. Several representative partitioning policies \ncombinations  of  O-,  B-  and  C-  bits  to  partition  memory/cache \ninto different  quotas.   Each policy  represents  one  resource  usage \ncharacteristic  and  has  its  own  friendly  (suitable)  workloads, \nwhich performs better on this policy than on any other policies. \n2.2 Understanding Memory Features \n2.1.1 Key Observations and Classification   \nBased  on  numerous  experiments  across  over  200  workloads  in \nreal  system  (Figure  3),  we  find  that  compared  to  DRAM,  the \namount of available cache resource has a much greater impact on \napplication  performance.  Thus,  we  classify  applications  into \nfour categories:  Core  Cache  Fitting  (CCF),  LLC  High  (LLCH), \nLLC  Middle  (LLCM)  and  LLC  Thrashing  (LLCT),  based  on \ntheir  performance drop caused  by  cache  quota  reduction  from \n8/8 (entire cache) to 1/8.  CCF applications (e.g., hmmer), do not \ndegrade  significantly  when  using  fewer  LLC  resources  as  their \nworking  sets  fit  into  the  L1  and  L2  private  caches.  LLCT \napplications,  such  as  libquantum,  are  also  insensitive  to  cache \nquotas,  but  due  to  cache  thrashing  behavior  rather  than  small \nworking  set  sizes.  LLCH  applications  such  as  mcf  suffer  the \nworst  performance  degradations  from  reduced  cache  quotas  due \nto their large resource requirements. \n2.1.2 Dynamic Application Classification  \nTo enable online optimization, we need to classify an application \nbased  on  its  run-time  characteristics.  We  found  the  number \nof hot  pages  (active  pages  used  in  a  particular  time  interval \nand can be  identified by  the access bit  in PTE) can reflect an \napplications  LLC  demand  due  to  the  DRAM  row-buffer \nlocality.   Based  on  this  insight,  we  devise  a  classification \nalgorithm  implemented  as  two  kernel  tasks,  JOB1  and  JOB2, \nwhich  sample  the  page  access  patterns  periodically.  JOB1  is \nresponsible  for  collecting  the  number  of  hot  pages  by  clearing \nthe  access_bit  and  examining  pages  with  access_bit  =1  at  the \nend  of  each  sampling  period.  JOB2  uses  an  array  of  page  access \ncounters  to  record  the  number  of  accesses  for  each  page.  JOB2 \ngroups  the  counter  values  into  ranges  and  computes  a  weighted \npage  distribution  (WPD)  to  reflect  page  reference  locality.  Our \nalgorithm  can  accurately  classify  applications  into  one  of  the \nfour  categories  detailed  in  Section  2.1.1  by  comparing  the \nnumber of hot pages and WPD with trained thresholds.  \n2.3 Memory Allocation Policy Selection \nWe  adopt  a  data  mining  approach  to  quantitatively  study  the \nimpacts  of  various  memory  allocation  schemes  on  over  2000 \nworkloads.  We run  each  workload  with  different  policies  and \nrecord \nthe  performance \nimprovements  achieved  by \nthe \ncorresponding  policies.  Based  on  the  correlation  between  the \nclassification  and  performance  gains  on  different  policies,  we \ncreate a set of rules to select the policy of vertical management. \n2.3.1 Partitioning Rules \nFirst, most workloads  that contain at  least one LLCT application \nperform  best  on  A/C-VP.  Second,  a  dominating percentage  of \nworkloads  containing  LLCH  but  not  LLCT  perform  best  on \nbank-only  partitioning.    Third,  most  workloads  with  LLCM  but \nno LLCT or LLCH applications achieve best performance results \nwith a modest cache  partitioning scheme such  as A-VP  and  B- \n\nFigure 2. Memory allocation policy decision tree (PDT) \n\n  \n\n  \n\n  \n\n  \n\n \nFigure  3.  Performance  improvement  of  various  polices  for  214 \nworkloads (A/B/C-VP are memory policies introduced in Table 1). \n \nVP. Based on  the  rules  and  their priorities  relative  to  each other, \nwe generate  a memory management policy decision  tree  (PDT) \nshown  in  Figure  2.  The  PDT  is  useful  for  choosing  appropriate \npolicies  for  diverse  workloads.  Moreover,  for  multi-threaded \nworkloads, Park et al. [12] argues that a random page-interleaved \nallocation  scheme  (Table  1)  outperforms  other  schemes.  Thus, \nwe add this policy to handle multi-threaded workloads in PDT. \n2.3.2 Coalescing Rules \nDespite  the  advantage  in  eliminating  interference,  a  pure \npartitioning  based  approach  is  not  always  preferable  since  it \nlimits  the  available  resource  and  can  harm  the  performance  for \nresource  hungry  applications  (e.g.,  LLCH).  Thus,  we  extend  the \nPDT with  several coalescing  rules  that  can  be  used  to merge  the \npartitioned  resource  quotas  among  certain  types  of  applications. \nUsing  the  data  mining  approach,  we  find  the  LLCH  and  LLCM \napplications  should  be coalesced  together  to  share  the  cache \nquota, while LLCT  and CCF  applications  should  be coalesced \nrespectively to share a small cache quota. \n3. Experimental Results  \nWe  implement  PDT  with  partitioning  and  coalescing  rules  in \nLinux  kernel.  Extensive  experiments  show  that  our  framework \noutperforms  the  unmodified  Linux  kernel  and  achieves  up  to  11% \nperformance  gains  over  prior  techniques.  Our  results  also  show \nthat  higher  performance  can  be  achieved  by  adaptively  choosing \nworkloads  best-fit  memory  policy.  Illustrated  in  Figure  3,  half \nof  the  workloads  fall  into  region  4,  meaning  their  performance \ndegrade  in  shared  memory  conditions  where  cache  resource  is \nbeing  limited/partitioned, but bank partitioning can benefit  them. \nBy contrast, some workloads  in region 1 can achieve above 10% \nperformance gains via x-VP to eliminate interference. \n4. Future Impact  \nDRAM  and  cache technology has  been undergoing remarkable \n\n\fchanges.  In  contrast  to  the  fast-paced  changes  in  the  memory \nhierarchy, the legacy memory management strategies such as the \norder-based, \ninterleaved  memory/cache  allocation  schemes \nadopted \nlargely \nin  commodity  OS  and  hardware  remain \nunchanged.  These  existing  strategies  manage  memory  resource \nblindly  in  that  they  are  not  aware  of  the  architecture  features \nand  applications  memory  characteristics,  leading  to  a  more \ngeneric but  less efficient approach. We develop a practical, cost-\neffective  way  to  make  the  OS  conscious  about  running \napplications  and  the  underlying  architecture,  enabling  a  more \nadaptive  and  efficient  way  of  utilizing  memory  resources.  Our \ncontributions may have  the  following  long-term  impacts on both \nacademia and industry: \n4.1 Academic Impact \n(1)  New  insight  in  memory  optimization:  We  conduct  a \ncomprehensive  study  in  page-coloring  based  partitioning.  We \nfurther  propose  a  new  vertical memory management mechanism \nto  control  the  entire  memory  hierarchy,  and  thus  eliminate  the \nmemory  interference  in  the  entire  memory  hierarchy.  Our  key \ninsight is the overlapped address bits (O-bits) in physical address \nmapping.  Using  O-bits,  we  enable  memory  and  cache  vertical \nmanagement that significantly mitigates the memory interference \nissue.  Additionally,  our  idea  to  utilize  architecture  features  (O-, \nB-,  C-  address  mapping  bits)  enriches  memory  allocation \npolicies  and  enlarges  the  space  of  memory  optimization.  Our \nstudy  brings  new  opportunities  and  design  patterns  in  the  areas \nof high performance computing and memory architecture. \n(2) New method  for  application-aware  computing/allocation: \nNumerous prior studies demonstrate  that many  important system \noptimizations  cannot  be  achieved without  leveraging  application \nbehavior.  In  our  work,  we  devise  an  online  application \nclassification method and  implement  it as Linux kernel modules. \nTo  the  best  of  our  knowledge,  this  is  the  first  approach  that \ncaptures  dynamic  application  memory  and  cache  usage  patterns \nin  real  production  settings,  without  the  help  of  hardware  based \nperformance  counters,  long-running  simulations,  or  pin-based \nprofiling.  The  accuracy  of  our  approach  is  verified  by  off-line \nprofiling. Our work opens a new path for researchers  to  leverage \nthe knowledge of running workloads for system optimizations.  \n(3)  New  perspective  for  designing  future  OS:  To  assist  the \napplication-aware  policy  selection  process,  we  studied  a  large \namount  of  workloads  running  on  different  memory  allocation \npolicies.  Using  data  mining  to  analyze  the  results,  we  generate \npartitioning  and  coalescing  rules  used  to  appropriately  partition \nresources  while  allowing  non-interfering  programs  to  live \ntogether  for  resource  sharing.  Our  large  result  set  provides \nvaluable  reference  for  studying \nthe \nimpact  of  memory \nscheduling/allocation methods on diverse workloads.  In the long \nterm,  we  believe  that  our  approach,  including  (1)  and  (2),  will \nmotivate  researchers  in  the  related  fields  to  build  more \nintelligent  modern  OS  that  can  understand  and  learn  from \napplication behavior  to and adapt  its own behaviors  to maximize \nresource utilization and performance.   \n(4)  New  approach  for  hybrid  memory  management.  We \nrestructured  the  conventional  memory  strategy  and  firstly \nintroduced  the  Vertical  approach.  We  use  this  approach  for \nhybrid  DRAM-NVM  memory  management  and  devise  Memos \n[19,20]  in  Linux  kernel.  Our  first  step  results  show  memos  can \nbenefit the hybrid memory performance and the NVM lifetime.  \n4.2 Industry Impact    \nThe  benefits  of  our  proposed  vertical  memory  management  to \nindustrial world are multifold. (1) It adds both cache and DRAM \ninto  the  OS  management  pool,  and  thus  potentially  benefits  the \noverall  system  performance  by  simultaneously  reducing  cache \nand  DRAM  contention,  a  critical  problem  faced  by  many  cloud \nproviders  such  as  Amazon,  Google  and  VMware.  (2)  It \nsignificantly enlarges  the memory management policy space and \n\nbrings  greater  flexibility  for  diverse  application  needs  in \ncommercial data center and production environments. Moreover, \napplication memory access and usage patterns are captured using \na  practical,  page-table  sampling  based  approach  that  only  adds  a \nvery low overhead. (3) It helps reduce the energy cost and access \nlatency  of  emerging  NVMs.  In  particular,  memory-partitioning \ntechniques  is  more  effective  in  NVM  cases  where  row  buffer \nmiss  latency  and  energy  is  larger.  (4)  It  segregates  applications \nwith  high  latency-sensitivity  versus  those  with  bandwidth-\nsensitivity  accesses  (e.g.,  stream-like  application),  thus  ensuring \nbetter  QoS  and  fairness.  Particularly,  our  partitioning  and \ncoalescing  techniques  can  be  used  together  to  handle  dynamic \nworkload  changes  in  production  environments,  thus  having  a \nprofound  influence  on  efficient  resource  isolation,  virtualization \nand  consolidation,  which  are  critical  and  have  a  significant \nimpact on the trend of moving to the cloud.    \n    By  restructuring  the  buddy  system  in  Linux  kernel,  we \nimplement  the  HVR  framework  as  an  all-in-one  solution  that \ncombines  Horizontal,  Vertical  Partitioning  and  Random \nallocation  [12].  We  believe  our  prototype  demonstrates  the \nfeasibility of  a more  intelligent memory management  strategy  in \nmodern  OS  design  for  addressing  the  emerging  challenges  in \nfuture  complicated  computing  environments.  It  would  require  a \nminimal  effort  to  port  our  prototype  to  production  settings  to \nbenefit diverse commercial workloads. \nReferences \n[1]  D.  Chandra,  F.  Guo,  S.  Kim,  and  Y.  Solihin.  Predicting  Inter-Thread  Cache \nContention on a Chip Multi-Processor Architecture. In HPCA-2005. \n[2]  S. Cho  and L.  Jin. Managing  distributed,  shared L2  caches  through OS-level \npage allocation. In MICRO-2006. \n[3]  X.  Ding,  et.al.  SRM-buffer:  an  OS  buffer  management  technique  to  prevent \nlast level cache from thrashing in multicores. In EuroSys-2011. \n[4] X. Ding, K. Wang, and X. Zhang. ULCC: a user-level  facility  for optimizing \nshared cache performance on multicores. In PPoPP-2011. \n[5]  M.K.  Jeong,  D.H.  Yoon,  D.  Sunwoo,  et.al.  Balancing  DRAM  locality  and \nparallelism in shared memory CMP systems. In HPCA-2012. \n[6]  Y.  Kim,  et.al.Thread  cluster  memory  scheduling:  Exploiting  differences  in \nmemory access behavior. In MICRO-2010. \n[7]  J.  Lin,  et.al.  Gaining  insights  into multicore  cache  partitioning:  Bridging  the \ngap between simulation and real systems. In HPCA-2008. \n[8]  L.  Liu,  et.al.  A  software  memory  partition  approach  for  eliminating  bank-\nlevel interference in multicore systems. In PACT-2012. \n[9]  L.  Liu  et.  al.  BPM/BPM+:  Software-based  Dynamic  Memory  Partitioning \nMechanisms  for  Mitigating  DRAM  Bank-/Channel-level  Interferences \nin \nMulticore Systems. In ACM TACO-2014 \n[10] L. Liu et. al. Going Vertical in Memory Management: Handling Multiplicity \nby Multi-policy. In ISCA-2014 \n[11] W. Mi, X. Feng,  J. Xue,  and Y.  Jia. Software-hardware  cooperative DRAM \nbank partitioning for chip multiprocessors. In NPC-2010. \n[12]  H.  Park,  S.  Baek,  J.  Choi,  D.  Lee,  and  S.H.  Noh,  Regularities  considered \nharmful:  forcing  randomness  to memory  accesses  to  reduce  row  buffer  conflicts \nfor multi-core, multi-bank systems. In ASPLOS-2013. \n[13]  S.  Rixner,  W.J.  Dally,  U.J.  Kapasi,  P.  Mattson,  and  J.D.  Owens.  Memory \naccess scheduling. In ISCA-2000. \n[14]  L.  Soares,  et.al.  Reducing  the  harmful  effects  of  last-level  cache  polluters \nwith an OS-level, software-only pollute buffer. In MICRO-2008. \n[15]  X.  Zhang,  S.  Dwarkadas,  and  K.  Shen.  Towards  practical  page  coloring-\nbased multicore cache management. In EuroSys-2009. \n[16]  S. Zhuravlev,  S. Blagodurov,  and A.  Fedorova. Addressing  shared  resource \ncontention in multicore processors via scheduling. In ASPLOS-2010. \n[17]  J.  Lee  and  H.  Kim.  TAP:  A  TLP-aware  cache  management  policy  for  a \nCPU-GPU heterogeneous architecture. In HPCA-2012. \n[18] M. K. Qureshi et.al. Utility-based cache partitioning: A  low-overhead, high-\nperformance, runtime mechanism to partition shared caches. In MICRO-2006. \n[19]  L.  Liu,  H.  Yang,  M.  Xie  et.al.  Memos:  A  Full  Hierarchy  Hybrid  Memory \nManagement Framework. In ICCD-2016. \n[20]  L.  Liu,  M.  Xie  and  H.  Yang.  Memos:  Revisiting  Hybrid  Memory \nManagement in Modern Operating System. In arXiv:1703.07725. \n\n\f", 
        "tag": "Operating Systems", 
        "link": "https://arxiv.org/list/cs.OS/new"
    }, 
    {
        "text": "Wireless Health Monitoring using Passive WiFi\nSensing\n\nUsman Mahmood Khan, Zain Kabir, Syed Ali Hassan\nSchool of Electrical Engineering & Computer Science (SEECS), National University of Sciences & Technology\n(NUST), Islamabad, Pakistan {13beeukhan, 13beezkabir, ali.hassan}@seecs.nust.edu.pk\n\n7\n1\n0\n2\n \nr\np\nA\n \n3\n \n \n]\nH\nO\n.\ns\nc\n[\n \n \n1\nv\n0\n2\n6\n0\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstractThis paper presents a two-dimensional phase ex-\ntraction system using passive WiFi sensing to monitor three basic\nelderly care activities including breathing rate, essential tremor\nand falls. Specically, a WiFi signal is acquired through two\nchannels where the rst channel is the reference one, whereas the\nother signal is acquired by a passive receiver after reection from\nthe human target. Using signal processing of cross-ambiguity\nfunction, various features in the signal are extracted. The entire\nimplementations are performed using software dened radios\nhaving directional antennas. We report the accuracy of our system\nin different conditions and environments and show that breathing\nrate can be measured with an accuracy of 87% when there are\nno obstacles. We also show a 98% accuracy in detecting falls and\n93% accuracy in classifying tremor. The results indicate that\npassive WiFi systems show great promise in replacing typical\ninvasive health devices as standard tools for health care.\nKeywordsPassive WiFi sensing; breathing rate measurement;\nessential\ntremor classication; fall detection; phase extraction;\nDoppler; SDRs\n\nI .\n\nIN TRODUC T ION\n\nOver the past few years, there has been a growing interest in\nubiquitous health monitoring [1-3]. One of the areas which\nbeneted most from this surge in the interest is elderly care\n[4][5]. Today, we see wearable devices that continuously mon-\nitor vital health signs [6], track essential tremor in Parkinson\npatients [7], generate alarms if there are any falls, and do more.\nHowever, such devices present several challenges: they intrude\nwith users routine activities, they have to be worn all the time\neven during sleep, and they have to be frequently recharged.\nWith the advances in wireless sensing, it has become possible\nto track and localize human motion [8-10]. In this paper, we\nexplore if we can tap onto these advances to monitor three\nbasic elderly care activities; i.e., breathing, tremor and falls.\nBreathing involves continuous inhale and exhale move-\nments and may be used to study the subjects physiological\nstate, her stress levels, or even emotions like fear and relief.\nTremor is a movement disorder in which the subject expe-\nriences rhythmic shaking of a body part such as hands [7].\nIt is highly prevalent among older people and although not\nlife threatening, it causes great inconvenience in social and\ndaily life settings, such as writing and eating. Falls are also\nhighly common among the ageing population and detecting\nthem early is integral to effective interventions and subsequent\ntreatments.\nDespite the ubiquitous nature of wireless networks, a\nlittle work has been done on exploring their suitability for\nhealth applications. This is concerning since the typical health\n\nmonitoring techniques are inconvenient; they require contact\nwith human body, and most of them are intrusive. For example,\ncurrent breath monitoring solutions require a chest band [6],\nnasal probe [11], or pulse oximeter [12]. Technologies that\nare more comfortable such as wrist bands tend to be erro-\nneous and unreliable. Tremor and fall monitoring devices are\nsimilarly inconvenient. However, the bigger concern is that\ncontact devices are not suitable for elderly health care. Using\na technology round the clock may be cumbersome or even\ndemeaning for the elderly. Worse, they may be in a condition\nsuch as dementia where they cant remember to put on the\ndevice once they have worn it off.\nRelated literature on target detection and localization fo-\ncuses on both active and passive radar techniques. Active\nradars employ dedicated transmitters, use high bandwidth and\noften require complex antenna arrays to go along with them\n[8]. In contrast, passive radars utilize the transmitted signals\nin the environment such as WiFi and cellular signals for target\nmonitoring. Passive radars offer numerous advantages when\ncompared to active radars, i.e., they are low-cost and covert\ndue to their receive-only nature, they operate license free due\nto no bandwidth requirements, and they offer better Doppler\nresolution due to the possibility of higher integrations times.\nTo the best of the authors knowledge, this is the rst\ncomprehensive study on elderly-focused health care applica-\ntions using passive WiFi sensing. Specically, this research\nintroduces a vital health wireless device (Wi-Vi) and makes\nthe following contributions:\n\n\nProposes a phase extraction system in 2-D to de-\ntermine breathing rate, classify essential tremor and\ndetect falls.\nCarries out extensive experiments to study the accu-\nracy of elderly care applications in different environ-\nments and conditions.\n\n\n\nA. Related Work\nMultiple approaches have been proposed for human mo-\ntion detection and other applications using passive sensing.\nKotaru et.al presented an indoor localization system using the\nchannel state information (CSI) and received signal strength\ninformation (RSSI) [9]. Similar approaches have been used\nfor other applications such as keystroke identication [13], in-\nhome activity analysis [10], and virtual drawing [14]. Some\nwork has been done on using active radars for breathing\nrate measurements. WiZ uses frequency modulated continuous\nwave (FMCW) signal chirps to detect breathing rate in both\nline-of-sight (LOS) and non line-of-sight (NLOS) scenarios\n\n\ftime is a function of total number of samples N and the\nsampling frequency fs . Accordingly, the Doppler resolution\nof the system is given as\n\nd =\n\nfs\nN\n\n.\n\n(3)\n\nEquation (3) shows that a long integration time is required to\nachieve a ne Doppler resolution. For example, an integration\ntime of 5 seconds yields a Doppler resolution of 0.2Hz. This\nposes two challenges to Vi-Wis target applications which\ninvolve small scale body motions. First, due to the nature\nof these applications, multiple Doppler shifts will emerge in\na single integration cycle resulting in an inherent ambiguity;\nsecond, positive and negative Doppler shifts may be present in\na single cycle and thus cancel out the effects of one another.\nIn order to address these issues, Vi-Wis operations consist of\nthe following three steps:\n\n\nExtract phase information off the reected surveillance\nsignals through cross-correlation.\nDevelop a 2-D system model to identify phase varia-\ntions in two separate planes.\nAnalyze and process phase variations to determine\nbreathing rate, classify tremor and detect human falls.\n\n\n\n\n\nIn what follows, we present an elaborate approach to imple-\nment these steps.\n\nA. Phase Extraction\nLet a small scale movement in a single plane is given by m.\nIf the wavelength of transmitted signal is given by , then the\nphase  associated with this movement can by described as\n\n =\n\n2m\n\n\n.\n\n(4)\n\nSince the reference and the surveillance antennas are spa-\ntially separate, the small scale human motion is observed by\nsurveillance antenna only. It follows then that the surveillance\nsignal has an associated phase shift which is positive when\nthe movement is towards the antenna and negative when the\nmovement is away from it. Assuming that there is a single\nreection at the surveillance antenna from the desired small\nscale motion, the surveillance signal is given as\n\ns[n] = Axsource [n +  ]ei2fd n ,\n\n(5)\n\nwhere xsource [n] is the source signal, fd is the Doppler shift,\nand A is the signal amplitude. In the simplest case when\nthere is no movement, the Doppler shift is zero and the above\nequation simplies to a delayed version of transmitted source\nsignal with no phase shift. When there is a movement towards\nthe plane of surveillance signal, the Doppler shift increases,\nresulting in a positive phase shift, and vice versa. Hence, by\ntracking phase variations in the reected signal, we can also\ntrack the movements in the environment.\nIf we divide the data into B batches and assume that the\ntime delay between reference and the surveillance signals is  ,\n\nFig. 1. Essential components of a passive radar system. Both surveillance and\nreference antennas pick up signals from the access point, but the surveillance\nsignal contains an added Doppler shift due to human movement.\n\n[8]. Other similar works include an ultra wide-band radar\nwith short transmit pulses to avoid reections from surround-\ning objects [15], a continuous wave (CW) radar for clinical\nmeasurements [16], and a CW radar for indoor environment\n[17]. Essential tremor monitoring is mostly unexplored in the\nwireless domain and is typically done using electromyography\nsensors, accelerometers and gyroscopes [18].\nThe rest of the paper is organized as follows. In section\n2, the phase extraction theory is discussed followed by an\nexplanation of the 2-D system model and subsequent signal\nprocessing in section 3. Section 4 elaborates on the imple-\nmentation procedure and section 5 lists the key results and\nlimitations of this research.\n\nI I . TH EORY O F O PERAT ION\nThe observed frequency shift due to relative motion of trans-\nmitter and receiver is given by the well-know phenomenon of\nDoppler frequency, and is written as\nv\nc\n\nfd = fo\n\ncos(),\n\n(1)\n\nwhere fo is the carrier frequency of the transmitted signal, v\nis the speed of relative motion, c is the speed of light, and  is\nthe angle of relative motion between transmitter and receiver.\nIn a passive radar sensing system (shown in Fig. 1), one or\nmultiple WiFi access points (APs) are used as transmitters,\nand spatially directional high gain antennas with narrow beam-\nwidths are used to obtain the reference and the surveillance\nsignals. One approach to obtain target range and Doppler\ninformation is by using the cross ambiguity function (CAF),\nN(cid:88)\nwhich can be represented in its discrete form as\nn=0\n\nr[n]s [n +  ]ei2fd\n\n[ , f ] =\n\n(2)\n\nn\nN ,\n\nwhere r [n] is the reference signal, s[n] is the surveillance\nsignal,  is the path delay from access point to the surveillance\nreceiver, and N is the total number of samples in a single\nwindow. For the system given in (2), the total integration\n\nAccess PointSurveillance AntennaReference Antennay (m)x (m)12123453\fFig. 2. Displacement axes during vital health care activities. Dotted lines\nshow inactive axes for a specic activity.\n\nthen the cross correlation result in time domain for each batch\nb, where b  {1, 2, ..., B }, is represented as\nNb(cid:88)\nn=1\n\nr [ib + n   ]s [ib + n],\n\ny [b] =\n\n(6)\n\nwhere Nb is the number of samples in each batch, and ib\nis the starting sample in each batch. In (6), (.) denotes the\nconjugate operator on a complex number. We note that since\npassive radars have a limited range resolution, therefore, \ncan be set to the maximum time delay our system is likely to\nencounter. If the sampling frequency is fs and the time delay\nbetween the reference and surveillance signals is tlag , then the\ndelay is given as\n\n =\n\n.\n\n(7)\n\nfs\ntlag\n\nIn typical small scale motions such as breathing, the time delay\nis in the order of nano seconds (ns). Hence, we can assume  to\nbe zero in context of our applications. In order to get smoother\ntransitions in y [b] (and consequently in [b]), we consider an\nx% overlap between consecutive batches. The exact choice of\nx is governed by systems latency requirements. The phase of\neach batch, [b], can now be found by taking inverse tangent\nof real and imaginary parts of complex series y [b], i.e.,\n[b] = tan1 (cid:60)(y [b])\n(cid:61)(y [b])\n\n(8)\n\n.\n\n[b] =\n\nAlthough [b] encodes small scale body motions,\nit also\ncaptures reections off static objects such as furniture and\nwalls. In order to remove these time invariant phase shifts,\nwe normalize [b] to zero mean and unit variance, i.e.,\n(cid:113) 1\n(cid:80)B\n[b]  b\ni=bW +1 ([i]  b )2\nW\nB(cid:88)\nwhere W denotes the window length, and b is given as\ni=bW +1\n\nb =\n\n1\nW\n\n(10)\n\n[i].\n\n(9)\n\n,\n\nI I I . PA S S IV E RADAR SY ST EM MOD EL IN 2 -D\nFigure (2) shows the displacements involved in the three\nactivities of human breathing, essential tremor and human fall,\nrespectively. Human respiration involves movements along a\nsingle axis which can be quantied by placing a surveillance\nantenna perpendicular to human chest. In contrast, essential\ntremor involves motion in two different planes which requires a\n\nminimum of two antennas placed perpendicular to one another.\nFinally, the motion involved in human fall is also along a single\naxis but perpendicular to the breathing motion.\nIn order to detect phase variations in these applications, Vi-\nWi uses two surveillance antennas surv1 and surv2 with the\nfollowing polar coordinate geometry:\n\nsurv1 :  =\n2\n\n2\n\n,  = 0,\n\n4\n\nsurv2 :  =\n\n(12)\n\n(11)\n\n,  =\n\n.\n\nwhere  denotes the angles surv1 and surv2 make with the x-y\nplane in radians. The choice of  for surv2 presents an interest-\ning problem. Since, the antennas are highly directive, picking\na  close to 90 degrees implies that the target movement has\nto be very close to the antenna to be detected. In contrast, \nof 0 degrees does not pick any movement in the x-z plane\ndue to Doppler shift being 0. As a compromise, we choose a\n of 45 degrees. In such a conguration, correlation signal at\nsurv2 is given by s2 [n] and is formed by the superimposition\n(cid:88)\n(cid:88)\nof Doppler shifts in x-y and x-z planes, and can be represented\nby the following equation, i.e.,\nfdfdxz\nfdfdxy\n\nAx[n +  ]ei2fd n +\n\n(13)\nwhere fdxy and fdxz contain a set of Doppler shifts in x-y and\nx-z planes, respectively. Similarly, correlation signal at surv1\nis given by s1 [n] and is represented by the following equation,\n(cid:88)\ni.e.,\nfdfdxy\n\nAx[n +  ]ei2fd n .\n\ns1 [n] =\n\ns2 [n] =\n\n(14)\n\nAx[n +  ]ei2fd n ,\n\nBecause of the geometry of surveillance antennas, extracted\nphase information from 1 [n] also leaks into 2 [n]. In order for\ns2 [n] to only track movements in x-z plane, Vi-Wi calculates\n(cid:48)\n2 [n] as\n\n2 [n] = 2 [n]  1 [n],\n(cid:48)\n\n(15)\n\n\n\nwhere 1 [n] and 2 [n] are phase signals calculated through\n(cid:48)\nEquation (8) from s1 [n] and s2 [n] respectively, and \n2 [n] is a\nmodied phase signal to account for information leakage from\nx-y plane to x-z plane.\n\nA. Feature Extraction\nAfter Vi-Wi extracts phases from surveillance signals, it pro-\nceeds by extracting features in following order:\n\n\nVi-Wi determines if there is any activity in the envi-\nronment.\nVi-Wi classies the dominant activity (the activity that\nresults in the highest Doppler shift)\nVi-Wi extracts specic activity information.\n\n\nTo illustrate these steps, let us consider the phase signals\n1 [n] and 2 [n] associated with surveillance signals s1 [n]\nand s2 [n], against each activity. Fig. 3(a) shows that when\n\n\n\n\fTremor measurement is similar to breathing rate measurement,\nbut with two main variations. First, the dominant essential\ntremor frequency is in range 4-11 H z as compared to 0.5-2\nH z in breathing. Second, the tremor measurements are taken\nacross both x-y and x-z planes instead of just x-y plane.\n\nAccurately determining the frequency in such resolution range\nis challenging. Moreover, the precise frequency estimate of\nessential tremor does not provide any added value to the health\nprofessionals. Therefore, we instead focus our experiments\non classifying tremor as either low or high. The dominant\nfrequency is measured using the same approach as in last\npassage, and then a classication decision is made based on\nwhether the frequency is above or below a certain threshold.\nWe note that if the frequency is too large or too small, we\ndiscard the activity as random motion.\nFall detection\n\nFall detection is done by locating an instance of time when\nthere is a spike in 2 [n] and 1 [n]\nis at. We note that\nfall detection can be done continuously even when Vi-Wi is\nmonitoring subjects breathing rate or essential tremor.\n\nIV.\n\nIM P LEM EN TAT ION\n\nThe implementation settings are shown in Fig. 4 and explained\nbelow.\n\nA. Hardware\n\nThe passive sensing system used in the experiments utilizes\nUSRP B200 software dened radio with an omni-directional\nantenna as an access point. The access point transmits orthog-\nonal frequency division multiplexed symbols at a data rate\nof 3 Mb/s with a code rate of 1\n2 and quadrature phase shift\nkeying modulation. With this conguration, the transmit power\nof the WiFi source is estimated to be around -60 dBm. At the\nreceiver end, we have log-periodic directional antennas with\n5dBi gain and 60 degree beam-width as shown in Fig. 4 on\nsurveillance antenna 1 and 2. The signals received at these\nantennas are digitized through a Spartan 6 XC6SLX75 FPGA\nand 61.44 MS/s, 12 bit ADC. For non line-of-sight experiments\n(with obstacles), we use a 30 dB RF power amplier with the\ntransmit antenna.\n\nB. Software\n\nWe implement Python blocks in GNU Radio for signal pro-\ncessing operations. The code runs in real-time and the display\nis updated on Python matplotlib library every 2 seconds.\n\nC. Ground Truth\n\nIn our experiments, we used a pulse oximeter to establish\nground truth for breathing rate measurements. However, our\nstudy of essential tremor was limited as we could not nd a\nreliable device to estimate tremor. Therefore, we developed a\nbinary classication metric and determined if the tremor was\nhigh or low. In future studies, a more reliable ground truth\ncould be established by using a reliable tremor monitoring\ndevice.\n\nFig. 3. Tracking 1 and 2 during breathing, tremor and human fall.\n\na person inhales, her chest moves towards the device resulting\nin a positive Doppler shift; when she inhales, her chest moves\naway from the device causing a negative Doppler. Fig. 3(c) and\n3(d) show variations in surveillance signals when the dominant\nactivity is essential tremor. In such a scenario, both 1 and 2\nalternate between positive and negative Doppler shifts. Finally,\na human fall causes an aperiodic variation in 2 as shown in\nFig. 3(f).\nHowever, there may be instances when a user moves her\nlimbs or makes some other dominant motion. To deal with\nsuch scenarios, Vi-Wi operates on a window of 20 seconds\nand determines if 1 and 2 are periodic. If both are aperiodic,\nwe detect a random movement such as limb motion. However,\nif only 2 is aperiodic, this indicates a sudden movement in\nx-z plane and we detect a fall. If 1 is periodic and 2 is\naperiodic, we recognize this as breathing motion in x-y plane\nonly. If both are periodic, we detect tremor activity.\nIn the following section, we show how Vi-Wi determines the\nspecics of an activity once it has been classied.\nBreathing Rate Measurement\nFundamentally, the breathing rate can be extracted by tak-\ning fast Fourier transform (FFT) of the phase signal and\nsubsequently picking the FFT peak. However, this does not\nprovide an accurate estimate since the frequency resolution\nis quite small. Specically, for a window size of 10 seconds,\nthe frequency resolution is only 0.1H z or approximately 6\nbreaths per second. In order to improve this resolution, we\nutilize a well known property that the dominant frequency of\na signal can be accurately measured by doing regression on\nthe phase of the complex time-domain signal [19]. Since, the\nphase of complex signal 1 [n] is linear, its slope corresponds\nto breathing rate estimate in H ertz .\nTremor Classication\n\n\fFig. 4. Experiment Setup. The breathing rate of the subject is tracked and\nthe activity is displayed on a personal computer.\n\nFig. 6. Accuracy comparison of health activities. Breathing rate accuracy\nis determined by comparing actual breaths per minute to monitored breaths.\nTremor classication accuracy is determined the ability to classify slow (4-\n7Hz) and fast (7-11Hz) tremor. In our experiments, fall detection has the\nhighest accuracy, while breathing has the least. Breathing measurement also\nundergoes the highest drop in accuracy from LOS to NLOS environment.\n\nFig. 5.\nExperiment settings for LOS and NLOS environments. There is a\nsingle access point, a reference antenna and two surveillance antennas. In\nNLOS setting, an additional metal block is placed to occlude the view of\ntarget.\n\nV. EX PER IM EN TA L EVALUAT ION\n\nTo evaluate the performance of Vi-Wi, we performed exper-\niments in a standard ofce environment with and without\nobstacles. A simple metal block was used as an obstacle, and it\nblocked all line of sight signals towards the receive antennas.\n\nFig. 6 shows Vi-Wis accuracy for different elderly-care ac-\ntivities before and after an obstacle is placed in the line of\nsight of receiver. We observe that Vi-Wi is able to detect falls\nwith the highest accuracy followed by tremor classication and\nbreathing rate measurement. In the presence of an obstacle, the\naccuracy of all three activities considerably drops but we note\nthat it can be improved by increasing the APs transmit power\nor using antennas with higher gains. Here, we note several\nfactors that may account for some of the inaccuracies in these\nsmall scale motion measurements. First, the wavelength of the\nWiFi signal is in order of centimeters and thus, slight geometry\n\nFig. 7. Change in breathing rate accuracy with distance. Transmit power is\nset to 20 dBm. At a distance of 1 meter, the breathing rate accuracy is the\nhighest at 87%, and drops to 84% when the target is 5 meters away.\n\nchanges of human body may cause additional phase variations.\nSecond, at each instance of time, a slightly different body part\nmay be detected causing deviation in phase variations. Finally,\nthese small scale motions are unlikely to be regular and a small\nchange in orientation may cause a large phase deviation.\n\nHaving xed the transmit power at 20 dBm, we validated\nthe performance of Vi-Wi as the distance of the target to device\nchanged as shown in Fig. 7. At all the distances, the accuracy\nremained above 84% while the highest accuracy (87%) was\nachieved at the nearest distance of 1 meter. We then asked\nthe participants to change their orientations and sit at slight\nangles facing the receive antennas. In particular, we asked the\nsubjects to sit at three angles facing the antenna: 300 , 600\nand 900 . At each distance, changing the sitting orientation\ndid not have any signicant effect on breathing rate accuracy.\n\nReference AntennaSurveillance Antenna-1Surveillance Antenna-2Access PointSubjectx (m)y (m)0.510.511.521.5Access PointxyplaneReference Antenna0.7 m1 m0.4 m0.5 mxzplaneSurveillance Antennas0.3 mLOS Obstaclex (m)y (m)0.511.5Access PointxyplaneReference Antenna0.7 m1 m0.4 m0.5 mxzplaneSurveillance Antennas0.3 mExperiment setting with obstacleExperiment setting without obstacle\fon various aspects of Vi-Wi. First, monitoring vital health\nactivities in presence of motion and background artefacts is\nan interesting challenge. Second, additional health activities\nsuch as human gait can be investigated. Third, research can\nfocus on using multiple access points for analyzing activities\nof more than one target.\nBeyond health, passive radar sensing offers a number of\npromising future directions. By utilizing the access points\nalready available in buildings and intelligently assigning them\nto the targets of interest, one can achieve almost ubiquitous\nsensing.\n\n[1]\n\ncare.\n\nR E FER ENC E S\nInformationWeek. Health-monitoring devices market outpaces telehealth.\nhttp://www.informationweek.com/mobile/health-monitoring-devices-\nmarket-outpaces-telehealth/d/d-id/1104636.\n[2] M. M. Baig, H. J. Gholamhosseini, Smart health monitoring systems:\nan overview of design and modeling, Journal of Medical Systems, 2013.\n[3] MIT Technology Review. How cell phones are transforming health care\nin Africa. https://www.technologyreview.com/s/519041/how-cell-phones-\nare-transforming-health-care-in-africa/\n[4] Healthcare Drive. How technologies can help the elderly age at\nhome. http://www.healthcaredive.com/news/how-technologies-can-help-\nthe-elderly-age-at-home/436386/\n[5] Xinhua.\nelderly\nand\nhealth\nChina\nplans\nsmart\nhttp://news.xinhuanet.com/english/2017-02/17/c 136062355.htm\n[6] Vivonoetics.\ndual-band.\nor\nsingle\nRespiration:\nhttp://vivonoetics.com/products/vivosense/ analyzing/respiration-single-\nor-dual-band/.\n[7] D. V. Forrest, The tremometer: a convenient device to measure postural\ntremor from lithium and other causes, The Journal of Neuropsychiatry\n& Clinical Neurosciences, 1990.\n[8] F. Adib, Z. Kabelac, D. Katabi, R. C. Miller, 3D tracking via body radio\nreections, Usenix NSDI, 2014.\n[9] M. Kotaru, et. al, SpotFi: decimeter level localization using WiFi,\nSIGCOMM, 2015.\n[10] Y. Wang, et. al. E-eyes: device-free location-oriented activity identi-\ncation using ne-grained WiFi signatures, MobiCom, 2014.\n[11] GigaOm. Could a breath-monitoring headset\nimprove your health?\nhttp://gigaom.com/2013/09/20/could-a-breath-monitoring-headset-\nimprove-your-health/.\n[12] C. Guard, Wireless wrist band pulse oximeter CMS-50FW with soft-\nware for sleep study, http://www.clinicalguard.com/wireless-wristband-\npulse-oximeter-cms50fw-with-software-for-sleep-study-p-455.html.\n[13] K. Ali, A. X. Liu, W. Wang, M. Shahzad, Keystroke recognition using\nWiFi signals, MobiCom, 2015.\n[14] Y. Wang, et. al, RF-IDraw: virtual touch screen in the air using RF\nsignals, SIGCOMM, 2014.\nJ. Qiuchi, Y. Jian, Y. Yinan, P. Bjorkholm, T. McKelvey, Detection of\nbreathing and heartbeat by using a simple UWB radar system, Antennas\nand Propagation (EuCAP), 2014.\n[16] D. Dei, et. al, Non-contact detection of breathing using a microwave\nsensor, Sensors, 2009.\n[17] T. Bo, et. al, Wi-Fi based passive human motion sensing for in-home\nhealthcare applications, Internet of Things (WF-IoT), 2015.\n[18] G. Grimaldi, M. Manto, Neurological tremor: sensors, signal process-\ning and emerging applications, Sensors, 2010.\n[19] A. Oppenheim, R. Schafer, J. Buck, et. al, Discrete-time signal\nprocessing, Prentice Hall, 1989.\n\n[15]\n\nFig. 8. Change in breathing rate accuracy with transmit power. Distance of\nthe target is set to 1 meter. The accuracy improves by around 11% when the\ntransmit power is changed from -30 dBm (78.5%) to 20 dBm (87%).\n\nThis is because during breathing, the chest expands in all\ndirections and even though the subject is facing sideways, his\nchest movements can be detected. However, we noticed that\nchanging the subject orientation caused tremor classication\naccuracy to drop signicantly. This is because unlike breathing,\ntremor motion is concentrated along xed directions and as the\norientation of hand changes, Doppler shift can no longer be\nmonitored.\nOur nal experiment was to analyze the effect of transmit\npower on system accuracy as shown in Fig. 8. Our custom\ndesigned AP offered a maximum power of -10 dBm. By\napplying an external 30 dB RF power amplier, Vi-Wis\nbreathing rate measurement accuracy improved by 7.4%. There\nwere similar improvements in the accuracies of tremor and fall\nactivities with increase in transmit power.\n\nA. Limitations\nIn this section, we discuss some of the limitations of Vi-Wi:\n\n\nVi-Wi assumes that there are no motion or background\nartefacts in the environment. While Vi-Wi can work\nreasonably well\nin presence of background noise,\nits performance is severely affected when there are\nmultiple movements.\nVi-Wi may be prone to detecting non-human motion.\nFor example,\nit may mistake fall of stick in the\nenvironment as a human fall.\nSince, Vi-Wi requires a minimum signal-to-noise ratio\n(SNR), it works in a limited range of 5 meters. Beyond\nthis distance, the accuracy of Vi-Wi drops signcantly.\n\n\n\n\n\nV I . CONC LU S ION AND FU TUR E D IR EC T ION S\nThe health applications using passive radar sensing have been\nlargely untapped. The traditional methods to monitor breathing\nrate, tremor and falls are invasive and inconvenient. The mod-\nern methods using active radars require dedicated transmitters\nand high bandwidth. In contrast, Vi-Wi offers covert, license-\nfree and convenient health monitoring. Future works can focus\n\n\f", 
        "tag": "Other Computer Science", 
        "link": "https://arxiv.org/list/cs.OH/new"
    }, 
    {
        "text": "On the equivalence between multiclass PS(cid:173)type\nscheduling policies\n\nKonstantin Avrachenkov\nTejas Bodas\nINRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nF\nP\n.\ns\nc\n[\n \n \n1\nv\n2\n2\n7\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nABSTRACT\n\nConsider a single server queue serving a multiclass popula-\ntion. Some popular scheduling policies for such a system\n(and of interest in this paper) are the discriminatory pro-\ncessor sharing (DPS), discriminatory random order service\n(DROS), generalized processor sharing (GPS) and weighted\nfair queueing (WFQ). The aim of this paper is to show a\ncertain equivalence between these scheduling policies for the\nspecial case when the multiclass population have identical\nand exponential service requirements. In fact, we show the\nequivalence between two broader classes of policies that gen-\neralize the above mentioned four policies. We specically\nshow that the so journ time distribution for a customer of a\nparticular class in a system with the DPS (GPS) scheduling\npolicy is a constant multiple of the waiting time distribution\nof a customer of the same class in a system with the DROS\n(respectively WFQ) policy.\n\n1.\n\nINTRODUCTION\n\nConsider a single server queue with multiclass customers\nwhere the customers have independent and identical service\nrequirements independent of their class. Suppose that there\nare N customer classes and assume that the service require-\nment of a customer is exponentially distributed with rate\n. Let i denote the arrival rate for a Class i customer,\ni = 1 . . . N . Let PN\n and  = PN\ni=1 i = , i = i\ni=1 i .\nFurther, let pi denote a weight parameter associated with a\nClass i customer. Additionally, for the purpose of stability,\nassume throughout that  < . Some examples of schedul-\ning policies used in such multiclass queues are DPS, DROS,\nGPS and WFQ. In this paper, we will show that the so journ\ntime distribution of any Class i customer with DPS (GPS)\nscheduling policy is equal to the waiting time distribution\nof a Class i customer in a system with DROS (respectively\nWFQ) policy under the assumption of i.i.d exponential ser-\nvice requirements. A quick overview of these four scheduling\npolicies is given below.\nScheduling policies such as GPS and DPS are variants\nof the processor sharing policy and can serve multiple cus-\ntomers from the system simultaneously. In case of GPS, a\nseparate queue is maintained for each customer class and\nthe total service capacity of the server is shared among cus-\ntomers of the dierent classes in proportion to the weights pi .\nThe GPS scheduling policy is often considered as a general-\n\nCopyright is held by author/owner(s).\n\nization of the head-of-line processor sharing policy (HOLPS)\nas described in [6, 14, 15]. (Refer [16, 17] for details about\nHOLPS). As a generalization of HOLPS, GPS maintains a\nFIFO scheduling policy within each queue for a class and\nonly the head-of-line customers of dierent classes are al-\nlowed to share the processor. The share of the server for a\nhead-of-line Class i customer is proportional to the weight pi\nand is independent of the number of other customers in the\nqueue. The service rate received by the customer is precisely\npi\ngiven by\nwhere j = 1 if the queue has at least\nPN\nj=1 pj j\none class j customer and j = 0 otherwise. Refer Parekh\nand Gallager [11], Zhang et al. [12] for an early analysis of\nthe model.\nIn case of DPS, the total service capacity is shared among\nall the customers present in the system and not just among\nthe head-of-line customers of dierent classes. The share of\nthe server for a customer of a class is not only in proportion\nto the class weight, but also depends on the number of multi-\nclass customers present in the queue. In particular, a Class i\npi\ncustomer in the system is served at a rate of\nwhere\nPN\nj=1 pj nj\nnj denotes the number of Class j customers in the system.\nThe DPS system was rst introduced by Kleinrock [5] and\nsubsequently analyzed by several authors [13, 8, 7, 9].\nThe DROS and WFQ scheduling policies are also char-\nacterized by an associated weight for each customer class.\nHowever these policies are not a variant of the processor\nsharing policies and hence their respective server can only\nserve one customer at a time. The DROS and WFQ policies\ndier in their exact rule for choosing the next customer. In\nthe DROS policy, the probability of choosing a customer for\nservice depends on the weights and the number of customers\nof the dierent classes in the queue. A Class i customer is\npi\nwhere nj denotes\nthus chosen with a probability of\nPN\nj=1 pj nj\nthe number of Class j customers in the system. DROS policy\nis also know as relative priority policy and was rst intro-\nduced by Haviv and wan der Wal [7]. For more analysis of\nthis policy we refer to [3, 10]. In the WFQ policy, a separate\nqueue for each class is maintained and the next customer is\nchosen randomly from among the head-of-line customers of\ndierent classes. As in case of the GPS scheduling policy,\na FIFO scheduling policy is used within each queue for a\nclass. WFQ can be seen as a packetised version of GPS and\nthe probability of choosing a head-of-line Class i customer\npi\nfor service is given by\nwhere j is as dened ear-\nPN\nj=1 pj j\nlier. Refer Demers [14] for the detailed analysis of the WFQ\npolicy.\nIt is interesting to note that for a Class i customer, the ser-\n\n\fvice rate received in DPS and the probability of being cho-\npi\nsen next for service in case of DROS is given by\n.\nPN\nj=1 pj nj\nSimilarly, the service rate received in GPS and the prob-\nability of being chosen next for service in case of WFQ is\npi\n. This similarity in the scheduling rules motivates\nPN\nj=1 pj j\nus to compare the mean waiting times and so journ times\nof the multiclass customers with these scheduling policies.\nHaving assumed identical service requirements, we speci-\ncally show that the waiting time distribution of a Class i\ncustomer in a system with DROS (WFQ) scheduling pol-\nicy is  times the so journ time distribution of any Class i\ncustomer with DPS (resp. GPS) scheduling policy. This is\na generalization of [2], where the equivalence has been es-\ntablished between single-class processor sharing and random\norder service discipline. The coupling technique which we\nuse also builds upon the technique used in [2].\nThe rest of the paper is organized as follows. In the next\nsection, we introduce a generalized notion of multiclass pro-\ncessor sharing (mPS) and random order service (mROS)\npolicies. The DPS, GPS, DROS and WFQ policies will turn\nout to be special cases of mPS and mROS. In Section 3, we\nshow that the mean so journ time of a Class i customer with\nmPS scheduling is equivalent to the mean waiting time of\na Class i customer with mROS policy. As a special case,\nthis proves the mentioned equivalences among the four mul-\nticlass scheduling policies.\n\n2. GENERALIZED MULTICLASS SCHEDUL(cid:173)\nING POLICIES\n\nIn this section, we will describe two multiclass scheduling\npolicies that are a generalization of policies such as DPS,\nDROS, GPS and WFQ. The two policies are based on the\nprocessor sharing and random order service mechanism and\nwill be labeled as mPS and mROS respectively.\nThe mPS scheduling policy is a multiclass processor shar-\ning policy and can serve multiple customers simultaneously.\nA separate queue for each customer class is maintained and\na FIFO sequencing policy is used within each queue of a\nclass. The mPS scheduling policy is parameterized by a vec-\ntor  of length N that characterizes the maximum number\nof customers of each class that can be served simultaneously\nwith other customers. We shall henceforth use the nota-\ntion mPS( ) where  = [1 , . . . , N ]. Here  denotes the\nset of customers that can be served simultaneously. Recall\nthe denition that ni denotes the instantaneous number of\nClass i customers in the queue, where i = 1, . . . , N . Let\ni ( n) denote the number of Class i customers under service\nwhen the conguration of total multiclass customers is n.\nThen, clearly i ( n) = min (ni , i ). To lighten notation, we\nshall drop the dependence on n and use only i when the\ncontext is clear.\nIn other words, if ni  i , then all the\nClass i customers in the queue are being served simultane-\nously for i = 1, . . . , N . However if ni > i , then only the\nrst i customers of Class i in its queue are served simul-\ntaneously. Recall that due to the FIFO sequencing policy\nwithin each class, only the rst i customers in the queue\nare always served. For an mPS( ) scheduling policy with\na conguration of n = [n1 , . . . , nN ] multiclass customers in\nthe system, the service rate received by a particular Class i\npi\ncustomer in service is given by\n. When i = ,\nPN\nj=1 pj j\nfor i = 1 to N , the corresponding scheduling policy will be\n\ndenoted by mPS( ). In this case, i = min (ni , ) = ni\nand therefore mPS( ) corresponds to the DPS scheduling\npolicy. Similarly if e = [1, . . . , 1], then mPS(e) corresponds\nto the GPS scheduling policy where only the head-of-line\ncustomers of each class can be served.\nIn a similar manner, we can dene the mROS( ) schedul-\ning policy where  = [1 , . . . , N ] and  denotes the set\nof customers from which the subsequent customer is cho-\nsen for service. As in case of the mPS policy, note that a\nseparate FIFO queue for each customer class is also main-\ntained for the mROS system. At any given time, the rst\ni = min (ni , i ) customers are candidates for being cho-\nsen for service while the remaining ni  i customers have\nto wait for their turn. For an mROS( ) scheduling policy\nwith a conguration of n = [n1 , . . . , nN ] waiting customers,\na Class i customer within the rst i customers in its queue\npi\nwill be chosen next for service with probability\n.\nPN\nj=1 pj j\nAs in case of the mPS scheduling, mROS( ) corresponds\nto the DROS policy whereas mROS(e) corresponds to the\nWFQ policy.\n\nRemark 1. A policy closely related to the mPS discipline\nis the limited processor sharing (LPS) policy. LPS is a single\nclass processor sharing policy parametrized by an integer c\nwhere c denotes the maximum number of customers that can\nbe served simultaneously. Here c =  corresponds to the\nprocessor sharing policy while c = 1 corresponds to FCFS\npolicy. LPS can also be viewed as a special case of the mPS\npolicy when there is a single service class for the arriving\ncustomers. See [1, 18] more more details about the LPS-c\npolicy.\n\nHaving introduced the generalized multiclass scheduling\npolicies, we shall now establish a relation between the so-\njourn time of a Class i customer in mPS system with the\nwaiting time of a Class i customer in mROS system.\n\n3. COMPARING THE SOJOURN AND WAIT(cid:173)\nING TIME DISTRIBUTIONS IN MPS AND\nMROS\n\nThe analysis in this section is inspired from that in [2]\nwhere a similar result is established for the case of a sin-\ngle class of population. Let n now denote a vector corre-\nsponding to the number of customers of each class present\nin the queueing system at an arrival instant. We have n =\n(n1 , . . . , nN ) where ni denotes the number of Class i cus-\ntomers at the arrival instant. Suppose PN\ni=1 ni = n. Let ran-\ndom variable Si ( , n) denote the conditional so journ time\nexperienced by an arriving Class i customer in an mP S ( )\nsystem when it sees a conguration of n customer on ar-\nrival. The corresponding unconditional random variable will\nbe denoted by Si ( ). We shall occasionally use the notation\nmP S ( , n) to denote the mPS( ) system with n customers.\nAlong similar lines, let the random variable Wi ( , n) de-\nnote waiting time (time until chosen for service) experi-\nenced by an arriving Class i customer in the mROS sys-\ntem conditioned on the fact that it sees a conguration n\nof waiting customers. This system will be often denoted\nas mROS ( , n) and the unconditional random variable will\nbe denoted by Wi ( ). Let P and P denote the probability\ndistribution of the random variables Si ( , n) and Wi ( , n)\nrespectively. (The dependence of these distributions on n\n\n\fhave been supressed for notational convenience.) We now\nstate the main result of this paper.\n\nTheorem 1. P (S i ( ) > t) = P (W i ( ) > t) for i =\n1, . . . , N .\n\n(1)\n\nProof. As in [2], our aim is to rst provide a coupling\n(cid:16) Si ( , n), Wi ( , n)(cid:17) with the corresponding law denoted by\nP such that\n Si ( , n) D= S i ( , n) and Wi ( , n) D= W i ( , n)\n P (cid:16) Si ( , n) = Wi ( , n)(cid:17) = 1\nThe second requirement will help us show that the two\ndistributions P and P are equal. This follows from the cou-\npling inequality\nP  P(cid:13)(cid:13)  2P (cid:16) Si ( , n) 6= Wi ( , n)(cid:17) .\n(cid:13)(cid:13)\nSuch a coupling is precisely obtained as follows.\nConsider two tagged Class i customers X and Y that ar-\nrive to a mP S ( , n) and a mROS ( , n) system respectively.\nThis means that at the arrival instant of customer X in\nthe mP S ( , n) system, there are ni Class i customers al-\nready present in the system. Similarly, at the arrival instant\nof customer Y in mROS ( , n), there are ni customers of\nClass i that are waiting for service in the queue. Recall that\n = [1 , . . . , N ] where i in the mPS system denotes the\nnumber of Class i customers that are receiving service. In\nthe mROS system, i denotes those (waiting) Class i cus-\ntomers from which the next customer could be chosen. Note\nthat since PN\ni=1 ni = n, with the arrival of customer X, the\nmP S ( , n) system has n + 1 customers. Similarly, with the\narrival of customer Y , the mROS ( , n) system has n+ 2 cus-\ntomers of which one customer is in service and the remaining\nn + 1 customers (including customer Y) are waiting for ser-\nvice. We will now specify the rule for forming the required\ncoupling. Since the customers can be distinguished by their\nclass index and also the position in their respective queues,\nwe couple the n + 1 customers in mP S ( , n) with the n + 1\nwaiting customers in the mROS ( , n) system based on their\nclass and queue position. The coupling must be such that\nthe coupled customers belong to the same class and invari-\nably have the same queue position in their respective queues.\nIt goes without saying that the tagged customers X and Y\nare also coupled. As in [2], we also couple the subsequent\narriving customers and let D1 , D2 . . . denote i.i.d random\nvariables with an exponential distribution of rate . These\nrandom variables correspond to service times of a customer\nin service in mROS ( ). At the service completion epoch,\npick a pair of coupled customers randomly from the set of\n customers. The random picking is with a distribution\nsuch that a Class i pair from the  customers is chosen with\npi\nprobability\n. If the chosen pair belongs to Class k,\nPN\nj=1 pj j\nthen a class k customer departs from the mPS system while\nsuch a customer is taken for service in mROS. This process\nis repeated till the tagged pair (X, Y ) leaves the system.\nClearly, this joint probability space is so constructed that\nthe random variables Si ( , n) = Wi ( , n) Pa.s. From Eq.\n(1), this implies that\n\nS i ( , n) D= W i ( , n).\n\n(2)\n\nNow let random variables N mP S (resp. N mROS\n) denote\n1\nthe conguration of the total customers (resp. waiting cus-\ntomers in case of mROS system) as seen by a Class i arrival.\nThe subscript 1 in N mROS\nis used to indicate a busy server.\n1\nWe have the unconditional probabilities given by the follow-\ning.\nP (S i ( ) > t) = Xn\n\nP (N mP S = n)P (S i ( , n) > t).\n\n(3)\n\nP (N mROS\n1\n\n= n)P (W i ( , n) > t). (4)\n\nSimilarly we have\nP (W i ( ) > t) = Xn\nNow if suppose P (N mROS\n= n) = P (N mP S = n) is\n1\ntrue, then from Eq. (2), the statement of the theorem follows\nand this would complete the proof. In the following lemma,\n= n) = P (N mP S =\nwe shall prove that indeed P (N mROS\n1\nn).\n\nLemma 1. P (N mROS\n1\nsuch that |n|  0.\n\n= n) = P (N mP S = n) for n\n\nProof. We rst simplify the notations as follows. Let\n ( n) := P (N mP S = n) and  (1, n) := P (N mROS\n= n).\n1\nLet  (0, 0) denote the probability that the mROS system\nhas no customers and is idle. The statement of the lemma\nnow requires us to prove that (1, n) =  ( n). To prove this\nresult, consider the balance equation for the mPS system\nwhere  shall denote the stationary invariant distribution\nfor the system. The assumption  <  implies that the\nunderlying Markov process is ergodic and hence the station-\nary distribution  is unique. For n such that |n|  0, the\ndetailed balance equations for the mPS( ) system are\nXi=1   i ( n)pi\nj=1 pj j ( n) ! 1{| n|>0} ) ( n)\nM\nPM\nM\nXi=1\ni 1{ni>0} ( n  ei )\nXi=1   i ( n + ei )pi\nj=1 pj j ( n + ei ) !  ( n + ei ).\nM\nPM\nNow since\nXi=1   i ( n)pi\nj=1 pj j ( n) ! = 1,\nM\nPM\nthe balance equations can be written as\n\n( +\n\n+\n\n=\n\n( + 1{| n|>0} ) ( n) =\n\nM\nXi=1\ni1{ni >0} ( n  ei )\nj=1 pj j ( n + ei ) !  ( n + ei ).\nXi=1   i ( n + ei )pi\nM\nPM\nSimilarly, the detailed balance equations for the mROS( )\nsystem are as follows for n such that |n|  0.\n\n(5)\n\n+\n\n\f+\n\n(6)\n\nM\nXi=1\n( + 1{| n|>0} ) (1, n) =\ni1{ni >0}  (1, n  ei )\nXi=1   i ( n + ei )pi\n.!  (1, n + ei )\nM\nPM\nj=1 pj j ( n + ei )\nAdditionally, the idle system should satisfy\n (0, 0) =  (1, 0).\n(7)\nwhere  (0, 0) = 1   is the probability that the system is\nempty.\nNow again, the assumption  <  implies that the un-\nderlying Markov process is ergodic and hence the station-\nary distribution  is also unique. Therefore to prove the\nlemma, it is sucient to check if the detailed balance equa-\ntions for the mROS system given by Eq. (6) are satised\nwhen  (1, n) =  ( n).\nNow from Eq. (6) and assuming that (1, n) =  ( n), we\nhave\n\n\n\n\n\nM\nXi=1\n( + 1{| n|>0} )  (1, n) \ni1{ni >0}  (1, n  ei )\nXi=1   i ( n + ei )pi\nj=1 pj j ( n + ei ) !  (1, n + ei )\nM\nPM\nM\nXi=1\ni1{ni >0}  ( n  ei )\n= ( + 1{| n|>0} ) ( n) \nj=1 pj j ( n + ei ) !  ( n + ei ) = 0.\nXi=1   i ( n + ei )pi\nM\nPM\nThe last equality follows from Eq. (5) after dividing through-\nout by . Similarly,\n (0, 0)   (1, 0) =  (0, 0)   (0)\n=  (  (0, 0)   (0))\n=  (  (0, 0)  (1  ))\n= 0.\n(8)\nHere the third equality is from the fact that  (0) = (1  ) is\nthe probability that the mPS( ) system is empty. Clearly,\nsubstituting  (1, n) =  ( n), satises the balance equations\nfor the mROS system. Since  is the unique invariant dis-\ntribution, the statement of the lemma follows.\n\nWe now have the following corollary that establishes the\ndesired equivalence between DPS (GPS) and DROS (resp.\nWFQ) scheduling policies. Note that the result is true only\nfor the case when all customers have identically distributed\nservice requirements. The equivalence result need not be\ntrue in general when the customer classes dier in their ser-\nvice requirements.\n\nCorollary 1.\n\n P (S i ( ) > t) = P (W i ( ) > t)\nwhere S i ( ) denotes the sojourn time of a Class i cus-\ntomer in DPS system and W i ( ) denotes the waiting\ntime of a Class i customer in DROS system.\n\n P (S i (e) > t) = P (W i (e) > t)\nwhere S i (e) denotes the sojourn time of a Class i cus-\ntomer in GPS system and W i (e) denotes the waiting\ntime of a Class i customer in WFQ system.\n\n4. ACKNOWLEDGEMENTS\n\nThe authors would like to thank Dr. Rudesindo Queija\nfrom CWI, Amsterdam for the several discussions on this\nwork.\n\n5. REFERENCES\n\n[1] B. Avi-Itzhak and S. Haln, Expected response times\nin a non-symmetric time sharing queue with a limited\nnumber of service positions,Proceedings of ITC 12,\n1988.\n[2] S.C. Borst, O.J. Boxma, J.A. Morrison and R.\nNunez-Queija, The equivalence between processor\nsharing and service in random order Operations\nResearch Letters, vol. 31, no. 4, pp. 254262, July 2003.\n[3] M. Haviv and J. van der Wal, Waiting times in queues\nwith relative priorities, Operations Research Letters,\nvol. 35, no. 5, pp. 591594, September 2007.\n[4] M. Haviv and J. van der Wal, Mean so journ times for\nphase-type discriminatory processor sharing systems,\nEuropean Journal of Operational Research, vol. 189, no.\n2, pp. 375386, September 2008.\n[5] L. Kleinrock, Time-shared systems: a theoretical\ntreatment. Journal of ACM, 14(2):242261, April 1967.\n[6] S. Aalto, U. Ayesta, S. Borst, V. Misra and R.\nNunez-Queija, Beyond Processor Sharing, ACM\nSigmetrics Performance Evaluation Review, vol. 34, pp.\n3643, 2007,\n[7] M. Haviv and J. van der Wal, Equilibrium strategies\nfor processor sharing and random queues with relative\npriorities. Probability in the Engineering and\nInformational Sciences, null(4):403412, October 1997.\n[8] M. Haviv and J. van der Wal, Waiting times in queues\nwith relative priorities. Operation Research Letters,\n35:591594, 2007.\n[9] J. Kim, and B. Kim,  So journ time distribution in\nM/M/1 queue with discriminatory processor sharing,\nPerformance Evaluation, vol. 58, pp. 341365, July\n2004,\n[10] J. Kim, J. Kim and B. Kim,  Analysis of M/G/1\nqueue with discriminatory random order service policy,\nPerformance Evaluation, vol. 68, pp. 256270, 2011,\n[11] A. K. Parekh and R. G. Gallager, A generalized\nprocessor sharing approach to ow control in integrated\nservices networks: The single-node case. IEEE/ACM\nTransactions on Networking, 1(3):344357, 1993.\n[12] Z. Zhang, D. Towsley and J. Kurose, Statistical\nanalysis of generalized processor sharing scheduling\ndiscipline. In Proceedings of SIGCOMM, pages 6877,\n1994.\n[13] G. Fayolle, I. Mitrani and R. Iasnogorodski, Sharing\na processor among many job classes, Journal of ACM,\nvol. 27, no. 3, pp. 519532, July 1980.\n[14] A. Demers, Analysis and Simulation of a Fair\nQueueing Algorithm, Internetworking: Research and\nExperience, vol. 1, pp. 326, 1990.\n\n\f[15] A. Kumar, D. Manjunath and J. Kuri, \nCommunication Networking: An Analytical Approach,\nElsevier, 2004.\n[16] G. Fayolle and R. Iasnogorodski, Two Coupled\nProcessors: The reduction to a Riemann-Hilbert\nProblem, Wahrscheinlichkeitstheorie verw Gebiete,\nvol. 47, pp. 325351, 1979.\n[17] J. Morrison, Head of the line processor sharing for\nmany symmetric queues with nite capacity, Queueing\nSystems, vol. 14, pp. 215237, 1993.\n[18] J. Zhang et al, Diusion limits of limited processor\nsharing queues,The Annals of Applied Probability, vol.\n21, pp. 745799, 2011.\n\n\f", 
        "tag": "Performance", 
        "link": "https://arxiv.org/list/cs.PF/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nL\nP\n.\ns\nc\n[\n \n \n1\nv\n4\n1\n8\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nBilateral Proofs of Safety and Progress Properties\nof Concurrent Programs\n\nJayadev Misra\nUniversity of Texas at Austin, misra@utexas.edu\n\nApril 5, 2017\n\nAbstract\n\nThis paper suggests a theomisra@utexas.edury of composable speci-\ncation of concurrent programs that permits: (1) verication of program\ncode for a given specication, and (2) composition of the specications of\nthe components to yield the specication of a program. The specication\nconsists of both terminal properties that hold at the end of a program ex-\necution (if the execution terminates) and perpetual properties that hold\nthroughout an execution. We devise (1) proof techniques for verication,\nand (2) composition rules to derive the specication of a program from\nthose of its components. We employ terminal properties of components\nto derive perpetual properties of a program and conversely. Hence, this\nproof strategy is called bilateral. The compositional aspect of the theory\nis important in assembling a program out of components some of whose\nsource code may not be available, as is increasingly the case with cross-\nvendor program integration.\n\nKeywords: Program specication, concurrent programming, verication,\ncomposition, safety and progress properties.\n\n1 Introduction\n\nFour decades of intensive research has failed to yield a scalable solution to the\nproblem of concurrent program design and verication. While there have been\nvast improvements in our understanding, the theory and practice in this area\nlag considerably behind what has been achieved for sequential programs. Very\nsmall programs, say for synchronization, are proved manually, though the proof\nmethods are mostly unscalable. Larger programs of practical signicance, say\ncache coherence protocols, are typically proved using model checking, which\nimposes size limitations. Programs from dierent vendors are rarely assembled\nto run concurrently.\nWe believe that the problem stems from the lack of a theory of composable\nspecication for concurrent programs. Sequential imperative programs enjoy\n\n1\n\n\fsuch a theory, introduced by Hoare [7], in which a program is specied by a pair\nof predicates, called its pre- and postcondition. The theory successfully permits:\n(1) verication of program code for a given specication, and (2) composition of\nthe specications of the components to yield the specication of a program. A\nfundamental concept is invariant that holds at specic program points, though\ninvariant is not typically part of the program specication. Termination of a\nprogram is proved separately.\nA specication of a concurrent program typically includes not just the pre-\nand postconditions but properties that hold of an entire execution, similar to\ninvariants. A typical specication of a thread that requests a resource, for ex-\nample, may state that: (1) once it requests a resource the thread waits until the\nresource is granted, and (2) once the resource is granted the thread will even-\ntually release it. The rst property is an instance of a safety property and the\nsecond of a progress property, see Lamport [10] and Owicki and Lamport [14].\nCall the postcondition of a program for a given precondition to be a termi-\nnal property. And a property that holds throughout an execution a perpetual\nproperty. Terminal properties compose only for sequential programs, though\nnot for concurrent programs, and they constitute the essence of the assertional\nproof method of Hoare. Safety and progress are typical perpetual properties.\nThis paper suggests a theory of composable specication of concurrent pro-\ngrams with similar goals as for sequential programs. The specication consists of\nboth terminal and perpetual properties. We devise (1) proof techniques to ver-\nify that program code meets a given specication, and (2) composition rules to\nderive the specication of a program from those of its components. We employ\nterminal properties of components to derive perpetual properties of a program\nand conversely. Hence, this proof strategy is called bilateral. The compositional\naspect of the theory is important in assembling a program out of components\nsome of whose source code may not be available, as is increasingly the case with\ncross-vendor program integration.\nThe Hoare-proof theory for sequential programs is known to be sound and\nrelatively-complete. A sound and relatively-complete proof theory for concur-\nrent programs that use a very limited set of program constructs, known as\nUnity, appears in Chandy and Misra [2] and Misra [11]. This paper combines\nboth approaches to yield a theory applicable to concurrent programs written\nin conventional languages. (The soundness and relatively-completeness of this\ntheory has not been proved yet.)\nWe treat three examples of varying complexity in detail in this paper. First,\na program that implements a distributed counter is used to illustrate the various\nconcepts of the theory in stages through out the paper. Appendix B includes a\nproof of a mutual exclusion program, a system of tightly-coupled components.\nUnlike traditional proofs, it is based on composing the specications of the\ncomponents. Appendix C includes proof of a recursively-dened concurrent\nprogram, where code of one component is only implicit, again using composable\nspecications. We know of no other proof technique that can be used to prove\nthis example program.\n\n2\n\n\f2 Program and Execution Model\n\n2.1 Program Structure\n\nThe syntax for programs and its components is given below.\n\n::= guard  body\naction\nf , g :: component ::= action | f [] g | seq(f0 , f1 ,    fn )\n::= f\nprogram\n\nAn action has a guard, which is a predicate, and a body which is a piece of\ncode. Execution of the body in a state in which the guard holds is guaranteed to\nterminate; we assume that this guarantee is independently available. Details of\nexecution semantics is given Section 2.2. A guard that is true is often dropped.\nAn action without a guard is non-blocking and a guarded action blocking.\nA structured component is either: (1) a join of the form f [] g where f\nand g are its direct subcomponents, or (2) seq(f0 , f1 ,    fn ) where the direct\nsubcomponents, fi , are combined using some sequential language construct.\nA join models concurrent execution, as we describe in Section 2.2. And seq\ndenotes any sequential language construct for which proof rules are available.\nThus, the typical constructs of sequencing, if-then-else and do-while are seq\nconstructs. A subcomponent of any component is either its direct subcomponent\nor a subcomponent of some direct subcomponent. Note that a component is\nnever empty.\nJoin construct is commutative and associative. The syntax permits arbitrary\nnesting of sequential and concurrency constructs, so, (f [] g ) ; (f  [] g  ) is a\nprogram with f , f  , g and g  as components.\nA program is a component that is meant to be executed alone.\n\nAccess rights to variables A variable named in a component is accessible\nto it. Variable x is local to f if f has exclusive write-access to x during any of\nits executions. Therefore, any accessible variable of a component in a sequential\nprogram is local to it. However, a local variable of f [] g may not be local to\neither f or g . An accessible non-local variable x of f is shared ; x is shared with\ng if x is also accessible to g . Note that x is local to f and shared in g if f has\nexclusive write-access to x and g can only read x.\nA program is executed alone, so, all its accessible variables are local to it.\nA local predicate of f is a predicate in which every variable is a local variable\nof f . Therefore, true and false are local to all components. It follows that the\nvalue of a local predicate of f remains unchanged in any execution as long as f\ndoes not take a step.\n\n2.2 Program execution\n\nA step is an instance of the execution of an action. A step of action b   is\nexecuted as follows: evaluate b without preemption and if it is true, immediately\nexecute  to completion, again without preemption this is called an eective\n\n3\n\n\fexecution of the action else the program state (and its control point) are\nunaltered this is called an ineective execution. Thus, in an eective execution\nof b  , b holds when the execution of  begins. Ineective execution models\nbusy-waiting whereby the action execution is retried at some future moment.\nExecution of a join starts simultaneously for all its direct subcomponents,\nand terminates when they all do. At any moment during the execution, the\nprogram control may reside simultaneously in many of its subcomponents. Ex-\necution rules for a seq are the traditional ones from sequential programming,\nwhich ensures that the program control is within one direct subcomponent at\nany moment. The subcomponent itself may be a join which may have multiple\ncontrol points of its own.\nInitially the program control is at the entry point of the program. In any\ngiven state the program takes a step by choosing a control point before an action\nand executing, eectively or ineectively, the action. If there is no such control\npoint, the program has terminated. The choice of control point for a step is\narbitrary, but sub ject to the following fairness constraint: every control point is\nchosen eventually for execution, so that no component is ignored forever. This\nis the same constraint advocated by Dijkstra [5] in his classic paper. In contrast\nto a terminated execution a deadlocked execution attempts executions of certain\nactions ineectively forever, and the control resides permanently at the points\npreceding each of these actions.\nAn execution is a sequence of steps that can not be extended.\nInnite\nexecutions do not have an end state. A nite execution either terminates or is\ndeadlocked, and it has an end state. It simplies the proof theory considerably\nto imagine that every execution is innite, by extending each nite execution\nby an innite number of stutter steps that repeat the end state forever.\n\n2.3 Example: Distributed counter\n\nWe consider an abstraction of a distributed counter protocol in Blumofe [1].\nThe proof in this paper is based on a proof by Ra jeev Joshi and the one given\nin Chapter 6 of Misra [11].\nThe protocol f that implements counter ctr is the join of a nite number\nof threads, fj , given below. Each fj has local variables oldj and newj . Below,\neach assignment statement and guarded statement is an action. The following\nform of the if statement was introduced by Dijkstra [4]; in each execution of the\nstatement an alternative whose guard holds is executed without preemption.\n\ninitially ctr = 0\nfj ::\ninitially oldj , newj = 0, 0\nloop\n\nnewj := oldj + 1;\nif [ ctr = oldj  ctr := newj\n| ctr 6= oldj  oldj := ctr ]\n\nforever\n\n4\n\n\fIt is required to show that ctr behaves as a counter, that is: (1) ctr never\ndecreases and it increases only by 1, and (2) ctr increases eventually. Both of\nthese are perpetual properties. There is no terminal property of interest since\nthe program never terminates.\n\n3 Introduction to the proof theory\n\n3.1 Specication\n\nA specication of component f is of the form {r} f {Q\ns} where r and\ns are the pre- and postconditions, and Q is a set of perpetual properties. The\nmeaning of such a specication is as follows. For any execution of f starting in\nan r-state,\n\n1. if the execution terminates, the end state is an s-state, and\n\n2. every property in Q holds for the execution.\n\nWe give proof rules for pre- and postconditions in the following section.\nProof rules for perpetual properties appear in subsequent sections, for safety\nproperties in Section 4 and for progress properties in Section 5.\n\nTerminology Write {r} f {s} when Q is irrelevant in the discussion, and\n{r} f {Q\n} when s is irrelevant. Further for q  Q, write q in f  when r is\nunderstood from the context, and just q when both f and r are understood.\nAn inference rule without explicit mention of f or r denotes that the rule applies\nfor any f and r.\n\nVariables named in properties A property of component f includes pred-\nicates that name accessible variables of f , and other variables, called free vari-\nables. A property is implicitly universally quantied over its free variables. Any\ninaccessible variable named in a property denotes a free variable, and, being a\nbound variable, may be renamed.\n\n3.2 Local Annotation\n\nLocal Annotation of component f associates assertions with program points\nsuch that each assertion holds whenever program control reaches the associated\npoint in every execution of any program in which f is a component. Thus, a\nlocal annotation yields precondition for the execution of each action of f , and\nvalid pre- and postcondition of f in any environment. Since the execution envi-\nronment of f is arbitrary, only the predicates that are local to f are unaected\nby executions of other components. Therefore, a local annotation associates\npredicates local to each point of f , as explained below.\nLocal annotation is dened by the program structure. First, the proof rule\nfor an action is as follows:\n\n5\n\n\f{p  b}  {q}\n{p} b   {q}\n\nTo construct a local annotation of f = seq(f0 , f1 ,    fn ), construct local\nannotation of each fi using only the local variables of fi as well as those of\nf . Then construct an annotation of f using the proof rules for seq from the\nsequential program proof theory. Observe that the local variables of f are local\nto each fi because in a sequential execution among the direct subcomponents\nof f each fi has exclusive write-access to these variables during its execution.\nTo construct a local annotation of f = g [] h, construct local annotations of\neach of g and h using only their local variables. Then construct an annotation\nof f using the proof rule given below. Note that the proof rule is valid only\nbecause the assertions in g and h are local to those components.\n\n{r} g {s}\n{r} h {s}\n{r  r } g [] h {s  s}\n\nObserve that a local variable of f is not necessarily local to g or h unless\nthey have exclusive write-access to it. Henceforth, all annotations in this paper\nare local annotations.\nA shortcoming of local annotation is that a variable that is local to f [] g but\nshared by both f and g can not appear in a local annotation by the application\nof these rules alone. The invariance meta-rule, given in Section 3.4, overcomes\nthis problem.\n\n3.3 Example: Distributed Counter, contd.\n\nConstruct a local annotation of fj for the example of Section 2.3. Below, we\nhave labeled the actions to refer to them in subsequent proofs.\n\nfj ::\ninitially oldj , newj = 0, 0\n{true}\nloop\n{true}\nj :: newj := oldj + 1;\n{newj = oldj + 1}\nif [ j :: {newj = oldj + 1} ctr = oldj  ctr := newj {true}\n| j :: {newj = oldj + 1} ctr 6= oldj  oldj := ctr\n{true}\n\n]\n{true}\nforever\n\n6\n\n\f3.4 Meta-rules\n\nThe following general rules apply for specications.\n\n (lhs strengthening; rhs weakening)\n\n{r} f {Q\ns}\nr  r, s  s , Q  Q, r and s are local to f\n{r } f {Q\ns}\n\n (Conjunction; Disjunction)\n\n{r} f {Q\n{r } f {Q\n{r  r } f {Q  Q\n{r  r } f {Q  Q\n\ns}\ns}\ns  s }\ns  s }\n\nJustications for the meta-rules The lhs strengthening and rhs weakening\nrules are inspired by similar rules for Hoare-triples. Additionally, since the\nproperties in Q are independent, any number of them may be removed.\nFor the conjunction rule, let the set of executions of f starting in r-state be r-\nexecutions, and, similarly r -executions. The r  r -executions is the intersection\nof r-executions and r -executions. Therefore, the postcondition of any execution\nin r  r -executions satises s  s and every property in Q or Q , justifying the\nconjunction rule. The arguments for the disjunction rule are similar.\n\n4 Safety Properties\n\nA safety property is perpetual. We develop a safety property, co, and its special\ncases, taken from Misra [11]. Safety properties are derived from local annota-\ntions and/or safety properties of the subcomponents of a component.\n\n4.1 Safety Property co\n\nWrite p co q in component f , for predicates p and q that may not be local to\nf , to mean that eective execution of any action of f in a p-state establishes a\nq -state. Observe that an ineective execution preserves p. Thus, given p co q :\n(1) in any execution of f once p holds it continues to hold until q is established,\nthough q may never be established, and (2) as a composition rule, p co q holds\nin component i it holds in every subcomponent of it.\nFor an annotated component, co is dened by the following proof rule.\n\n{r} f {s}\nFor every action b   with precondition pre in the annotation :\n{pre  b  p}  {q}\n{r} f {p co q\ns}\n\n7\n\n\fAs an example, the statement every change in integer variable ctr can only\nincrement its value may be formalized as ctr = m co ctr = m  ctr = m + 1,\nfor all integer m.\n\n4.2 Special cases of co\n\nDene special cases of co for component f : stable, constant and invariant.\nGiven predicate p and expression e, in any execution of f : (1) stable p means\nthat p remains true once it becomes true, (2) constant e that the value of\ne never changes, and (3) invariant p that p is always true, including after\ntermination, if the program terminates. Formally, in f\n\nstable p\n p co p\nconstant e  (c :: stable e = c)\ninvariant p  initially p and stable p\n\nObserve that invariant true (hence, stable true) and stable false are prop-\nerties of every component. A variable for which f has no write-access is constant\nin f , and so is any expression constructed out of such variables.\nDerived rules for co and some of its special cases, which are heavily used\nin actual proofs, are given in Appendix A.1. It follows from the derived rules\nthat a safety property of a program is a property of all its components, and\nconversely, as given by the inheritance rule below.\n\n4.3 Meta-rules\n\n1. (Inheritance) If any safety property (co or any of its special cases) holds\nin all subcomponents of f then it holds in f . More formally, for safety\nproperties  ,\n\nGiven:\n\n(i :: {ri} fi {si})\n{r} f {s}\n\nAssert:\n\n(i :: {ri} fi {\n{r} f {\n\ns}\n\nsi})\n\n2. (Invariance) A local invariant of a component, i.e., a local predicate that is\ninvariant in the component, can be substituted for true, and vice versa, in\nany predicate in an annotation or property of the component. In particu-\nlar, for a program all variables are local, so any invariant can be conjoined\nto an assertion including the postcondition.\n\nJustications for the meta-rules\nInheritance rule is based on the fact that\nif a safety property holds for all components of f it holds for f as well. Given\nthe proof rule at left the inheritance proof rule at right can be asserted for any\nset of safety properties  .\nThe invariance rule is from Chandy and Misra [2] where it is called the\nsubstitution axiom. One consequence of the rule is that a local invariant of\nf [] g , that may not be a local predicate of either f or g , could be conjoined to\npredicates in an annotation of f [] g . Additionally, all variables in a program\nare local; so, any invariant can be substituted for true in a program.\n\n8\n\n\f4.4 Example: Distributed Counter, contd.\n\nFor the example of Section 2.3 we prove that ctr behaves as a counter in that\nits value can only be incremented, i.e., ctr = m co ctr = m  ctr = m + 1 in\nf . Using the inheritance rule, it is sucient to prove this property in every\ncomponent fj . In fj , only j may change the value of ctr; so we need only show\nthe following whose proof is immediate:\n\n{ctr = m  newj = oldj + 1  ctr = oldj } ctr := newj {ctr = m  ctr = m + 1}\n\n5 Progress Properties\n\nWe are mostly interested in progress properties of the form if predicate p holds\nat any point during the execution of a component, predicate q holds eventually.\nHere eventually includes the current and all future moments in the execution.\nThis property, called leads-to, is dened in Section 5.3 (page 12). First, we\nintroduce two simpler progress properties, transient and ensures. Transient is a\nfundamental progress property, the counterpart of the safety property stable.\nIt is introduced because its proof rules are easy to justify and it can be used\nto dene ensures. However, it is rarely used in program specication because\nensures is far more useful in practice. Ensures is used to dene leads-to.\n\n5.1 Progress Property: transient\n\nIn contrast to a stable predicate that remains true once it becomes true, a\ntransient predicate is guaranteed to be falsied eventually. That is, predicate\np is transient in component f implies that if p holds at any point during an\nexecution of f , p holds then or eventually in that execution. In temporal logic\nnotation p is transient is written as p. Note that p may not continue to\nhold after p has been falsied. Predicate false is transient because false  true,\nand, hence false holds whenever false holds. Note that given p transient in f ,\np holds at the termination point of f because, otherwise, f can take no further\nsteps to falsify p.\nThe proof rules are given in Figure 5.1. Below, postf is a local predicate of\nf that is initially false and becomes true only on the termination of f . Such a\npredicate always exists, say, by encoding the termination control point into it.\nFor a non-terminating program, postf is false. Proof of postf is a safety proof.\n\nJustications The formal justication is based on induction on the program\nstructure: show that the basis rule is justied and then inductively prove the\nremaining rules. We give informal justications below.\nIn the basis rule the hypotheses guarantee that each action of f is eectively\nexecuted whenever p holds, and that the execution establishes p. If no action\ncan be executed eectively, because precondition of no action holds, the program\nhas terminated and postf holds. Hence, p  postf , i.e.(p  postf ), hold\neventually in all cases. Note that if pre  p then pre  p is false and the\n\n9\n\n\f--------------------------------------------------------------------------------\n\n (Basis)\n\n{r} f {s}\nFor every action b   of f with precondition pre :\npre  p  b\n{pre  p}  {p}\n{r} f {transient p  postf\n\ns}\n\n (Sequencing)\n{r} f {transient p  postf\n{postf } g {transient p\n{r} f ; g {transient p\n\npostf }\n}\n}\n\n (Concurrency)\n{r} f {transient p\n{r} f [] g {transient p\n\n}\n\n}\n\n (Inheritance)\n(i :: {ri} fi {si })\n{r} f {s}\n\nGiven:\n\nAssert:\n\n(i :: {ri } fi {transient p\n{r} f {transient p\n\ns}\n\nsi })\n\n--------------------------------------------------------------------------------\n\nFigure 1: Denition of transient\n\nhypotheses are vacuously satised. If f never terminates, postf always holds\nand p is guaranteed eventually.\nThe next two rules, for sequential and concurrent composition, have weaker\nhypotheses. The sequencing rule is based on an observation about a sequence\nof actions, ;  . To prove transient p it is sucient that  establish p\nor that it execute eectively, thus establishing post , and that  establish p.\nThe sequencing rule generalizes this observation to components. Being a local\npredicate, postf can not be falsied by any concurrently executing component,\nso it holds as long as the control remains at the termination point of f .\nIn the concurrency rule, as a tautology g either establishes p eventually,\nthus guaranteeing the desired result, or preserves p forever. In the latter case,\nf establishes p since transient p holds in f .\nThe inheritance rule applies to a program with multiple components.\nIt\nasserts that if the property holds in each component fi then it holds in program\nf . To see this consider two cases: f is seq or join, and argue by induction on\nthe program structure.\nFor seq f : if p holds at some point before termination of f it is within exactly\none direct subcomponent fi , or will do so without changing any variable value.\nFor example, if control precedes execution of ifb then f0 else f1 then it will\nmove to a point preceding f0 or f1 after evaluation of b without changing the\n\n10\n\n\fstate. Note that fi may be a join, so there may be multiple program points\nwithin it where control may reside simultaneously, but all controls reside within\none direct subcomponent of seq f at any moment. From the hypothesis, that\ncomponent, and hence, the program establishes p eventually.\nFor a join, say f [] g : Consider an execution in which, say, f has not ter-\nminated when p holds. From the arguments for the concurrency rule, using\nthat transient p in f , eventually p is established in that execution. Similar\nremarks apply for all executions in which g has not terminated. And, if both\nf and g have terminated, then p holds from the denition of transient for\neach component.\n\n\nNotes\n\n1. The basis rule by itself is sucient to dene an elementary form of tran-\nsient. However, the transient predicate then has to be extremely elaborate,\ntypically encoding control point of the program, so that it is falsied by\nevery action of the component. The other rules permit simpler predicates\nto be proven transient.\n\n2. Basis rule is the only rule that needs program code for its application,\nothers are derived from properties of the components, and hence, permit\nspecication composition.\n\n3. It is possible that p is eventually falsied in every execution of a component\nthough there is no proof for transient p. To see this consider the program\nf [] g in which every action of f falsies p only if for some predicate q ,\np  q holds as a precondition, and every action of g falsies p only if p  q\nholds as a precondition, and neither component modies q . Clearly, p\nwill be falsied eventually, but this fact can not be proved as a transient\nproperty; only p  q and p  q can be shown transient. As we show later,\np leads-to p.\n\n5.2 Progress Property: ensures\n\nProperty ensures for component f , written as p en q with predicates p and q ,\nsays that if p holds at any moment in an execution of f then it continues to\nhold until q holds, and q holds eventually. This claim applies even if p holds\nafter the termination of f . For initial state predicate r, it is written formally as\n{r} f {p en q\n} and dened as follows:\n\n{r} f {p  q co p  q , transient p  q\n{r} f {p en q\n}\n\n}\n\nWe see from the safety property in the hypothesis that once p holds it con-\ntinues until q holds, and from the transient property that eventually q holds.\nCorresponding to each proof rule for transient, there is a similar rule for en-\nsures. These rules and additional derived rules for en are given in Appendix A.3\n(page 16).\n\n11\n\n\fExample: Distributed counter, contd. We prove a progress property of\nthe annotated program from Section 3.3, reproduced below.\n\nfj ::\ninitially oldj , newj = 0, 0\n{true}\nloop\n{true}\nj :: newj := oldj + 1;\n{newj = oldj + 1}\nif [ j :: {newj = oldj + 1} ctr = oldj  ctr := newj {true}\n| j :: {newj = oldj + 1} ctr 6= oldj  oldj := ctr\n{true}\n\n]\n{true}\nforever\n\nOur ultimate goal is to prove that for any integer m if ctr = m at some\npoint during an execution of f , eventually ctr > m. To this end let auxiliary\nvariable nb be the number of threads fj for which ctr 6= oldj . We prove the\nfollowing ensures property, (E), that says that every step of f either increases\nctr or decreases nb while preserving ctrs value. Proof uses the inheritance rule\nfrom Appendix A.3 (page 16). For every fj and any m and N :\n\nctr = m  nb = N en ctr = m  nb < N  ctr > m in fj\n\n(E)\n\nWe use the rules for en given in Appendix A.3 (page 16). First, to prove\n(E) in g ; h, for any g and h, it is sucient to show that g terminates and (E) in\nh. Hence, it is sucient to show that (E) holds only for the loop in fj , because\ninitialization always terminates. Next, using the inheritance rule, it is sucient\nto show that (E) holds only for the body of the loop in fj . Further, since\nj always terminates, (E) needs to be shown only for the if statement. Using\ninheritance, prove (E) for j and j .\nIn each case, assume the precondition\nctr = m  nb = N of if and the preconditions of j and j . The postcondition\nctr = m  nb < N  ctr > m is easy to see in each of the following cases:\n\nj :: {ctr = m  nb = N  newj = oldj + 1  ctr = oldj }\nctr := newj\n{ctr = m  nb < N  ctr > m}\n\nj :: {ctr = m  nb = N  newj = oldj + 1  ctr 6= oldj }\noldj := ctr\n{ctr = m  nb < N  ctr > m}\n\n5.3 Progress Property: Leads-to\n\nThe informal meaning of p 7 q (read: p leads-to q) is if p holds at any point\nduring an execution, q holds eventually. Unlike en, p is not required to hold\nuntil q holds.\n\n12\n\n\fLeads-to is dened by the following three rules, taken from Chandy and\nMisra [2]. The rules are easy to justify intuitively.\n\n (basis)\n\np en q\np 7 q\n\n (transitivity)\n\np 7 q , q 7 r\np 7 r\n\n (disjunction) For any (nite or innite) set of predicates S\n\n(p : p  S : p 7 q)\n(p : p  S : p) 7 q\n\nDerived rules for 7 are given in Appendix A.4 (page 18).\nleads-to is not\nconjunctive, nor does it obey the inheritance rule, so even if p 7 q holds in both\nf and g it may not hold in f [] g .\n\nExample: Distributed counter, contd. We show that for the example of\nSection 2.3 the counter ctr increases without bound. The proof is actually quite\nsimple. We use the induction rule for leads-to given in Appendix Section A.4.2.\nThe goal is to show that for any integer C , true 7 ctr > C . Below, all\nproperties are in f .\n\nctr = m  nb = N en ctr = m  nb < N  ctr > m\nproven in Section 5.2\nctr = m  nb = N 7 ctr = m  nb < N  ctr > m\nApplying the basis rule of leads-to\nctr = m 7 ctr > m\nInduction rule, use the well-founded order < over natural numbers\ntrue 7 ctr > C , for any integer C\nInduction rule, use the well-founded order < over natural numbers.\n\n6 Related Work\n\nThe earliest proof method for concurrent programs appears in Owicki and\nGries [13]. The method works well for small programs, but does not scale\nup for large ones. Further it is limited to proving safety properties only. There\nis no notion of component specication and their composition. Lamport [10]\nrst identied leads-to for concurrent programs as the appropriate generaliza-\ntion of termination for sequential programs (progress is called liveness in that\npaper). Owicki and Lamport [14] is a pioneering paper.\nThe rst method to suggest proof rules in the style of Hoare [7], and thus a\nspecication technique, is due to Jones [8, 9]. Each component is annotated as-\nsuming that its environment preserves certain predicates. Then the assumptions\nare discharged using the annotations of the various components. The method\nthough is restricted to safety properties only. A similar technique for message\ncommunicating programs was proposed in Misra and Chandy [12].\n\n13\n\n\fThe approaches above are all based on specifying allowed interface behav-\niors using compositional temporal logics. Since 2000, a number of proposals[6, 3]\nhave instead used traditional Hoare triples and resource/ob ject invariants, but\nextending state predicates to include permissions describing how locations can\nbe used, and suitably generalizing the Hoare rule for disjoint parallel compo-\nsition. However, these proposals typically address only global safety and local\ntermination properties, not global progress properties (as this paper does).\nA completely dierent approach is suggested in the UNITY theory of Chandy\nand Misra [2], and extended in Misra [11]. A restricted language for describing\nprograms is prescribed. There is no notion of associating assertions with pro-\ngram points. Instead, the safety and progress specication of each component\nis given by a set of properties that are proved directly. The specications of\ncomponents of a program can be composed to derive program properties. The\ncurrent paper extends this approach by removing the syntactic constraints on\nprograms, though the safety and progress properties of UNITY are the ones\nused in this paper.\nOne of the essential questions in these proof methods is to propose the appro-\npriate preconditions for actions. In Owicki and Gries [13] theory it is postulated\nand proved. In UNITY the programmer supplies the preconditions, which are\noften easily available for event-driven systems. Here, we derive preconditions\nthat remain valid in any environment; so there can be no assertion about shared\nvariables. The theory separates local precondition (obtained through annota-\ntion) from global properties that may mention shared variables.\nThe proof strategy described in the paper is bilateral in the following sense.\nAn invariant, a perpetual property, may be used to strengthen a postcondition,\na terminal property, using the invariance rule. Conversely, a terminal property,\npostcondition postf of f , may be employed to deduce a transient predicate, a\nperpetual property.\nSeparation logic [16] has been eective in reasonong about concurrently ac-\ncessed data structures. We are studying its relationship to the work described\nhere.\n\nAcknowledgment\nI am truly grateful to Ernie Cohen of Amazon and Jose\nMeseguer of the Univ. of Illinois at Urbana, for reading the manuscript thor-\noughly and several times, endless discussions and substantive comments that\nhave shaped the paper. Presenting this work at the IFIPS Working Group 2.3\nand the Programming Languages lunch at the University of Texas at Austin\nhas sharpened my understanding; I am grateful to the students and the other\nattendees, in particular to Dhananjay Ra ju.\n\n14\n\n\fA Appendix: Derived Rules\n\nA.1 Derived Rules for co and its special cases\n\nThe derived rules for co are given in Figure 2 and for the special cases in\nFigure 3. The rules are easy to derive; see Chapter 5 of Misra [11].\n\nfalse co q\n\np co true\n\np co q , p\n\n co q\n\n\n\np  p\n\n co q  q\n\n\n\n(conjunction)\n\np co q\n\np  p\n\n co q\n\n(lhs strengthening)\n\np co q , p\n\n co q\n\n\n\np  p\n\n co q  q\n\n\n\n(disjunction)\n\np co q\n\np co q  q\n\n (rhs weakening)\n\nFigure 2: Derived rules for co\n\nThe top two rules in Figure 2 are simple properties of Hoare triples. The\nconjunction and disjunction rules follow from the conjunctivity, disjunctivity\nand monotonicity properties of the weakest precondition, see Dijkstra [4] and\nof logical implication. These rules generalize in the obvious manner to any set\nnite or innite of co-properties, because weakest precondition and logical\nimplication are universally conjunctive and disjunctive.\nThe following rules for the special cases are easy to derive from the denition\nof stable , invariant and constant . Special Cases of co\n\n (stable conjunction, stable disjunction)\np co q , stable r\np  r co q  r\np  r co q  r\n\n (Special case of the above)\n\nstable p , stable q\nstable p  q\nstable p  q\n\n\n\n\n\ninvariant p , invariant q\ninvariant p  q\ninvariant p  q\n\n{r} f {stable p\n{r  p} f {p}\n\n}\n\n{r} f {constant e\n}\n{r  e = c} f {e = c}\n\n (constant formation) Any expression built out of constants is constant.\n\nFigure 3: Derived rules for the special cases of co\n\n15\n\n\fA.2 Derived Rules for transient\n\n transient false.\n\n (Strengthening) Given transient p, transient p  q for any q .\n\nTo prove transient false use the basis rule. The proof of the strengthening\nrule uses induction on the number of applications of the proof rules in deriving\ntransient p. The proof is a template for proofs of many derived rules for ensures\nand leads-to. Consider the dierent ways by which transient p can be proved\nin a component. Basis rule gives the base case of induction.\n\n1. (Basis) In component f , p is of the form p  postf for some p . Then in\nsome annotation of f where action b   has the precondition pre:\n(1) pre  p  b, and (2) {pre  p}  {p}.\n(1) From predicate calculus pre  p  q  b, and\n(2) from Hoare logic {pre  p  q}  {p}. Applying the basis rule,\ntransient p  q  postf , i.e., transient p  q .\n\n2. (Sequencing) In f ; g , transient p  postf in f and transient p in g . In-\nductively, transient p  q  postf in f and transient p  q in g . Applying\nthe sequencing rule, transient p  q .\n\n3. (Concurrency, Inheritance) Similar proofs.\n\nA.3 Derived Rules for en\n\nA.3.1 Counterparts of rules for transient\n\nThis set of derived rules correspond to the similar rules for transient. Their\nproofs are straight-forward using the denition of en.\n\n (Basis)\n\n{r} f {s}\nFor every action b   with precondition pre in the annotation :\npre  p  q  b\n{pre  p  q}  {q}\n{r} f {p en p  s  q\n\ns}\n\n (Sequencing)\n\n{r} f {p en p  postf  q\n{postf } g {p en q\n{r} f ; g {p en q\n\n}\n}\n\npostf }\n\n (Concurrency)\n\np en q in f\np  q co p  q in g\np en q in f [] g\n\n16\n\n\f (Inheritance) Assuming the proof rule at left the inheritance proof rule at\nright can be asserted.\n\nGiven:\n\n(i :: {ri } fi {si})\n{r} f {s}\n\nAssert:\n(i :: {ri} fi {p en q\n{r} f {p en q\n\ns}\n\nsi})\n\nA.3.2 Additional derived rules\n\nThe following rules are easy to verify by expanding each ensures property by its\ndenition, and using the derived rules for transient and co. We show one such\nproof, for the PSP rule. Observe that ensures is only partially conjunctive and\nnot disjunctive, unlike co.\n\n1. (implication)\n\np  q\np en q\nConsequently, false en q and p en true for any p and q .\n\n2. (rhs weakening)\n\np en q\np en q  q \n\n3. (partial conjunction)\n\np en q\np en q\np  p en q\n\n4. (lhs manipulation)\n\np  q  p  p  q\np en q  p en q\nObserve that p  q  p  q and p  q  p  q . So, p and q are\ninterchangeable in all the proof rules. As special cases, p  q en q \np en q  p  q en q .\n\n5. (PSP) The general rule is at left, and a special case at right using stable r\nas r co r.\n\n(PSP)\n\np en q\nr co s\np  r en (q  r)  (r  s)\n\n(Special case)\np en q\nstable r\np  r en q  r\n\n6. (Special case of Concurrency)\n\np en q in f\nstable p in g\np en q in f [] g\n\nProof of (PSP): From the hypotheses:\n\ntransient p  q\np  q co p  q\nr co s\n\nWe have to show:\n\n17\n\n\n\n(1)\n(2)\n(3)\n\n\ftransient p  r  (q  r)  (r  s)\np  r  (q  r)  (r  s) co p  r  q  r  r  s\n\n(4)\n(5)\n\nFirst, simplify the term on the rhs of (4) and lhs of (5) to p  r  q  s.\nProof of (4) is then immediate, as a strengthening of (1). For the proof of (5),\napply conjunction to (2) and (3) to get:\n\np  r  q co p  s  q  s\n {expand both terms in rhs}\np  r  q co p  r  s  p  r  s  q  r  s  q  r  s\n {lhs strengthening and rhs weakening}\np  r  q  s co p  r  q  r  r  s\n\nA.4 Derived Rules for leads-to\n\nThe rules are taken from Misra [11] where the proofs are given. The rules are\ndivided into two classes, lightweight and heavyweight. The former includes rules\nwhose validity are easily established; the latter rules are not entirely obvious.\nEach application of a heavyweight rule goes a long way toward completing a\nprogress proof.\n\nA.4.1 Lightweight rules\np  q\np 7 q\n\n1. (implication)\n\n2. (lhs strengthening, rhs weakening)\np 7 q\np  p 7 q\np 7 q  q \n\n3. (disjunction)\n\n7 qi )\n(i :: pi\n(i :: pi ) 7 (i :: qi )\n\nwhere i is quantied over an arbitrary nite or innite index set, and pi , qi\nare predicates.\n\n4. (cancellation)\n\np 7 q  r , r 7 s\np 7 q  s\n\nA.4.2 Heavyweight rules\n\n1. (impossibility)\n\np 7 false\np\n\n2. (PSP) The general rule is at left, and a special case at right using stable r\nas r co r.\n\n18\n\n\f(PSP)\n\np 7 q\nr co s\np  r 7 (q  r)  (r  s)\n\n(Special case)\np 7 q\nstable r\np  r 7 q  r\n\n3. (induction) Let M be a total function from program states to a well-\nfounded set (W, ). Variable m in the following premise ranges over W .\nPredicates p and q do not contain free occurrences of variable m.\n\n(m :: p  M = m 7 (p  M  m)  q)\np 7 q\n\n4. (completion) Let pi and qi be predicates where i ranges over a nite set.\n\n(i ::\n7 qi  b\npi\nqi co qi  b\n\n)\n(i :: pi ) 7 (i :: qi )  b\n\nA.4.3 Lifting Rule\n\nThis rule permits lifting a leads-to property of f to f [] g with some modi-\ncations. Let x be a tuple of some accessible variables of f that includes all\nvariables that f shares with g . Below, X is a free variable, therefore universally\nquantied. Predicates p and q name accessible variables of f and g . Clearly,\nany local variable of g named in p or q is treated as a constant in f .\n\np 7 q in f\nr  x = X co x = X  r in g\np 7 q  r in f [] g\n\n(L)\n\nAn informal justication of this rule is as follows. Any p-state in which\nr holds, q  r holds. We show that in any execution of f [] g starting in a\n(p  r)-state q or r holds eventually. If r is falsied by a step of f then r\nholds. Therefore, assume that every step of f preserves r. Now if any step of\ng changes the value of x then it falsies r from the antecedent, i.e., r holds.\nSo, assume that no step of g modies x. Then g does not modify any accessible\nvariable of f ; so, f is oblivious to the presence of g , and it establishes q .\nAs a special case, we can show\n\n(L)\n\np 7 q in f\np  x = M 7 q  x 6= M in f [] g\n\nThe formal proof of (L) is by induction on the structure of the proof of\np 7 q in f . See\nhttp://www.cs.utexas.edu/users/psp/unity/notes/UnionLiftingRule.pdf\nfor details.\n\n19\n\n\fB Example: Mutual exclusion\n\nWe prove a coarse-grained version of a 2-process mutual exclusion program due\nto Peterson [15]. The given program has a nite number of states, so it is\namenable to model-checking. In fact, model-checking is a simpler alternative\nto an axiomatic proof. We consider this example primarily because this is an\ninstance of a tightly-coupled system where the codes of all the components are\ntypically considered together to construct a proof. In contrast, we construct a\ncomposable specication of each component and combine the specications to\nderive a proof.\n\nB.1 Program\n\nThe program has two processes M and M  . Process M has two local boolean\nvariables, try and cs where try remains true as long as M is attempting to enter\nthe critical section or in it and cs is true as long as it is in the critical section;\nM  has try  and cs . They both have access to a shared boolean variable turn.\nIt simplies coding and proof to postulate an additional boolean variable turn\nthat is the complement of turn, i.e., turn  turn.\nThe global initialization and the code for M , along with a local annota-\ntion, is given below. The code of M  is the dual of M , obtained by replacing\neach variable in M by its primed counterpart. Henceforth, the primed and the\nunprimed versions of the same variable re duals of each other.\nThe unrelated computation below refers to computation preceding the\nattempt to enter the critical section that does not access any of the relevant\nvariables. This computation may or may not terminate in any iteration.\n\ninitially cs, cs = false, false  global initialization\n\nM : initially try = false\n{try , cs}\nloop\n unrelated computation that may not terminate;\n\n{try , cs} :\n\ntry , turn := true, true;\n\n{try , cs}\n\n : try   turn\n\n cs := true;  Enter critical section\n\n{try , cs}\nforever\n\n :\n\ntry , cs := false, false  Exit critical section\n\nGiven that M  is the dual of M , from any property of M obtain its dual as\na property of M  . And, for any property of M [] M  its dual is a property of\nM  [] M , i.e., M [] M  , thus reducing the proof length by around half.\n\nRemarks on the program The given program is based on a simplication\nof an algorithm in Peterson [15]. In the original version the assignment in \n\n20\n\n\fmay be decoupled to the sequence try := true; turn := true. The tests in \nmay be made separately for each disjunct in arbitrary order. Action  may\nbe written in sequential order try := false; cs := false. These changes can be\neasily accommodated within our proof theory by introducing auxiliary variables\nto record the program control.\n\nB.2 Safety and progress properties\n\nIt is required to show in M [] M  (1) the safety property: both M and M  are\nnever simultaneously within their critical sections, i.e., invariant (cs  cs ),\nand (2) the progress property: any process attempting to enter its critical section\nwill succeed eventually, i.e., try 7 cs and try  7 cs ; we prove just try 7 cs\nsince its dual also holds.\n\nB.2.1 Safety proof: invariant (cs  cs )\n\nWe prove below:\n\ninvariant cs  try in M [] M \ninvariant cs  try  turn in M [] M \n\n(S1)\n(S2)\n\nMutual exclusion is immediate from (S1) and (S2), as follows.\n\ncs  cs\n {from (S1) and its dual}\ncs  try  cs  try \n {rewriting}\n(cs  try )  (cs  try  )\n {from (S2) and its dual}\nturn  turn\n {turn and turn are complements}\nfalse\n\nProofs of (S1) and (S2) First, we show the following stable properties of\nM , which constitute its safety specication, from which (S1) and (S2) follow.\n\nstable cs  try in M\nstable try  turn in M\nstable cs  try   turn in M\n\n(S3)\n(S4)\n(S5)\n\nThe proofs of (S3), (S4) and (S5) are entirely straight-forward, but each\nproperty has to be shown stable for each action. We show the proofs in Table 1,\nwhere each row refers to one of the predicates in (S3  S5) and each column\nto an action. An entry in the table is either: (1) post: p claiming that since\np is a postcondition of this action the given predicate is stable, or (2) una.\nmeaning that the variables in the given predicate are unaected by this action\nexecution. The only entry that is left out is for the case that  preserves (S5):\ncs  try   turn ; this is shown as follows. The guard of  can be written as\n\n21\n\n\ftry   turn and execution of  aects neither try  nor turn , so try   turn\nis a postcondition, and so is cs  try   turn .\n\n(S3)\n(S4)\n(S5)\n\ncs  try\ntry  turn\ncs  try   turn\n\n\n\n\npost: cs\npost: try\npost: cs\npost: turn una.: try , turn post: try\npost: cs\nsee text\npost: cs\n\nTable 1: Proofs of (S3), (S4) and (S5)\n\nNow we are ready to prove (S1) and (S2). The predicates in (S1) and (S2)\nhold initially because initially cs, cs = false, false. Next, we show these predi-\ncates to be stable. The proof is compositional, by proving each predicate to be\nstable in both M and M  . The proof is simplied by duality of the codes.\n\n (S1) stable cs  try in M [] M  :\n\nstable cs  try in M\nstable cs  try in M \nstable cs  try in M [] M \n\n, (S3)\n, cs and try are constant in M \n, Inheritance rule\n\n (S2) stable cs  try  turn in M [] M  :\n\nstable try  turn in M\nstable cs  try  turn in M\nstable cs  try  turn in M \nstable cs  try  turn in M [] M \n\n, (S4)\n, cs constant in M\n, dual of (S5)\n, Inheritance rule\n\nB.2.2 Progress proof: try 7 cs in M [] M \n\nFirst, we prove a safety property:\n\ntry  cs co try  cs in M [] M \n\n(S6)\n\nTo prove (S6) rst prove try cs co try  cs in M , which is entirely straight-\nforward. Now variables try and cs are local to M , therefore stable try  cs in\nM  , and through rhs weakening, try  cs co try  cs in M  . Using inheritance\n(S6) follows.\n\nNext we prove a progress property:\n\ntry  (try   turn ) 7 try in M [] M \n\n(P)\n\nFirst, prove try  (try   turn ) en try in M , using the sequencing rule for\nen ; intuitively, in a try  (try   turn )-state the program control is never at\n, execution of  terminates while preserving try  (try   turn ), and execution\nof  establishes try .\nUsing duality on (S4) get stable try   turn in M  , i.e., stable try  \nturn in M  . And try is local to M , so stable try in M  . Conjoining these two\n\n22\n\n\fproperties, stable try  (try   turn ) in M  . Apply the concurrency rule for\nen with stable try  (try   turn ) in M  and try  (try   turn ) en try\nin M to conclude that try  (try   turn ) en try in M [] M  .\nNow apply the basis rule for 7 to conclude the proof of (P).\n\n\n\nProof of try 7 cs in M [] M  : All the properties below are in M [] M  .\nConclude from (P), using lhs strengthening,\n\ntry  try  7 try\ntry  turn 7 try\n\nThe main proof:\n\n(P1)\n(P2)\n\ntry   turn 7 try \n, duality applied to (P2)\ntry  try   turn 7 try \n, lhs strengthening\ntry  (try   turn) 7 try , rewriting (P) using turn  turn\ntry 7 try  try \n, disjunction of the above two\ntry 7 try  (try   try )\n, rewriting the rhs\n, cancellation using (P1)\ntry 7 try\n, (S6)\ntry  cs co try  cs\n, PSP of above two\ntry  cs 7 cs\ntry 7 cs\n, disjunction with try  cs 7 cs\n\nC Example: Associative, Commutative fold\n\nWe consider a recursively dened program f where the code of f1 is given and\nfk+1 is dened to be f1 [] fk . This structure dictates that the specication sk\nof fk must permit proof of (1) s1 from the code of f1 , and (2) sk+1 from s1\nand sk , using induction on k . This example illustrates the usefulness of the\nvarious composition rules for the perpetual properties. The program is not easy\nto understand intuitively; it does need a formal proof.\n\nC.1\n\nInformal Problem Description\n\nGiven is a bag u on which  is a commutative and associative binary operation.\nDene u, fold of u, to be the result of applying  repeatedly to all pairs of\nelements of u until there is a single element. It is required to replace all the\nelements of u by u. Program fk , for k  1, decreases the size of u by k while\npreserving its fold. That is, fk transforms the original bag u to u such that:\n(1) u = u , and (2) |u| + k = |u |, provided |u | > k , where |u| is the size of\nu. Therefore, execution of fn1 , where n is the size of u and n > 1, computes\na single value in u that is the fold of the initial bag.\nBelow, get(x) removes an item from u and assigns its value to variable x.\nThis operation can be completed only if u has an item. And put(x  y ), a\nnon-blocking action, stores x  y in u. The formal semantics of get and put are\ngiven by the following assertions where u is constant:\n\n23\n\n\f{u = u} get(z ) {u = u  {z }}\n{u = u} put(z ) {u = u  {z }}\n\nThe fold program fk for all k , k  1, is given by:\n\nf1 = |u| > 0  get(x); |u| > 0  get(y ); put(x  y )\nfk+1 = f1 [] fk , k  1\n\nSpecication and proof of safety properties appear in Section C.2, next, and\nprogress properties in Section C.3. Observe that\n\nC.2 Terminal property\n\nThe relevant safety property of fk , for all k , k  1, is a terminal property:\n\n{u = u} fk {u = u , |u| + k = |u |}\n\nThis property can not be proved from a local annotation alone because it\nnames the shared variable u in its pre- and postconditions. We suggest an\nenhanced safety property using certain auxiliary variables.\n\nAuxiliary variables The following auxiliary variables of fk are local to it:\n\n1. wk : the bag of items removed from u that are, as yet, unfolded. That is,\nevery get from u puts a copy of the item in wk , and put(x  y ) removes x\nand y from wk . Initially wk = {}.\n\n2. nhk : the number of halted threads where a thread halts after completing a\nput. A get does not aect nhk and a put increments it. Initially nhk = 0.\n\nIntroduction of any auxiliary variable auxk for fk follows a pattern: (1)\nspecify the initial value of auxk , (2) dene aux1 by modifying the code of f1 ,\ncorresponding to the basis of a recursive denition, and (3) dene auxk+1 in\nterms of aux1 and auxk .\nWe adopt this pattern for dening wk and nhk for all k , k  1. First, the\ninitial values of wk and nhk are {} and 0, respectively. Second, w1 and nh1 are\ndened below in f1 . Third, wk+1 = w1  wk and nhk+1 = nh1 + nhk , for all k .\nThe modied program for f1 is as follows where h   i is an action.\n\nf1 = |u| > 0  hget(x); w1 := w1  {x}i;\n|u| > 0  hget(y ); w1 := w1  {y}i;\nhput(x  y ); w1 := w1  {x, y}; nh1 := 1i\n\nNote: The denition of auxiliary variables is problematic for nh2 , for instance;\nby denition, nh2 = nh1 + nh1 . However, these are two dierent instances of\nnh1 referring to the local variables of the two instances of f1 in f2 . This is not\na problem in the forthcoming proofs because the proofs always refer to indices\n1, k and k + 1, and, nhk is treated as being dierent from nh1 .\n\n24\n\n\fSpecication of safety property The safety specication of fk , for all k ,\nk  1, is given by:\n\n{u = u , wk = {}, nhk = 0}\nfk\n{constant (u  wk ), constant |u| + |wk | + nhk\nwk = {}, nhk = k}\n\n(S)\n\n Proof of (S) for f1 : Construct the following local annotation of f1 .\n\n{w1 = {}, nh1 = 0}\n|u| > 0  hget(x); w1 := w1  {x}i;\n{w1 = {x}, nh1 = 0}\n|u| > 0  hget(y ); w1 := w1  {y}i;\n{w1 = {x, y}, nh1 = 0}\nhput(x  y ); w1 := w1  {x, y}; nh1 := 1i\n{w1 = {}, nh1 = 1}\n\nThe perpetual and terminal properties of f1 in (S) are easily shown using\nthis annotation and employing the semantics of get and put.\n\n Proof of (S) for fk+1 : by induction on k . Use the following abbreviations in\nthe proof.\n\nak  wk = {}  nhk = 0\nbk  constant (u  wk ), constant |u| + |wk | + nhk\nck  wk = {}  nhk = k\n\n{a1} f1 {b1\n, from the annotation of f1\nc1}\n, wk , nhk constant in f1\nc1}\n{a1} f1 {bk+1\n, inductive hypothesis\nck }\n{ak } fk {bk\n{ak } fk {bk+1\n, w1 , nh1 constant in fk\nck }\n, join proof rule on (1,2)\n{a1  ak } f1 [] fk {c1  ck }\nc1  ck }, inheritance on (1,2)\n{a1  ak } f1 [] fk {bk+1\nck+1 }\n, ak+1  a1  ak and c1  ck  ck+1\n{ak+1 } fk+1 {bk+1\n\n(1)\n\n(2)\n\nThe terminal property {u = u} fk {u = u ,\n|u| + k = |u |} follows\nfrom (S) as follows. The initial values of (u  wk ) and |u| + |wk | + nhk are,\nrespectively, u and |u |, so deduce that invariant (u  wk ) = u and also\ninvariant |u| + |wk | + nhk = |u |. Given that fk is a program whose terminal\nproperties are being proved, we may apply the invariance rule of Section 4.3\nwith these invariants. So, deduce u = u , |u| + k = |u | as a postcondition\nof fk with the given precondition.\n\n25\n\n\fC.3 Specication and proof of progress property\n\nThe relevant progress property is that if u has more than k elements initially,\nfk terminates eventually. That is, |u | > k 7 nhk = k in fk . Initially |u | =\n|u| + |wk | + nhk , so it is enough to prove |u| + |wk | + nhk > k 7 nhk = k .\nAbbreviate |u| + |wk | + nhk > k by pk and nhk = k by qk so that for all k ,\nk  1, the required progress property is:\n\npk 7 qk in fk\n\n(P)\n\nThe proof of (P) is by induction on k , as shown in Sections C.3.1 and C.3.2.\n\nC.3.1 Progress proof, p1 7 q1 in f1\n\nWe show that p1 en q1 , from which p1 7 q1 follows by applying the basis rule\nof leads-to. We reproduce the annotation of f1 for easy reference.\n\n{w1 = {}, nh1 = 0}\n|u| > 0  hget(x); w1 := w1  {x}i;\n{w1 = {x}, nh1 = 0}\n|u| > 0  hget(y ); w1 := w1  {y}i;\n{w1 = {x, y}, nh1 = 0}\nhput(x  y ); w1 := w1  {x, y}; nh1 := 1i\n{w1 = {}, nh1 = 1}\n\nNext, prove p1 en q1 using the sequencing rule of en, from Section A.3.1.\nIt amounts to showing that if p1 holds before any action then the action is\neectively executed and q1 holds on completion of f1 . As shown in Section C.2\np1 is stable, and from the annotation q1 holds on completion of f1 . Therefore, it\nsuces to show that if p1 holds initially then every action is eectively executed.\nThe put action is always eectively executed. Using the given annotation, the\nverication conditions for the two get actions are shown below in full:\n\nw1 = {}  nh1 = 0  |u| + |w1 | + nh1 > 1  |u| > 0, and\nw1 = {x}  nh1 = 0  |u| + |w1 | + nh1 > 1  |u| > 0\n\nThese are easily proved.\n\nC.3.2 Progress proof, pk+1 7 qk+1 in fk+1\n\nThe main part of the proof uses the observation that every action, put and\nget, reduces a well-founded metric. The metric is the pair (|u| + |wk |, |u|) and\nthe order relation is lexicographic. Clearly, the metric is bounded from below\nbecause each set size is at least 0. The crux of the proof is to show that if\nprogram fk is not terminated there is some action that can be executed, i.e.,\nthere is no deadlock. Henceforth, abbreviate (|u| + |wk |, |u|) by zk and any pair\nof non-negative integers by n.\n\n26\n\n\fFirst, observe that zk can only decrease or remain the same in fk , that is,\nstable zk (cid:22) n in fk , where (cid:22) is the lexicographic ordering. The proof is by\ninduction on k and it follows the same pattern as all other safety proofs. In f1 ,\ninformally, every eective get preserves |u| + |w1 | and decreases |u|, and a put\ndecreases |u| + |w1 |. For the proof in fk+1 : from above, stable z1 (cid:22) n in f1 , and\ninductively stable zk (cid:22) n in fk . Since constant wk in f1 and constant w1 in\nfk , stable zk+1 (cid:22) n in both f1 and fk . Apply the inheritance rule to conclude\nthat stable zk+1 (cid:22) n in fk+1 .\nThe progress proof of pk+1 7 qk+1 in fk+1 is based on two simpler progress\nresults, (P1) and (P2).\n(P1) says that any execution starting from pk+1 re-\nsults in the termination of either f1 or fk . And, (P2) says that once either\nf1 or fk terminates the other component also terminates. The desired result,\npk+1 7 qk+1 , follows by using transitivity on (P1) and (P2).\n\npk+1 7 pk+1  (q1  qk ) in fk+1\npk+1  (q1  qk ) 7 qk+1 in fk+1\n\n(P1)\n(P2)\n\nThe proofs mostly use the derived rules of leads-to from Section A.4. Note\nthat zk+1 includes all the shared variables between f1 and fk , namely u, so that\nthe lifting rule can be used with zk+1 . Also note that\npk+1  (p1  pk ), pk+1  q1  pk , pk+1  qk  p1 and q1  qk  qk+1 . As shown\nin Section C.2 pk is constant, hence stable, and qk is also stable in fk .\n\nProof of (P1) pk+1 7 pk+1  (q1  qk ) in fk+1 : Below all properties are in\nfk+1 . Lifting rule refers to rule (L) of Section A.4.3. We have already shown\np1 7 q1 and, inductively, pk 7 qk .\n\n, Lifting rule on p1 7 q1 in f1\np1  zk+1 = n 7 q1  zk+1 6= n\npk  zk+1 = n 7 qk  zk+1 6= n\n, Lifting rule on pk 7 qk in fk\n(p1  pk )  zk+1 = n 7 (q1  qk )  zk+1 6= n\n, disjunction\n(p1  pk )  zk+1 = n 7 (q1  qk )  zk+1  n\n, (PSP) with stable zk+1 (cid:22) n\npk+1  (p1  pk )  zk+1 = n 7 pk+1  (q1  qk )  pk+1  zk+1  n\n, conjunction with stable pk+1\npk+1  zk+1 = n 7 pk+1  zk+1  n  pk+1  (q1  qk )\n, pk+1  (p1  pk )\n, induction rule of leads-to\n\npk+1 7 pk+1  (q1  qk )\n\n27\n\n\fProof of (P2) pk+1  (q1  qk ) 7 qk+1 in fk+1 : Below all properties are in\nfk+1 .\n\n, Lifting rule on p1 7 q1 in f1\np1  zk+1 = n 7 q1  zk+1 6= n\n, conjunction with stable zk+1 (cid:22) n\np1  zk+1 = n 7 q1  zk+1  n\nqk  p1  zk+1 = n 7 qk  q1  qk  zk+1  n\n, conjunction with stable qk\npk+1  qk  p1  zk+1 = n 7 pk+1  qk  q1  pk+1  qk  zk+1  n\n, conjunction with stable pk+1\npk+1  qk  zk+1 = n 7 pk+1  qk  zk+1  n  pk+1  qk  q1\n, pk+1  qk  p1\n, induction rule of leads-to\n, rhs weakening\n, similarly\n, disjunction and q1  qk  qk+1\n\npk+1  qk 7 pk+1  q1  qk\npk+1  qk 7 q1  qk\npk+1  q1 7 q1  qk\npk+1  (q1  qk ) 7 qk+1\n\nD References\n\nReferences\n\n[1] Robert D. Blumofe. Executing Multithreaded Programs Eciently. PhD\nthesis, Massachusetts Institute Of Technology, September 1995.\n\n[2] K. Mani Chandy and Jayadev Misra. Paral lel Program Design: A Foun-\ndation. Addison-Wesley, 1988.\n\n[3] Ernie Cohen, Michal Moskal, Wolfram Schulte, and Stephan Tobies. Local\nverication of global invariants in concurrent programs. In Computer Aided\nVerication, 22nd International Conference, CAV 2010, Edinburgh, UK,\nJuly 15-19, 2010. Proceedings, pages 480494, 2010.\n\n[4] Edsger W. Dijkstra. Guarded commands, nondeterminacy, and the formal\nderivation of programs. Communications of the ACM, 8:453457, 1975.\n\nIn F. Genuys, editor,\n[5] E.W. Dijkstra. Cooperating sequential processes.\nProgramming Languages, pages 43112. Academic Press, 1968.\n\n[6] Thomas Dinsdale-Young, Mike Dodds, Philippa Gardner, Matthew J.\nParkinson, and Viktor Vafeiadis. Concurrent abstract predicates. In Pro-\nceedings of the 24th European Conference on Object-oriented Programming,\nECOOP10, pages 504528, Berlin, Heidelberg, 2010. Springer-Verlag.\n\n[7] C.A.R. Hoare. An axiomatic basis for computer programming. Communi-\ncations of the ACM, 12:576580,583, 1969.\n\n[8] C. B. Jones. Specication and design of (parallel) programs. In Proceedings\nof IFIP83, pages 321332. North-Holland, 1983.\n\n28\n\n\f[9] C. B. Jones. Tentative steps toward a development method for interfering\nprograms. TOPLAS, 5(4):596619, 1983.\n\n[10] Leslie Lamport. Proving the correctness of multiprocess programs. IEEE\nTrans. on Software Engineering, SE-3(2):125143, Mar 1977.\n\n[11] Jayadev Misra. A Discipline of Multiprogramming. Monographs in Com-\nputer Science. Springer-Verlag New York Inc., New York, 2001.\n\n[12] Jayadev Misra and K. Mani Chandy. Proofs of networks of processes. IEEE,\nSE,7(4):417426, July 1981.\n\n[13] S. Owicki and D. Gries. Verifying properties of parallel programs: an\naxiomatic approach. Communications of the ACM, 19:279285, May 1976.\n\n[14] S. Owicki and Leslie Lamport. Proving liveness properties of concurrent\nprograms. ACM Transactions on Programming Languages and Systems,\n4(3):455495, July 1982.\n\n[15] G.L. Peterson. Myths about the mutual exclusion problem. Information\nProcessing Letters, 12(3):115116, June 1981.\n\n[16] John C. Reynolds. Separation logic: A logic for shared mutable data struc-\ntures.\nIn Proceedings of the 17th Annual IEEE Symposium on Logic in\nComputer Science, IEEE Computer Society, Washington, DC, USA, 2002.\n\n29\n\n\f", 
        "tag": "Programming Languages", 
        "link": "https://arxiv.org/list/cs.PL/new"
    }, 
    {
        "text": "Landmark Guided Probabilistic Roadmap Queries\n\nBrian Padena,b , Yannik Nagera,b , and Emilio Frazzolia\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nO\nR\n.\ns\nc\n[\n \n \n1\nv\n6\n8\n8\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstract A landmark based heuristic is investigated for\nreducing query phase run-time of the probabilistic roadmap\n(PRM) motion planning method. The heuristic is generated\nby storing minimum spanning trees from a small number\nof vertices within the PRM graph and using these trees to\napproximate the cost of a shortest path between any two vertices\nof the graph. The intermediate step of preprocessing the graph\nincreases the time and memory requirements of the classical\nmotion planning technique in exchange for speeding up indi-\nvidual queries making the method advantageous in multi-query\napplications. This paper investigates these trade-offs on PRM\ngraphs constructed in randomized environments as well as a\npractical manipulator simulation. We conclude that the method\nis preferable to Dijkstras algorithm or the A algorithm with\nconventional heuristics in multi-query applications.\n\nI . IN TRODUC T ION\nThe probabilistic roadmap (PRM) [1] is a cornerstone of\nrobot motion planning. It is widely used in practice or as\nthe foundation for more complex planning algorithms. The\nmethod is divided into two phases: the PRM graph is rst\nconstructed followed by, potentially multiple, shortest path\nqueries on this graph to solve motion planning problems.\nFor a single motion planning query, a feasibility checking\nsubroutine executed repeatedly during PRM construction\ndominates run-time. However, once the PRM is constructed\nit can be reused for multiple motion planning queries or\nmodied slightly according to minor changes in the envi-\nronment. Applicability to multi-query problems is one of the\nadvantages of the PRM over tree-based planners such as\nRapidly exploring Random Trees (RRT) [2] and Expansive\nSpace Trees (EST) [3] which are tailored to single-query\nproblems.\nRecent efforts have focused on ne tuning various aspects\nof PRM-based motion planning for real-time applications.\nHighly parallelized feasibility checking using FPGAs was\nrecently developed in [4] to alleviate this computational bot-\ntleneck during the construction phase. The sparse roadmap\nspanner was introduced in [5] to reduce memory required to\nstore the PRM and speed up the query phase by keeping\nonly a sparse subgraph with near-optimality properties.\nIn this paper we examine the effectiveness of a landmark\nbased admissible heuristic for reducing the running time of\nthe query phase of the PRM. The landmark heuristic was\noriginally developed for vehicle routing problems in road\nnetworks [6] where many shortest path queries are solved\non a single graph. In theory, any amount of time spent\npreprocessing the graph is negligible in comparison to the\n\na The authors are with The Institute for Dynamic Systems and Control\nat ETH (email: {padenb,ynager,emilio.frazzoli}@ethz.ch).\nb The rst two authors made equal contributions to this work.\n\ntime spent solving shortest path queries if sufciently many\nqueries must be solved. This observation suggests solving\nthe all-pairs shortest path problem in order to answer each\nrouting problem in constant time with respect to graph size.\nHowever, the memory required to store a solution to the\nall-pairs shortest is prohibitive for large road networks. The\nlandmark heuristic provides a trade-off between memory\nrequirements and query times by solving a small number of\nsingle-source shortest path problems and using their solutions\nto construct an effective heuristic for a particular graph.\nThis investigation is inspired by the similarities between\nroad networks and the PRM; multiple path queries are solved\non both graphs and both graphs are, in practice, too large to\nstore an all pairs shortest path solution in memory.\nA useful feature of the landmark heuristic is that it can be\nused together with the sparse roadmap spanner and FPGA-\nbased collision checking for a compounded speedup over a\nstandard PRM implementation. Based on the results pre-\nsented in this paper, we conclude that the landmark heuristic\nis effective on PRM graphs; solving shortest path queries\nas much as 20 times faster than Dijkstras algorithm and\ntwice as fast as the Euclidean distance-based heuristic in\ncluttered environments. The downside to the approach is that\nconstructing the heuristic requires preprocessing the graph\nwhich adds to the computation time required before the PRM\ncan be used for motion planning queries.\nAn overview of the motion planning problem is presented\nin Section II, followed by a review of the build and query\nphases of the PRM method. Section III introduces the land-\nmark heuristic, discusses its admissibility and the complexity\nof its construction, and illustrates its utility with a simple\nshortest path problem. However, to better understand the\neffectiveness of the landmark heuristic in general, we con-\nstruct randomized environments with a quantiable degree\nof clutter and run numerous motion planning queries on\nthese environments to obtain the average case performance.\nThe environment construction and experimental results are\npresented in Section IV. In Section V we evaluate the\nlandmark heuristic on a simulation of the Kinova Jaco robotic\nmanipulator and nd the landmark heuristic to be effective on\nrealistic robot models. Lastly, we conclude with a discussion\nof our experimental observations in Section VI.\n\nI I . MOT ION P LANN ING PROB L EM\nThe following optimal motion planning problem will be\naddressed: Let Xfree be an open, bounded subset of Rd , and\n the set of continuous curves from [0, 1] to Rn . Then let\nfree be the subset of  whose image is contained in Xfree .\nThe cost objective is a function c :   [0, ) that assigns a\n\n\fcost to each curve in Rd . The cost function must be additive\nin the sense that if two curves 1,2   satisfy 1 ([0, 1]) \n2 ([0, 1]) then c(1 )  c(2 ).\nAn individual motion planning query on Xfree consists of\nnding a curve    free from an initial state x0  Xfree to\na goal state xg  Xfree . That is,   (0) = x0 and   (1) = xg .\nThe subset of curves in free which satisfy these additional\nendpoint constraints are denoted sol . In addition to nding a\ncurve in sol , we would like a curve   which approximately\nminimizes the cost objective,\n\n(1)\n\n(c( )) + ,\n\nc(  ) < inf\nsol\nfor a xed  > 0. An approximate minimization is often used\nfor two reasons: the rst is that the problem may not admit\na minimum, and second, without further assumptions on the\ncost objective and geometry of Xfree there are no practical\ntechniques available for obtaining exact solutions when they\nexist.\n\nA. Probabilistic Roadmaps\n\nThe set free has innite dimension so the conventional\napproach to obtaining approximate solutions to motion plan-\nning problems is to construct a graph on Xfree whose vertices\nare points in Xfree . To avoid confusion with curves on Xfree ,\na path is a sequence of vertices in a graph {xi } such that\n(xi , xi+1 ) is an edge in the graph. Curves in free are\napproximated using paths in the graph by associating each\nedge of the graph with the line segment between the two\nvertices making up that edge. The PRM method falls into\nthis category of approximations to free .\nThe PRM method [7] is a popular variation of the\nPRM because it generates a sparse graph with the following\nproperty: if x0 and xg belong to a connected subset of Xfree ,\nthen for any xed  > 0, the probability that the PRM graph\ncontains a curve   free satisfying\n\n(c( )) + ,\nc( ) < inf\nsol\n(cid:107) (0)  x0(cid:107) < ,\n(cid:107) (1)  xg (cid:107) < ,\n\n(2)\n\nconverges to 1 as the number of vertices is increased.\n\nB. Graph Construction Phase\n\nThe construction phase of the PRM method is summa-\nrized in Algorithm 1. The nearest(r, x, VPRM ) subroutine\nreturns the points v  VPRM \\ {x} such that (cid:107)x v(cid:107) < r . The\nsubroutine sample(Xfree ) in Algorithm 1 returns a randomly\nsampled point from the uniform distribution supported on\nXfree . The subroutine collisionFree(x, v) returns true if\nthe line segment connecting x to v is an element of free\nand false otherwise. In reference to line 2 of Algorithm 1,\n is the Legesgue measure on Rd , and B1 (0) is the ball of\nradius 1 centered at 0.\n\n(cid:17) (cid:16) log(n)\n(cid:17)(cid:17)1/d\n(cid:16)\n(cid:16) (Xfree )\nAlgorithm 1 PRM\n1: VPRM  ; EPRM  \n2: r =\n(2 + 2/d)\nn\n(B1 (0))\n3: for i = 1, . . . , n\nVPRM  VPRM  {sample(Xfree )}\n4:\n5: for x  VPRM\nU  nearest(r, x, VPRM )\n6:\nfor v  U \\ {x}\n7:\nif collisionFree(x, v)\n8:\nEPRM = EPRM  {(x, v)}\n9:\n10: return (VPRM , EPRM )\n\nC. Motion Planning Query Phase\nAfter construction, paths in the graph (VPRM , EPRM ) can\nbe used to solve motion planning queries. One subtlety is that\nthe probability of x0 and xg being present in the PRM graph\nis zero. There are a number of practical ways to resolve this\nissue, but to keep the exposition as concise as possible we\nwill simply select the nearest vertex x0  VPRM to x0 and\nxg  VPRM to xg as an approximation in light of (2).\nOnce initial and nal states x0 and xg are selected, the\nmotion planning query reduces to a shortest path problem\non the PRM graph with edge weights determined by the\ncost of the line segments between vertices of the graph.\nAlgorithm 2 summarizes the A algorithm for nding a\nshortest path in the PRM graph from x0 to xg . The function\nparent : VPRM  VPRM  {NULL} is used to keep track of\nthe shortest path from x0 to each vertex examined by the\nalgorithm. Initially, parent maps all vertices of the graph\nto NULL, but is redened in each iteration of the algorithm as\nshorter paths from x0 to vertices in the graph are found. The\nfunction label : VPRM  [0, ] maps each vertex to the\ncost of the shortest known path reaching that vertex from x0 .\nThe function label initially maps all vertices to , but is\nupdated at each iteration with the cost of newly discovered\npaths.\nA set Q of vertices represents a priority queue. The\ndistinguishing feature of the A algorithm is the ordering of\nvertices in the priority queue according to the labeled cost of\nthe vertex plus a heuristic estimate of the remaining cost to\nreach the goal h : VPRM  [0, ]. The subroutine pop(Q)\nreturns a vertex x  Q such that\nx  argmin\n{label( ) + h( )}\nQ\nThe heuristic h is called admissible if it never overestimates\nthe cost to reach the goal from a particular vertex. The A\nalgorithm is guaranteed to return the shortest path from x0\nif the heuristic in equation (3) is admissible.\nThe pathToRoot subroutine returns the sequence of\nvertices {vi}i=1,...,N , terminating at vN = x0 , generated by\nthe recursion\n\n(3)\n\nvi+1 = parent(vi ),\nv1 = xg ,\nIf pathToRoot is evaluated in Algorithm 2, then its output\nis a shortest path from x0 to xg .\n\n(4)\n\n\fFor graphs with nonnegative edge-weights the heuristic\nh(x) = 0 for all x  VPRM is clearly admissible. In this\nspecial case, the A algorithm is equivalent to Dijkstras\nalgorithm. However, the more closely h underestimates the\noptimal cost from each vertex to xg the fewer iterations\nrequired by the A algorithm to nd the shortest path from\nx0 to xg . Therefore, it is desirable to use a heuristic which\nestimates the optimal cost to reach the goal as closely as\npossible.\nAlgorithm 2 The A algorithm\n1: Q  x0 ;\n2: label( x0 )  0\n3: while Q (cid:54)= \nv  pop(Q)\n4:\nif v = xg\n5:\nreturn pathToRoot(xg )\n6:\nS  neighbors(v)\n7:\nfor w  S\n8:\nif label(v) + cost(v , w) < label(w)\n9:\nlabel(w)  label(v) + cost(v , w)\n10:\nparent(w)  v\n11:\nQ  Q  {w}\n12:\n13: return NO SOLUTION\n\n(5)\n\nc( ) =\n\nWhen the cost functional is simply the length of the path,\nas in equation (5), the canonical heuristic is the Euclidean\ndistance between x0 to xg which is the length of the optimal\n(cid:90) 1\npath in the absence of obstacles.\n(cid:107) (cid:48) (t)(cid:107)2 dt\n0\nThe Euclidean distance heuristic is specic to shortest path\nobjectives, and may not be admissible for cost functionals\nother than (5).\nI I I . TH E LANDMARK H EUR I ST IC\nThe landmark heuristic is tailored to a particular graph and\nrequires preprocessing the graph before it can be used in the\nA algorithm. The resulting heuristic is admissible regardless\nof cost functional and environment making it a very general\napproach to obtaining an admissible heuristic.\nThe idea behind the landmark heuristic is as follows: Let\nd : VPRM  VPRM  [0, ] be the function which returns\nthe cost of a shortest path from one vertex of the graph\nto another; taking the value  if no path exists. It follows\nfrom the denition that d satises the triangle inequality.\nConsider a vertex xl  VPRM that will represent a landmark.\nRearranging the triangle inequality with xl and xg yields\n|d(x, xl )  d(xl , xg )|  d(x, xg ) x  VPRM .\n(6)\nThus, the left hand side of (6) is a lower bound on cost of\nthe shortest path to xg . While computing d explicitly would\nrequire solving the all-pairs shortest path problem, only the\nsolution to the single-source shortest path problem from xl\nis required to evaluate (6).\nWhen the lower bound in (6) is evaluated at a vertex\nx that lies on or near to the shortest path from xl to xg\n\nFig. 1. Geometric illustration showing how the triangle inequality can be\nrearranged to obtain a lower bound on the minimum cost path from x to\nxg .\n\nor vice-versa it provides a surprisingly close estimate of\nthe minimum cost path from x to xg . Figure 1 illustrates\nthis lower bound. However, obtaining an effective heuristic\nfor all origin-destination pairs requires having a collection\nof landmarks Vl  VPRM . The landmark heuristic then\nleverages (6) for each landmark:\n{|d(x, xl )  d(xl , xg )|}.\n\nh(x, xg ) = max\nxlVl\nTo simplify the analysis presented in this paper, each\nlandmark is an i.i.d. random variable selected from the\nuniform distribution on VPRM . However, other selection rules\ncan be used to improve the heuristic.\n\n(7)\n\nA. Complexity of the Landmark Heuristic\nGenerating the function d(, xl ) for an individual land-\nmark requires solving a single-source shortest path problem\nwhich can be accomplished with Dijkstras algorithm in\nO(|VPRM | log(|VPRM |)) time1 where |  | denotes the car-\ndinality of a set. Thus, the time complexity of constructing\nthe heuristic is in O(|Vl |  |VPRM | log(|VPRM |)). From this\nobservation it is clear that this heuristic is only useful in\ninstances where the number of motion planning queries that\nwill be evaluated on the PRM graph will be greater than |Vl |\nsince this many shortest path queries can be solved in the\ntime required to construct the heuristic. Then evaluating the\nlandmark heuristic (7) requires looking up the optimal cost\nto a landmark 2  |Vl | times so the complexity of (7) is linear\nin the number of landmarks.\nStoring the cost of the shortest path to each vertex from\na landmark for use in (7) requires O(|VPRM |) memory per\nlandmark for a total memory requirement in O(|Vl |  |VPRM |).\nThe next question is how many landmarks should be used?\nA natural choice is to select a xed fraction of the PRM\nvertices to be landmarks. That is, |Vl | =   |VPRM | for\nsome constant . This results in O(|VPRM |2 ) space required\nto store the heuristics lookup tables in memory. However,\nwith just than 16 landmarks, the landmark heuristic has been\nobserved to speed up routing queries by a factor of 9 to 16\non city to continent-scale road networks. On a PRM with a\nshortest path objective, this observation can be made precise\nas stated in the next result.\n\n1 This assumes the PRM graph is constructed using Algorithm 1 which\nhas O(|VPRM | log(|VPRM |) edges [7].\n\n\fFig. 2. Shortest path from x0 (red circle) to xg (green circle) on a PRM\ngraph. Paths are computed with the Euclidean distance as a heuristic (left)\nand the landmark-based heuristic (right). Colored markers represent vertices\nexamined in each search with color indicating the relative cost to reach that\nvertex.\nLemma 1. If the number of landmarks relative to the number\nof vertices is given by |Vl | =   |VPRM | for   (0, 1], then\n(8)\nh(x, xg ) = d(x, xg ),\nlim\n|VPRM |\n\nalmost surely.\n\nThe proof can be found in the appendix. With increasing\ngraph size and an arbitrarily small fraction of vertices as-\nsigned to landmarks, the landmark heuristic will converge to\nthe solution of the all-pairs shortest path problem.\n\nB. Demonstration of the Landmark Heuristic\nTo demonstrate the advantages of using the landmark\nheuristic, it was compared with Dijkstras algorithm and A\nwith the Euclidean distance heuristic in a bug-trap environ-\nment. A PRM was constructed in the bug trap environment\naccording to Algorithm 1 with a density of 1000 vertices\nper unit area for a total of 69, 272 vertices. The Landmark\nheuristic was then constructed with 100 landmarks (0.14%\nof vertices) obtained by randomly sampling from the vertices\nof the graph.\nFigure 2 shows the environment and vertices expanded by\nthe A algorithm using Euclidean distance as a heuristic and\nthe landmark heuristic. The A algorithm with Euclidean\ndistance heuristic required 58, 145 iterations and 351ms to\nnd the shortest path; a marginal difference in performance\nin comparison to the 69, 180 iterations and 334ms required\nby Dijkstras algorithm. In contrast, the A algorithm with\nthe landmark heuristic required only 3, 338 iterations and\n49ms to nd the shortest path.\nThe results of this demo can be reproduced with the\nimplementation of the landmark heuristic available in [8].\n\nIV. EVALUAT ION IN RANDOM I Z ED ENV IRONM EN T S\nEnvironments with randomly placed obstacles provides a\nsimple and easily reproducible benchmark for motion plan-\nning algorithms [9], [10]. In this paper, the degree of clutter\nin these randomly generated environments is quantied as\nthe probability of the line segment connecting two randomly\nsampled points being contained in Xfree .\n\nFig. 3. A PRM graph on a randomized environment with P(clear) =\n0.5.\nA. Environment Generation\nA Poisson forest with intensity  of circular obstacles with\nradius r is used as a random environment This is simulated\nover a sample window S = [1, 1]2 by sampling the number\nof obstacles N from the Poisson distribution\n\nfN (n) =\n\n,\n\n(9)\n\n((S ))n e(S )\nn!\nand then placing these obstacles randomly by sampling\nfrom uniform distribution on S . The subset of S occupied\nby the circular obstacles is denoted Xobs . Then we select\nXfree = [0.5, 0.5]2 \\Xobs . Embedding Xfree in S simplies\nsubsequent calculations by eliminating boundary effects of\nthe sample window.\nLet Z1 and Z2 be independent random variables with the\nuniform distribution on Xfree , and let clear denote the\nevent that the line segment connecting Z1 and Z2 remains\nin Xfree .\nThe next derivation relates the obstacle intensity  to the\nmarginal probability P(clear). Observe that a line segment\nintersects a circular obstacle of radius r if and only if the\ncircle of radius r swept along this line segment contains the\nobstacle center. If the obstacle is placed by sampling from\nthe uniform distribution on S , the probability of collision is\nsimply the ratio of the swept area of the circle along the\nline segment and the area of S . Thus, conditioned on the\nnumber of obstacles N and the points Z1 , Z2 , the probability\n(cid:18) (S )  r2  2r(cid:107)Z1  Z2(cid:107)\n(cid:19)N\nof clear is\nP(clear|N , Z1 , Z2 ) =\n.\n(S )\n(10)\nThen the marginal probability P(clear) for a given obstacle\nintensity  can be calculated by combining (9) and (10) to\nobtain\n(cid:82)\n(cid:82)\n(cid:80)\nP (clear) =\nXfree\nXfree\nnN\n\nP (clear|n,z1 ,z2 )fN (n)\n((Xfree ))2\n\n(dz1 )(dz2 ).\n(11)\n\n\fIn all of the numerical experiments of the next section\nrandom environments with obstacle radius r = 0.05 were\nused.\n\nB. Numerical Experiments\nExperiments were designed to evaluate how the effective-\nness of the landmark heuristic varies with with the parameter\nP(clear) and to validate Lemma 1. To facilitate obtaining\nthe results in a reasonable time, experiments were run in\nparallel on the central high-performance cluster EULER\n(Erweiterbarer, Umweltfreundlicher, Leistungsfhiger ETH-\nRechner) of ETH Zrich. Each compute node consists of\ntwo 12-Core Intel Xeon E5-2680 processors with clock rates\nvarying between 2.5-3.5 GHz.\nIn the rst set of trials a single random environment\nwas sampled with P(clear) = 0.05. Three PRM graphs\nwere constructed on this environment with 40,000, 60,000\nand 80,000 vertices. On each PRM, 700 landmark heuris-\ntics were constructed, 100 each for landmark quantities\n|Vl |  {10, 30, 50, 70, 90, 110, 130}. Then for each landmark\nheuristic, a random shortest path query is solved using A\nwith the landmark heuristic.\nIn the second set of trials, 20 logarithmically spaced\nvalues for the parameter P(clear) from 0.01 to 1.0 were\nselected. For each of these values 100 random environments\nwere generated according to the construction outlined in\nSection IV-A. A PRM with 100,000 vertices per unit area\nwas constructed on each environment with 100, 000 ver-\ntices per unit area. Then for each PRM, the 9 landmark\nheuristics were constructed with landmark quantities |Vl | \n{10, 30, 50, 70, 90, 110, 130, 150, 170}. Finally, for each of\nthe 9 landmark heuristics, 100 shortest path queries were\nevaluated on each PRM using A .\n\nC. Results\nThe rst experiment, summarized in Figure 4, revealed\nhow the effectiveness of the landmark heuristic varied with\nthe fraction of vertices assigned to landmarks as well as\nwith varying graph sizes. We observed a rapid reduction in\niterations required to nd a solution relative to Dijkstras\nalgorithm with just 0.2% of vertices assigned to landmarks.\nSecondly, the number of iterations required to nd a solution\nwith A relative to that of Dijkstras algorithm decreased\nwith increasing graph size. This validates Lemma 1 since\nthe number of iterations required by A decreases with an\nimproving estimate of the optimal cost to reach the goal.\nIn the second experiment we observed that the effective-\nness of the Euclidean distance heuristic rapidly diminishes\nwith increasing clutter, while the the landmark heuristic was\nmuch less sensitive to P(clear). This is summarized in\nFigure 5 where the landmark heuristic reduced the number of\niterations required to nd a solution by a factor greater than\n20 in highly cluttered environments whereas the Euclidean\ndistance heuristic reduced the number of iterations by less\nthan a factor of 3.\nThis experiment also showed the diminishing returns of\nincreasing the number of landmarks in terms of iteration\n\nFig. 4.\nEffectiveness of the landmark heuristic increases with both the\nfraction of vertices assigned to landmarks and the number of vertices in the\ngraph.\n\nFig. 5.\nEffectiveness of the landmark heuristic in comparison to the\nEuclidean distance heuristic with varying degrees of clutter quantied by\nP(clear).\ntime. Recall that evaluating the landmark heuristic in (7)\nrequired checking the triangle equality for each landmark.\nIn Figure 6 the average running time of the A algorithm\nwith the landmark heuristic reaches a minimum with with\n50 landmarks.\n\n0%0.1%0.2%0.3%0.4%0.5%%Landmarks0.000.050.100.150.200.25AiterationsDijkstraiterations40000vertices60000vertices80000vertices102101100P(clear)0.000.050.100.150.200.250.30AiterationsDijkstraiterationsA10LandmarksA30LandmarksA50LandmarksA90LandmarksA130LandmarksA170LandmarksAEuclidian\fobjectives were considered for this problem, a shortest\npath objective and a minimum mechanical work objective.\nMotivation for using the shortest path objective is that the\nEuclidean distance is available as an admissible heuristic. On\nthe other hand, minimizing the mechanical work required\nto execute the motion is a more natural objective that is\nlikely similar to the motion a human would use for the\ntask. The drawback to the latter objective is that there is\nno obvious heuristic to inform the A search. Since the\nlandmark heuristic is admissible regardless of the objective\nit was applicable for this objective.\nA 100, 000 vertex PRM was constructed followed by the\nconstruction of a landmark heuristic with 50 landmarks. A\nminimum work path was computed in 48 iterations and\n7.5ms using the landmark heuristic while Dijkstras algo-\nrithm required 25, 207 iterations and 209.4ms. A shortest\npath was computed in 36 iterations and 5.6ms using the\nlandmark heuristic while using the Euclidean distance re-\nquired 1, 238 iterations and 14.4ms. Figure 7 illustrates the\nminimum energy motion that was computed.\n\nV I . CONCLU S ION\nThe landmark heuristic is well known in the vehicle\nrouting literature where it has been shown to reduce shortest\npath query times by a factor of 9 to 16 on city to continent-\nscale road networks. Multi-query applications of the PRM in\nrobot motion planning have striking similarities with vehicle\nrouting problems in road networks in that shortest path\nqueries are evaluated repeatedly on a large graph. The goal\nof this investigation was to evaluate the effectiveness of the\nlandmark heuristic in robotic motion planning applications.\nSince the heuristic is based on preprocessing the PRM\ngraph, our hypothesis was that its effectiveness would be\nindependent of how densely cluttered the environment was\na useful feature for complex planning tasks.\nTo make this evaluation, we constructed a randomized\nenvironment parameterized by the probability that the line\nsegment between two random points did not intersect obsta-\ncles. The average case relative performance of the landmark\nheuristic relative to the Euclidean distance heuristic was then\nmeasured through numerous randomized trials. Additionally,\nthe performance of the landmark heuristic was evaluated on\na manipulator arm model in a realistic planning scenario.\nThe landmark heuristic was empirically observed to be\nless sensitive to environment clutter than the Euclidean\ndistance heuristic. For the range of parameters evaluated,\nthe query times were reduced by a factor of 5 to 20 in\ncomparison to Dijkstras algorithm. Secondly, a theoretical\nanalysis showed that, with a xed fraction of PRM vertices\nassigned to be landmarks, the landmark heuristic converges\nto the optimal cost between any origin-destination pair with\nincreasing graph size. This analysis was then validated in our\nexperimental results.\nThe landmark heuristic is an effective heuristic for query-\ning large PRM graphs. In particular, it is more effective than\nthe Euclidean distance heuristic in all but nearly obstacle\nfree problem instances. However,\nthe preprocessing time\n\nFig. 6. Running time of shortest path queries using the landmark heuristic\nand Euclidean distance heuristic normalized by the running time using\nDijkstras algorithm.\n\nFig. 7.\nStill frames of the minimum mechanical work motion generated\nby the Jaco robotic arm when using a PRM with 100000 vertices and A\nwith the landmark heuristic.\nV. ROBOT MAN I PU LATOR EXAM PL E\n\nTo demonstrate suitability of the landmark-based heuristic\nfor realistic manipulator models, we use a model of the six\ndegree of freedom Jaco manipulator by Kinova Robotics.\nTo simulate a complex planning task the arm must nd a\ncollision free motion through a window terminating with\nthe end effector near the ground to simulate reaching for\nan object.\nThe landmark heuristic was implemented in the Open\nMotion Planning Library (OMPL) [11] and the problem was\nsolved using the MoveIt [12] software tool. Two planning\n\n102101100P(clear)0.000.050.100.150.200.250.300.35AquerytimeDijkstraquerytimeA10LandmarksA30LandmarksA50LandmarksA90LandmarksA130LandmarksA170LandmarksAEuclidian\frequired to construct the heuristic makes it only suitable\nfor multi-query applications where the heuristic will be used\nrepeatedly on the same graph. A valuable direction for future\ninvestigation would be an efcient update to the heuristic\nwhen small changes are made to the PRM as a result of\nchanges in the workspace.\n\nfor constants ,   (0, 1) that depend only on the dimension\nd of Xfree . One can readily verify that\nthis expression\nconverges to zero as |VPRM |  . Thus, the probability\nBr/2 (zi )(cid:1) has a landmark as a neighbor almost surely\n(cid:0)(cid:83)\nthat there is at least one landmark in each Br/2 (zi ) for\ni  K converges 1. It follows that every vertex x  VPRM \nas |VPRM |  . Therefore, for at least one landmark l , the\niK\noptimal cost from x to l satises d(x, l )  r . Thus,\n{|d(x, xl )  d(xl , xg )|}\nh(x, xg ) = max\nxlVl\n d(x, l )  d(xg , l )\nExpanding d(x, l ) with the triangle inequality between x,\nl , and xg yields\nh(x, xg )  d(x, xg )  d(xg , xn )  d(xg , xn )\n(18)\n d(x, xg )  2r\nCombining (6) and (17) we have d(x, xg )  2r  h(x, xg ) \nd(x, xg ), and since r  0 as |VPRM |   we obtain\non (cid:83)\nh(x, xg ) = d(x, xg ),\nlim\n|VPRM |\nBr/2 (zi ). The desired result (8) then follows in light\niK\nof (14).\n\n(17)\n\n(19)\n\nR E FER ENC E S\n[1] L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Overmars, Prob-\nabilistic roadmaps for path planning in high-dimensional conguration\nspaces, IEEE transactions on Robotics and Automation, vol. 12, no. 4,\npp. 566580, 1996.\n[2] S. M. LaValle and J. J. Kuffner, Randomized kinodynamic planning,\nThe International Journal of Robotics Research, vol. 20, no. 5,\npp. 378400, 2001.\n[3] D. Hsu, J.-C. Latombe, and R. Motwani, Path planning in expansive\nconguration spaces, in Robotics and Automation, 1997. Proceed-\nings., 1997 IEEE International Conference on, vol. 3, pp. 27192726,\nIEEE, 1997.\n[4] S. Murray, W. Floyd-Jones, Y. Qi, D. Sorin, and G. Konidaris, Robot\nmotion planning on a chip, in Robotics: Science and Systems, 2016.\n[5] J. D. Marble and K. E. Bekris, Asymptotically near-optimal planning\nwith probabilistic roadmap spanners, IEEE Transactions on Robotics,\nvol. 29, no. 2, pp. 432444, 2013.\n[6] A. V. Goldberg and C. Harrelson, Computing the shortest path: A\nsearch meets graph theory, in Proceedings of the sixteenth annual\nACM-SIAM symposium on Discrete algorithms, pp. 156165, Society\nfor Industrial and Applied Mathematics, 2005.\n[7] S. Karaman and E. Frazzoli, Sampling-based algorithms for optimal\nmotion planning, The International Journal of Robotics Research,\nvol. 30, no. 7, pp. 846894, 2011.\n[8] B. Paden, Y. Nager, and E. Frazzoli, Landmark guided probabilistic\nroadmap queries, 2017. Available at: https://github.com/\nbapaden/Landmark_Guided_PRM/releases/tag/v0.\n[9] J. D. Gammell, S. S. Srinivasa, and T. D. Barfoot, Batch informed\ntrees (bit*): Sampling-based optimal planning via the heuristically\nguided search of implicit random geometric graphs, in 2015 IEEE In-\nternational Conference on Robotics and Automation (ICRA), pp. 3067\n3074, IEEE, 2015.\n[10] S. Karaman and E. Frazzoli, High-speed ight in an ergodic forest, in\nRobotics and Automation (ICRA), 2012 IEEE International Conference\non, pp. 28992906, IEEE, 2012.\n[11] I. A. Sucan, M. Moll, and L. E. Kavraki, The Open Motion Planning\nLibrary, IEEE Robotics & Automation Magazine, vol. 19, pp. 7282,\nDecember 2012. http://ompl.kavrakilab.org.\n[12] I. A. Sucan and S. Chitta, Moveit!, 2016.\n\n(14)\n\n(15)\n\nA P P END IX\nThe proof of Lemma 1 requires some additional notation.\nThe symbol  denotes the Lebesgue measure on Rd so that\nthe uniform probability measure of a measurable subset S\nof Xfree is given by (S )/(Xfree ). Since each landmark\nis an i.i.d. random variable with the uniform distribution on\n} can be viewed as\nXfree , the set of landmarks {l1 , ..., l|Vl |\na random variable on the product space, denoted X |Vl |\nfree . The\nprobability of li  Si for subsets Si of Xfree is given by the\nproduct measure m:\n(cid:1) = m (cid:0)S1  ...  S|Vl |\n(cid:1)\nP (cid:0){l1 , ...l|Vl |\n= (cid:81)|Vl |\n}  S1  ...  S|Vl |\n(Si )\ni=1\n(Xfree )\n(12)\nNext, an \u0001-net on Xfree is a subset {z1 , ..., zk } of Xfree\n1) Xfree  (cid:83)M\nsuch that\ni=1 B\u0001 (zi ),\n2) B\u0001/2 (zi )  B\u0001/2 (zj ) =  i (cid:54)= j .\nBased on these two properties it is clear that the number of\npoints k making up an \u0001-net on Xfree is bounded by\n k  (Xfree )\n(Xfree )\n(13)\n(B\u0001 ())\n(B\u0001/2 ())\n.\nObserve that not every B\u0001 (zi )  Xfree is convex since it\nmay intersect the boundary of Xfree . The the index set K \n{1, ..., k} will identify open balls of the \u0001-net which have a\nconvex intersection with Xfree . As the \u0001-net becomes ner,\na greater fraction of points will lie on the interior of Xfree\n(cid:91)\nwith a distance to the boundary greater than \u0001 so\nlim\n\u00010\niK\nProof (Lemma 1). Consider the \u0001-net described above with\n\u0001 = r/2, half the connection radius of the PRM. Note that\nevery vertex in Br/2 (zi ) is connected by a line segment for\ni  K.\nthe landmarks {l1 , ..., l|Vl |\n} \nThe probability that\n(cid:17)\n(cid:16)\n(l1 , ..., l|Vl | )  (cid:83)\nBr/2 (zi ) =  for some i  K can be written as\n (cid:80)\nr/2 (zi ))|Vl |\n(B c\nP\n(cid:17)|Vl |\n(Br/4 )  (cid:16)\niK\nr/2 (zi ))|Vl | )\nm((B c\niK\n (Xfree )\n1  (Br/2 (zi ))\n(Xfree )\nBy inserting the expression for r in Algorithm 1 and replac-\ning |Vl | with   |VPRM |, the last expression in (15) simplies\n(cid:18)\n(cid:19)|VPRM |\nto\n1    log(|VPRM |)\n|VPRM |\n\n|VPRM |\nlog(|VPRM |)\n\nB\u0001 (zi ) = Xfree .\n\n \n\n,\n\n(16)\n\n\n\n\f", 
        "tag": "Robotics", 
        "link": "https://arxiv.org/list/cs.RO/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nI\nS\n.\ns\nc\n[\n \n \n1\nv\n0\n6\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nThe impact of random actions on opinion dynamics\n\nAmir Leshemand Anna Scaglione\n\nApril 7, 2017\n\nAbstract\n\nOpinion dynamics have fascinated researchers for centuries. The\nability of societies to learn as well as the emergence of irrational herd-\ning are equally evident. The simplest example is that of agents that\nhave to determine a binary action, under peer pressure coming from\nthe decisions observed. By modifying several popular models for opin-\nion dynamics so that agents internalize actions rather than smooth\nestimates of what other people think, we are able to prove that almost\nsurely the actions nal outcome remains random, even though actions\ncan be consensual or polarized depending on the model. This is a the-\noretical conrmation that the mechanism that leads to the emergence\nof irrational herding behavior lies in the loss of nuanced information\nregarding the privately held beliefs behind the individuals decisions.\n\nMathematical models of herding phenomena prove the old saying that\nactions speak louder than words. A classical result shows that sequential\nBayesian learners give rise to dysfunctional information cascades because of\nthe overwhelming eect of the actions that are observed. In this work we\ndemonstrate that a variety of popular opinion dynamics models, modied so\nthat the agents internalize the eect of random actions rather than smooth\nestimates their peers beliefs, create herding, polarized groups or do not\nconverge to a xed belief. Because of the generality of our analysis, our\nwork cements the notion that actions are poor representations of private\ninformation and lead to unpredictable social herding phenomena.\nThere is a vast literature on the sub ject of opinion dynamics, aimed at\nmodeling how beliefs propagate across a social fabric. In the late 1700s [1]\nCorresponding Author: Amir Leshem, Faculty of Engineering, Bar-Ilan University,\nRamat-Gan, 52900, Israel. Phone: +972-35317409. email: leshema@ biu.ac.il.\nDept. of EE Arizona State University.\n\n1\n\n\frst noted that averaging individuals beliefs would, by the law of large num-\nbers, lead to a noise free average opinion and precise estimate in the limit [1],\na concept later popularized broadly in the early 1900s [2]. The models used\nto derive these insights are very similar to the Bayesian multi-agent models\nthat are pervasive in signal processing [3, 4, 5]. While in signal processing\ndecision criteria are often based on minimizing the expected Bayesian risk,\nin economics, the decision model of rational agents has been long modeled\nas maximizing the expected utility in the Bayesian sense. Furthermore, a\nsubstantial body of the economics literature assumes that agents reveal im-\nperfectly their private beliefs through their utility maximizing action. The\nseminal paper by Bikhchandani, Hirshleifer and Welch (BHW) [6] (see also\n[7]) highlighted how pathologies, called informational cascades, seem to de-\nvelop when agents make a zero-one decisions (for example, zero is bad\nand one is good), instead of averaging continuous values as in [2]. The\nBHW paper highlights the signicant impact of processing discrete actions,\nas opposed to continuous belief values, on the nal behavior of the group.\nThis impact leads to what is often referred to as herding behavior [8]. Other\nopinion diusions models and interactions of agents in social networks are\nreviewed in e.g. [9, 10, 11, 12, 13, 14].\nIn the last forty years, a urry of models for opinion dynamics has\nemerged in the statistical physics literature [12], inspired by ideas aimed\nat predicting macroscopic behavior that emerges from simple random par-\nticles interactions. Among them the voter model, introduced in [15], is one\nof the simplest non-ergodic opinion diusion models that leads to herding.\nThis more closely resemble the zero-one interactions in [6] since the agents\nhave two discrete opinions and copy at random one of their neighbors. This\nmodel was later mapped onto that of random walkers that coalesce upon\nencounter by Liggett [16, 17] which made it analytically tractable. An in-\nteresting variant is the one introduced by Mobilia in [18] where, using a\nmean-eld approach, the author studied the eect of having stubborn agents\nin the network that, by not changing their opinion, attempt to inuence the\nnal equilibrium point. In recent work [19, 20, 21] re-examined this question\nand analytically characterized its behavior by generalizing the approach in-\ntroduced by Liggett and providing a solution for the strategic placement of\nstubborn agents in the voter model. Stubborn agents were also considered\nby Jia et al. [22]\nSzna jd and coauthors in [23] considered a dierent discrete opinion dy-\nnamic in which it takes two agents to convince their neighbors of the cor-\nrectness of their opinion. By contrast, Galam in [24], proposed a variant in\nwhich the opinions of the agents switched to the prevalent opinion (a ma-\n\n2\n\n\fjority rule) in their neighborhood. Since Szna jd dynamics are a special case\nof the general sequential probabilistic model (GPM) in [25], only two dier-\nent phases are possible: either consensus, or coexistence of the two opinions\nin equal proportions. Interestingly, in the presence of stubborn agents with\nasynchronous updates, opinion uctuations exist and convergence is not cer-\ntain [26].\nContinuous belief models and updates, even in the presence of non-\nlinearities (e.g. bounded condence) are also a popular way to model so-\ncial dynamics of opinions. A popular form is the Hegselmann-Krause (HK)\nmodel (or the bounded condence (BC) model) [27, 28], where opinions are\nrepresented by a real number and are updated synchronously, by averaging\nall the agents opinions that dier by less than a condence level \u0001 (set to 1\nin [28]). As long as the network is connected, the agents reach a consensus\n[29] with the average belief in the network. A randomized variant of the HK\nwas introduced by DeGroot in [30] and studied in e.g.\n[31, 32, 33, 34] in\nwhich two randomly chosen neighbors meet and adjust their opinions only\nwhen their opinion distance is below a threshold. These models lead either\nto consensus or polarization, but opinions do converge.\nNote that in the economics literature, due to the Bayesian update rule,\nthe agents have a continuous belief even if their action and decision variables\nare discrete (the probability mass function of the decision variable given the\nprivate information and history of the actions that were observed). Gener-\nalizing the analysis in [6] to arbitrary social graphs is both intractable and\nperhaps futile, given the ample evidence that social agents are not rational\n[35].\nIn statistical physics the acronym CODA (continuous opinions discrete\nactions) was coined much later in [36, 37]. This class of models follows the\ngeneral idea from economics of assuming that the agents internalize an es-\ntimate of the beliefs of the others obtained by observing their actions. But\ncontrary to the BHW model, CODA models postulate simple mechanisms of\nopinion contagion, without oering signicant detail on the interpretation of\nthe models and providing mostly numerical characterization of their asymp-\ntotic behavior. The strong guarantees and mathematical characterizations\nof the limiting behavior is completely known only for the simple chain of\nsequential interactions treated in [6].\nOur main results is to oer strong theoretical guarantees that the same\nemergent behavior of random herding in the BHW model holds more broadly.\nOur work supports the argument that it is the discrete and random nature\nof what the agents internalize about the beliefs held by their neighbors that\nproduces the formation of random herds, i.e. herds whose nal decision out-\n\n3\n\n\fcome that cannot be predicted with certainty and is, potentially, nefariously\nwrong.\n\n1 Consensus under the social pressure of actions\n\nWe assume each agent has only two options to chose from, and we label them\nchoice 0 and choice 1. Assume these choices are made by ipping a coin that\nis biased by a belief variable xn (t), generating an action an (t)  {0, 1}. For\neach n, an (t) is a Bernoulli random variable with probability xn (t).\nTo capture the agents tendency to conform to social norms, we assume\nthat the agents perturb their belief xn (t) by moving towards the weighted\nmean of their neighbors actions. The agents update their belief as a convex\ncombination of the frequency of action 1 over their neighborhood and their\nxn (t + 1) = (1  )xn (t) +  (cid:80)N\nprior belief, i.e.:\nk=1 wnk ak (t)\nWe also require that (cid:80)\nwhere wnk is a weight function corresponding to the relative weight that\nagent n gives to the action of agent k if (n, k) is and edge and zero otherwise.\nk wn,k = 1. We require that the matrix W dened\nby: will be the transition matrix of an irreducible Markov chain but we\ndo not require wn,n = 0 since it might be that agent n is inuenced by its\nown action (even though it is random and determined by its internal belief\nxn (t). Basic results from spectral graph theory show that this condition is\nequivalent to having a network that has a single component, where we have\nweighted edges between agents n, k whenever wn,k (cid:54)= 0. Note that xn (t) is\nalways an number in [0, 1] and assuming that not all xn (0) are identically 0 or\nidentically 1, we may assume without loss of generality that they are strictly\nbetween 0 and 1 (since this will happen with probability 1 in nite time by\nthe averaging property). Introducing the vectors x(t) = (x1 (t), . . . , xN (t))T\nand a(t) = (a1 (t), . . . , aN (t)) we note that:\nE{a(t)|x(t)} = x(t),\n\n(1)\n\n(2)\n\nand\n\nCov{a(t)|x(t)} = diag {v1 (t), ..., vn (t)}\n(3)\nwhere vj (t) = xj (t)(1  xj (t)) is the variance of agent j at time t. Further-\nN(cid:89)\nmore,\nn=1\n\nn (t)(1  xn (t))(1an ) .\nxan\n\nP r (a(t) = a|x(t)) =\n\n(4)\n\n4\n\n\fThe value of  determines the inuence of the actions on the belief of each\nagent1 . Let us dene also the matrix W which has elements wm,n and dene\nW = (1  )I + W.\n(5)\nWe note that for all 0    1 both W and W are stochastic matrices\nwith the same right and left eigenvectors, and spectral radius 1. Let  be\nthe left eigenvector of W corresponding to the largest eigenvalue 1 (nor-\nmalized such that 1T  = 1) and remember that the corresponding right\neigenvector is 1. By the Perron-Frobenius theorem we have  >> 0, i.e. all\nits components are strictly positive. Denoting by\nJ (cid:44) 1T ,\n\n(6)\n\nBy standard theory of stochastic matrices for all  the spectral radius of\nW  J 0 < (W  J) < 1 and WJ = J.\nIntroducing the vectors\nx(t) = (x1 (t), . . . , xN (t))T and a(t) = (a1 (t), . . . , aN (t)), we can also note\nthat (1) can be written in vector form as follows:\nx(t + 1) = (1  )x(t) + Wa(t)\n\n(7)\n\nFrom (7) it is straightforward to prove the following:\nLemma 1 In general E{x(t + 1)|x(t)} = Wx(t) and E{Jx(t + 1)|x(t)} =\nJx(t).\n\nThe proof of the lemma is provided in the supplementary material. This\nlemma claries the connection with the classic opinion dynamics model in-\ntroduced by DeGroot [30], which is identical to our model in expectation.\nThe weights in the matrix W in the DeGroot model typically areinterpreted\nas being the trust individuals place on each other. The average dynamics are\nalso a classic example of the so called Average Consensus Gossiping (AGC)\nalgorithms [38]. For a connected network is well known that these dynamics\nlead to consensus on the state. Note however (7) is not deterministic and\nthe fact that actions are discrete leads to very dierent dynamics than AGC\nas the following lemma shows:\n\nLemma 2 The network has only two stationary states: x(t) = 1 and x(t) =\n0.\n\n1We assume that  is independent of n for notational simplicity, but all arguments go\nthrough even when n depends on the player.\n\n5\n\n\fProof: It is clear that in both cases, x(t) = 0 or x(t) = 1, if we evaluate (4)\nfor x(t) = 0 P r (a(t) = 0|0) = 1 and x(t) = 1 P r (a(t) = 1|1) = 1. This\nimplies (7) leads to a xed point x(t + 1) = x(t). Any other belief level will\nplace a non-zero probability in all a(t + 1)  {0, 1}N therefore making it\npossible to deviate from the previous belief for every  > 0.\nInterestingly, the fact that there are stationary states, does not suce\nto prove that the beliefs indeed converge to the stationary states. This\nconvergence is the main result. Denote the expected value of x(t) by x (t):\nx (t) = E{x(t)}.\n\n(8)\n\nLet the network weighted sample average of the beliefs according to the\nstationary distribution  at time t be:\n\nq(t) = T x(t).\n\n(9)\n\nThe next lemma leverages results in the AGC literature [38]:\n\n(10)\n\nLemma 3 The expected value in (8) is such that\nt+ x (t) = E{q(0)}1.\nlim\nThe simple proof of this lemma is relegated to the supplementary material\nfor brevity. While this property is not surprising, what is interesting is the\nfollowing result:\nt xn (t)  {0, 1}(cid:17)\n(cid:16)\nTheorem 1 The mechanism in (1) leads to herding, i.e.\nn,\nlim\nMoreover, the limit is identical to al l agents, i.e., either al l agents end up\nwith belief 0 or al l of them end up with belief 1.\n\n= 1.\n\nP r\n\nThe rst part of the proof of this lemma is similar to that used in [39] albeit\nfor a dierent opinion diusion model to show that the process converges\nto a random variable with probability 1. However, in contrast to [39], our\nmodel leads to herding while their model leads almost surely to consensus\non the true value of the parameter. The second part resorts to the Lesbegue\ndominated convergence theorem to prove that the limiting random variable\nmust be equal to either 0 or 1:\nProof: We know that the only stationary belief levels are x = 0 or x = 1,\nsince for every other vector there is a positive probability that the average\n\n6\n\n\faction will cause a deviation up to , however, in contrast to the voters\nmodel this is insucient, since when the initial state has all beliefs strictly\nbetween 0 and 1, at any given stage the beliefs will be in the open interval\n(0, 1) since they are convex combinations of the current belief with a num-\nber in [0, 1] and convergence is not achieved in nite time. Since W is a\nstochastic matrix. Let q(t) = T x(t) be the average belief level in the net-\nwork according to the stationary distribution of W . We will show that q(t)\nis a martingale process with respect to the beliefs. To that end we observe\nthat:\nq(t + 1) = T x(t + 1) = T [(1  )x(t) + (W)a(t)] .\nwhere a(t) is the action vector at times t. Computing the expectation of\nboth sides conditioned on x(t) we observe that:\nE {q(t + 1)|x(t)} = T Wx(t) = T x(t) = q(t),\n\n(12)\n\n(11)\n\nsince  is a left eigenvector of W with an eigenvalue equal to 1 and by\nthe Perron-Frobenius theorem all its elements are strictly positive, since the\nnetwork is connected (which is equivalent to the Markov chain dened by\nW is irreducible. This implies that q(t) is a martingale with respect to\nthe sigma-algebra determined by the sequence of beliefs x(t). By denition\nq(t) is a bounded sequence, as the weighted mean of a vector with elements\nbetween 0 and 1. By the martingale convergence theorem the sequence q(t)\nmust converge to a random variable q with probability 1. We need to show\nthat q is almost surely either 0 or 1,\nConsider now for t > 1 the sequence q(t) = q(t)  q(t  1), i.e. the\nmartingale dierence sequence. For each t by the martingale property,\nE {q(t)} = 0.\n(13)\nSince  is a probability vector and for all n, t 0  xn (t)  1 we have that\nfor all n, t |q(t)| < 1 as well as |q(t)| < 1. Therefore, by the almost sure\nconvergence of q(t)\n\nlim\nt var(q(t)) = 0, a.s.\n\n(14)\n\nwhich implies that also\nt var(q(t)|q(t  1)) = 0, a.s.\nlim\nSimilarly q(t) < 1 and converges almost surely to 0. Hence by the Lesbegue\ndominated convergence theorem q(t) converges to 0 in the mean square\n\n(15)\n\n7\n\n\fvar(q(t)|q(t  1)) = E (cid:8)q2 (t)|q(t  1)(cid:9)\nsense. If we express explicitly the conditional variance of q(t)\n= 2 (cid:80)N\n= 2 (cid:80)N\nn=1 2\nnvar(an (t))\nnxn (t)(1  xn (t)).\nn=1 2\nSince for all n, n > 0 the MS convergence implies that\n n.\nt xn (t)(1  xn (t)) = 0\nlim\nSimple probabilistic computation shows that either for all n\n\nlim\nt xn (t) = 0 or\n\nlim\nt xn (t) = 1.\n\n(16)\n\n(17)\n\n(18)\n\nIn fact, if for some j, n which are connected, xn (t) converges to 1 while\nxj (t) converges to 0, since the actions are independent, there will be with\nprobability 1 innitely many deviations of at least min {|Wn,k | : wn,k (cid:54)= 0} of\nboth xn (t) and xj (t), contradicting the convergence of both xn (t) and xj (t).\nThis implies that herding must be achieved with probability 1, which\nends the proof of the main theorem.\nIt should be noted that while the limit variables xn () have values\n0, 1, each initial condition has a dierent probability of converging to 0 or\n1, which implies that what outcomes prevails, wrong or right, is random\nand therefore social pressure can lead to umpredictable collective behavior,\nin spite of the initial information the agents have available to forge their\nopinion. We would like to determine its distribution given an initial value\nor a distribution of initial values. The next corollary will show that xn () is\na binomial random variable with mean q(0). This implies that the variance\nof xn () is q(0) (1  q(0)). In fact, the combination of the lemma above\nand the main theorem leads to the following corollary:\n\nCorollary 1 Let x(0) be the agents initial belief level vector. The prob-\nability that the agents herd to action an (t) = 1 is q(0) = 1T x(0), i.e.\nlimt+ P r(xn (t) = 1) = q(0).\nProof: Since, as t  +, the network can only be in one of the two xed\npoints x(t) = 1 or x(t) = 0, the probability that it will endup on one of\nthese two states is, as indicated in Lemma 3, equal to the average initial\nbelief.\nInterestingly, no matter what the initial state is, we cannot predict the\noutcome of the herding for sure. No matter how close the population belief\n\n8\n\n\fis to 0 or 1 initially, it is always possible that they will herd towards the op-\nposite action. If it is indeed true that the social pressure mechanism is based\non internalizing the actions of ones peers, we cannot predict accurately the\noutcome of the social dynamics. Applying this to market behavior, where\npeople favor one product over the other, this indicates a quality brand that\nhas high probability of capturing a large percentage of the market if indi-\nviduals were to choose in isolation, may actually fail and other rms with\nworse products have a ghting chance to capture it. Furthermore, we can see\nthat even in large markets driven only by the consumers repeated actions,\na winner takes al l cascade eventually happens, and one product wins the\nma jority of the market share irrespective of the initial individual evidence\nto the quality of the product. This may not always be the case however.\nIn the next section we generalize our analysis rst to random interactions\nand show the same type of herding phenomenon happens as well in that\ncontext. We also show that when social pressure is mitigated by mistrust\nfor what deviates excessively from ones prior belief, then the society may\nsplit in multiple herds.\n\n(19)\n\n1.1 Randomized interactions\nA simple variation of our model is one that captures random interactions\nxn (t + 1) = (1  )xn (t) +  (cid:80)N\namong the agents. In this case (1) becomes\nk=1 wnk (t)ak (t)\nwhere the weights wnk (t) are non zero only for those individuals who ob-\nserve and are aected by their peers actions in that particular epoch. We\nassume wnk (t) are random processes independent from the actions the nodes\nperformed, which continue to be modeled as in the previous section. Cor-\nrespondingly, we can dene the random matrices W(t) and W (t) as be-\nfore.\nInterestingly this variation of the model hardly changes the result\nand random herding ensues also in this case almost surely. Let us now\nassume that these random matrices are ergodic processes and denote by\nW = E{W(t)}. If all nodes will speak innitely often with peers and the\naverage W = E{W(t)} is a stochastic matrix with eigenvalue 1 with multi-\nplicity 1, then we can easily generalize the proof of our main theorem, using\nthe same denition of  as before:\n\nCorollary 2 The statement of Theorem 1 holds unchanged if the interac-\ntions are random and W = E{W(t)}.\n\n9\n\n\fProof:\n\nIn this case we dene:\n\nq(t) = T x(t)\n\nIn this case, like before:\nE {q(t + 1)|x(t)} = T Wx(t) = T x(t) = q(t),\n\n(20)\n\n(21)\n\nbut now the average is also with respect to the process W(t). Nonetheless\neverything else from this point on follows the steps of the proof of Theorem\n1.\n\n2 Bounded condence models\n\n2.1 Bounded condence under the inuence of actions\n\nAn interesting generalization of our model is the update with bounded-\ncondence, where the nth agent updates happen only where the obser-\nvations are suciently close to the agent own disposition. A natural option\nis to analyze a model similar to the HK model proposed in [27] for continu-\nous opinion dynamics. In the HK model, agents mix their belief only with\nagents whose belief suciently close to their own. In our case, agents update\ntheir belief only if the empirical distribution of the actions of its neighbors\nis suciently close to xn (t). We introduce the function:\n(x) = xu(|x|   )\n\n(22)\n\nin which the condence threshold  < 1 and u(x) is the Heaviside (i.e. the\nunit step) function. The agents update with bounded condence in its\n(cid:32) N(cid:88)\n(cid:33)\nneighbors is modeled as:\nwnk ak (t)  xn (t)\nxn (t + 1) = xn (t) + \nk=1\nWe note right away that (0  0) = (1  1) = 0, which implies that x(t) = 1\n(cid:33)\n(cid:32)(cid:12)(cid:12)(cid:12) N(cid:88)\n(cid:12)(cid:12)(cid:12)x(t)\n(cid:12)(cid:12)(cid:12) > \nor x(t) = 0 are, once again xed points. Let us denote by:\nwnk ak (t)  xn (t)\nk=1\nn (x(t)) = (1  Pn (x(t))).\n\nPn (x(t)) = P\n\n(25)\n\nLet:\n\n.\n\n(23)\n\n(24)\n\n10\n\n\fWe can dene a matrix W(x(t)) in the same way as (5), except that we\nreplace  with a diagonal matrix with diagonal elements n (x(t)).\nIt is\nstraightforward to see that\nE{x(t + 1)|x(t)} = W(x(t))x(t)\n\nwhich are smooth non-linear dynamics equivalent to the HK model. In the\naction-based HK dynamics a range of stationary beliefs are possible and, also\nthe beliefs may never converge even in distribution. In fact, next we provide\nan example where the action-based HK model exhibits much more complex\nbehavior than the polarization phenomenon observed in the classical HK\nmodel. Let us illustrate this fact with an example. Consider a network with\nfour nodes, with an initial beliefs vector x = (0, 0.45, 0.55, 1)T , a condence\n .\n 0\nthreshold  = 0.25 and a mixing matrix:\n0\n0\n1\n0.25 0.25\n0\n0.5\nW =\n0.5\n0\n0.25 0.25\n1\n0\n0\n0\n(cid:26)x1 (t)+u(|1x1 (t)|   )(1x1 (t)) a2 (t) = 1\nIn this case the dynamics of the four nodes are as follows. For the rst node:\n(cid:26)x1 (t) = 0\nx1 (t)u(x1 (t)   )x1 (t)\na2 (t) = 0\na2 (t) = 1\n(1)x1 (t) = 0 a2 (t) = 0\n\nx1 (t + 1) =\n\n(26)\n\n(27)\n\n(28)\n\n=\n\nand the reason why x1 (t) = 0 is that we assumed that x1 (0) = 0 and all\nlater values remain unchanged. Similarly, without giving a proof we can\nshow that x4 (t) = 1 (just by symmetry 1  x4 (t) behaves like x1 (t)). For\nnode number two, when update is activated, the node has a new state that\nis a convex combination of x2 (t) with either 0.25 or 0.5. Therefore:\n(1  )x2 (t) +  0.25  x2 (t + 1)  (1  )x2 (t) +  0.5\n\n(29)\n\nand, since the node starts at x2 (0) = 0.45, the subsequent values will have\nto remain conned in the interval (0.25, 0.5). For similar reasons x3 (t) will\nremain in the interval (0.5, 0.75). However, they both will change randomly.\nIn fact, x2 (t) will change due to a3 (t) and viceversa x3 (t) will change based\non a2 (t). Therefore no convergence in distribution is attained and the opin-\nions will continue to uctuate indenitely in the intervals discussed above.\nThis is shown numerically in the next section in Fig. 4.\n\n11\n\n\f2.2 Reinforcement model with random pairwise interactions\n\nWe propose an alternative model for the update. We assume that if agents\nn observe at random one of its neighbors, say the kth neighbor, make the\nsame decision, agent n will adjust the belief as follows:\nxn (t + 1) = xn (t) +  [an (t)  ak (t)](an (t)  xn (t)),\n\n(30)\n\nwhere  [x] is the Kronecker delta function.\nWe view this model as a mechanism that reinforces behavior, because it\ntends to decrease or increase the belief based on the frequency with which\nones action is repeated in the neighborhood. In this case the only xed points\nfor the dynamics x have integer entries, but unlike either (12) or (21), in this\ncase the network may never coalesce in taking a single action, but opposite\ndecisions may persist as well. The expectation of these dynamics given a\nrandom pair (n, k) is:\nE{xn (t + 1)|x(t), (n, k)} = xn (t)[1  (1  xn (t))(1  xk (t))]\n xn (t).\n\n(31)\n\nSince this holds for each random choice of (n, k) we observe that from the\nprevious equations, taking the expectation also over the random choices of\n(n, k) it is clear that the state is a sub-martingale:\nE{xn (t + 1)|x(t)}  xn (t)\n\n(32)\n\nNext we prove the following lemma:\nt xn (t)  {0, 1}(cid:17)\n(cid:16)\nTheorem 2 The mechanism in (30) leads to\nn,\nlim\nbut not necessarily herding, which in turn means society wil l be polarized in\ngeneral.\n\n= 1.\n\nP r\n\nProof: The argument follows a similar line of reasoning as our previous\ntheorem. Because of the dominated convergence theorem the state xn (t)\nmust converge in the mean square sense. We can see then that the sequence\nynk (t) =  [an (t)  ak (t)](an (t)  xn (t)) must go to zero in the mean square\nsense, because each pair is selected at random innitely often. This can\nhappen in only two cases, either the conditional variance of an (t)  xn (t)\ngoes to zero or  [an (t)  ak (t)] = 0 with probability one. If we assume the\n\n12\n\n\frst is not true, then the second implies that the independent actions of the\nrandom pair of agents are the same, which can only be if they have equal\nprobability xn (t) = xk (t)  {0, 1}; while this cannot happen in nite time\nit can happen at the limit.\nIf this does not happen then the variance of\nan (t) in the limit must be zero which again implies that in the limit the\nprobability converges to either 1 or 0. Both lead to the same conclusion,\nwhich proves the theorem.\n\n3 Simulated experiments\n\n3.1 Consensus under the social pressure of actions\n\nTo demonstrate the herding and clustering phenomenon we proved to be\ntrue in Section 1, we performed several Monte-Carlo trials.\nIn the rst\nexperiments we randomly picked a random sample graph shown in Figure\n1.\n\nFigure 1: The random graph used for simulations.\n\nThen we picked randomly 5000 initial vectors x(0) whose entries are\nindependent and drawn to have a mean  n E{xn (0)} = p0 varying from 0.2\nto 0.8 and chose uniform weights wkn = 1/deg(k), where deg(k) denotes the\ndegree of node k (the graph is undirected).\nFigure 2 shows the histogram of the limit for various random realizations\n\n13\n\n-2-1.5-1-0.500.511.52-2.5-2-1.5-1-0.500.511.522.512345678910\fFigure 2: Histograms of the nal belief for p0 = 0.2 and p0 = 0.6.\n\nof the process for initial belief levels of p0 = 0.2, 0.6. Figure 3 shows clearly\nthat the actions are Bernoulli random variables whose parameter is correctly\npredicted by the theorem.\n\n3.2 Action-based HK model\n\nIn Figure 4 we simulated the dynamics discussed in Section 2 pertaining\nfour nodes with W in (26). Note that there is no agreement but also no\nconvergence of belief, in contrast to the classical HK model and the claims\nmade in Section 2 are corroborated by the simulation results. Furthermore\nthe random process of the opinions of the agents is very complex, as one can\nnotice from the complex non linear dynamics of the mean. This shows that\nit is much harder to predict a form of polarization because the random pro-\ncesses x2 (t), x3 (t) are governed by non-linear stochastic dynamics that are\nMarkovian but non homogeneous, whose trends are dicult to manipulate\nmathematically to obtain optimum forecasts. Either than resorting to the\nbounds we mentioned, accurate predictions become intractable, even when\nthe initial conditions are known.\n\n3.3 Reinforcement model\n\nIn Section 2 we introduced a new model in which nodes move their belief\nwhen their action is identical to one of their neighbors, selected at random.\nIn this case we showed that the network in Figure 1, with uniform initial\nbelief equal to 0.5, converges to possibly multiple herds. This is illustrated\nin our three numerical simulation of the dynamics of the beliefs in Figure 5\nwhere we can clearly observe the emergence of herding towards belief 1 and\n\n14\n\n00.20.40.60.8100.10.20.30.40.50.60.70.80.91p0=0.200.20.40.60.8100.10.20.30.40.50.60.70.80.91p0=0.6\fFigure 3: Mean and variance of the limit point as a function of p0 .\n\nbelief 0 by all nodes in the top and bottom gures respectively, as well as\nthe emergence of polarized groups, whose belief is 1 or 0, in the middle plot.\nThis corroborate the statement in Theorem 2.\n\n4 Conclusions\n\nIn this paper we examined an opinion diusion model in which agents update\ntheir belief based on their neighbors empirical distribution of the actions.\nThe use of the observed actions leads to herding unlike continuous state\nupdates when neither bounded condence nor stubborn agents are present\nin the system showing that the trend observed in the BHW model extends\nto other settings.\n\n5 Appendix\n\nProof of Lemma 3\n\nWe now prove prove Lemma 3 Taking the expectation on both sides of 7,\nwe obtain:\n\nx (t + 1) = (1  )E{x(t)} + WE{E{a(t)|x(t)}}\ntx (0)\n= Wx (t) = W\n\n(33)\n\n15\n\n0.10.20.30.40.50.60.70.80.900.51E{xn(0)}=p0E{xn()}  simulationtheoretical0.10.20.30.40.50.60.70.80.90.050.10.150.20.25E{xn(0)}=p0Var(xn())  simulationtheoretical\fFigure 4: The state of the four node network with W in (26).\n\nwhich is the AGC update that converges to consensus in the limit. In fact\n\nt+ Wt = J\nlim\n\n(34)\n\nand\n\nJx (0) = 1T x (0)\n(35)\nwhich proves the claim, since E{q(0)} = T x (0). In addition to the lemmas\nand theorems in the main paper, the following result holds\n\nLemma 4 In general\n\nand\n\nE{x(t + 1)|x(t)} = Wx(t)\n\nE{Jx(t + 1)|x(t)} = Jx(t).\n\nProof:\nE{x(t + 1)|x(t)} = (1  )x(t) + WE{a(t + 1)|x(t)}\n= (1  )x(t) + Wx(t) = Wx(t).\n\n(36)\n\n(37)\n\n(38)\n\n16\n\n012345678910x 10400.10.20.30.40.50.60.70.80.91  x1(t)x2(t)x3(t)x4(t)\fFigure 5: Three experiments of the dynamics of the agents beliefs with same\ninitial beliefs that converge to herding (top 0 and bottom 1) and polarization\n(middle).\n\nThe second claim follows easily since JW = J. This shows that in expec-\ntation the belief follows the same dynamics as the DeGroot model [30].\n\nSimulations of herding under random pairwise interactions\n\nWe also tested the dynamics where each time two random nodes average\ntheir actions, discussed in the Randomized interactions section, and use\nthe averaged action to update their mutual beliefs. Figure 6 describes the\nrandom graph used. In this case we describe the course of two dynamics\nof the population, both where all nodes had an initial belief of 0.9, i.e. all\nnodes were very inclined to take action 1. The top Figure 7 presents a\ndynamics converging to 1. We present the lowest belief, the highest belief\nand the mean belief. As can be seen when the dynamics converge to 1 this\nhappens quite rapidly, because of the initial state of all nodes. The bottom\nof Figure 7 presents another instance of the dynamics, where initial beliefs\nwere identical to the previous case, but the limiting value of the herding was\n0. With the initial conditions dened in the simulation, this occurs only 10%\nof the simulations. Still this is surprising, as it would be expected that the\n\n17\n\n\finitial belief will have signicant impact on the results of the herding. As\npredicted by the main theorem, the direction of the herding is unexpected,\nand governed by the random actions of the agents.\n\nFigure 6: 10 nodes networks used for the asynchronous dynamics.\n\n18\n\n\fReferences\n\n[1] de Caritat (marquis de Condorcet) JAN (1785) Essai sur lapplication\nde lanalyse `a la probabilite des decisions rendues `a la pluralitee des\nvoix. (De lImprimerie royale).\n\n[2] Galton F (1907) Vox populi. Nature 75:450451.\n\n[3] Vikram Krishnamurthy ONG, Hamdi M (2014) Interactive sensing and\ndecision making in social networks. Foundations and Trends in Signal\nProcessing 7(1-2):1196.\n\n[4] Krishnamurthy V, Poor HV (2014) A tutorial on interactive sensing in\nsocial networks.\n\n[5] Sayed AH (2014) Adaptation, learning, and optimization over networks.\nFoundations and Trends in Machine Learning 7(4-5):311801.\n\n[6] Bikhchandani S, Hirshleifer D, Welch I (1992) A theory of fads, fashion,\ncustom, and cultural change as informational cascades in Journal of\nPolitical Economy. Vol. 100, pp. 9921026.\n\n[7] Lohmann S (1994) Dynamics of informational cascades: The monday\ndemonstrations in leipzig, east germany, 1989-1991 in World Politics.\nVol. 47, pp. 42101.\n\n[8] Chamley C (2004) Rational herds: Economic models of social learning.\n(Cambridge University Press).\n\n[9] Jackson M (2008) Social and Economic Networks. (Princeton University\nPress).\n\n[10] Easley D, Kleinberg J (2010) Networks, Crowds, and Markets: Reason-\ning About a Highly Connected World. (Cambridge University Press).\n\n[11] Acemoglu D, Dahleh M, Lobel I, Ozdaglar A (2010) Bayesian learning\nin social networks in Review of Economic Studies. Vol. 78, pp. 1201\n1236.\n\n[12] Castellano C, Fortunato S, Loreto V (2009) Statistical physics of social\ndynamics. Reviews of modern physics 81(2):591.\n\n[13] Krishnamurthy V, Poor HV (2013) Social learning and bayesian games\nin multiagent signal processing: How do local and global decision mak-\ners interact? IEEE Signal Processing Magazine 30(3):4357.\n\n19\n\n\f[14] Friedkin NE (2006) A structural theory of social inuence. (Cambridge\nUniversity Press) Vol. 13.\n\n[15] Cliord P, Sudbury A (1973) A model for spatial conict. Biometrika\n60(3):581588.\n\n[16] Liggett TM (1985) Particle systems.\n\n[17] Holley RA, Liggett TM (1975) Ergodic theorems for weakly interacting\ninnite systems and the voter model. The annals of probability pp.\n643663.\n\n[18] Mobilia M (2003) Does a single zealot aect an innite group of voters?\nPhysical review letters 91(2):028701.\n\n[19] Yildiz E, Ozdaglar A, Acemoglu D, Scaglione A (2010) The voter model\nwith stubborn agents extended abstract in Communication, Control,\nand Computing (Al lerton), 2010 48th Annual Al lerton Conference on.\n(IEEE), pp. 11791181.\n\n[20] Yildiz E, Acemoglu D, Ozdaglar A, Saberi A, Scaglione A (2011) Dis-\ncrete opinion dynamics with stubborn agents. SSRN eLibrary.\n\n[21] Yildiz E, Ozdaglar A, Acemoglu D, Saberi A, Scaglione A (2013) Bi-\nnary opinion dynamics with stubborn agents. ACM Transactions on\nEconomics and Computation 1(4):19.\n\n[22] Jia P, MirTabatabaei A, Friedkin NE, Bullo F (2015) Opinion dynamics\nand the evolution of social power in inuence networks. SIAM review\n57(3):367397.\n\n[23] Szna jd-Weron K, Szna jd J (2000) Opinion evolution in closed commu-\nnity. International Journal of Modern Physics C 11(06):11571165.\n\n[24] Galam S (2002) Minority opinion spreading in random geometry. The\nEuropean Physical Journal B-Condensed Matter and Complex Systems\n25(4):403406.\n\n[25] Galam S (2005) Local dynamics vs. social mechanisms: A unifying\nframework. EPL (Europhysics Letters) 70(6):705.\n\n[26] Acemoglu D, Como G, Fagnani F, Ozdaglar A (2013) Opinion uctua-\ntions and disagreement in social networks. Mathematics of Operations\nResearch 38(1):127.\n\n20\n\n\f[27] Hegselmann R, Krause U (2002) Opinion dynamics and bounded con-\ndence models, analysis and simulations in Jounral of Articial Societies\nand Social Simulation. Vol. 5.\n\n[28] Blondel V, Hendrickx J, Tsitsiklis J (2009) On krauses multi-agent con-\nsensus model with state-dependent connectivity in IEEE Trans. Auto.\nControl. Vol. 54, pp. 25862597.\n\n[29] Tsitsiklis J (1984) Ph.D. thesis (Dept. of Electrical Engineering and\nComputer Science, M.I.T., Boston, MA).\n\n[30] DeGroot M (1974) Reaching a consensus in Journal of American\nStatistcal Association. Vol. 69, pp. 118121.\n\n[31] Deuant G, Neau D, Amblard F, Weisbuch G (2000) Mixing beliefs\namong interacting agents in Adv. Compl. Syst. Vol. 3, pp. 8798.\n\n[32] Weisbuch G, Deuant G, Amblard F, Nadal J (2001) Interacting agents\nand continuous opinions dynamics in Heterogeneous agents, interac-\ntions, and economic performance.\n\n[33] Weisbuch G (2004) Bounded condence and social networks in The Eu-\nropean Physical Journal B - Condensed Matter and Complex Systems.\nVol. 38, pp. 339343.\n\n[34] Li L, Scaglione A, Swami A, Zhao Q (2013) Consensus, polarization\nand clustering of opinions in social networks. Selected Areas in Com-\nmunications, IEEE Journal on 31(6):10721083.\n\n[35] Kahneman D (2003) Maps of bounded rationality: Psychology for be-\nhavioral economics. American economic review pp. 14491475.\n\n[36] Martins AC (2008) Continuous opinions and discrete actions in opin-\nion dynamics problems.\nInternational Journal of Modern Physics C\n19(04):617624.\n\n[37] Martins AC, Pereira CdB, Vicente R (2009) An opinion dynamics model\nfor the diusion of innovations. Physica A: Statistical Mechanics and\nits Applications 388(15):32253232.\n\n[38] Dimakis A, Kar S, Moura J, Rabbat M, Scaglione A (2010) Gossip\nalgorithms for distributed signal processing. Proceedings of the IEEE\n98(11):1847 1864.\n\n21\n\n\f[39] Jadbabaie A, Molavi P, Sandroni A, Tahbaz-Salehi A (2012) Non-\nbayesian social learning. Games and Economic Behavior 76(1):210225.\n\n22\n\n\fFigure 7: Opinion dynamics herding to 1 (top) and to 0 (bottom), xn (0) =\n0.9, n.\n\n23\n\n0501001502002503000.40.50.60.70.80.91txn(t)0500100015002000250030003500400000.10.20.30.40.50.60.70.80.91txn(t)\f", 
        "tag": "Social and Information Networks", 
        "link": "https://arxiv.org/list/cs.SI/new"
    }, 
    {
        "text": "A Syntactic Neural Model for General-Purpose Code Generation\n\nPengcheng Yin\nLanguage Technologies Institute\nCarnegie Mellon University\npcyin@cs.cmu.edu\n\nGraham Neubig\nLanguage Technologies Institute\nCarnegie Mellon University\ngneubig@cs.cmu.edu\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nL\nC\n.\ns\nc\n[\n \n \n1\nv\n6\n9\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstract\n\nWe consider the problem of parsing natu-\nral language descriptions into source code\nwritten in a general-purpose programming\nlanguage like Python.\nExisting data-\ndriven methods treat this problem as a lan-\nguage generation task without considering\nthe underlying syntax of the target pro-\ngramming language.\nInformed by previ-\nous work in semantic parsing, in this pa-\nper we propose a novel neural architecture\npowered by a grammar model to explicitly\ncapture the target syntax as prior knowl-\nedge. Experiments nd this an effective\nway to scale up to generation of complex\nprograms from natural language descrip-\ntions, achieving state-of-the-art results that\nwell outperform previous code generation\nand semantic parsing approaches.\n\nIntroduction\n1\nEvery programmer has experienced the situation\nwhere they know what they want to do, but do\nnot have the ability to turn it into a concrete im-\nplementation. For example, a Python programmer\nmay want to sort my list in descending order,\nbut not be able to come up with the proper syn-\ntax sorted(my list, reverse=True) to real-\nize his intention. To resolve this impasse, it is\ncommon for programmers to search the web in\nnatural language (NL), nd an answer, and mod-\nify it into the desired form (Brandt et al., 2009,\n2010). However,\nthis is time-consuming, and\nthus the software engineering literature is ripe\nwith methods to directly generate code from NL\ndescriptions, mostly with hand-engineered meth-\nods highly tailored to specic programming lan-\nguages (Balzer, 1985; Little and Miller, 2009;\nGvero and Kuncak, 2015).\n\nIn parallel, the NLP community has developed\nmethods for data-driven semantic parsing, which\nattempt to map NL to structured logical forms ex-\necutable by computers. These logical forms can be\ngeneral-purpose meaning representations (Clark\nand Curran, 2007; Banarescu et al., 2013), for-\nmalisms for querying knowledge bases (Tang and\nMooney, 2001; Zettlemoyer and Collins, 2005;\nBerant et al., 2013) and instructions for robots or\npersonal assistants (Artzi and Zettlemoyer, 2013;\nQuirk et al., 2015), among others. While these\nmethods have the advantage of being learnable\nfrom data, compared to the programming lan-\nguages (PLs) in use by programmers, the domain-\nspecic languages targeted by these works have a\nschema and syntax that is relatively simple.\nRecently, Ling et al. (2016) have proposed a\ndata-driven code generation method for high-level,\ngeneral-purpose PLs like Python and Java. This\nwork treats code generation as a sequence-to-\nsequence modeling problem, and introduce meth-\nods to generate words from character-level mod-\nels, and copy variable names from input descrip-\ntions. However, unlike most work in semantic\nparsing, it does not consider the fact that code has\nto be well-dened programs in the target syntax.\nIn this work, we propose a data-driven syntax-\nbased neural network model tailored for genera-\ntion of general-purpose PLs like Python.\nIn or-\nder to capture the strong underlying syntax of the\nPL, we dene a model that transduces an NL state-\nment into an Abstract Syntax Tree (AST; Fig. 1(a),\n 2) for the target PL. ASTs can be deterministi-\ncally generated for all well-formed programs us-\ning standard parsers provided by the PL, and thus\ngive us a way to obtain syntax information with\nminimal engineering. Once we generate an AST,\nwe can use deterministic generation tools to con-\nvert the AST into surface code. We hypothesize\nthat such a structured approach has two benets.\n\n\fProduction Rule\nExplanation\nRole\nCall (cid:55) expr[func] expr*[args] keyword*[keywords] Function Call (cid:46) func: the function to be invoked (cid:46) args: arguments list\n(cid:46) keywords: keyword arguments list\nIf (cid:55) expr[test] stmt*[body] stmt*[orelse]\n(cid:46) test: condition expression (cid:46) body: statements inside\nthe If clause (cid:46) orelse: elif or else statements\nFor (cid:55) expr[target] expr*[iter] stmt*[body]\n(cid:46) target: iteration variable (cid:46) iter: enumerable to iterate\nstmt*[orelse]\nover (cid:46) body: loop body (cid:46) orelse: else statements\nFunctionDef (cid:55) identier[name] arguments*[args]\nFunction Def. (cid:46) name: function name (cid:46) args: function arguments\nstmt*[body]\n(cid:46) body: function body\n\nIf Statement\n\nFor Loop\n\nTable 1: Example production rules for common Python statements (Python Software Foundation, 2016)\n\nFirst, we hypothesize that structure can be used\nto constrain our search space, ensuring generation\nof well-formed code. To this end, we propose a\nsyntax-driven neural code generation model. The\nbackbone of our approach is a grammar model\n( 3) which formalizes the generation story of a\nderivation AST into sequential application of ac-\ntions that either apply production rules ( 3.1), or\nemit terminal tokens ( 3.2). The underlying syn-\ntax of the PL is therefore encoded in the grammar\nmodel a priori as the set of possible actions. Our\napproach frees the model from recovering the un-\nderlying grammar from limited training data, and\ninstead enables the system to focus on learning the\ncompositionality among existing grammar rules.\nXiao et al. (2016) have noted that this imposition\nof structure on neural models is useful for seman-\ntic parsing, and we expect this to be even more im-\nportant for general-purpose PLs where the syntax\ntrees are larger and more complex.\nSecond, we hypothesize that structural informa-\ntion helps to model information ow within the\nneural network, which naturally reects the recur-\nsive structure of PLs. To test this, we extend a\nstandard recurrent neural network (RNN) decoder\nto allow for additional neural connections which\nreect the recursive structure of an AST ( 4.2).\nAs an example, when expanding the node (cid:63) in\nFig. 1(a), we make use of the information from\nboth its parent and left sibling (the dashed rectan-\ngle). This enables us to locally pass information\nof relevant code segments via neural network con-\nnections, resulting in more condent predictions.\nExperiments ( 5) on two Python code gener-\nation tasks show 11.7% and 9.3% absolute im-\nprovements in accuracy against the state-of-the-art\nsystem (Ling et al., 2016). Our model also gives\ncompetitive performance on a standard semantic\nparsing benchmark.\n2 The Code Generation Problem\nGiven an NL description x, our task is to generate\nthe code snippet c in a modern PL based on the in-\n\ntent of x. We attack this problem by rst generat-\ning the underlying AST. We dene a probabilistic\ngrammar model of generating an AST y given x:\np(y |x). The best-possible AST y is then given by\np(y |x).\n(1)\ny = arg max\ny\ny is then deterministically converted to the corre-\nsponding surface code c.1 While this paper uses\nexamples from Python code, our method is PL-\nagnostic.\nBefore detailing our approach, we rst present\na brief introduction of the Python AST and its\nunderlying grammar. The Python abstract gram-\nmar contains a set of production rules, and an\nAST is generated by applying several production\nrules composed of a head node and multiple child\nnodes. For instance, the rst rule in Tab. 1 is\nused to generate the function call sorted() in\nFig. 1(a). It consists of a head node of type Call,\nand three child nodes of type expr, expr* and\nkeyword*, respectively. Labels of each node are\nnoted within brackets.\nIn an AST, non-terminal\nnodes sketch the general structure of the target\ncode, while terminal nodes can be categorized into\ntwo types: operation terminals and variable ter-\nminals. Operation terminals correspond to basic\narithmetic operations like AddOp.Variable termi-\nnal nodes store values for variables and constants\nof built-in data types2 . For instance, all terminal\nnodes in Fig. 1(a) are variable terminal nodes.\n3 Grammar Model\nBefore detailing our neural code generation\nmethod, we rst introduce the grammar model at\nits core. Our probabilistic grammar model denes\nthe generative story of a derivation AST. We fac-\ntorize the generation process of an AST into se-\nquential application of actions of two types:\n A P PLYRU LE[r] applies a production rule r to\nthe current derivation tree;\n\n1We use astor library to convert ASTs into Python code.\n2 bool, float, int, str.\n\n\f(2)\n\nFigure 1: (a) the Abstract Syntax Tree (AST) for the given example code. Dashed nodes denote terminals. Nodes are labeled\nwith time steps during which they are generated. (b) the action sequence (up to t14 ) used to generate the AST in (a)\nexample, in Fig. 1(b), the rule Call (cid:55) expr. . .\n G ENTOKEN[v ] populates a variable terminal\nexpands the frontier node Call at time step t4 , and\nnode by appending a terminal token v .\nits three child nodes expr, expr* and keyword*\nare added to the derivation.\nA P PLYRU LE actions grow the derivation AST\nby appending nodes. When a variable terminal\nnode (e.g., str) is added to the derivation and be-\ncomes the frontier node, the grammar model then\nswitches to G ENTOK EN actions to populate the\nvariable terminal with tokens.\nUnary Closure Sometimes, generating an AST\nrequires applying a chain of unary productions.\nFor instance, it takes three time steps (t9  t11 )\nto generate the sub-structure expr* (cid:55) expr (cid:55)\nName (cid:55) str in Fig. 1(a). This can be effectively\nreduced to one step of A P P LYRU LE action by tak-\ning the closure of the chain of unary productions\nand merging them into a single rule: expr* (cid:55)\nstr. Unary closures reduce the number of actions\nneeded, but would potentially increase the size of\nthe grammar.\nIn our experiments we tested our\nmodel both with and without unary closures ( 5).\n3.2 GENTOKEN Actions\nOnce we reach a frontier node nft that corresponds\nto a variable type (e.g., str), G ENTOK EN actions\nare used to ll this node with values. For general-\npurpose PLs like Python, variables and constants\nhave values with one or multiple tokens. For in-\nstance, a node that stores the name of a function\n(e.g., sorted) has a single token, while a node\nthat denotes a string constant (e.g., a=hello\nworld) could have multiple tokens. Our model\ncopes with both scenarios by ring G ENTOKEN\nactions at one or more time steps. At each time\n\nFig. 1(b) shows the generation process of the tar-\nget AST in Fig. 1(a). Each node in Fig. 1(b) in-\ndicates an action. Action nodes are connected by\nsolid arrows which depict the chronological order\nof the action ow. The generation proceeds in\ndepth-rst, left-to-right order (dotted arrows rep-\nresent parent feeding, explained in  4.2.1).\nFormally, under our grammar model, the prob-\nT(cid:89)\nability of generating an AST y is factorized as:\np(y |x) =\np(at |x, a<t ),\nt=1\nwhere at is the action taken at time step t, and a<t\nis the sequence of actions before t. We will explain\nhow to compute Eq. (2) in  4. Put simply, the\ngeneration process begins from a root node at t0 ,\nand proceeds by the model choosing A P PLYRU LE\nactions to generate the overall program structure\nfrom a closed set of grammar rules, then at leaves\nof the tree corresponding to variable terminals, the\nmodel switches to G ENTOKEN actions to gener-\nate variables or constants from the open set. We\ndescribe this process in detail below.\n3.1 A PPLYRULE Actions\nA P P LYRU LE actions generate program structure,\nexpanding the current node (the frontier node at\ntime step t: nft ) in a depth-rst,\nleft-to-right\ntraversal of the tree. Given a xed set of produc-\ntion rules, A P P LYRU LE chooses a rule r from the\nsubset that has a head matching the type of nft ,\nand uses r to expand nft by appending all child\nnodes specied by the selected production. As an\n\nExprrootexpr[value]Callexpr*[args]keyword*[keywords]Namestr(sorted)expr[func]exprNamestr(my_list)keywordstr(reverse)expr[value]Namestr(True)\u0000\u0000\u0000\u0000\u0000\u0000(\u0000\u0000_\u0000\u0000\u0000\u0000, \u0000\u0000\u0000\u0000\u0000\u0000\u0000=\u0000\u0000\u0000\u0000)t1t2t3t4t5t6t7t8t9t10t11t12t14rootExprExprexpr[value]exprCallexprNameNamestr\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000[sorted]\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000[</n>]expr*exprexprNameNamestr\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000[my_list]keyword*keywordCallexpr[func] expr*[args] keyword*[keywords]ActionFlowParentFeedingtiApplyRuletiGenerateTokentiGenTokenwithCopyt13\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000[</n>]t1t2t3t4t5t6t7t8t4t9t10t11t12t13t4t14t15t16t17t16t18t19t20t21t0(a)(b)\u0000\u0000\u0000\u0000 \u0000\u0000_\u0000\u0000\u0000\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000Input:Code:. . .\fstep, G ENTOKEN appends one terminal token to\nthe current frontier variable node. A special </n>\ntoken is used to close the node. The grammar\nmodel then proceeds to the new frontier node.\nTerminal tokens can be generated from a pre-\ndened vocabulary, or be directly copied from the\ninput NL. This is motivated by the observation\nthat the input description often contains out-of-\nvocabulary (OOV) variable names or literal values\nthat are directly used in the target code. For in-\nstance, in our running example the variable name\nmy list can be directly copied from the the input\nat t12 . We give implementation details in  4.2.2.\n4 Estimating Action Probabilities\nWe estimate action probabilities in Eq. (2) using\nattentional neural encoder-decoder models with an\ninformation ow structured by the syntax trees.\n4.1 Encoder\nFor an NL description x consisting of n words\n{wi}n\ni=1 ,\nthe encoder computes a context sen-\nsitive embedding hi for each wi using a bidi-\nrectional Long Short-Term Memory (LSTM) net-\nwork (Hochreiter and Schmidhuber, 1997), simi-\nlar to the setting in (Bahdanau et al., 2014). See\nsupplementary materials for detailed equations.\n4.2 Decoder\nThe decoder uses an RNN to model the sequential\ngeneration process of an AST dened as Eq. (2).\nEach action step in the grammar model naturally\ngrounds to a time step in the decoder RNN. There-\nfore, the action sequence in Fig. 1(b) can be in-\nterpreted as unrolling RNN time steps, with solid\narrows indicating RNN connections. The RNN\nmaintains an internal state to track the generation\nprocess ( 4.2.1), which will then be used to com-\npute action probabilities p(at |x, a<t ) ( 4.2.2).\n4.2.1 Tracking Generation States\nOur implementation of the decoder resembles a\nvanilla LSTM, with additional neural connections\n(parent feeding, Fig. 1(b)) to reect the topological\nstructure of an AST. The decoders internal hidden\nstate at time step t, st , is given by:\n(3)\nst = fLSTM ([at1 : ct : pt : nft ], st1 ),\nwhere fLSTM () is the LSTM update function.\n[:] denotes vector concatenation. st will then be\nused to compute action probabilities p(at |x, a<t )\nin Eq. (2). Here, at1 is the embedding of the pre-\nvious action. ct is a context vector retrieved from\n\nFigure 2: Illustration of a decoder time step (t = 9)\ninput encodings {hi} via soft attention. pt is a\nvector that encodes the information of the parent\naction. nft denotes the node type embedding of\n3 . Intuitively, feeding\nthe current frontier node nft\nthe decoder the information of nft helps the model\nto keep track of the frontier node to expand.\nAction Embedding at We maintain two action\nembedding matrices, WR and WG . Each row in\nWR (WG ) corresponds to an embedding vector\nfor an action A P PLYRU LE[r] (G ENTOKEN[v ]).\nContext Vector ct The decoder RNN uses soft at-\ntention to retrieve a context vector ct from the in-\nput encodings {hi} pertain to the prediction of the\ncurrent action. We follow Bahdanau et al. (2014)\nand use a Deep Neural Network (DNN) with a sin-\ngle hidden layer to compute attention weights.\nParent Feeding pt Our decoder RNN uses ad-\nditional neural connections to directly pass infor-\nmation from parent actions. For instance, when\ncomputing s9 , the information from its parent ac-\ntion step t4 will be used. Formally, we dene the\nparent action step pt as the time step at which\nthe frontier node nft is generated. As an exam-\nple, for t9 , its parent action step p9 is t4 , since\nnf9 is the node (cid:63), which is generated at t4 by the\nA P PLYRU LE[Call(cid:55). . .] action.\nWe model parent\ninformation pt from two\nsources: (1) the hidden state of parent action spt ,\nand (2) the embedding of parent action apt . pt is\nthe concatenation. The parent feeding schema en-\nables the model to utilize the information of par-\nent code segments to make more condent predic-\ntions. Similar approaches of injecting parent in-\nformation were also explored in the S EQ2TR EE\nmodel in Dong and Lapata (2016)4 .\n3We maintain an embedding for each node type.\n4 S EQ2TR EE generates tree-structured outputs by condi-\n\nh1h2sortmy_listh3h4h5indescendings9p9a8nf9GenToken[</n>]ApplyRule[Call]ParentStates4a4+expr*s8ApplyRuleGenTokentypeof?order......nf9nonterminalvariableterminalembeddingofnodetypeembeddingofc9\f4.2.2 Calculating Action Probabilities\nIn this section we explain how action probabilities\np(at |x, a<t ) are computed based on st .\nA PPLYRULE The probability of applying rule r\nas the current action at is given by a softmax5 :\np(at = A P PLYRU LE[r]|x, a<t ) =\nsoftmax(WR  g(st ))\n(cid:124)  e(r)\n(4)\nwhere g() is a non-linearity tanh(W  st + b), and\ne(r) the one-hot vector for rule r .\nGENTOKEN As in  3.2, a token v can be gener-\nated from a predened vocabulary or copied from\nthe input, dened as the marginal probability:\np(at = G ENTOK EN[v ]|x, a<t ) =\np(gen|x, a<t )p(v |gen, x, a<t )\n+ p(copy|x, a<t )p(v |copy, x, a<t ).\nThe selection probabilities p(gen|) and p(copy|)\nare given by softmax(WS  st ).\nThe prob-\nability of generating v from the vocabulary,\np(v |gen, x, a<t ), is dened similarly as Eq. (4),\nexcept that we use the G ENTOK EN embedding\nmatrix WG , and we concatenate the context vector\nct with st as input. To model the copy probability,\nwe follow recent advances in modeling copying\nmechanism in neural networks (Gu et al., 2016;\nJia and Liang, 2016; Ling et al., 2016), and use a\npointer network (Vinyals et al., 2015) to compute\nthe probability of copying the i-th word from the\ninput by attending to input representations {hi}:\n(cid:80)n\np(wi |copy, x, a<t ) =\nexp((hi , st , ct ))\n,\ni(cid:48)=1 exp((hi(cid:48) , st , ct ))\nwhere () is a DNN with a single hidden layer.\nSpecically, if wi is an OOV word (e.g., my list,\nwhich is represented by a special <unk> token in\nencoding), we directly copy the actual word wi to\nthe derivation.\n\n4.3 Training and Inference\nGiven a dataset of pairs of NL descriptions xi and\ncode snippets ci , we parse ci into its AST yi and\ndecompose yi into a sequence of oracle actions un-\nder the grammar model. The model is then op-\ntimized by maximizing the log-likelihood of the\noracle action sequence. At inference time, we\nuse beam search to approximate the best AST y\nin Eq. (1). See supplementary materials for the\npseudo-code of the inference algorithm.\ntioning on the hidden states of parent non-terminals, while\nour parent feeding uses the states of parent actions.\n5We do not show bias terms for all softmax equations.\n\nDataset\nTrain\nDevelopment\nTest\nAvg. tokens in description\nAvg. characters in code\nAvg. size of AST (# nodes)\n\nHS\n533\n66\n66\n39.1\n360.3\n136.6\n\nD JANGO I F TTT\n16,000\n77,495\n5,171\n1,000\n758\n1,805\n7.4\n14.3\n41.1\n62.2\n7.0\n17.2\n\nStatistics of Grammar\nw/o unary closure\n# productions\n# node types\nterminal vocabulary size\nAvg. # actions per example\nw/ unary closure\n# productions\n# node types\nAvg. # actions per example\n\n100\n61\n1361\n173.4\n\n222\n96\n6733\n20.3\n\n100\n57\n141.7\n\n237\n92\n16.4\n\n1009\n828\n0\n5.0\n\n\n\n\n\nTable 2: Statistics of datasets and associated grammars\n\n5 Experimental Evaluation\n5.1 Datasets and Metrics\nHEARTHSTONE (HS) dataset (Ling et al., 2016)\nis a collection of Python classes that implement\ncards for the card game HearthStone. Each card\ncomes with a set of elds (e.g., name, cost, and\ndescription), which we concatenate to create the\ninput sequence. This dataset is relatively difcult:\ninput descriptions are short, while the target code\nis in complex class structures, with each AST hav-\ning 137 nodes on average.\nD JANGO dataset (Oda et al., 2015) is a collection\nof lines of code from the Django web framework,\neach with a manually annotated NL description.\nCompared with the HS dataset where card imple-\nmentations are somewhat homogenous, examples\nin D JANGO are more diverse, spanning a wide va-\nriety of real-world use cases like string manipula-\ntion, IO operations, and exception handling.\nI FTTT dataset (Quirk et al., 2015) is a domain-\nspecic benchmark that provides an interest-\ning side comparison. Different from HS and\nD JANGO which are in a general-purpose PL, pro-\ngrams in I FT TT are written in a domain-specic\nlanguage used by the IFTTT task automation\nApp. Users of the App write simple instruc-\ntions (e.g., If Instagram.AnyNewPhotoByYou\nThen Dropbox.AddFileFromURL) with NL de-\nscriptions (e.g., Autosave your Instagram photos\nto Dropbox). Each statement inside the If or\nThen clause consists of a channel (e.g., Dropbox)\nand a function (e.g., AddFileFromURL)6 . This\n\n6Like Beltagy and Quirk (2016), we strip function param-\n\n\fsimple structure results in much more concise\nASTs (7 nodes on average). Because all examples\nare created by ordinary Apps users, the dataset\nis highly noisy, with input NL very loosely con-\nnected to target ASTs. The authors thus provide a\nhigh-quality ltered test set, where each example\nis veried by at least three annotators. We use this\nset for evaluation. Also note I F TTTs grammar has\nmore productions (Tab. 2), but this does not imply\nthat its grammar is more complex. This is because\nfor HS and D JANGO terminal tokens are generated\nby G ENTOK EN actions, but for I FT TT, all the code\nis generated directly by A P P LYRU LE actions.\nMetrics As is standard in semantic parsing, we\nmeasure accuracy, the fraction of correctly gen-\nerated examples. However, because generating an\nexact match for complex code structures is non-\ntrivial, we follow Ling et al. (2016), and use token-\nlevel BLEU-4 with as a secondary metric, dened\nas the averaged BLEU scores over all examples.7\n5.2 Setup\nPreprocessing All\ninput descriptions are tok-\nenized using N LTK. We perform simple canoni-\ncalization for D JANGO, such as replacing quoted\nstrings in the inputs with place holders. See sup-\nplementary materials for details. We extract unary\nclosures whose frequency is larger than a thresh-\nold k (k = 30 for HS and 50 for D JANGO).\nConguration The size of all embeddings is 128,\nexcept for node type embeddings, which is 64.\nThe dimensions of RNN states and hidden layers\nare 256 and 50, respectively. Since our datasets are\nrelatively small for a data-hungry neural model,\nwe impose strong regularization using recurrent\ndropouts (Gal and Ghahramani, 2016), together\nwith standard dropout layers added to the inputs\nand outputs of the decoder RNN. We validate the\ndropout probability from {0, 0.2, 0.3, 0.4}. For\ndecoding, we use a beam size of 15.\n5.3 Results\nEvaluation results for Python code generation\ntasks are listed in Tab. 3. Numbers for our sys-\n\neters since they are mostly specic to users.\n7These two metrics are not ideal: accuracy only measures\nexact match and thus lacks the ability to give credit to seman-\ntically correct code that is different from the reference, while\nit is not clear whether BLEU provides an appropriate proxy\nfor measuring semantics in the code generation task. A more\nintriguing metric would be directly measuring semantic/func-\ntional code equivalence, for which we present a pilot study\nat the end of this section (cf. Error Analysis). We leave ex-\nploring more sophisticated metrics (e.g. based on static code\nanalysis) as future work.\n\nRetrieval System\nPhrasal Statistical MT\nHierarchical Statistical MT\nNM T\nS EQ2TR EE\nS EQ2TR EEUNK\nL PN\nOur system\nAblation Study\n frontier embed.\n parent feed.\n copy terminals\n+ unary closure\n unary closure\n\nHS\n\nD JANGO\n\nACC\n0.0\n0.0\n0.0\n1.5\n1.5\n13.6\n4.5\n16.2\n\n16.7\n10.6\n3.0\n\n10.1\n\nB LEU\n62.5\n34.1\n43.2\n60.4\n53.4\n62.8\n65.6\n75.8\n\n75.8\n75.7\n65.7\n\n74.8\n\n\n\nBL EU\n18.6\n47.6\n35.9\n63.4\n44.6\n58.2\n77.6\n84.5\n\n83.8\n84.3\n61.7\n83.3\n\nACC\n14.7\n31.5\n9.5\n45.1\n28.9\n39.4\n62.3\n71.6\n\n70.7\n71.5\n32.3\n70.3\n\n\n\nTable 3: Results on two Python code generation tasks.\nResults previously reported in Ling et al. (2016).\n\ntems are averaged over three runs. We compare\nprimarily with two approaches:\n(1) Latent Pre-\ndictor Network (L PN), a state-of-the-art sequence-\nto-sequence code generation model (Ling et al.,\n2016), and (2) S EQ2TR EE, a neural semantic pars-\ning model (Dong and Lapata, 2016). S EQ2TR EE\ngenerates trees one node at a time, and the tar-\nget grammar is not explicitly modeled a priori,\nbut implicitly learned from data. We test both\nthe original S EQ2TR EE model released by the au-\nthors and our revised one (S EQ2TR EEUNK) that\nuses unknown word replacement to handle rare\nwords (Luong et al., 2015). For completeness,\nwe also compare with a strong neural machine\ntranslation (NM T) system (Neubig, 2015) using a\nstandard encoder-decoder architecture with atten-\ntion and unknown word replacement8 , and include\nnumbers from other baselines used in Ling et al.\n(2016). On the HS dataset, which has relatively\nlarge ASTs, we use unary closure for our model\nand S EQ2TR EE, and for D JANGO we do not.\nSystem Comparison As in Tab. 3, our model\nregisters 11.7% and 9.3% absolute improvements\nover L PN in accuracy on HS and D JANGO. This\nboost in performance strongly indicates the impor-\ntance of modeling grammar in code generation.\nFor the baselines, we nd L PN outperforms oth-\ners in most cases. We also note that S EQ2TR EE\nachieves a decent accuracy of 13.6% on HS , which\nis due to the effect of unknown word replacement,\nsince we only achieved 1.5% without it. A closer\n\n8For NM T, we also attempted to nd the best-scoring syn-\ntactically correct predictions in the size-5 beam, but this did\nnot yield a signicant improvement over the NM T results in\nTab. 3.\n\n\fFigure 3: Performance w.r.t reference AST size on D JANGO\n\nFigure 4: Performance w.r.t reference AST size on HS\n\ncomparison with S EQ2TR EE is insightful for un-\nderstanding the advantage of our syntax-driven ap-\nproach, since both S EQ2TR EE and our system out-\nput ASTs: (1) S EQ2TR EE predicts one node each\ntime step, and requires additional dummy nodes\nto mark the boundary of a subtree. The sheer num-\nber of nodes in target ASTs makes the prediction\nprocess error-prone. In contrast, the A P PLYRU LE\nactions of our grammar model allows for gener-\nating multiple nodes at a single time step. Em-\npirically, we found that in HS, S EQ2TR EE takes\nmore than 300 time steps on average to generate a\ntarget AST, while our model takes only 170 steps.\n(2) S EQ2TR EE does not directly use productions\nin the grammar, which possibly leads to grammat-\nically incorrect ASTs and thus empty code out-\nputs. We observe that the ratio of grammatically\nincorrect ASTs predicted by S EQ2TR EE on HS\nand D JANGO are 21.2% and 10.9%, respectively,\nwhile our system guarantees grammaticality.\nAblation Study We also ablated our best-\nperforming models to analyze the contribution of\neach component. frontier embed. removes the\nfrontier node embedding nft from the decoder\nRNN inputs (Eq. (3)). This yields worse results on\nD JANGO while gives slight improvements in ac-\ncuracy on HS. This is probably because that the\ngrammar of HS has fewer node types, and thus\nthe RNN is able to keep track of nft without de-\npending on its embedding. Next, parent feed.\nremoves the parent feeding mechanism. The ac-\ncuracy drops signicantly on HS, with a marginal\ndeterioration on D JANGO. This result is interest-\ning because it suggests that parent feeding is more\nimportant when the ASTs are larger, which will\nbe the case when handling more complicated code\ngeneration tasks like HS . Finally, removing the\npointer network (copy terminals) in G ENTO -\n\nCHANN EL FU LL TR EE\n\nClassical Methods\nposclass (Quirk et al., 2015)\nLR (Beltagy and Quirk, 2016)\nNeural Network Methods\nNM T\nNN (Beltagy and Quirk, 2016)\nS EQ2TR EE (Dong and Lapata, 2016)\nDoubly-Recurrent NN\n(Alvarez-Melis and Jaakkola, 2017)\nOur system\n parent feed.\n frontier embed.\n\n81.4\n88.8\n\n87.7\n88.0\n89.7\n90.1\n\n90.0\n89.9\n90.1\n\n71.0\n82.5\n\n77.7\n74.3\n78.4\n78.2\n\n82.0\n81.1\n78.7\n\nTable 4: Results on the noise-ltered I F TTT test set of >3\nagree with gold annotations (averaged over three runs), our\nmodel performs competitively among neural models.\n\nK EN actions gives poor results, indicating that it\nis important to directly copy variable names and\nvalues from the input.\nThe results with and without unary closure\ndemonstrate that, interestingly, it is effective on\nHS but not on D JANGO. We conjecture that this is\nbecause on HS it signicantly reduces the number\nof actions from 173 to 142 (c.f., Tab. 2), with the\nnumber of productions in the grammar remaining\nunchanged.\nIn contrast, D JANGO has a broader\ndomain, and thus unary closure results in more\nproductions in the grammar (237 for D JANGO\nvs. 100 for HS), increasing sparsity.\nPerformance by the size of AST We further in-\nvestigate our models performance w.r.t. the size\nof the gold-standard ASTs in Figs. 3 and 4. Not\nsurprisingly, the performance drops when the size\nof the reference ASTs increases. Additionally, on\nthe HS dataset, the BLEU score still remains at\naround 50 even when the size of ASTs grows to\n200, indicating that our proposed syntax-driven\napproach is robust for long code segments.\nDomain Specic Code Generation Although this\nis not the focus of our work, evaluation on I F TTT\nbrings us closer to a standard semantic parsing set-\nting, which helps to investigate similarities and\ndifferences between generation of more compli-\ncated general-purpose code and and more limited-\ndomain simpler code. Tab. 4 shows the results,\nfollowing the evaluation protocol in (Beltagy and\nQuirk, 2016) for accuracies at both channel and\nfull parse tree (channel + function) levels. Our\nfull model performs on par with existing neu-\nral network-based methods, while outperforming\nother neural models in full tree accuracy (82.0%).\nThis score is close to the best classical method\n(LR), which is based on a logistic regression\n\n01020304050Reference AST Size (# nodes)0.00.20.40.60.81.0PerformanceBLEUacc50100150200250Reference AST Size (# nodes)0.00.20.40.60.81.0PerformanceBLEUacc\finput <name> Brawl </name> <cost> 5 </cost> <desc>\nDestroy all minions except one (chosen randomly)\n</desc> <rarity> Epic </rarity> ...\npred. class Brawl(SpellCard):\ndef\ninit (self):\nsuper(). init (Brawl, 5, CHARACTER CLASS.\nWARRIOR, CARD RARITY.EPIC)\ndef use(self, player, game):\nsuper().use(player, game)\ntargets = copy.copy(game.other player.minions)\ntargets.extend(player.minions)\nfor minion in targets:\nminion.die(self)\n\nA\n\nref.\n\nminions = copy.copy(player.minions)\nminions.extend(game.other player.minions)\nif len(minions) > 1:\nsurvivor = game.random choice(minions)\nfor minion in minions:\nif minion is not survivor: minion.die(self)\n\nB\n\ninput join app cong.path and string locale into a le\npath, substitute it for localedir.\npred. localedir = os.path.join(\napp config.path, locale) \u0013\ninput self.plural is an lambda function with an argument\nn, which returns result of boolean expression n not\nequal to integer 1\npred. self.plural = lambda n: len(n) \u0017\nref.\nself.plural = lambda n: int(n!=1)\n\nTable 5: Predicted examples from HS (1st) and D JANGO.\nCopied contents (copy probability > 0.9) are highlighted.\n\nmodel with rich hand-engineered features (e.g.,\nbrown clusters and paraphrase). Also note that the\nperformance between NM T and other neural mod-\nels is much closer compared with the results in\nTab. 3. This suggests that general-purpose code\ngeneration is more challenging than the simpler\nI FT TT setting, and therefore modeling structural\ninformation is more helpful.\nCase Studies We present output examples in\nTab. 5. On HS, we observe that most of the\ntime our model gives correct predictions by lling\nlearned code templates from training data with ar-\nguments (e.g., cost) copied from input. However,\nwe do nd interesting examples indicating that the\nmodel learns to generalize beyond trivial copy-\ning. For instance, the rst example is one that our\nmodel predicted wrong  it generated code block\nA instead of the gold B (it also missed a function\ndenition not shown here). However, we nd that\nthe block A actually conveys part of the input in-\ntent by destroying all, not some, of the minions.\nSince we are unable to nd code block A in the\ntraining data, it is clear that the model has learned\nto generalize to some extent from multiple training\ncard examples with similar semantics or structure.\nThe next two examples are from D JANGO. The\nrst one shows that the model learns the usage\nof common API calls (e.g., os.path.join), and\n\nhow to populate the arguments by copying from\ninputs. The second example illustrates the dif-\nculty of generating code with complex nested\nstructures like lambda functions, a scenario worth\nfurther investigation in future studies. More exam-\nples are attached in supplementary materials.\nError Analysis To understand the sources of er-\nrors and how good our evaluation metric (exact\nmatch) is, we randomly sampled and labeled 100\nand 50 failed examples (with accuracy=0) from\nD JANGO and HS, resp. We found that around 2%\nof these examples in the two datasets are actually\nsemantically equivalent. These examples include:\n(1) using different parameter names when dening\na function; (2) omitting (or adding) default values\nof parameters in function calls. While the rarity of\nsuch examples suggests that our exact match met-\nric is reasonable, more advanced evaluation met-\nrics based on statistical code analysis are denitely\nintriguing future work.\nFor D JANGO, we found that 30% of failed\ncases were due to errors where the pointer net-\nwork failed to appropriately copy a variable name\ninto the correct position. 25% were because the\ngenerated code only partially implementated the\nrequired functionality.\n10% and 5% of errors\nwere due to malformed English inputs and pre-\nprocessing errors, respectively. The remaining\n30% of examples were errors stemming from mul-\ntiple sources, or errors that could not be easily cat-\negorized into the above. For HS, we found that\nall failed card examples were due to partial imple-\nmentation errors, such as the one shown in Table 5.\n\n6 Related Work\n\nCode Generation and Analysis Most existing\nworks on code generation focus on generating\ncode for domain specic languages (DSLs) (Kush-\nman and Barzilay, 2013; Raza et al., 2015; Man-\nshadi et al., 2013), with neural network-based ap-\nproaches recently explored (Parisotto et al., 2016;\nBalog et al., 2016). For general-purpose code gen-\neration, besides the general framework of Ling\net al. (2016), existing methods often use language\nand task-specic rules and strategies (Lei et al.,\n2013; Raghothaman et al., 2016). A similar line\nis to use NL queries for code retrieval (Wei et al.,\n2015; Allamanis et al., 2015). The reverse task of\ngenerating NL summaries from source code has\nalso been explored (Oda et al., 2015; Iyer et al.,\n2016). Finally, there are probabilistic models of\n\n\fsource code (Maddison and Tarlow, 2014; Nguyen\net al., 2013). The most relevant work is Allama-\nnis et al. (2015), which uses a factorized model\nto measure semantic relatedness between NL and\nASTs for code retrieval, while our model tackles\nthe more challenging generation task.\nSemantic Parsing Our work is related to the\ngeneral topic of semantic parsing, where the tar-\nget logical forms can be viewed as DSLs. The\nparsing process is often guided by grammatical\nformalisms like combinatory categorical gram-\nmars (Kwiatkowski et al., 2013; Artzi et al.,\n2015), dependency-based syntax (Liang et al.,\n2011; Pasupat and Liang, 2015) or task-specic\nformalisms (Clarke et al., 2010; Yih et al., 2015;\nKrishnamurthy et al., 2016; Misra et al., 2015; Mei\net al., 2016). Recently, there are efforts in design-\ning neural network-based semantic parsers (Misra\nand Artzi, 2016; Dong and Lapata, 2016; Nee-\nlakantan et al., 2016; Yin et al., 2016). Several\napproaches have be proposed to utilize grammar\nknowledge in a neural parser, such as augmenting\nthe training data by generating examples guided\nby the grammar (Kocisk y et al., 2016; Jia and\nLiang, 2016). Liang et al. (2016) used a neu-\nral decoder which constrains the space of next\nvalid tokens in the query language for question\nanswering. Finally, the structured prediction ap-\nproach proposed by Xiao et al. (2016) is closely\nrelated to our model in using the underlying gram-\nmar as prior knowledge to constrain the genera-\ntion process of derivation trees, while our method\nis based on a unied grammar model which jointly\ncaptures production rule application and terminal\nsymbol generation, and scales to general purpose\ncode generation tasks.\n\n7 Conclusion\nThis paper proposes a syntax-driven neural code\ngeneration approach that generates an abstract\nsyntax tree by sequentially applying actions from\na grammar model. Experiments on both code gen-\neration and semantic parsing tasks demonstrate the\neffectiveness of our proposed approach.\n\nAcknowledgment\nWe are grateful to Wang Ling for his generous help\nwith L PN and setting up the benchmark. We also\nthank Li Dong for helping with S EQ2TR EE and\ninsightful discussions.\n\nReferences\nMiltiadis Allamanis, Daniel Tarlow, Andrew D. Gor-\ndon, and Yi Wei. 2015. Bimodal modelling of\nIn Proceedings\nsource code and natural language.\nof ICML. volume 37.\n\nDavid Alvarez-Melis and Tommi S. Jaakkola. 2017.\nTree-structured decoding with doubly recurrent neu-\nral networks. In Proceedings of ICLR.\n\nYoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.\nBroad-coverage CCG semantic parsing with AMR.\nIn Proceedings of EMNLP.\n\nYoav Artzi and Luke Zettlemoyer. 2013. Weakly su-\npervised learning of semantic parsers for mapping\ninstructions to actions. Transaction of ACL 1(1).\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014.\nNeural machine translation by\nCoRR\njointly learning to align and translate.\nabs/1409.0473.\n\nMatej Balog, Alexander L. Gaunt, Marc Brockschmidt,\nSebastian Nowozin, and Daniel Tarlow. 2016.\nDeepcoder: Learning to write programs. CoRR\nabs/1611.01989.\n\nRobert Balzer. 1985. A 15 year perspective on au-\nIEEE Trans. Software Eng.\ntomatic programming.\n11(11).\n\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Griftt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract meaning representation\nfor sembanking. In Proceedings of the 7th Linguis-\ntic Annotation Workshop and Interoperability with\nDiscourse, LAW-ID@ACL.\n\nI. Beltagy and Chris Quirk. 2016.\nImproved seman-\ntic parsers for if-then statements. In Proceedings of\nACL.\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of EMNLP.\n\nJoel Brandt, Mira Dontcheva, Marcos Weskamp, and\nScott R. Klemmer. 2010. Example-centric program-\nming: integrating web search into the development\nenvironment. In Proceedings of CHI .\n\nJoel Brandt, Philip J. Guo, Joel Lewenstein, Mira\nDontcheva, and Scott R. Klemmer. 2009. Two stud-\nies of opportunistic programming: interleaving web\nforaging, learning, and writing code. In Proceedings\nof CHI .\n\nStephen Clark and James R. Curran. 2007. Wide-\ncoverage efcient statistical parsing with CCG and\nlog-linear models. Computational Linguistics 33(4).\n\nJames Clarke, Dan Goldwasser, Ming-Wei Chang, and\nDan Roth. 2010. Driving semantic parsing from the\nworlds response. In Proceedings of CoNLL.\n\n\fLi Dong and Mirella Lapata. 2016. Language to logical\nform with neural attention. In Proceedings of ACL.\n\nGreg Little and Robert C. Miller. 2009. Keyword pro-\ngramming in java. Autom. Softw. Eng. 16(1).\n\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Proceedings of NIPS.\n\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.\nIncorporating copying mechanism in\nLi. 2016.\nIn Proceedings of\nsequence-to-sequence learning.\nACL.\n\nTihomir Gvero and Viktor Kuncak. 2015. Interactive\nIn Proceedings\nsynthesis using free-form queries.\nof ICSE.\n\nSepp Hochreiter and J urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation 9(8).\n\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2016. Summarizing source code\nIn Proceedings of\nusing a neural attention model.\nACL.\n\nRobin Jia and Percy Liang. 2016. Data recombination\nfor neural semantic parsing. In Proceedings of ACL.\n\nTom as Kocisk y, G abor Melis, Edward Grefenstette,\nChris Dyer, Wang Ling, Phil Blunsom,\nand\nKarl Moritz Hermann. 2016. Semantic parsing with\nIn Pro-\nsemi-supervised sequential autoencoders.\nceedings of EMNLP.\n\nJayant Krishnamurthy, Oyvind Tafjord, and Aniruddha\nKembhavi. 2016. Semantic parsing to probabilistic\nprograms for situated question answering. In Pro-\nceedings of EMNLP.\n\nNate Kushman and Regina Barzilay. 2013. Using se-\nmantic unication to generate regular expressions\nfrom natural language. In Proceedings of NAACL.\n\nTom Kwiatkowski, Eunsol Choi, Yoav Artzi, and\nLuke S. Zettlemoyer. 2013.\nScaling semantic\nparsers with on-the-y ontology matching. In Pro-\nceedings of the EMNLP.\n\nTao Lei, Fan Long, Regina Barzilay, and Martin C. Ri-\nnard. 2013. From natural language specications to\nprogram input parsers. In Proceedings of ACL.\n\nChen Liang, Jonathan Berant, Quoc Le, Kenneth D.\nForbus, and Ni Lao. 2016. Neural symbolic ma-\nchines: Learning semantic parsers on freebase with\nweak supervision. CoRR abs/1611.00020.\n\nPercy Liang, Michael I. Jordan, and Dan Klein. 2011.\nLearning dependency-based compositional seman-\ntics. In Proceedings of ACL.\n\nWang Ling, Phil Blunsom, Edward Grefenstette,\nKarl Moritz Hermann, Tom as Kocisk y, Fumin\nWang, and Andrew Senior. 2016. Latent predictor\nIn Proceedings of\nnetworks for code generation.\nACL.\n\nThang Luong,\nIlya Sutskever, Quoc V. Le, Oriol\nVinyals, and Wojciech Zaremba. 2015. Addressing\nthe rare word problem in neural machine translation.\nIn Proceedings of ACL.\n\nChris J. Maddison and Daniel Tarlow. 2014. Structured\nIn Pro-\ngenerative models of natural source code.\nceedings of ICML. volume 32.\n\nMehdi Hafezi Manshadi, Daniel Gildea, and James F.\nAllen. 2013. Integrating programming by example\nand natural language programming. In Proceedings\nof AAAI .\n\nHongyuan Mei, Mohit Bansal, and Matthew R. Wal-\nter. 2016. Listen, attend, and walk: Neural mapping\nof navigational instructions to action sequences. In\nProceedings of AAAI .\n\nDipendra K. Misra and Yoav Artzi. 2016. Neural shift-\nIn Proceedings of\nreduce CCG semantic parsing.\nEMNLP.\n\nDipendra Kumar Misra, Kejia Tao, Percy Liang, and\nAshutosh Saxena. 2015. Environment-driven lex-\nIn Pro-\nicon induction for high-level instructions.\nceedings of ACL.\n\nArvind Neelakantan, Quoc V. Le, and Ilya Sutskever.\nInducing latent pro-\n2016. Neural programmer:\nIn Proceedings of\ngrams with gradient descent.\nICLR.\n\nGraham Neubig. 2015.\nlamtram: A toolkit for lan-\nguage and translation modeling using neural net-\nworks. http://www.github.com/neubig/lamtram.\n\nTung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh\nNguyen, and Tien N. Nguyen. 2013. A statistical\nsemantic language model for source code. In Pro-\nceedings of ACM SIGSOFT .\n\nYusuke Oda, Hiroyuki Fudaba, Graham Neubig,\nHideaki Hata, Sakriani Sakti, Tomoki Toda, and\nSatoshi Nakamura. 2015.\nLearning to generate\npseudo-code from source code using statistical ma-\nchine translation (T). In Proceedings of ASE.\n\nEmilio Parisotto, Abdel-rahman Mohamed, Rishabh\nSingh, Lihong Li, Dengyong Zhou, and Pushmeet\nKohli. 2016. Neuro-symbolic program synthesis.\nCoRR abs/1611.01855.\n\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of ACL.\n\nPython Software Foundation. 2016. Python abstract\ngrammar. https://docs.python.org/2/library/ast.html.\n\nChris Quirk, Raymond J. Mooney, and Michel Galley.\n2015. Language to code: Learning semantic parsers\nfor if-this-then-that recipes. In Proceedings of ACL.\n\n\fMukund Raghothaman, Yi Wei, and Youssef Hamadi.\n2016.\nSWIM: synthesizing what i mean: code\nsearch and idiomatic snippet synthesis. In Proceed-\nings of ICSE.\n\nMohammad Raza, Sumit Gulwani, and Natasa Milic-\nFrayling. 2015. Compositional program synthesis\nIn Proceed-\nfrom natural language and examples.\nings of IJCAI .\n\nLappoon R. Tang and Raymond J. Mooney. 2001. Us-\ning multiple clause constructors in inductive logic\nprogramming for semantic parsing. In Proceedings\nof ECML.\n\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Proceedings of NIPS.\n\nYi Wei, Nirupama Chandrasekaran, Sumit Gul-\nand Youssef Hamadi. 2015.\nwani,\nBuild-\ning\nbing\ndeveloper\nassistant.\nTechni-\ncal\nhttps://www.microsoft.com/en-\nreport.\nus/research/publication/building-bing-developer-\nassistant/.\n\nChunyang Xiao, Marc Dymetman, and Claire Gardent.\n2016. Sequence-based structured prediction for se-\nmantic parsing. In Proceedings of ACL.\n\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and\nJianfeng Gao. 2015. Semantic parsing via staged\nquery graph generation: Question answering with\nknowledge base. In Proceedings of ACL.\n\nPengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao.\n2016. Neural enquirer: Learning to query tables in\nnatural language. In Proceedings of IJCAI .\n\nLuke Zettlemoyer and Michael Collins. 2005. Learn-\ning to map sentences to logical form structured clas-\nsication with probabilistic categorial grammars. In\nProceedings of UAI .\n\n\fquoted string literals (e.g., verbose name is a\nstring cache entry). We therefore replace quoted\nstrings with indexed placeholders using regular\nexpression. After decoding, we run a post-\nprocessing step to replace all placeholders with\ntheir actual values.\n(2) For descriptions with\ncascading variable reference (e.g., call method\nself.makekey), we append after the whole variable\nname with tokens separated by .\n(e.g., append\nself and makekey after self.makekey). This gives\nthe pointer network exibility to copy either par-\ntial or whole variable names.\nGenerate Oracle Action Sequence To train our\nmodel, we generate the gold-standard action se-\nquence from reference code. For I FT TT, we sim-\nply parse the ofcially provided ASTs into se-\nquences of A P P LYRU LE actions. For HS and\nD JANGO, we rst convert the Python code into\nASTs using the standard ast module. Values\ninside variable terminal nodes are tokenized by\nspace and camel case (e.g., ClassName is tok-\nenized to Class and Name). We then traverse the\nAST in pre-order to generate the reference action\nsequence according to the grammar model.\n\nD Additional Decoding Examples\nWe provide extra decoding examples from the\nD JANGO and HS datasets, listed in Table 6 and Ta-\nble 7, respectively. The model heavily relies on the\npointer network to copy variable names and con-\nstants from input descriptions. We nd the source\nof errors in D JANGO is more diverse, with most\nincorrect examples resulting from missing argu-\nments and incorrect words copied by the pointer\nnetwork. Errors in HS are mostly due to partially\nor incorrectly implemented effects. Also note that\nthe rst example in Table 6 is semantically cor-\nrect, although it was considered incorrect under\nour exact-match metric. This suggests more ad-\nvanced evaluation metric that takes into account\nthe execution results in future studies.\n\nSupplementary Materials\nA Encoder LSTM Equations\n\nSuppose the input natural language description x\nconsists of n words {wi}n\ni=1 . Let wi denote the\nembedding of wi . We use two LSTMs to process\nx in forward and backward order, and get the se-\nquence of hidden states {(cid:126)hi}n\ni=1 and { (cid:126)hi}n\ni=1 in\nthe two directions:\n(cid:126)hi = f \nLSTM (wi , (cid:126)hi1 )\n(cid:126)hi = f \nLSTM (wi , (cid:126)hi+1 ),\nwhere f \nLSTM and f \nLSTM are standard LSTM up-\ndate functions. The representation of the i-th\nword, hi , is given by concatenating (cid:126)hi and (cid:126)hi .\n\nB Inference Algorithm\n\nGiven an NL description, we approximate the best\nAST y in Eq. 1 using beam search. The inference\nprocedure is listed in Algorithm 1.\nWe maintain a beam of size K . The beam is\ninitialized with one hypothesis AST with a single\nroot node (line 2). At each time step, the decoder\nenumerates over all hypotheses in the beam. For\neach hypothesis AST, we rst nd its frontier node\nnft (line 6). If nft is a non-terminal node, we col-\nlect all syntax rules r with nft as the head node\nIf nft is a variable\nto the actions set (line 10).\nterminal node, we add all terminal tokens in the\nvocabulary and the input description as candidate\nactions (line 13). We apply each candidate action\non the current hypothesis AST to generate a new\nhypothesis (line 15). We then rank all newly gen-\nerated hypotheses and keep the top-K scored ones\nin the beam. A complete hypothesis AST is gener-\nated when it has no frontier node. We then convert\nthe top-scored complete AST into the surface code\n(lines 18-19).\nWe remark that our inference algorithm can\nbe implemented efciently by expanding multi-\nple hypotheses (lines 5-16) simultaneously using\nmini-batching on GPU.\n\nC Dataset Preprocessing\nInfrequent Words We replace word types whose\nfrequency is lower than d with a special <unk>\ntoken (d = 3 for D JANGO, 3 for HS and 2 for\nI FT TT).\nCanonicalization We perform simple canonical-\nization for the D JANGO dataset:\n(1) We ob-\nserve that\ninput descriptions often come with\n\n\fAlgorithm 1: Inference Algorithm\nInput\n: NL description x\nOutput: code snippet c\n1 call Encoder to encode x\n2 Q = {y0 (root )}\n3 for time step t do\nQ(cid:48) = \n4\nforeach hypothesis yt  Q do\n5\nnft = FrontierNode(yt )\n6\nA = \n7\nif nft is non-terminal then\n8\nforeach production rule r with nft as the head node do\n9\nA = A  {A P P LYRU LE[r ]}\n10\n11\nforeach terminal token v do\n12\nA = A  {G ENTOKEN[v ]}\n13\nforeach action at  A do\n14\ny (cid:48)\nt = ApplyAction(yt , at )\n15\nQ(cid:48) = Q(cid:48)  {y (cid:48)\nt }\n16\nQ = top-K scored hypotheses in Q(cid:48)\n17\n18 y = top-scored complete hypothesis AST\n19 convert y to surface code c\n20 return c\n\nelse\n\n(cid:46) Initialize a beam of size K\n\n(cid:46) Initialize the set of candidate actions\n\n(cid:46) A P P LYRU LE actions for non-terminal nodes\n\n(cid:46) G ENTOKEN actions for variable terminal nodes\n\ninput for every i in range of integers from 0 to length of result, not included\npred. for i in range(0, len(result)): \u0013\nref. for i in range(len(result)):\ninput call the function blankout with 2 arguments: t.contents and B, write the result to out.\nref. out.write(blankout(t.contents, B))\npred. out.write(blankout(t.contents, B)) \u0013\npred. code list.append(foreground[v]) \u0013\nref. code list.append(foreground[v])\ninput zip elements of inner result and inner args into a list of tuples, for every i item and i args in the result\nref. for i item, i args in zip(inner result,\npred. for i item, i args in zip(inner result,\ninner args): \u0013\ninner args):\ninput activate is a lambda function which returns None for any argument x.\nref. activate = lambda x: None\npred. activate = lambda x: None \u0013\ninput if elt is an instance of Choice or NonCapture classes\nref. if isinstance(elt, (Choice, NonCapture)):\npred. if isinstance(elt, Choice): \u0017\ninput get translation function attribute of the object t, call the result with an argument eol message, substitute the result for\nresult.\npred. translation function = getattr(t,\nref. result = getattr(t, translation function)(\ntranslation function) \u0017\neol message)\ninput for every s in strings, call the function force text with an argument s, join the results in a string, return the result.\npred. return .join(force text(s)) \u0017\nref. return .join(force text(s) for s in strings)\ninput for every p in parts without the rst element\npred. for p in p[1:]: \u0017\nref. for p in parts[1:]:\ninput call the function get language, split the result by -, substitute the rst element of the result for base lang.\nref. base lang = get language().split()[0]\npred. base lang = get language().split()[0] \u0017\n\nTable 6: Predicted examples from D JANGO dataset. Copied contents (copy probability > 0.9) are highlighted\n\n\finput <name> Burly Rockjaw Trogg </name> <cost> 5 </cost> <attack> 3 </attack> <defense> 5 </defense>\n<desc> Whenever your opponent casts a spell, gain 2 Attack. </desc> <rarity> Common </rarity> ...\npred. class BurlyRockjawTrogg(MinionCard):\ninit (self):\ndef\nsuper(). init (Burly Rockjaw Trogg, 4, CHARACTER CLASS.ALL, CARD RARITY.COMMON)\ndef create minion(self, player):\nreturn Minion(3, 5, effects=[Effect(SpellCast(player=EnemyPlayer()),\nActionTag(Give(ChangeAttack(2)), SelfSelector()))]) \u0013\n\ninput <name> Maexxna </name> <cost> 6 </cost> <attack> 2 </attack> <defense> 8 </defense> <desc> Destroy\nany minion damaged by this minion. </desc> <rarity> Legendary </rarity> ...\npred. class Maexxna(MinionCard):\ninit (self):\ndef\nsuper(). init (Maexxna, 6, CHARACTER CLASS.ALL, CARD RARITY.LEGENDARY,\nminion type=MINION TYPE.BEAST)\ndef create minion(self, player):\nreturn Minion(2, 8, effects=[Effect(DidDamage(), ActionTag(Kill(),\nTargetSelector(IsMinion())))]) \u0013\n\ninput <name> Hellre </name> <cost> 4 </cost> <attack> -1 </attack> <defense> -1 </defense> <desc> Deal 3\ndamage to ALL characters. </desc> <rarity> Free </rarity> ...\npred. class Hellfire(SpellCard):\ninit (self):\ndef\nsuper(). init (Hellfire, 4, CHARACTER CLASS.WARLOCK, CARD RARITY.FREE)\n\ndef use(self, player, game):\nsuper().use(player, game)\nfor minion in copy.copy(game.other player.minions):\nminion.damage(player.effective spell damage(3), self) \u0017\nref. class Hellfire(SpellCard):\ninit (self):\ndef\nsuper(). init (Hellfire, 4, CHARACTER CLASS.WARLOCK, CARD RARITY.FREE)\n\ndef use(self, player, game):\nsuper().use(player, game)\ntargets = copy.copy(game.other player.minions)\ntargets.extend(game.current player.minions)\ntargets.append(game.other player.hero)\ntargets.append(game.current player.hero)\nfor minion in targets:\nminion.damage(player.effective spell damage(3), self)\nreason Partially implemented effect: only deal 3 damage to opponents characters\ninput <name> Darkscale Healer </name> <cost> 5 </cost> <attack> 4 </attack> <defense> 5 </defense> <desc>\nBattlecry: Restore 2 Health to all friendly characters. </desc> <rarity> Common </rarity> ...\npred. class DarkscaleHealer(MinionCard):\ninit (self):\ndef\nsuper(). init (Darkscale Healer, 5, CHARACTER CLASS.ALL,\nCARD RARITY.COMMON, battlecry=Battlecry(Damage(2),\nCharacterSelector(players=BothPlayer(), picker=UserPicker())))\n\ndef create minion(self, player):\nreturn Minion(4, 5) \u0017\nref. class DarkscaleHealer(MinionCard):\ninit (self):\ndef\nsuper(). init (Darkscale Healer, 5, CHARACTER CLASS.ALL,\nCARD RARITY.COMMON, battlecry=Battlecry(Heal(2), CharacterSelector()))\n\ndef create minion(self, player):\nreturn Minion(4, 5)\nreason Incorrect effect: damage 2 health instead of restoring. Cast effect to all players instead of friendly players only.\n\nTable 7: Predicted card examples from HS dataset. Copied contents (copy probability > 0.9) are highlighted.\n\n\f", 
        "tag": "Software Engineering", 
        "link": "https://arxiv.org/list/cs.SE/new"
    }, 
    {
        "text": "REVISITING THE PROBLEM OF AUDIO-BASED HIT SONG PREDICTION USING\nCONVOLUTIONAL NEURAL NETWORKS\n\nLi-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen\nResearch Center for Information Technology Innovation, Academia Sinica, Taiwan\nMachine Learning Research Team, KKBOX Inc., Taiwan\n\n7\n1\n0\n2\n \nr\np\nA\n \n5\n \n \n]\nD\nS\n.\ns\nc\n[\n \n \n1\nv\n0\n8\n2\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nABSTRACT\nBeing able to predict whether a song can be a hit has impor-\ntant applications in the music industry. Although it is true\nthat the popularity of a song can be greatly affected by exter-\nnal factors such as social and commercial inuences, to which\ndegree audio features computed from musical signals (whom\nwe regard as internal factors) can predict song popularity is\nan interesting research question on its own. Motivated by the\nrecent success of deep learning techniques, we attempt to ex-\ntend previous work on hit song prediction by jointly learning\nthe audio features and prediction models using deep learning.\nSpecically, we experiment with a convolutional neural net-\nwork model that takes the primitive mel-spectrogram as the\ninput for feature learning, a more advanced JYnet model that\nuses an external song dataset for supervised pre-training and\nauto-tagging, and the combination of these two models. We\nalso consider the inception model to characterize audio infor-\nmation in different scales. Our experiments suggest that deep\nstructures are indeed more accurate than shallow structures\nin predicting the popularity of either Chinese or Western Pop\nsongs in Taiwan. We also use the tags predicted by JYnet to\ngain insights into the result of different models.\nIndex Terms Hit song prediction, deep learning, con-\nvolutional neural network, music tags, cultural factors\n\n1. INTRODUCTION\n\nThe popularity of a song can be measured a posteriori ac-\ncording to statistics such as the number of digital downloads,\nplaycounts, listeners, or whether the song has been listed in\nthe Billboard Chart once or multiple times. However, for mu-\nsic producers and artists, it would be more interesting if song\npopularity can be predicted a priori before the song is ac-\ntually released. For music streaming service providers, an\nautomatic function to identify emerging trends or to discover\npotentially interesting but not-yet-popular artists is desirable\nto address the so-called long tail of music listening [1]. In\nacademia, researchers are also interested in understanding the\n\nThis work was partially supported by the Ministry of Science and Tech-\nnology of Taiwan under Contracts 104-2221-E-001-029-MY3 and 105-2221-\nE-001-019-MY2.\n\nfactors that make a song popular [2,3]. This can be formulated\nas a pattern recognition problem, where the task is to gener-\nalize observed association between song popularity measure-\nments and feature representation characterizing the songs in\nthe training data to unseen songs [4].\nOur literature survey shows that this automatic hit song\nprediction task has been approached using mainly two differ-\nent information sources: 1) internal factors directly relating\nto the content of the songs, including different aspects of au-\ndio properties, song lyrics, and the artists; 2) external factors\nencompassing social and commercial inuences (e.g. concur-\nrent social events, promotions or album cover design).\nThe majority of previous work on the internal factors of\nsong popularity are concerned with the audio properties of\nmusic. The early work of Dhanaraj and Logan [4] used sup-\nport vector machine to classify whether a song will appear\nin music charts based on latent topic features computed from\naudio Mel-frequency cepstral coefcients (MFCC) and song\nlyrics. Following this work, Pachet et al. [5] employed a large\nnumber of audio features commonly used in music informa-\ntion retrieval (MIR) research and concluded that the features\nthey used are not informative enough to predict hits, claiming\nthat hit song science is not yet a science. Ni et al. [6] took\na more optimistic stand, showing that certain audio features\nsuch as tempo, duration, loudness and harmonic simplicity\ncorrelate well with the evolution of musical trends. How-\never, their work analyzes the evolution of hit songs [79],\nrather than discriminates hits from non-hits. Fan et al. [10]\nperformed audio-based hit song prediction of music charts in\nmainland China and UK and found that Chinese hit song pre-\ndiction is more accurate than the UK version. Purely lyric-\nbased hit song prediction was relatively unexplored, except\nfor the work presented by Singhi and Brown [11].\nOn the other hand, on external factors, Salganik et al. [12]\nshowed that the song itself has relatively minor role than the\nsocial inuences for deciding whether a song can be a hit.\nZangerla et al. [13] used Twitter posts to predict future charts\nand found that Twitter posts are helpful when the music charts\nof the recent past are available.\nTo our best knowledge, despite its recent success in var-\nious pattern recognition problems, deep learning techniques\nhave not be employed for hit song prediction. In particular,\n\n\fin speech and music signal processing, convolutional neural\nnetwork (CNN) models have exhibited remarkable strength in\nlearning task-specic audio features directly from data, out-\nperforming models based on hand-crafted audio features in\nmany prediction tasks [1416].\nWe are therefore motivated to extend previous work on\naudio-based hit song prediction by using state-of-the-art\nCNN-based models, using either the primitive,\nlow-level\nmel-spectrogram directly as the input for feature learning, or\na more advanced setting [17] that exploits an external mu-\nsic auto-tagging dataset [18] for extracting high-level audio\nfeatures. Moreover, instead of using music charts, we use a\ncollection of user listening data from KKBOX Inc., a leading\nmusic streaming service provider in East Asia. We formulate\nhit song prediction as a regression problem and test how we\ncan predict the popularity of Chinese and Western Pop music\namong Taiwanese KKBOX users, whose mother tongue is\nMandarin. Therefore, in addition to testing whether deep\nmodels outperform shallow models in hit song prediction, we\nalso investigate how the culture origin of songs affects the\nperformance of different CNN models.\n\n2. DATASET\n\nBecause we are interested in discriminating hits and non-hits,\nwe nd it informative to use the playcounts a song receives\nover a period of time from streaming services to dene song\npopularity and formulate a regression problem to predict song\npopularity. In collaboration with KKBOX Inc., we obtain a\nsubset of user listening records contributed by Taiwanese lis-\nteners over a time horizon of one year, from Oct. 2012 to\nSep. 2013, involving the playcounts of close to 30K users\nfor around 125K songs. Based on the language metadata pro-\nvided by KKBOX, we compile a Mandarin subset featuring\nChinese Pop songs and a Western subset comprising of songs\nsung mainly in English. There are more songs in the West-\nern subset but the Mandarin songs receive more playcounts\non average, for Mandarin is the mother tongue of Taiwanese.\nThe following steps are taken to gain insights into the data\nand for data pre-processing. First, as the songs in our dataset\nare released in different times, we need to check whether we\nhave to compensate for this bias, for intuitively songs released\nearlier can solicit more playcounts. We plot in Fig. 1 the\naverage playcounts of songs released in different time peri-\nods, where Q1 denotes the rst three months starting from\nOct. 2012 and Q1 the most recent three months before Oct.\n2012, etc. The y-axis is in log scale but the actual values are\nobscured due to a condentiality agreement with KKBOX.\nFrom the dash lines we see that the average playcounts from\ndifferent time periods seem to be within a moderate range in\nthe log scale for both subsets, exempting the need to compen-\nsate for the time bias by further operations.\nSecond, we dene the hit score of a song according to the\nmultiplication of its playcount in log scale and the number of\n\nFig. 1. The average playcounts (in log scale) of songs released\nin different time periods.\n\nFig. 2. The distribution of hit scores (see Section 2 for deni-\ntion) in the (left) whole and (right) test sets.\n\nusers (also in log scale) who have listened to the song. We\nopt for not using the playcounts only to measure song pop-\nularity because it is possible that the playcount of a song is\ncontributed by only a very small number of users.\nThird, to make our experimental results on the two subsets\ncomparable, we sample the same amount of 10K songs in our\nexperiment for both subsets. These songs are those with the\nhighest playcounts within the subset. It can be seen from Fig.\n2 that the distributions of hit scores of the sampled songs are\nsimilar. The solid lines in Fig. 1 show that after this sampling\nthe time bias among the sampled songs remains moderate.\nFinally, we randomly split the songs to have 8K, 1K, and\n1K songs as the training, validation, and test data for each\nof the subsets. Although it may be more interesting to split\nthe songs according to their release dates so as to learn from\nthe past and predict the future, we leave this as a future work.\nOur focus here is to study whether deep models perform better\nthan shallow models in audio-based hit song prediction.\nThe scale and the time span of the dataset are deemed\nappropriate for this study. Unlike previous work on musical\ntrend analysis that may involve more than ten years worth of\ndata (e.g. [6], [19]), for the purpose of our work we want to\navoid changes in public music tastes and therefore it is better\nto use listening records collected within a year.\n\n\f3. METHODS\n\nWe formulate hit song prediction as a regression problem and\ntrain either shallow or deep neural network models for pre-\ndicting the hit scores. Given the audio representation xn for\neach song n in the training set, the objective is to optimize the\nparameters  of our model f () by minimizing the squared er-\n(cid:80)\nror between the ground truth yn and our estimate, expressed\nn (cid:107)yn  f (xn )(cid:107)2\nas min\n2 . As described below, a total\nnumber of six methods are considered, All of them are im-\nplemented based on the lasagne library [20], and the model\nsettings such as learning rate update strategy, dropout rate,\nand numbers of feature maps per layer are empirically tuned\nby using the validation set.\n\n3.1. Method 1 (m1): LR\n\nAs the simplest method, we compute 128-bin log-scaled mel-\nspectrograms [21] from the audio signals and take the mean\nand standard deviation over time, leading to a 256-dim feature\nvector per song. The feature vectors are used as the input to\na single-layer shallow neural network model, which is effec-\ntively a linear regression (LR) model. The mel-spectrograms\nare computed by short-time Fourier transform with 4,096-\nsample, half-overlapping Hanning windows, from the middle\n60-second segment of each song, which is sampled at 22 kHz.\nIn lasagne, we can implement the LR model by a dense layer.\n\n3.2. Method 2 (m2): CNN\n\nGoing deeper, we use the mel-spectrograms directly as the in-\nput, which is a 128 by 646 matrix for there are 646 frames\nper song, to a CNN model. Our CNN model consists of two\nearly convolutional layers, with respectively 128-by-4 and 1-\nby-4 convolutional kernels, and three late convolutional lay-\ners, which all has 1-by-1 convolutional kernels. Unlike usual\nCNN models, we do not use fully connected layers in the lat-\nter half of our model for such fully convolutional model has\nbeen shown more effective for music [14, 17, 22].\n\n3.3. Method 3 (m3): inception CNN\n\nThe idea of inception was introduced in GoogLeNet for visual\nproblems [23]. It uses multi-scale kernels to learn features.\nWe make an audio version of it by adding two more parallel\nearly convolutional layers with different sizes: 132-by-8 and\n140-by-16, as illustrated in the bottom-right corner of Fig. 3.\nTo combine the output of these three kernels by concatena-\ntion, the input mel-spectrogram needs to be zero-padded.\n\n3.4. Method 4 (m4): JYnet (a CNN model) + LR\n\nWhile generic audio features such as mel-spectrogram may be\ntoo primitive to predict hits, we employ a state-of-art music\nauto-tagging system referred to as the JYnet [17] to compute\n\nFig. 3. Architecture of the investigated CNN models.\n\nhigh-level tag-based features. JYnet is another CNN model\nthat also takes the 128-bin log-scaled mel-spectrograms as the\ninput, but the model is trained to make tag prediction using\nthe MagnaTagATune dataset [18]. The output is the activa-\ntion scores of 50 music tags, including genres, instruments,\nand other performing related tags such as male vocal, female\nvocal, fast and slow. From the output of JYnet (i.e. 50-dim\ntag-based features), we learn another LR model for predicting\nhit scores, as illustrated in the bottom-left corner of Fig. 3.\n\n3.5. Methods 5 and 6 (m5 & m6): Joint Training\n\nWe also try to combine (m4) with (m2) or (m3) to exploit\ninformation in both the mel-spectrograms and tags, leading to\n(m5) and (m6). Instead of simply combining the results of\nthe two models f1 () and f2 () being combined, we add\nanother layer on top of them for joint training, as illustrated\n(cid:88)\nin Fig. 3. The learning objective becomes:\n(cid:107)yn  wf1 (xn )  (1  w)f2 (xn )(cid:107)2\n2 ,\nn\n\nmin\nw,1 ,2\n\n(1)\n\nwhere w determines their relative weight.\nIn this way, we\ncan optimize the model parameters of both models jointly.\nHowever, when method 4 is used in joint training we only\nupdate the parameters of its LR part, as JYnet is treated as an\nexternal, pre-trained model in our implementation.\n\n4. EXPERIMENTAL RESULTS\n\nWe train and evaluate the two data subsets separately. For\nevaluation, the following four metrics are considered:\n Recall@100: Treating the 100 songs (i.e. 10%) with\nthe highest hit scores among the 1,000 test songs as the\nhit songs, we rank all the test songs in descending order\nof the predicted hit scores and count the number of hit\nsongs that occur in the top 100 of the resulting ranking.\n nDCG@100: normalized discounted cumulative gain\n(nDCG) is another popular measure used in ranking\n\n\fMethod\nrecall\n(m1) audio+LR\n0.1900\n(m2) audio+CNN\n0.2300\n(m3) audio+inception CNN 0.2500\n(m4) tag+LR\n0.2400\n(m5) (m2)+(m4)\n0.2500\n(m6) (m3)+(m4)\n0.3000\n\nTable 1. Accuracy of Hit Song Prediction\nMandarin subset\nKendall\nnDCG\n0.1679\n0.1997\n0.2334\n0.1806\n0.2286\n0.2369\n0.1671\n0.2372\n0.2018\n0.2558\n0.2927\n0.2665\n\nSpearman\n0.2480\n0.2678\n0.3374\n0.2473\n0.2971\n0.3894\n\nrecall\n0.1400\n0.1300\n0.1800\n0.2000\n0.1800\n0.2100\n\nWestern subset\nKendall\nnDCG\n0.0674\n0.1271\n0.1294\n0.1031\n0.1093\n0.1989\n0.0918\n0.1774\n0.1300\n0.1791\n0.2413\n0.1341\n\nSpearman\n0.1002\n0.1564\n0.1636\n0.1372\n0.1941\n0.1996\n\nproblems [24]. It is computed in a way similar to re-\ncall@100, but the positions of recalled hit songs in the\nranking list are taken into account.\n Kendalls  : we directly compare the ground truth and\npredicted rankings of the test songs in hit scores (with-\nout dening which songs are hit songs) and compute a\nvalue that is based on the number of correctly and in-\ncorrectly ranked pairs [25].\n Spearmans : the rank correlation coefcient (consid-\nering the relative rankings but not the actual hit scores)\nbetween the ground truth and predicted rankings.\nThe result is shown in Table 1, which is obtained by aver-\naging the result of 10 repetition of each method. The follow-\ning observations can be made. First, by comparing the result\nof (m1), (m2) and (m3), we see that better result in most of\nthe four metrics is obtained by using deeper and more compli-\ncated models for both subsets. This suggests the effectiveness\nof deep structures for this task. Furthermore, by comparing\nthe result of the two subsets, we see that audio-based hit song\nprediction is easier for the Mandarin subset, conrming the\nndings of Fan et al. [10].\nSecond, as both (m1) and (m4) use LR for prediction, by\ncomparing their result we see that the tag-based method (m4)\noutperforms the simple audio-based method (m1) in all the\nfour metrics for the Western subset, demonstrating the effec-\ntiveness of the JYnet tags. This is however not the case for\nthe Mandarin subset for Kendalls  and Spearmans .\nThird, from the result of (m5) and (m6), we see that the\njoint learning structure can further improve the result for both\nsubsets. The best result is obtained by (m6) in all metrics.\nTo gain insights, we employ JYnet to assign genre labels\nto all the test songs and examine the distribution of genres in\nthe top-50 hit songs determined by either automatic models or\nthe ground truth. For each song, we pick the genre label that\nhas the strongest activation as predicted by JYnet. The result-\ning genre distributions are shown in Fig. 4. We see from the\nresult of ground truth that the Western hits have more diverse\ngenres. The predominance of Pop songs in the Mandarin\nsubset might explain why 1) hit song prediction in this subset\nis easier and 2) (m4) alone cannot improve  and . More-\nover, for the Western subset, we see that the genre distribu-\ntion of (m4) is more diverse than that of (m3), despite that\n\nFig. 4. The predominate tags (predicted by JYnet) for the\ntop-50 hit songs determined by different methods for the (top)\nMandarin and (bottom) Western subsets. From left to right:\n(a) the tag-based model (m4), (b) the audio-based model\n(m3), (c) the hybrid model (m6), and (d) the ground truth.\n\n(m3) achieves slightly higher nDCG and Spearmans . This\nmight imply that the ability to match the genre distribution of\nthe ground truth is another important performance indicator.\n\n5. CONCLUSION\n\nIn this paper, we have introduced state-of-the-art deep learn-\ning techniques to the audio-based hit song prediction prob-\nlem. Instead of aiming at classifying hits from non-hits, we\nformulate it as a regression problem. Evaluations on the lis-\ntening data of Taiwanese users of a streaming company called\nKKBOX conrms the superiority of deep structures over shal-\nlow structures in predicting song popularity. Deep structures\nare in particular important for Western songs, as simple shal-\nlow models may not capture the rich acoustic and genre di-\nversity exhibited in Western hits. For future work, we hope\nto understand what our neural network models actually learn,\nto compare against more existing methods (preferably using\nthe same datasets), and to investigate whether our models can\npredict future charts or emerging trends.\n\n\f6. REFERENCES\n\n[1] H. Silk, R. Santos-Rodriguez, C. Mesnage, T. De Bie,\nand M. McVicar,\nData science for the detection of\nemerging music styles, EPSRC, pp. 46, 2014.\n\n[2] S. McClary, Studying popular music, vol. 10, 1991.\n\n[3] P. D. Lopes, Innovation and diversity in the popular\nmusic industry, 1969 to 1990, American Sociological\nReview, vol. 57, no. 1, pp. 56, 1992.\n\n[4] R. Dhanaraj and B. Logan, Automatic prediction of\nhit songs, in Proceedings of International Society for\nMusic Information Retrieval, pp. 1115, 2005.\n\n[5] F. Pachet and P. Roy, Hit song science is not yet a sci-\nence, in Proceedings of International Society for Music\nInformation Retrieval, pp. 355360, 2008.\n\n[6] Y. Ni and R. Santos-Rodriguez, Hit song science once\nagain a science, International Workshop on Machine\nLearning and Music, pp. 23, 2011.\n\n[7] R. M. MacCallum, M. Mauch, A. Burt, and A. M. Leroi,\nEvolution of music by public choice, in Proceedings\nof the National Academy of Sciences, vol. 109, no. 30,\npp. 1208112086, 2012.\n\n[8] M. Mauch, R. M. MacCallum, M. Levy, and A. M.\nLeroi, The evolution of popular music: USA 1960-\n2010, Royal Society Open Science, vol. 2, no. 5, pp.\n150081, 2015.\n\n[9] J. Serr `a, A. Corral, M. Bogu n a, M. Haro, and J. L. Ar-\ncos, Measuring the evolution of contemporary western\npopular music, Scientic Reports, vol. 2, pp. 16, 2012.\n\n[10] J. Fan and M. Casey, Study of Chinese and UK hit\nsongs prediction, 10th International Symposium on\nComputer Music Multidisciplinary Research (CMMR),\npp. 640652, 2013.\n\n[11] A. Singhi and D. G. Brown, Hit song detection using\nlyric features alone, in Proceedings of International\nSociety for Music Information Retrieval, 2014.\n\n[12] M. J. Salganik, P. S. Dodds, and D. J. Watts, Experi-\nmental study of inequality and cultural market, Science,\nvol. 311, no. 5762, pp. 854856, 2006.\n\n[13] E. Zangerle, M. Pichl, B. Hupfauf, and G. Specht, Can\nmicroblogs predict music charts ? an analysis of the\nrelationship between # nowplaying tweets and music\ncharts, in Proceedings of International Society for Mu-\nsic Information Retrieval, 2016.\n\n[14] K. Choi, G. Fazekas, and M. Sandler, Automatic tag-\nging using deep convolutional neural networks, in Pro-\nceedings of International Society for Music Information\nRetrieval, 2016.\n\n[15] O. Abdel-Hamid, A.-R. Mohamed, H. Jiang, L. Deng,\nG. Penn, and D. Yu, Convolutional neural networks for\nspeech recognition, IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing, pp. 15331545,\n2014.\n\n[16] S. Dieleman and B. Schrauwen, End-to-end learning\nin IEEE International Conference\nfor music audio,\non Acoustics, Speech and Signal Processing, 2014, pp.\n69646968.\n\n[17] J.-y. Liu and Y.-h. Yang, Event localization in Music\nauto-tagging, in Proceedings of the 24th ACM interna-\ntional conference on Multimedia, 2016.\n\n[18] E. Law, K. West, M. I. Mandel, M. Bay, and J. S.\nDownie, Evaluation of algorithms using games: the\ncase of music tagging, in Proceedings of International\nSociety for Music Information Retrieval, 2009.\n\n[19] S. Kinoshita, T. Ogawa, and M. Haseyama, Popular\nmusic estimation based on topic model using time infor-\nmation and audio features, IEEE 3rd Global Confer-\nence on Consumer Electronics (GCCE), pp. 102103,\n2014.\n\n[online]\n[20] lasagne,\nhttps://lasagne.\nreadthedocs.org/en/latest/.\n\n[21] S. Dieleman and B. Schrauwen, Multiscale approaches\nto music audio feature learning, in Proceedings of In-\nternational Society for Music Information Retrieval, pp.\n116121, 2013.\n\n[22] E. Shelhamer, J. Long, and T. Daeedll, Fully convolu-\ntional networks for semantic segmentation, Computer\nVision and Pattern Recognition (CVPR), 2015.\n\n[23] C. Szegedy, W. Liu, Y. Jia, and P. Sermanet,\nGo-\ning deeper with convolutions, arXiv preprint arXiv:\n1409.4842, 2014.\n\n[24] Y. Wang, L. Wang, Y. Li, D. He, T.-Y. Liu, and W. Chen,\nA theoretical analysis of NDCG ranking measures, in\nProceedings of the Annual Conference on Learning The-\nory, pp. 130, 2013.\n\n[25] M. Kendall and J. D. Gibbons, Rank correlation meth-\nods, vol. 3, Oxford University Press, 1990.\n\n\f", 
        "tag": "Sound", 
        "link": "https://arxiv.org/list/cs.SD/new"
    }, 
    {
        "text": "7\n1\n0\n2\n \nr\na\nM\n \n9\n2\n \n \n]\nS\nD\n.\ns\nc\n[\n \n \n1\nv\n2\n3\n2\n0\n1\n.\n3\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nRecursive Method for the Solution\nof Systems of Linear Equations \n\nGennadi.I.Malaschonok\nTambov State University, 392622 Tambov, Russia\ne-mail: malaschonok@math-univ.tambov.su\n\nAbstract\n\nNew solution method for the systems of linear equations in commutative\nintegral domains is proposed. Its complexity is the same that the complexity\nof the matrix multiplication.\n\n1\n\nIntroduction\n\nOne of the rst results in the theory of computational complexity is the\nStrassen discovery of the new algorithm for matrix multiplication [1]. He\nchanged the classical method with the complexity O(n3 ) for the new algo-\nrithm with the complexity O(nlog2 7 ). This method may be used for a matrix\nin any commutative ring. He used matrix multiplication for the computation\nof the inverse matrix, of the determinant of a matrix and for the solution of\nthe systems of linear equations over an arbitrary eld with the complexity\nO(nlog2 7 ).\nMany authors improved this result. There is known now an algorithm\nof matrix multiplication with the complexity O(n2,37 ) (see D.Coppersmith,\nS.Winograd [2]).\nWe have another situation with the problems of the solution of systems\nof linear equations and of the determinant computation in the commutative\nrings. Dodgson [3] proposed a method for the determinant computation and\nthe solution of systems of linear equations over the ring of integer numbers\nwith the complexity O(n3 ). During this century this result was improved and\ngeneralized for arbitrary commutative integral domain due to Bareis [4] and\nthe author (see [5]  [8]). But the complexity is still O(n3 ).\nThere is proposed the new solution method for the systems of linear equa-\ntions in integral domains. Its complexity is the same that the complexity of\nthe matrix multiplication in integral domain.\n\nThis paper vas published in: Computational Mathematics (A. Sydow Ed, Proceedings of the\n15th IMACS World Congress, Vol.\nI, Berlin, August 1997 ), Wissenschaft & Technik Verlag,\nBerlin 1997, 475480. No part of this materials may be reproduced, stored in retrieval system, or\ntransmitted, in any form without prior permission of the copyright owner.\n\n1\n\n\fLet\n\naij xj = aim ,\n\nm1\nXj=1\nbe the system of linear equations with extended coecients matrix\n\ni = 1, 2, . . . , n\n\nA = (aij ), i = 1, . . . , n, j = 1, . . . , m.\n\nxj =\n\n, j = 1, . . . , n,\n\nwhose coecients are in integral domain R: A  Rnm .\nThe solution of such system may be written according to Cramers rule\njm  Pm1\np=n+1 xp n\nn\nj p\nn\nwhere xp , p = n + 1, . . . , m, are free variables and n 6= 0.\nn = |aij |,\ni = 1, . . . , n, j = 1, . . . , n, - denote the corner minors of the matrix A of\norder n, n\nij - denote the minors obtained by a substitution of the column\nj of the matrix A instead of the column i in the minors n , i = 1, . . . , n,\nj = n + 1, . . . , m. So we need to construct the algorithm of computation of\nthe minor n and the matrix G = ( n\nij ), i = 1, . . . , n, j = n + 1, n + 2, . . . , m.\nThat means that we must make the reduction of the matrix A to the\ndiagonal form\n\nIn denotes the unit matrix of order n.\n\nA  (n In , G).\n\n2 Recursive Algorithm\n\nFor the extended coecients matrix A we shall denote:\n\n\n\n\n the matrix, formed by the surrounding of the submatrix of an order k  1\nin the upper left corner by row i and column j ,\n\na1j\na1,k1\n  \na2j\na2,k1\n  \n...\n...\n. . .\n   ak1,k1 ak1,j\n  \nai,k1\naij\n\na12\na11\na22\na21\n...\n...\nak1,1 ak1,2\nai1\nai2\n\nAk\nij =\n\nij = det Ak\nak\nij ,\n\nij = aij , 0 = 1, k = ak\nkk , k\na1\nij  the determinant of the matrix, that is\nreceived from the matrix Ak\nkk after the substitution of the column i by the\ncolumn j .\nij and ak\nWe shall use the minors k\nij for the construction of the matricez\nr+1,k+1 ap\nap\n\nr+1,k+2\nr+2,k+1 ap\nap\n\nr+2,k+2\n...\n...\nap\nap\nl,k+2\nl,k+1\n\n   ap\nr+1,c\n   ap\nr+2,c\n...\n. . .\nap\n  \nl,c\n\nAr,l,(p)\nk ,c =\n\n\n\n\n2\n\n\fand\n\nGr,l,(p)\nk ,c =\n\np\nr+1,k+1\np\nr+2,k+1\n...\np\nl,k+1\n\n\n\n\n\n R(lr)(ck) , 0  k < n, k < c  n, 0  r < m, r < l  m,\n\np\nr+1,k+2\np\nr+2,k+2\n...\np\nl,k+2\n\n  \n  \n. . .\n  \n\np\nr+1,c\np\nr+2,c\n...\np\nl,c\n\n, Ar,l,(p)\nGr,l,(p)\nk ,c\nk ,c\n1  p  n.\nWe shall describe one recursive step, that makes the following reduction\nof the matrix A to the diagonal form\n\nwhere\n\nA  ( l Ilk , G)\n\nA = Ak ,l,(k+1)\nk ,c\n\n,\n\nG = Gk ,l,(l)\nl,c\n\n0  k < c  m, k < l  n, l < c. Note that if k = 0, l = n and c = m then\nwe get the solution of the system.\nWe can choose the arbitrary integer number s: k < s < l and write the\nmatrix A as the following:\nA = (cid:18) A1\nA2 (cid:19)\nwhere A1 = Ak ,s,(k+1)\n- the upper part of the matrix A consists of the s  k\nk ,c\nrows and A2 = As,l,(k+1)\n- the lower part of the matrix A.\nk ,c\n\n2.1 The rst step\n\nAs the next recurcive step we make the following reduction of the matrix\nA1  R(sk)(ck) to the diagonal form\n\nA1  (s Isk , G1\n2 ),\n\n2 = Gk ,s,(s)\nwhere G1\ns,c\n\n.\n\n2.2 The second step\n\nWe write the matrix A2 in the following way:\n\n1 , A2\nA2 = (A2\n2 )\n\n2 = As,l,(k+1)\n1 = As,l,(k+1)\nconsists of the rst s  k columns and A2\nwhere A2\ns,c\nk ,s\nconsists of the last c  s columns of the matrix A2 .\n2 = As,l,(s+1)\nThe matrix A2\nis obtained from the matrix identity (see the\ns,c\nproof in the next section):\n\nk  A2\n1  G1\n2  A2\n2 = s  A2\n2 .\n\nThe minors k must not equal zero.\n\n3\n\n\f2.3 The third step\n\nAs the next recurcive step we make the following reduction of the matrix\nA2\n2  R(ls)(cs) to the diagonal form\n\n2  ( l Ils , G2\nA2\n2 ),\n\n2 = Gs,l,(l)\nwhere G2\nl,c\n\n.\n\n2.4 The fourst step\n\nWe write the matrix G1\n2 in the following way:\n\n2 , G1\n2 = (G1\nG1\n2 )\n\n2 = Gk ,s,(s)\n2 = Gk ,s,(s)\nconsists of the rst l  s columns and G1\nwhere G1\nl,c\ns,l\nconsists of the last c  l columns of the matrix G1\n2 .\n2 = Gk ,s,(l)\nThe matrix G1\nis obtained from the matrix identity (see the\nl,c\nproof in the next section):\n\ns  G1\n2  G2\n2 =  l  G1\n2  G1\n2 .\n\nThe minors s must not equal zero.\nSo we get\nG = (cid:18) G1\n2 (cid:19)\n2\nG2\n\nand  l .\n\n2.5 Representation of the one recursive step\n\nWe can represent one recursive step as the following reduction of the matrix\nA:\n2 (cid:19) 2 (cid:18) s Isk G1\nA2 (cid:19) 1 (cid:18) s Isk G1\nA = (cid:18) A1\n2 (cid:19) 3\n2\n2\nA2\nA2\nA2\n0\n1\nG1\nG1\n3 (cid:18) s Isk G1\n2 (cid:19) 4 (cid:18)  l Isk\n0\n2 (cid:19) = (  l Ilk\n2\n2\n2\nG2\nG2\n l Ils\n l Ils\n0\n0\n\nG )\n\n3 The Proof of the Main Identities\n\n3.1 The rst matrix identity\n\nThe second step of the algorithm is based on the following matrix identity:\n\nk As,l,(s+1)\ns,c\n\n= sAs,l,(k+1)\ns,c\n\n As,l,(k+1)\nk ,s\n\n Gk ,s,(s)\ns,c\n\n.\n\nSo we must prove the next identities for the matrix elements\n\nij = s ak+1\nk as+1\nij \n\ns\nXp=k+1\n\nak+1\nip\n\n s\npj ,\n\n4\n\n\fi = s + 1, . . . , l; j = s + 1, . . . , c.\nij denote the minors that will stand in the place of the minors k after\nLet k\nthe replacement the row i by the row j . An expansion of the determinant ak+1\nij\naccording to the column j is the following\n\n0\n0\n...\n0\n0\n\nk\nriarj\n\nak+1\nij = k aij \n\n0\n  \n0\n  \n...\n. . .\n  \n0\n   k\nki\n\nk\nXr=1\nTherefore we can write the next matrix identity\n\n\n\n1\n0\n\n0\n1\n\n...\n...\n0\n0\n1i k\nk\n2i\n\n\na11\na12\n  \na1,s\na1j\na21\na22\n  \na2,s\na2j\n\n\n...\n...\n...\n...\n. . .\nas,1\nas,2\n  \nas,s\nas,j\nak+1\nak+1\n   ak+1\nak+1\ni1\ni2\ni,s\nij\nNote that ak+1\nip = 0 for p  k . Finaly we decompose the determinant of\nthe right matrix according to the last row and write the determinant identity\ncorrespondingly to this matrix identity.\n\n0\n0\n0\n0\n...\n...\n1\n0\n0 k\n\n  \n  \n. . .\n  \n  \n\n As+1\nij =\n\n=\n\n3.2 The second matrix identity\n\nThe fourth step of the algorithm bases on the matrix identity\n\nsGk ,s,(l)\nl,c\n\n=  lGk ,s,(s)\nl,c\n\n Gk ,s,(s)\ns,l\n\n Gs,l,(l)\nl,c\n\n.\n\nSo we must prove the next identities for the matrix elements:\n\nij =  l s\ns  l\nij \n\nl\nXp=s+1\n\nip   l\ns\npj ,\n\ni = k + 1, . . . , s; j = l + 1, . . . , c.\nj,i denote the algebraic adjunct of element aj,i in the matrix As\nLet  s\ns,s . An\nexpansion of the determinant s\nip according to the column i is the following\n\ns\nip =\n\n s\nq iaqp\n\ns\nXq=1\nTherefore we can write the next matrix identity:\n\n\n\n\n\n0\n  \n0\n  \n...\n. . .\n  \n0\n    s\ns,i\n\n0   \n0   \n...\n. . .\n0   \n0   \n\n1\n0\n...\n0\n s\n1i\n\n0\n1\n...\n0\n s\n2i\n\n0\n0\n...\n1\n0\n\n0\n0\n...\n0\n0\n\nAl+1\nij =\n\n5\n\n\f=\n\n\n\na1j\n   a1,l\na12\na11\na2j\n   a2,l\na22\na21\n\n\n...\n...\n...\n...\n. . .\nal,j\n  \nal,l\nal,1 al,2\ns\ns\ns\ns\n  \nij\ni2\ni1\ni,l\nii = s . So to nish the proof we must\nip = 0 for p  s and s\nNote that s\ndecompose the determinant of the right matrix according to the last row and\nwrite the determinant identity correspondingly to this matrix identity.\n\n4 Evaluation of Operations Number\n\nLet us have a method for matrix multiplications with the complexity M (n) =\nO(n2+ ), then for multiplication of two matrixes of order l  n and n  c we\nneed M (l  n, n  c) = O(lcn ) operations. Let us denote by S (n, m) the\ncomplexity of the recursive algorithm for the matrix A  Rnm .\nIf in the rst recursive step upper submatrix consists of the s rows, 1 \ns < n, then\n\nS (n, m) = S (s, m) + M ((n  s)  s, s  (m  s))+\n\n+S (n  s, m  s) + M (s  (n  s), (n  s)  (m  n)) + O(nm).\n\nFor a matrix with k rows we can choose the arbitrary s : 1  s  k  1.\nIf the process of partition is dichotomous, and the number of rows in the\nupper and lower submatrixes is the same in every step, then S (2n, m) satises\nthe recursive inequality:\n\nS (2n, m) = S (n, m) + M (n  n, n  (m  n)) + S (n, m  n)+\n\n+M (n  n, n  (m  2n)) + O(nm)  2S (n, m) + 2O(mn+1 ).\n\nSo we have\n\nS (2n, m)  nS (2, m) +\n\n(log2 n)1\nXi=0\n\nO((\n\nn\n2i )+1m)2i+1 =\n\n= nS (2, m) +\n\n2\n1  2 O((n  1)nm)\n\nS (2n, m)  O(mn+1 ).\n\nAnd nally\n\nOn the other hand\n\nS (2n, m) > M (n  n, n  (m  n)) = O(mn+1 ).\n\nTherefore\n\nS (2n, m) = O(mn+1 ).\n\nSo the complexity of this algorithm is the same that the complexity of the\nmatrix multiplication. In particular for m = n + 1 we have\n\nS (n, n + 1) = O(n2+ )\n\n6\n\n\fIt means that the solution of the system of linear equations needs (ac-\ncurate to the constant multiplier) the same number of operations that the\nmultiplication of two matrixes needs.\nWe can get the exact number of operations, that are necessary for the\nsolution of the system of linear equations of order n  m, in the case when on\nevery step upper submatrix is no less then lower submatrix and the number\nof rows in upper submatrix is some power of 2.\nLet F (s, s,  ) = M (( s)s, s(s))+M (s( s), ( s)( )),\nthen we obtain S (n, m):\n\nn\n2k , m  2k (\n\nn/2k \nlog2 n\n(F (2k , n  2k \nXi=1\nXk=1\nLet n = 2p . If we use simple matrix multiplications with complexity n3 than\nwe obtain\n\nF (2k1 , 2k1 , m  (i 1)2k ))\n\nn\n2k  1))+\n\nAnm = (6n2m  4n3  6nm + 3n2 + n)/6,\nMnm = (6n2m  4n3 + (6nm  3n2 ) log2 n  6nm + 4n)/6,\nDnm = ((6nm  3n2 ) log2 n  6nm  n2 + 6m + 3n  2)/6.\nHere we denote by Anm , Mnm , Dnm the numbers of additions/subtractions,\nmultiplications and divisions, and take into account that (6nm  2n2  6m +\n2)/6 divisions in the second step are divisions by 0 = 1, so they do not exist\nin Dnm .\nFor m=n+1 we obtain\n\nAn,n+1 = (2n3 + 3n2  5n)/6,\n\nMn,n+1 = (2n3 + (3n2 + 6n) log2 n  2n)/6,\nDn,n+1 = (3n2 log2 n  7n2 + 6n log2 n + 3n + 4)/6.\nThe general quantity of multiplication and division operations is about n3/3.\nWe can compare these results with one-pass algorithm, that was the best\nn,n+1 = (2n3 + 3n2  5n)/6, M O\nof all known algorithms (see [8]): AO\nn,n+1 =\nn,n+1 = (n3  7n + 6)/6, the general quantity of\n(n3 + 2n2  n  2)/2, DO\nmultiplication and division operations is about 2n3/3.\nIf we use Strassens matrix multiplications with complexity nlog2 7 than\nwe can obtain for n = 2p the general quantity of multiplication and division\noperations\n\nM DS\nn,m = n2 (log2 n  5/3) + 7/15nlog2 7 + (m  n)(n2 + 2n log2 n  n) + 6/5n\n\n\n\nlog2 n1\nn\n2i (8i  7i ){\nXi=1\nFor m = n + 1, n = 2p we get\n\nm  n\n2i\n\n  (\n\nn\n2i  2)\n\nm  n  2i+1 (m  n)/2i+1 \n2i\n\n}.\n\nM DS\nn,n+1 =\n\n7\n15\n\nnlog2 7 + n2 (log2 n \n\n2\n3\n\n) + n(2 log2 n +\n\n1\n5\n\n).\n\n7\n\n\f5 Example\n\nLet us consider next system over the integer numbers\n \n= \n\n\n\n\n\n\n\n\n\n\n5.1 Reduction of the matrix A1 = A02(1)\n05\nonal form\n\n1 1 1\n1\n2 0\n0\n1 2\n0 0\n2\n\n3\n1\n0\n1\n\nx1\nx2\nx3\nx4\n\n4\n4\n2\n1\n\nto the diag-\n\nWe make the next reduction:\n05  (2 I2 , G02(2)\nA02(1)\n25\n\n)\n\n5.1.1\n\n5.1.2\n\n5.1.3\n\n5.1.4\n\n05  (1 I1 , G01(1)\nA01(1)\n15\n\n) = (3; 1, 1, 1, 4)\n\n01 G01(1)\n15  A12(1)\n15 = 1A12(1)\n0A12(2)\n15 =\n= 3(2, 0, 1, 4)  (1)(1, 1, 1, 4) = (5, 1, 4, 8),\n\n0  1.\n\n15  (2 I1 , G12(2)\nA12(2)\n25\n\n) = (5; 1, 4, 8)\n\n1G01(2)\n25 = 2G01(1)\n25  G01(1)\n12 G12(2)\n25 =\n= 5(1, 1, 4)  (1)(1, 4, 8) = (6, 9, 12)\nG01(2)\n25 = (2, 3, 4)\n\nFinally we obtain\n\n(2 I2 , G02(2)\n25\n\n2 3 4\n) = (cid:18) 5 0;\n8 (cid:19)\n0 5; 1\n4\n\n2 = A24(3)\n5.2 Computation of the matrix A2\n25\n0A24(3)\n25 = 2A24(1)\n25  A24(1)\n02 G02(2)\n25 =\n0 2\n= 5  (cid:18) 2\n2 1 (cid:19)  (cid:18) 0 1\n1 0 (cid:19) (cid:18) 2 3\n0\n1\n4\n= (cid:18) 11 4 18\n13 9 (cid:19)\n2\n25 = (cid:18) 11 4 18\n13 9 (cid:19)\n0  1; A24(3)\n2\n\n4\n8 (cid:19) =\n\n8\n\n\f2 = A24(3)\n5.3 Reduction of the matrix A2\n25\nagonal form\n\nto the di-\n\nWe make the next reduction:\n\n25  (4 I2 G24(4)\nA24(3)\n45\n\n)\n\n5.3.1\n\n5.3.2\n\n5.3.3\n\n5.3.4\n\n25  (3 I1 , G23(3)\nA23(3)\n35\n\n) = (11; 4, 18)\n\n23 G23(3)\n35  A34(3)\n35 = 3A34(4)\n2A34(4)\n35 =\n= 11(13, 9)  (2)(4, 18) = (135, 135)\n\nA34(4)\n35 = (27, 27)\n\n35  (4 I1 , G34(4)\nA34(4)\n45\n\n) = (27, 27)\n\n34 G34(4)\n45  G23(3)\n45 = 4G23(3)\n3G23(4)\n45 =\n= 27(18)  (4)(27) = 594, G23(4)\n45 = (54)\n\nFinally, in step (3) we obtain\n\n(4 I2 , G24(4)\n45\n\n) = (cid:18) 27\n0\n\n0; 54\n27; 27 (cid:19)\n\n2 = G02(4)\n5.4 Computation of the matrix G1\n45\n24 G24(4)\n2G02(4)\n45 = 4G02(2)\n45  G02(2)\n45 =\n= 27 (cid:18) 4\n8 (cid:19)  (cid:18) 2 3\n4 (cid:19) (cid:18) 54\n27 (cid:19) = (cid:18) 135\n270 (cid:19)\n1\n45 = (cid:18) 27\n54 (cid:19)\nG02(4)\nThe solution of the system is the following:\n\n4 = 27; G04(4)\n45 =\n\n27\n54\n54\n27\n\n\n\n\n\n\n\n9\n\n\f5.5 Representation of the rst recursive step\n\nWe can represent the rst recursive step as the following\n\n5\n0\n0\n1\n\n4\n0\n2 3\n4\n5 1\n8\n0 2\n2\n1\n0\n0\n2 1\n\n4\n2 3\n5 0\n4\n0 5 1\n8\n0 54\n0 0\n27\n27 27\n0\n0 0\n\nA 1\n\n\n\n\n\n6 Conclusion\n\n3\n\n2\n\n4\n\n\n\n\n\n\n\n\n\n\n\n4\n5 0\n2 3\n0 5 1\n8\n4\n11 4 18\n0 0\n0 0 2\n13 9\n\n\n\n27\n0\n0\n54\n0 54\n27 27\n\n3\n\n\n\n\n27\n0\n0\n0\n\n0\n27\n0\n0\n\n0\n0\n27\n0\n\nThe described algorithm for the solution of the systems of linear equations\nover the integral domain includes the known one-pass method and the method\nof forward and back-up procedures [8]. If in every recursive step the partition\nof the matrix is such that the upper submatrix consists only of one row then\nit is the method of forward and back-up procedures. If the lower submatrix\nconsists only of one row in every step then it is the one-pass method.\nIf the process of partition is dichotomous and the numbers of rows in the\nupper and lower submatrixes are equal in every step, then the complexity of\nthe solution has the same order O(n2+ ) as the complexity of matrix multi-\nplication.\nThe computation of the matrix determinant and the computation of the\nadjugate matrix have the same complexity.\nThis method may be used in any commutative ring if the corner minors\nk , k = 1, 2, . . . , n, do not equal zero and are not zero divisors.\n\nReferences\n\n[1] V. Strassen. Gaussian Elimination is not optimal. Numerische\nMathematik, 1969, 13, 354356.\n\n[2] D. Coppersmith, S. Winograd. in Proc. 19th Annu ACM Symp.\non Theory of Comput., 1987, 16.\n\n[3] C.L. Dodgson. Condensation of determinants, being a new and\nbrief method for computing their arithmetic values. Proc. Royal\nSoc. Lond., 1866, A.15, 150155.\n\n[4] E.N. Bareiss. Sylvesters identity and multistep integer-preserving\nGaussian elimination. Math. Comput., 1968, 22, 565578.\n\n[5] G.I. Malaschonok. Solution of a system of linear equations in an\nintegral domain. USSR Journal of Computational Mathematics\nand Mathematical Physics, 1983, 23, 14971500.\n\n10\n\n\f[6] G.I. Malaschonok. On the solution of a linear equation system\nover commutative ring. Math. Notes of the Acad. Sci. USSR,\n1987, 42, N4, 543548.\n\n[7] G.I. Malaschonok. A new solution method for linear equation\nsystems over the commutative ring. In Int. Algebraic Conf., The-\nses on the ring theory, algebras and modules. Novosibirsk, 1989,\n8283.\n\n[8] G.I. Malaschonok. Algorithms for the solution of systems of linear\nequations in commutative rings. In Eective Methods in Algebraic\nGeometry, Edited by T. Mora and C. Traverso, Progress in Math-\nematics 94, Birkhauser, Boston-Basel-Berlin, 1991, 289298.\n\n11\n\n\f", 
        "tag": "Symbolic Computation", 
        "link": "https://arxiv.org/list/cs.SC/new"
    }, 
    {
        "text": "Control renement for discrete-time\ndescriptor systems: a behavioural approach\nvia simulation relations\n\nF. Chen  , S. Haesaert  , A. Abate  , and S. Weiland \n\n Department of Electrical Engineering\nEindhoven University of Technology, Eindhoven, The Netherlands\n Department of Computer Science\nUniversity of Oxford, Oxford, United Kingdom\n\n7\n1\n0\n2\n \nr\np\nA\n \n6\n \n \n]\nY\nS\n.\ns\nc\n[\n \n \n1\nv\n2\n7\n6\n1\n0\n.\n4\n0\n7\n1\n:\nv\ni\nX\nr\na\n\nAbstract: The analysis of industrial processes, modelled as descriptor systems,\nis often\ncomputationally hard due to the presence of both algebraic couplings and dierence equations\nof high order. In this paper, we introduce a control renement notion for these descriptor\nsystems that enables analysis and control design over related reduced-order systems. Utilising\nthe behavioural framework, we extend upon the standard hierarchical control renement for\nordinary systems and allow for algebraic couplings inherent to descriptor systems.\n\nKeywords: Descriptor systems, simulation relations, control renement, behavioural theory.\n\n1. INTRODUCTION\n\nComplex industrial processes generally contain algebraic\ncouplings in addition to dierential (or dierence) equa-\ntions of high order. These systems, referred to as descriptor\nsystems (Kunkel and Mehrmann, 2006; Dai, 1989), are\ncommonly used in the modelling of mechanical systems.\nThe presence of algebraic equations, or couplings, together\nwith large state dimensions renders numerical simulation\nand controller design challenging. Instead model reduction\nmethods (Antoulas, 2005) can be applied to replace the\nsystems with reduced order ones. Even though most meth-\nods have been developed for systems with only ordinary\ndierence equations, recent research also targets descriptor\nsystems (Cao et al., 2015).\nIn this paper, we newly target the use of descriptor systems\nof reduced order for the veriable design of controllers.\nA rich body of literature on verication and formal con-\ntroller synthesis exists for systems solely composed of\ndierence equations. This includes the algorithmic design\nof certiable (hybrid) controllers and the verication of\npre-specied requirements (Tabuada, 2009; Kloetzer and\nBelta, 2008). Usually, these methods rst reduce the orig-\ninal, concrete systems to abstract systems with nite or\nsmaller dimensional state spaces over which the verica-\ntion or controller synthesis can be run. A such controller\nobtained for the abstract system can be rened over the\nconcrete system leveraging the existence of a similarity re-\nlation, e.g., an (approximate) simulation relation, between\nthe two systems (Tabuada, 2009; Girard and Pappas,\n2011). For the application of these relations in control\nproblems, a hierarchical control framework is presented\nby (Girard and Pappas, 2009). Currently, the control syn-\nthesis over descriptor systems cannot be dealt with in this\nfashion due to the presence of algebraic equations.\n\nThe presence of similarity relations between descrip-\ntor systems has also been a topic under investigation\n\nin (Megawati and Van der Schaft, 2015). This work on\nsimilarity relations deals with continuous-time descriptor\nsystems that are unconstrained and non-deterministic, and\nfocuses on the conditions for bisimilarity and on the con-\nstruction of similarity relations. Instead in this work, we\nspecically consider the control renement problem for\ndiscrete-time descriptor systems via simulation relations\nwithin a behavioural framework, such that properties ver-\nied over the future behaviour of the abstract system are\nalso veried over the concrete controlled system. Within\nthe behavioural theory (Willems and Polderman, 2013),\na formal distinction is made between a system (its be-\nhaviour) and its representations, enabling us to investi-\ngate descriptor systems and renement control problems\nwithout having to directly deal with their inherent anti-\ncausality.\nIn the next section, we dene the notion of dynamical\nsystems and control within a behavioural framework and\nuse it to formalise the control renement problem. Subse-\nquently, Section 3 is dedicated to the exact control rene-\nment for descriptor systems and contains the main results\nof the paper. The last section closes with the conclusions.\n\n2. THE BEHAVIOURAL FRAMEWORK\n\n2.1 Discrete-time descriptor systems\n\nAs introduced by (Willems and Polderman, 2013), we\ndene dynamical systems as follows.\nDenition 1. A dynamical system  is dened as a triple\n = (T, W, B)\nwith the time axis T, the signal space W, and the behaviour\nB  WT . \n\nIn this denition, WT denotes the collection of all time-\ndependent functions w : T  W. The set of tra jectories\nor time-dependent functions given by B represents the\n\n\ftra jectories that are compatible with the system. This set\nis referred to as the behaviour of the system (Willems\nand Polderman, 2013). Generally, the representation of the\nbehaviour of a dynamical system by equations, such as a\nset of ordinary dierential equations, state space equations\nand transfer functions, is non-unique. Hence we distinguish\na dynamical system (its behaviour) from the mathematical\nequations used to represent its governing laws.\n\nWe consider dynamical systems evolving over discrete-time\n(T := N = {0, 1, 2, . . .}) that can be represented by a\ncombination of linear dierence and algebraic equations.\nThe dynamics of such a linear discrete-time descriptor\nsystem (DS) are dened by the tuple (E , A, B , C ) as\nEx(t + 1) = Ax(t) + Bu(t),\ny (t) = C x(t),\nwith the state x(t)  X = Rn , the input u(t)  U = Rp ,\nand the output y (t)  Y = Rk and t  N. Further, E , A \nRnn , B  Rnp and C  Rkn are constant matrices and\nwe presume that rank(B ) = p and rank(C ) = k .\n\n(1)\n\nWe say that a tra jectory w = (u, x, y ), with w : N  (U \nX  Y), satises (1) if for all t  N the equations in\n(1) evaluated at u(t), x(t), x(t + 1), y (t) hold. Then the\ncollection of all tra jectories w denes the ful l behaviour,\nor equivalently the input-state-output behaviour as\nBi/s/o := {(u, x, y )  (U  X  Y)N | (1) is satised}. (2)\nThe variable x is considered as a latent variable, therefore\nthe manifest, or equivalently the input-output behaviour\nassociated with (1) is dened by\nBi/o:= {(u, y )|x  XN s.t. (u, x, y )  Bi/s/o}.\nIf E is non-singular, we refer to the corresponding dynam-\nical system as a non-singular DS. In that case, we can\ntransform (1) into standard state space equations, as\nx(t + 1) = Ax(t) + Bu(t),\ny (t) = C x(t),\nwith A = E1A, B = E1B . Further Bi/s/o as in (2) is\n{(u, x, y )  (U  X  Y)N | (u, x, y ) s.t. (3) holds}.\nSimilarly, if E is non-singular, Bi/o can be dened by (3).\n\n(3)\n\nThe tuple with dynamics (1) denes a dynamical system\n evolving over the combined signal space W = U  X  Y\nwith behaviour B := Bi/s/o given in (2). Similarly, for W\nrestricted to input-output space, the tuple (N, U Y, Bi/o )\ndenes the manifest or induced dynamical system.\n\nWe are specically interested in the behaviour initialised\nat t = 0 with a given set of initial states X0  X. For this,\nwe say that a tra jectory w : N  (U  X  Y) is initialised\nwith X0 if (1) holds and x(0) = x0  X0 . Such a tra jectory,\ninitialised with x0  X0 , is also called the continuation\nof x0 . We refer to the collection of initialised tra jectories\nrelated to X0 as the initialised behaviour Binit\ni/s/o . This\nallows us to formalise our denition of the descriptor\nsystem evolving over N.\nDenition 2. (Discrete-time descriptor systems (DS)). A\n(discrete-time) descriptor system is dened as a dynamical\nsystem  initialised with X0 , whose behaviour can be\nrepresented by the combination of algebraic equations and\ndierence equations given in (1), that is\n\n := (T, W, B) = (N, U  X  Y, Binit\ni/s/o )\n\n(4)\n\nwith\n\n the time axis T := N = {0, 1, 2, . . .},\n the full signal space W := U  X  Y, and\n the initialised behaviour 1\nBinit\ni/s/o = {w  WN |w = (u, x, y ) s.t. (1)\nand s.t. x(0) = x0  X0 }.\n\n2.2 Control of descriptor systems\n\nController synthesis amounts to synthesising a system c ,\ncalled a controller, which, after interconnection with ,\nrestricts the behaviour B of  to desirable (or controlled)\ntra jectories. Thus, in the behavioural framework, control\nis dened through interconnections (or via variable sharing\nas specied next), rather than based on the causal trans-\nmission of signals or information, as in classical system the-\nory. Let 1 = (T, C1 W, B1) and 2 = (T, C2W, B2 ) be\ntwo dynamical systems. Then, as depicted in Fig. 1a and\ndened in (Willems and Polderman, 2013), the intercon-\nnection of 1 and 2 over W, denoted by  = 1 w 2\nwith the shared variable w  W, yields the dynamical\nsystem  = (T, C1  C2  W, B) with B = {(c1 , c2 , w) :\nT  C1  C2  W | (c1 , w)  B1 , (c2 , w)  B2 }.\n\nc1\n\nw\n\nc2\n\n1\n\n2\n\nB\n\nBc\n\nBc\n\n(a) The interconnected system \nobtained via the shared variables\nw in W between dynamical sys-\ntems 1 and 2 with signal spaces\nC1  W and C2  W.\n\n(b) The controlled behaviour\nBc = B  Bc is given as\nthe intersection of the behaviours\nof the dynamical system  and\nits controller c .\n\nFig. 1. The left gure (a) portrays the general interconnec-\ntion of two dynamical systems. In gure (b), the more\nspecic case of behavioural intersection for a system\nand its controller is depicted.\n\nObserve that w  WT contains the signals shared by\nboth 1 and 2 , while c1  CT\n1 only belongs to 1 and\nc2  CT\n2 only belongs to 2 . So, in the interconnected\nsystem, the shared variable w satises the laws of both\nB1 and B2 . Note that it is always possible to trivially\nextend the signal spaces of 1 and 2 (and the associated\nbehaviour) such that a full interconnection structure is\nobtained, that is, such that both C1 and C2 are empty and\nthe behaviour of the interconnected system is B = B1 \nB2 . Hence, a full interconnection of  = (T, W, B ) and\nc = (T, W, Bc ) is simply  w c = (T, W, B \nBc ), with the intersection of the behaviours, denoted by\nBc , as portrayed in Fig. 1b. That is, interconnection\nand intersection are equivalent in full interconnections.\n\nFurther, we dene a well-posed controller c for  as\nfollows.\nDenition 3. Consider a dynamical system  = (T, W, B),\nwith initialised behaviour as dened in (4). We say that a\nsystem c = (T, W, Bc ) is a wel l-posed controller for  if\nthe following conditions are satised:\n\n1 In the sequel the indexes init and i/s/o will be dropped.\n\n\f(1) Bc := B  Bc 6= {};\n(2) For every initial state x0  X0 , there exists a unique\ncontinuation in Bc .\n\nDenote with C() the collection of all well-posed con-\ntrollers for .\n\nWe want a controller that accepts any initial state of\nthe system. This is formalised in the second condition\nby requiring that for any initial state of , there exists a\nunique continuation in Bc . We elucidate the properties\nof a well-posed linear controller as follows.\nExample 1. For a system  as in (1), consider a control ler\nc , which is a DS, and has dynamics given as\n(5)\nEcx(t + 1) = Acx(t) + Bcu(t),\nwith Ec , Ac  Rncn and Bc  Rncp . Suppose that the\ncontrol ler shares the variables u and x with the system .\nThat is, w = (u, x). The interconnected system  w c\nyields the state evolutions of the combined system as\nBc(cid:21) u(t),\nAc(cid:21) x(t) + (cid:20) B\nEc(cid:21) x(t + 1) = (cid:20) A\n(cid:20) E\nand can be rewritten to\nu(t) (cid:21) = (cid:20) A\nAc(cid:21) x(t).\n(cid:20) E B\nEc Bc(cid:21) (cid:20)x(t + 1)\nIf for any x(t)  X, there exists a pair (x(t + 1), u(t)) such\nthat (7) holds, then this implies that for any initial state\nx0  X0 of  there exists a continuation in the control led\nbehaviour. In addition, if the pair (x(t + 1), u(t)) is unique\nfor any x(t)  X, then this continuation is unique and we\nsay that c  C(). This existence and uniqueness of the\npairs (x(t + 1), u(t)) depends on the solutions of the matrix\nequality (7). We use the classical results on the solutions\nof matrix equalities (cf. (Abadir and Magnus, 2005)) to\nconclude that the rst wel l-posedness condition is satised\nif and only if\nrank (cid:18)(cid:20) E B\nEc Bc(cid:21)(cid:19) = rank (cid:18)(cid:20) E B A\nEc Bc Ac(cid:21)(cid:19) .\nIf in addition,\n\n(6)\n\n(7)\n\n(8)\n\nrank (cid:18)(cid:20) E B A\nEc Bc Ac(cid:21)(cid:19) = n + p,\nthen the second condition is also satised and c  C().\n\n(9)\n\nOf interest is the design of well-posed controllers sub ject\nto specications over the future output behaviour of the\ncontrolled system. We thus consider specications dened\nover the output space. In order to analyse the output\nbehaviour, we introduce a pro jection map. For B  (W1 \nW2 )T we denote with W2 a pro jection given as\n2 |w1  WT\nW2 (B) := {w2  WT\n1 s.t. (w1 , w2 )  B}.\n\nWe focus here on nding a controller c for a given dynam-\nical system  such that the output behaviour Y (Bc )\nof the interconnected system satises some specications.\n\n2.3 Exact control renement & problem statement\n\nLet us refer to the original DS that represents the real\nphysical system as the concrete DS. It is for this system\nthat we would like to develop a well-posed controller.\nRecall that the DS is a dynamical system  with dynamics\n\n(E , A, B , C ) as in (1) and initialised with X0 . A well-posed\ncontroller for  is referred to c  C(). The controlled\nconcrete system is the interconnected system w c with\nthe shared variables w = (u, x).\n\nNow, we consider a simpler DS a , related to the concrete\nDS , with dynamics given as (Ea , Aa , Ba , Ca ) and ini-\ntialised with Xa0 . We assume that the synthesis of a well-\nposed controller ca for a is substantially easier than for\n. We refer to this simpler system a as the abstract DS,\nand we note that its signals take values ua(t), xa (t), ya (t)\nwith xa (t)  Xa = Rm , ua(t)  Ua = Rq , ya (t)  Ya =\nY = Rk and t  N. With respect to the concrete system,\nthe abstract DS is generally a reduced-order system. The\ncontrolled abstract system a wa ca is the intercon-\nnected system with the shared variables wa = (ua , xa ).\n\nIf we assume that we can compute a well-posed controller\nfor the abstract system, then the control synthesis problem\nreduces to a control renement problem.\nDenition 4. (Exact control renement). Let a and  be\nthe abstract and concrete DS, respectively. We say that\ncontroller c  C() renes the controller ca  C(a ) if\nY (Bc )  Y (Ba ca ).\nThen we formalise the exact control renement problem.\nProblem 1.\nLet a and  be the abstract and concrete\nDS, respectively. For any ca  C(a ), rene ca to c ,\ns.t. c  C() and Y (Bc )  Y (Baca ).\nIn the next section, we will show that the existence of\na solution to this problem hinges on certain conditions\ninvolving similarity relations between the concrete and\nabstract DS. For this, we will rst introduce simulation\nrelations to formally characterise this similarity.\n\n3. EXACT CONTROL REFINEMENT\n\n3.1 Similarity relations between DS\n\nWe give the notion of simulation relation as dened in\n(Tabuada, 2009) for transition systems and applied to\npairs of DS 1 and 2 that share the same output space\nY1 = Y2 = Y.\n\nDenition 5. Let 1 and 2 be two DS with respective\ndynamics (E1 , A1 , B1 , C1 ) and (E2 , A2 , B2 , C2 ) over state\nspaces X1 and X2 . A relation R  X1  X2 is called a\nsimulation relation from 1 to 2 , if (x1 , x2 )  R,\n(1) for all (u1 , x+\n1 )  U1  X1 sub ject to\nE1 x+\n1 = A1 x1 + B1u1\nthere exists (u2 , x+\n2 )  U2  X2 sub ject to\nE2 x+\n2 = A2 x2 + B2u2\n1 , x+\nsuch that (x+\n2 )  R, and\n\n(2) we have C1x1 = C2x2 .\n\nWe say that 1 is simulated by 2 , denoted by 1 (cid:22) 2 , if\nthere exists a simulation relation R from 1 to 2 and if in\naddition x10  X10 , x20  X20 such that (x10 , x20 )  R.\n\nWe call R  X1  X2 a bisimulation relation between 1\nand 2 , if R is a simulation relation from 1 to 2 and\nits inverse R1  X2  X1 is a simulation relation from\n\n\f2 to 1 . We say that 1 and 2 are bisimilar, denoted\nby 1 = 2 , if 1 (cid:22) 2 w.r.t. R and 2 (cid:22) 1 w.r.t. R1 .\nSimulation relations as dened above are transitive. Let\nR12 and R23 be simulation relations respectively, from 1\nto 2 and from 2 to 3 . Then a simulation relation from\n1 to 3 is given as a composition of R12 and R23 , namely\nR12 R23 = {(x1 , x3 ) | x2 : (x1 , x2 )  R12(x2 , x3 )  R23}.\nWe also have that 1 (cid:22) 2 and 2 (cid:22) 3 implies 1 (cid:22) 3\nand, in addition, 1 = 2 and 2 = 3 implies 1 = 3 .\nSimulation relations have also implications on the prop-\nerties of the output behaviours of the two systems. More\nprecisely, if a system is simulated by another system then\nthis implies output behaviour inclusion. This follows from\nProposition 4.9 in (Tabuada, 2009) and is formalised next.\nProposition 6. Let 1 and 2 be two DS with simulation\nrelations as dened in Denition 5. Then,\n1 (cid:22) 2 = Y (B1 )  Y (B2 ),\n1 = 2 = Y (B1 ) = Y (B2 ).\nSimulation relations can also be used for the controller\ndesign for deterministic systems such as nonsingular DS\n(Tabuada, 2009; Fainekos et al., 2007; Girard and Pappas,\n2009). This will be used in the next subsection, where we\nconsider the exact control renement for non-singular DS.\nAfter that, we introduce a transformation of a singular\nDS to an auxiliary nonsingular DS representation, referred\nto as a driving variable (DV) system. The exact control\nrenement problem is then solved based on the introduced\nnotions.\n\n3.2 Control renement for non-singular DS\n\nLet us consider the simple case where the concrete and\nabstract systems of interest are given with non-singular\ndynamics. For these systems, the existence of a simulation\nrelation also implies the existence of an interface function\n(Girard and Pappas, 2009), which is formulated as follows.\nDenition 7. (Interface). Let 1 and 2 be two non-\nsingular DS dened over the same output space Y with a\nsimulation relation R from 1 to 2 . A mapping F : U1 \nX1  X2 7 U2 is an interface related to R, if (x1 , x2 )  R\nand for all u1  U1 , u2 := F (u1 , x1 , x2 )  U2 is such that\n(x+\n1 , x+\n2 )  R with\n1 = A1x1 + B1 u1 and x+\nx+\n2 = A2x2 + B2 u2 .\nIt follows from Denition 5 that there exists at least one\ninterface related to R if two deterministic, or non-singular\nsystems are in a simulation relation. As such we can solve\nthe exact renement problem as follows.\nTheorem 8. Let 1 and 2 be two non-singular DS de-\nned over the same output space Y with dynamics\n(I , A1 , B1 , C1 ) and (I , A2 , B2 , C2 ), which are initialised\nwith X10 and X20 , respectively. If there exists a relation\nR  X1  X2 such that\n\n(1) R is a simulation relation from 1 to 2 , and\n(2) x20  X20 , x10  X10 s.t. (x10 , x20 )  R,\n\nthen for any controller c1  C(1 ), there exists a\ncontroller c2  C(2 ) that is an exact control renement\nfor c1 and thus achieves with\nY (B2c2 )  Y (B1c1 ).\n\nProof. Since R is a simulation relation from 1 to 2 ,\nthere exists an interface function F : U1  X1  X2  U2\nas given in Denition 7, cf (Tabuada, 2009; Girard and\nPappas, 2009). Additionally, due to (2) there exists a map,\nF0 : X20  X10 such that for all x20  X20 it holds that\n(F0 (x20 ), x20 )  R.\nNext, we construct the controller c2 that achieves exact\ncontrol renement for c1 as\nc2 := (1 w1 c1 ) w1 F ,\nwhere w1 = (u1 , x1 ) and where F := (N, W, BF ) is a\ndynamical system taking values in the combined signal\nspace with\nBF := {(x1 , u1 , x2 , u2 )  W|x10 = F0 (x20 ) and\nu2 = F (x1 , u1 , x2 )}.\nThe dynamical system c2 is a well-posed controller for\n2 with 2 w2 c2 sharing w2 = (u2 , x2 ). Denote with\nB2c2 the behaviour of the controlled system, then due\nto the construction of F it follows that B2c2 is non-\nempty and x20  X20 , x10  X10 such that (x10 , x20 ) has\na unique continuation in B2c2 . Furthermore it holds\nthat Y (B2 c2 )  Y (B1c1 ). \n\nThe design of the controller c2 that achieves exact control\nrenement for c1 is similar to that in (Tabuada, 2009),\nwhich also holds in the behavioural framework.\n\n3.3 Driving variable systems\n\nSince it is dicult to control and analyse a DS directly, we\ndevelop a transformation to a system representation that\nis in non-singular DS form and is driven by an auxiliary\ninput. We refer to this non-singular DS as the driving\nvariable (DV) system (Weiland, 1991). We investigate\nwhether the DS and the obtained DV system are bisimilar\nand behaviourally equivalent. Let us rst introduce with\na simple example the apparent non-determinism or anti-\ncausality in the DS. Later-on, we show the connections\nbetween a DS and its related DV system.\nExample 2. Consider the DS with dynamics (E , A, B , C )\ndened as\n0.5 iT\n0 0 0 i, A = h 1 0 0\n1 i, C = h 0\n0 0 1 i, B = h 1\nE = h 1 0 0\n1\n0.2\n0 0 1\n0 1 0\nand x(t) = [x1 (t) x2 (t) x3 (t)]T . In this case, the input\nu(t) = x3 (t) is constrained by the third state component.\nNow the state trajectories of (10) can be found as fol lows:\n\n(10)\n\n,\n\n for a given input sequence u : N  U, we have\nx2 (t) = u(t)u(t+1), and thus we can use this anti-\ncausal relation of the DS to nd the corresponding\nstate trajectories;\n alternatively, we can al low the next state x2 (t + 1)\nto be freely chosen, and for arbitrary state x2 (t),\nthe equations (10) impose constraints on the input\nsequence that is, therefore, no longer free as u(t) =\nx3 (t).\n\nWe embrace the latter, non-deterministic interpretation of\nthe DS.\n\nThis non-determinism can be characterised by introducing\nan auxiliary driving input of a so-called DV system. We\n\n\f(11)\n\nreorganise the state evolution of (1). For simplicity we omit\nthe time index in x(t) and u(t) and denote x(t + 1) as x+\nM (cid:20)x+\nu (cid:21) = Ax,\nwhere M = [E B ]. For any x, we notice that the pairs\n(u, x+ ) are non-unique due to the non-determinism related\nto x+ . If M has full row rank, then it has a right inverse.\nThis always holds when the DS is reachable (cf. Denition\n2-1.1 (Dai, 1989)). In that case we can characterise the\nnon-determinism as follows. Let M + be a right inverse of\nM such that M M + = I and N be a matrix such that\nim N = ker M and N T N = I . Then all pairs (u, x+ ) that\nare compatible with state x in (11) are parametrised as\n(cid:20)x+\nu (cid:21) = M +Ax + N s,\nwhere s is a free variable. We now claim that all transitions\n(x, u, x+ ) in (12) for some variable s satisfy (11). To see\nthis, multiply M on both sides of (12) to regain (11). Now\nassume that there exists a tuple (x, u, x+ ) satisfying (11)\nthat does not satisfy (12). Then there exists an s and a\nvector z 6= 0 that is not an element of the kernel of M and\nsuch that the right side of (12) becomes M +Ax + N s +\nz . Multiplying again with M , we infer that there is an\nadditional non-zero term M z and that (11) cannot hold.\nIn conclusion any transition of (11) is also a transition of\n(12) and vice versa.\nExample 3. [Example 2: contd] For the DS of Example 2,\nthe related DV system DV is developed as\nx(t + 1) = h 1 0 1\n0 1 1 ix(t) + h 0\n0 is(t)\n1\n0 0 0\nu(t) = [ 0 0 1 ]x(t)\ny (t) = [ 0 0.2 0.5 ]x(t).\n\n(12)\n\n(13)\n\nAs indicated by (13), the input u(t) is a function of\nthe state trajectory. The non-determinism of x2 (t + 1) is\ncharacterised by s(t) for which the auxiliary input s can\nbe freely selected.\n\n(14)\n\nLet us now formalise the notion of a driving variable\nrepresentation. We associate a driving variable repre-\nsentation with any given DS (1) by dening a tuple\n(Ad , Bd , Cu , Du , C ) and setting\n(cid:20)Ad\nCu (cid:21) = M +A, (cid:20)Bd\nDu (cid:21) = N ,\nwhere N  R(n+p)p has orthonormal columns, that is\nN T N = I . For any given DS, this tuple denes the driving\nvariable system DV = (N, W, BDV ), which maintains\nthe same set of initial states X0 and has dynamics\nx(t + 1) = Adx(t) + Bds(t)\nu(t) = Cu x(t) + Du s(t)\ny (t) = C x(t),\nthereby yielding the initialised behaviour\nBDV := {w  WN |w =(u, x, y ), s  SN\ns.t. (15) and x0  X0}.\nNext, we propose the following assumption for DS, which\nwill be used in the sequel to develop our main results.\nAssumption 1. The given DS  is a dynamical system\nwith dynamics (E , A, B , C ) such that M = [E B ] has\nfull row rank.\n\n(15)\n\nThe relationship between a DS and its related DV system\nis characterised as follows.\nTheorem 9. Let the DS  be given as in (1) satisfying\nAssumption 1 and let DV = (N, W, BDV ) be dened as\nin (15). Then\n(1)  and DV are bisimilar, that is,  = DV ,\n(2)  and DV have equal behaviour, i.e., BDV = B ,\n(3)  and DV have equal output behaviour, that is,\nY (B ) = Y (BDV ).\n\nProof. For the rst statement (1), we dene the diagonal\nrelation as I := {(x, x) | x  X}. Then I is a bisimulation\nrelation between  and DV , because by construction their\nstate evolutions can be matched, hence stay in I ; and they\nshare the same output map. In addition, since they have\nthe same set of initial states it follows that  = DV .\nThe second part (2) follows immediately from the deriva-\ntion of DV , because by construction all the transitions\nin  can be matched by those of DV and vice versa, in\naddition, they have the same output map. Hence, they\nshare the same signal space (U  X  Y) and we can\nconclude that  and DV have equal behaviour.\nAdditionally, we have that (2) implies (3); via Proposition\n6 also (1) implies (3). \n\n3.4 Main result: exact control renement for DS\n\nBased on the results developed in the previous subsections,\nwe now derive the solution to the exact control renement\nproblem in Problem 1. More precisely, sub ject to the\nassumption that there exists a simulation relation R from\na to , for which in addition holds that x0  X0 , xa0 \nXa0 s.t. (xa0 , x0 )  R, we show that for any ca  C(a ),\nthere exists a controller c for  that renes ca such that\nc  C() and Y (Bc )  Y (Baca ).\n\nIn the case of Assumption 1, we construct DV systems\nDV and DVa for the respective DS systems  and a\nas a rst step. For these systems, we develop the following\nresults on exact control renement:\n\ni) The exact control renement for the DV systems:\nDVa  C(DVa ), c\nc\nDV  C(DV ), s.t.\nY (cid:0)BDVc\nDVa (cid:1);\nDV (cid:1)  Y (cid:0)BDVa\nc\nii) The exact control renement from a to DVa :\nca  C(a ), c\nDVa  C(DVa ), s.t.\nY (cid:0)Baca (cid:1) = Y (cid:0)BDVa\niii) The exact control renement from DV to :\nc\nDV  C(DV ), c  C(), s.t.\nY (cid:16)BDVc\nDV (cid:17) = Y (Bc ) .\nIt will be shown that the combination of the elements\ni)iii) also implies the construction of the exact control\nrenement for the concrete and abstract DS.\n\nDVa (cid:1);\nc\n\ni) Exact control renement for the DV systems. From\nTheorem 9, we know that  = DV and a = DVa\nwith respective diagonal relations I := {(x, x)|x  X} and\nIa := {(xa , xa )|xa  Xa}. Hence as depicted in Fig. 2 and\nbased on the transitivity of simulation relations, we also\nderive that R is a simulation relation from DVa to DV .\n\n\f\n\n = DV ,\n\nw.r.t. I\n\nDV\n\nR\n(xa0 , x0 )  R\n\nR = Ia  R  I\n(xa0 , x0 )  R\n\na\n\na\n\n= DVa ,\n\nw.r.t. Ia\n\nDVa\n\n(16)\n\nFig. 2. Connection between DS and DV systems for the\nexact control renement.\nSince the DV systems DV and DVa share the same\ninitial states as the respective DS  and a , it also holds\nthat x0  X0 , xa0  Xa0 s.t. (xa0 , x0 )  R. According\nto Theorem 8, we know that we can do exact control\nrenement, that is, we have shown\nDVa  C(DVa ), c\nc\nDV  C(DV ), s.t.\nDV (cid:1)  Y (cid:0)BDVa\nDVa (cid:1).\nY (cid:0)BDVc\nc\nii) Exact control renement from a to DVa . Denote\nwith DVa the abstract DV system related to a , with dy-\nnamics (Ada , Bda , Cua , Dua , Ca ) and initialised with Xa0 .\nWe rst derive the static function Sa mapping transitions\nof a to the auxiliary input sa of DVa . From the denition\nof DV systems, we can also derive the transitions of DVa\nindexed with a, which is similar to the derivation of (12).\n(cid:20)x+\nua (cid:21) = M +\na\na Aaxa + Na sa .\nMultiplying N T\na on both sides of (16), Sa is derived as\na (cid:20)x+\nua (cid:21)N T\na M +\nSa : sa = Sa (x+\na , ua , xa ) = N T\na\na Aa xa . (17)\nSa maps the state evolutions of a wa ca to the auxiliary\ninput sa for DVa , where wa = (ua , xa ). Now, we consider\nthe exact control renement from the abstract DS to the\nabstract DV system.\nTheorem 10. Let a be the abstract DS with dynamics\n(Ea , Aa , Ba , Ca ) satisfying the condition of Assumption 1\nand let DVa be its related DV system with dynamics\n(Ada , Bda , Cua , Dua , Ca ) such that both systems are ini-\ntialised with Xa0 . Then, for any ca  C(a ), there exists\na controller c\nDVa  C(DVa ) that is an exact control\nrenement for ca as dened in Denition 4 with\nDVa (cid:1).\nY (cid:0)Baca (cid:1) = Y (cid:0)BDVa\nc\nProof. Denote with xa and xd\na the state variables of a\nand DVa , respectively. Next, we construct the controller\nc\nDVa that achieves exact control renement for ca as\nc\nDVa := (a wa ca ) wa Sa ,\nwhere wa = (ua , xa ) and where Sa := (N, W, BSa ) is a\ndynamical system with\na , sa )  W|xa0 = xd\nBSa := {(xa , ua , xd\na0 and\nsa = Sa (x+\na , ua , xa )}.\nThe dynamical system c\nDVa is a well-posed controller for\na c\na = (sa , xd\nDVa sharing wd\nDVa with DVa wd\na ). Denote\nthe behaviour of the controlled system.\nwith BDVa\nc\nDVa\n\nBy construction, we know that the set of the behaviour\nis non-empty and there is a unique continuation for any\nxd\na0  Xa0 . Further based on the construction of Sa , the\nbehaviour is such that xd\na (t) = xa (t), t  N. Additionally,\nsince a and DVa share the same set of initial states Xa0 ,\nit holds that Y (cid:0)Baca (cid:1) = Y (cid:16)BDVa\nDVa (cid:17) . \nc\nThe proof is actually constructive in the design of the\ncontroller c\nDVa that achieves exact control renement for\nca .\n\niii) Exact control renement from DV to . Now, we\nconsider the exact control renement from DV to .\nSuppose we are given a well-posed controller c\nDV for DV ,\nwhich shares the free variable s and the state variable x\nwith DV . We want to design a well-posed controller for\n over w = (u, x), for which we consider the dynamical\nsystem C = (N, W, B) over the signal space W = U \nX  S, the behaviour of which can be dened by\nd x(t + 1) = B T\nd Adx(t) + B T\nB T\nd Bds(t)\nu(t) = Cu x(t) + Dus(t).\n\n(18)\n\n(19)\n\n(20)\n\nThen the dynamics of the interconnected system  w C\nas a function of x and s is derived as\nd Bd(cid:21) s(t).\nd Ad (cid:21) x(t) + (cid:20) BDu\nd (cid:21) x(t + 1) = (cid:20)A + BCu\n(cid:20) E\nB T\nB T\nB T\nNote that A + BCu = EAd and BDu = EBd by\nmultiplying M = [E B ] on the left-hand side of the two\nequations in (14). Therefore, (19) is simplied to\nd (cid:21) Bds(t).\nd (cid:21) Adx(t) + (cid:20) E\nd (cid:21) x(t + 1) = (cid:20) E\n(cid:20) E\nB T\nB T\nB T\nFurthermore (cid:2)E T Bd (cid:3)T\nhas full column rank because the\nmatrix (cid:2)M T N (cid:3)T\nis square and has full rank. Hence\n(cid:2)E T Bd (cid:3)T\nhas a left inverse and the dynamics of  w C\nin (20) can be simplied as\nx(t + 1) = Adx(t) + Bd s(t),\nwhich is exactly the same as the state evolutions of DV\nas shown in (15). Next we construct c := C wd c\nDV\nwith wd = (s, xd ) and it is a well-posed controller for .\nThis allows us to state the following theorem regarding the\ncontrol renement from DV to .\nTheorem 11. Let  be the concrete DS with dynamics\n(E , A, B , C ) satisfying Assumption 1 and let DV be its\nrelated DV system with dynamics (Ad , Bd , Cu , Du , C ) such\nthat both systems are initialised with X0 . Then, for any\nc\nDV  C(DV ), there exists a controller c  C()\nthat is an exact control renement for c\nDV as dened in\nDenition 4 with\nY (cid:16)BDVc\nDV (cid:17) = Y (Bc ) .\nProof. Denote with x and xd the state variables of the\n and DV , respectively. Next, we construct the controller\nc that achieves exact control renement for c\nDV as\nc := C wd c\nDV ,\nwhere wd = (s, xd ) and the dynamics of C is dened as\n(18). Then, we can show that the dynamical system c is a\nwell-posed controller for . Based on the analysis of (20),\nit is shown that  w C = DV with w = (u, x), then we\n\n\fcan derive  w c = DV wd c\nDV . Therefore, we can\nDV (cid:1) = Y (cid:0)Bc (cid:1)\nconclude c  C() with Y (cid:0)BDVc\nimmediately follows from c\nDV  C(DV ). \n\nExact control renement for descriptor systems. We can\nnow argue that there exists exact control renement from\na to , as stated in the following result.\nTheorem 12. Consider two DS a (abstract,\ninitialised\nwith Xa0 ) and  (concrete, initialised with X0 ) satisfying\nAssumption 1 and let R be a simulation relation from a\nto , for which in addition holds that x0  X0 , xa0 \nXa0 s.t. (xa0 , x0 )  R. Then, for any ca  C(a ), there\nexists a controller c  C() such that\nY (Bc )  Y (cid:0)Baca (cid:1) .\nProof. Based on Assumption 1, we rst construct DV\nand DVa . Then to prove this we need to construct the\nexact control renement. This can be done based on\nthe subsequent control renements given in Theorem 10,\nTheorem 8 and Theorem 11. \n\nTheorem 12 claims the existence of such controller c that\nachieves exact control renement for ca . More precisely,\nwe have shown in the proof that the rened controller c\nis constructive, which provides the solution to Problem 1.\n\nTo elucidate how such an exact control renement is\nconstructed, we consider the following example.\nExample 4. [Example 2,3: contd] Consider the DS of\nExample 2 and its related DV system (cf. Example 3)\nsuch that both systems are initialised with X0 = {x0 |\nx0  [1, 1]3  R3}. According to Silverman-Ho algo-\nrithm (Dai, 1989), we can select an abstract DS a =\n(Ea , Aa , Ba , Ca ) that is the minimal realisation of  and\nis initialised with Xa0 = R2 , in addition\n0.2 ]T .\n0 ], Ca = [ 0.7\nEa = [ 0 0\n1 0 ], Aa = [ 1 0\n0 1 ], Ba = [ 1\nSimilarly, the related DV system DVa of a is given as\n0 0 ]xa (t) + (cid:2) 0\n1 (cid:3)sa (t)\nxa (t + 1) = [ 0 1\nua (t) = [ 1 0 ]xa (t)\nya (t) = [ 0.7 0.2 ]xa (t).\n\n(21)\n\nSubsequently,\nR := {(xa , x) | xa = Hx, xa  Xa , x  X}\nis a simulation relation from a to  with\nH = (cid:2) 0 0 1\n0 1 1 (cid:3).\nThis can be proved through verifying the two properties of\nDenition 5. In addition, the condition x0  X0 , xa0 \nXa0 s.t. (xa0 , x0 )  R holds. According to Theorem 12,\nwe can rene any ca  C(a ) to attain a wel l-posed\ncontrol ler c for  that solves Problem 1 as fol lows: Dene\nca  C(a ) with dynamics as\n[ 1 1 ]xa (t + 1) = [ 0.5 0.5 ]xa (t) + ua (t).\nThe control led system a wa ca is derived as\nxa (t + 1) = (cid:2) 0\n1\n0.5 0.5 (cid:3)xa (t)\nya (t) = [ 0.7 0.2 ]xa (t),\nwith wa = (ua , xa ) and ua (t) = [ 1 0 ]xa (t). Then a wa\nca is stable. According to Theorem 10, we derive the map\nSa for DVa as sa (t) = [ 0 1 ]xa (t + 1) = [ 0.5 0.5 ]xa (t).\nNext, the related interface from DVa to DV is developed\n\nas s(t) = sa (t)  [ 0 1 1 ]x(t). According to Theorem 11,\nwe derive the wel l-posed control ler c as\n[ 0 1 0 ]x(t + 1) = [ 0 1 1 ]x(t) + [ 0.5 0.5 ]xa (t)\nu(t) = [ 0 0 1 ]x(t),\nand the interconnected system  w c with w = (u, x), is\nderived as\nx(t + 1) = h 1 0 1\n0\n0 1 1 ix(t) + h 0\n0 ixa (t)\n0 1 1\n0.5 0.5\n0\ny (t) = [ 0 0.2 0.5 ]x(t).\nSince (xa , x)  R, that is xa = Hx,  w c can be\nsimplied by replacing xa (t):\nx(t + 1) = h 1 0\n1\n0 1 1 ix(t)\n0 0.5 1\ny (t) = [ 0 0.2 0.5 ]x(t).\nFinal ly, c  C() and Y (Bc )  Y (cid:0)Baca (cid:1) are\nachieved.\n4. CONCLUSION\n\nIn this paper, we have developed a control renement\nprocedure for discrete-time descriptor systems that is\nlargely based on the behavioural theory of dynamical\nsystems and the theory of simulation relations among\ndynamical systems. Our main results provide complete\nsolutions of the control renement problem for this class\nof discrete-time systems.\n\nThe exact control renement that has been developed\nin this work also opens the possibilities for approximate\ncontrol renement notions, to be coupled with approxi-\nmate similarity relations: these promise to leverage general\nmodel reduction techniques and to provide more freedom\nfor the analysis and control of descriptor systems.\n\nThe future research includes a comparison of the control\nrenement approach for descriptor systems to results in\nperturbation theory, as well as control renement for\nnonlinear descriptor systems.\n\nREFERENCES\n\nAbadir, K.M. and Magnus, J.R. (2005). Matrix algebra.\nCambridge University Press.\nAntoulas, A.C. (2005). Approximation of large-scale dy-\nnamical systems. SIAM.\nCao, X., Saltik, M., and Weiland, S. (2015). Hankel model\nreduction for descriptor systems.\nIn 2015 54th IEEE\nCDC, 46684673.\nDai, L. (1989). Singular control systems. Springer-Verlag\nNew York, Inc.\nFainekos, G.E., Girard, A., and Pappas, G.J. (2007). Hi-\nerarchical synthesis of hybrid controllers from tempo-\nral logic specications.\nIn International Workshop on\nHSCC, 203216.\nGirard, A. and Pappas, G.J. (2009). Hierarchical control\nsystem design using approximate simulation. Automat-\nica, 45(2), 566571.\nGirard, A. and Pappas, G.J. (2011). Approximate bisim-\nulation: A bridge between computer science and control\ntheory. European Journal of Control, 17(5), 568578.\nKloetzer, M. and Belta, C. (2008). A fully automated\nframework for control of linear systems from temporal\nlogic specications. IEEE Transactions on Automatic\nControl, 53(1), 287297.\n\n\fKunkel, P. and Mehrmann, V.L. (2006). Dierential-\nalgebraic equations: analysis and numerical solution.\nEuropean Mathematical Society.\nMegawati, N.Y. and Van der Schaft, A. (2015). Bisimula-\ntion equivalence of DAE systems. arXiv:1512.04689.\nTabuada, P. (2009). Verication and control of hybrid\nsystems: a symbolic approach.\nSpringer Science &\nBusiness Media.\nVan der Schaft, A. (2004). Equivalence of dynamical sys-\ntems by bisimulation. IEEE transactions on automatic\ncontrol, 49(12), 21602172.\nWeiland, S. (1991). Theory of approximation and dis-\nturbnace attenuation for linear systems. University of\nGroningen.\nWillems, J.C. and Polderman, J.W. (2013). Introduction\nto mathematical systems theory: a behavioral approach,\nvolume 26. Springer Science & Business Media.\n\n\f", 
        "tag": "Systems and Control", 
        "link": "https://arxiv.org/list/cs.SY/new"
    }
]
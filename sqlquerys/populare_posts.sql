INSERT INTO posts (postId,userId,title,body) VALUES (1,329,'Transferrable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence','Mieczys law Alo jzy K lopotek

Institute of Computer Science, Polish Academy of Sciences, Warszawa, Poland,
klopotek@ipipan.waw.pl

Abstract. This paper suggests a new interpretation of the Dempster-
Shafer theory in terms of probabilistic interpretation of plausibility. A
new rule of combination of independent evidence is shown and its preser-
vation of interpretation is demonstrated. 1

Introduction

Dempster Rule of Independent Evidence Combination has been criticized for
its failure to conform to probabilistic interpretation ascribed to belief and plau-
sibility function. Among those verifying DST (Dempster-Shafer-Theory, [2,12])
critically were Kyburg [7], Fagin [3], Halpern [6], Pearl [9], Provan [10], Cano
[1], just to mention a few.
As a way out of those diculties, we proposed in a recent book co-authored
by S.T.Wierzchon [18] three proposals for an empirical model of DST:

 the marginally correct approximation.
 the qualitative model
 the quantitative model

The marginally correct approximation assumes that the belief function shall
constitute lower bounds for frequencies, though only for the marginals, and not
for the joint distribution. Then, the reasoning process is expressed in terms
of the so-called Cano et al. conditionals - a special class of conditional belief
functions that are positive. This approach implies modication of the reasoning
mechanism, because the correctness is maintained only by reasoning forward.
Depending on the reasoning direction we need dierent Markov trees for the
reasoning engine.
Note that lower/upper bound interpretations have a long tradition for DST
[2,7] and have been heavily criticized [6]. The one that we presented in our

1 This is a preliminary version of the paper:
M.A. Kopotek: Transferable Plausibility Model - A Probabilistic Interpretation
of Mathematical Theory of Evidence O.Hryniewicz, J. Kacprzyk, J.Koronacki,
S.Wierzcho:
Issues
in Intelligent Systems Paradigms Akademicka Ocyna
Wydawnicza EXIT, Warszawa 2005 ISBN 83-87674-90-7, pp.107118

book diers from the known ones signicantly as we insist on dierent reason-
ing schemes (hypertrees) depending on which are our target variables, whose
values are to be inferred. This assures overcoming of the basic diculties with
lower/upper bound interpretations.
Our qualitative approach is based on the earlier rough set interpretations of
DST, but makes a small and still signicant distinction. All computations are
carried out in a strictly relational way, that is, indistinguishable ob jects in a
database are merged (no ob ject identities). The behavior under reasoning ts
strictly the DST reasoning model. Factors of well established hypergraph repre-
sentation (due to Shafer and Shenoy [14]) can be expressed by relational tables.
Conditional independence is well dened. However, there is no interpretation for
conditional belief functions in this model.
Rough set interpretations [15] were primarily developed for interpreting the
belief function in terms of decision tables. However, the Dempster-rule of evi-
dence combination was valid there only for the extended decision tables, not
easily derived from the original ones. In our interpretation, both the original
tables and the resultant tables dealt with when simulating Dempster-rule are
conventional decision tables and the process of combin');
INSERT INTO posts (postId,userId,title,body) VALUES (2,329,'Transferrable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence (part 2)','ing of decision tables is a
natural one (relational join operator).
Our rough set based interpretation may be directly applied in the domain of
multiple decision tables: independence of decision variables or Shenoys condi-
tional independence in the sense of DST may serve as an indication of possibility
of decomposition of the decision table into smaller but equivalent tables.
Furthermore, it may be applied in the area of Cooperative Query Answer-
ing [11]. The problem there is that a query posed to a local relational database
system may contain an unknown attribute. But, possibly, other co-operating
database systems know it and may explain it to the queried system in terms
of known attributes, shared by the various systems. The uncertainties studied
in the decision tables arise here in a natural way and our interpretation may
be used to measure these uncertainties in terms of DST (as a diversity of sup-
port). Furthermore, if several co-operating systems respond, then the queried
system may calculate the overall uncertainty measure using DST combination
of measures of individual responses.
The quantitative model assumed that the ob jects possess multivalued proper-
ties which are then lost in some physical properties and these physical processes
are described by DST belief functions (see e.g. [8])..
The quantitative model assumes that during the reasoning process one at-
taches labels to ob jects hiding some of their properties. There is a full agreement
with the reasoning mechanism of DST. Conditional independence and condi-
tional belief functions are well dened. We have also elaborated processes that
can give rise to well-controlled graphoidally structured belief functions. Thus,
sample generation for DST is possible. We elaborated also learning procedures
for discovery of graphoidal structures from data.
The quantitative model seems to be the best tting model for belief functions
created so far.

This frequency model diers from what was previously considered [16,17] in
that it assumes that reasoning in DST is connected with updating of variables
for individual cases. This is dierent from e.g. reasoning in probability where rea-
soning means only selection of cases. In this way, failures of previous approaches
could be overcome.
Many authors [13,16] question the need for an empirical model for DST and
point rather to theoretical properties of DST considered within an axiomatic
framework seeking parallels with the probability theory. Though it is true that
the probability theory may be applied within the framework of Kolmogorov ax-
ioms and quite useful results are derived in this way, one shall still point out that
the applicability of probability theory is signicantly connected with frequencies.
Both frequencies considered as naive probabilities, ore ones being probabilities
in the limit. Statistics is clearly an important part of the probabilistic world.
All three interpretations share a common drawback they are not sensu stricto
probabilistic. In the current paper we make an attempt of a purely probabilistic
vision of plausibility function.

2 Basics of the Dempster-Shafer Theory

We understand DST measures in a standard way (see [12]). Let  be a nite set
of elements called elementary events. Any subset of  is a composite event, or
hypothesis.  be called also the frame of discernment.

Denition 1. [12] Let  be a nite set of elements cal led elementary events.
The set  is cal led frame of discernment. Any subset of  be a composite event.
A basic probability assignment (bpa) function is any function m:2  [0, 1] such
that

m(A) = ON E

m() = 0,

A2
A2

0  X
AB

m(B )

We say that a bpa is ');
INSERT INTO posts (postId,userId,title,body) VALUES (3,329,'Transferrable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence (part 3)','vacuous i m( ) = ON E and m(A) = 0 for every A 6=  .

If ONE is equal 1, then we say that the belief function is normalized, otherwise
not (but ONE must be positive).

Denition 2. [12] Let a belief function be dened as Bel:2  [0, 1] so that
B el(A) = PBA m(B ). Let a plausibility function be Pl:2  [0, 1] with A2 P l(A) =
ON E  B el(  A), a commonality function be Q:2  {}  [0, 1] with
A2 {} Q(A) = PAB m(B ).
Denition 3. [12] The Rule of Combination of two Independent Belief Func-
tions B elE1 , B elE2 Over the Same Frame of Discernment (the so-cal led Dempster-
Rule), denoted

B elE1 ,E2 = B elE1  B elE2

is dened as fol lows: :

mE1 ,E2 (A) = c  X
B ,C ;A=BC

mE1 (B )  mE2 (C )

(c - constant normalizing the sum of m to 1).

Under multivariate settings  is a set of vectors in n-dimensional space
spanned by the set of variables X={ X1 , X2 , . . . Xn}. If A   , then by pro jec-
tion AY of the set A onto a subspace spanned by the set of variables Y  X we
understand the set B of vectors from A pro jected onto Y. Then marginalization
operator
follows:
as
dened
is
DST
of
mY (B ) = PA;B=AX m(A).

Denition 4. (See [?]) Let B be a subset of  , cal led evidence, mB be a basic
probability assignment such that mB (B ) = 1 and mB (A) = 0 for any A dierent
from B. Then the conditional belief function B el(.||B ) representing the belief
function B el conditioned on evidence B is dened as: B el(.||B ) = B el  B elB .

3 New Rule of Evidence Combination

Let us suggest now a totally new approach to understanding belief functions.
We assume the following interpretation of the plausibility function: P l (A) is
the maximum probability that an element from the set of events A occurs, given
the evidence  , where we assume the apriorical probability of all elementary
events is equal. Let 1 and 2 be two independent bodies of evidence, which are
represented numerically by plausibility functions P l1 and P l2 over some frame
of discourse  . We would like to obtain such an evidence updating rule P l that
P l3 = P l1 P l P l2 would have the semantics that under that interpretation
P l3 (A) is the maximum probability that an element from the set of events A
occurs, given the evidence P l1 , P l2 under the least conicting evidence.
Let us study in detail this assumption. First of all we have to tell what we
mean by independent evidence. Let  be an elementary event from the frame of
discernment  . The body of evidence 1 is independent of the body 2 if, for
each    , the probability of occurrence of evidence 1 is independent of the
occurrence of evidence 2. So we say that P r(1  2| ) = P r(1| )  P r(2| ).
How shall we understand the evidence, however. For any A   ) should hold
P l (A)  P r(A| ). Consequently, by the way, P l (A) + P l (/A)  1.
Now observe that P r(1  2 | ) = P r(1 | ) + P r(2 | ). As a consequence,
we have always that P l ({1}) + P l ({2})  P l ({1 , 2}).
Let us now turn to combining independent evidence.

P r( |1  2) = P r(1  2| ) 

P r( )
P r(1  2)

P r(1| )  P r(2| ) 

P r( )
P r(1  2)

P r( |1)  P r( |2) 

P r(1)  P r(2)
P r(1  2)  P r( )

So we can conclude that P l12 ( ) = P l1 ( )P l2 ( )c where c is a normalizing
factor (which needs to be chosen carefully).

But what about P r(1  2 |1  2) ? We know that P r(1  2 |1  2) =
P r(1 |1  2) + P r(2 |1  2) hence

P r(1  2 |1  2)

P r(1 |1)P r(1 |2)

P r(1)  P r(2)
P r(1  2)  P r(1 )

+P r(2 |1)P r(2 |2)

P r(1)  P r(2)
P r(1  2)  P r(2 )

As P r( ) is the same for all the  s, we get

P r(1  2 |1  2)

(P r(1 |1)  P r(1 |2) + P r(2 |1)  P r(2 |2)) 

P r(1)  P r(2)
P r(1  2)  P r( )

We can easily check that this translates to:

P l12 ({1 , 2}) =

max(P l1 (1 )  P l2 (1 ) + (P l1 ({1 , 2}  P l1(1 ))  (P l2 ({1 , 2}  P l2(1 ))

, P l1 (1 )  (P l2 ({1 , 2}  P l2 (2 )) + (P l1 ({1 , 2}  P l1 (1 ))  P l2 (');
INSERT INTO posts (postId,userId,title,body) VALUES (4,329,'Transferrable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence (part 4)','2 )

, P l1 (2 )  (P l2 ({1 , 2}  P l2 (1 )) + (P l1 ({1 , 2}  P l1 (2 ))  P l2 (1 )

, P l1 (2 )  P l2 (2 ) + (P l1 ({1 , 2}  P l1(2 ))  (P l2 ({1 , 2}  P l2(2 )))  c

where c is the normalizing factor mentioned earlier.
These formulas easily generalize for subsets of  with higher cardinality. The
normalizing factor should be chosen in such a way that P l12 ( ) = 1.
The generalization of P l for frames of discourse with cardinality higher than
3 runs along the following lines. To combine P l1 with P l2 we calculate:

 for each subset X of 
P lresult (X ) = P LX
1 V P l2 X ;

The operator  X does only a change of the domain of the P l function
keeping the values of P l for each subset of X and presuming that the discourse
frame consists only of X . In this way we get unnormalized P ls here, which are
not normalized during this operation.
The operator V , returning a numerical value, attempts identify such com-
binations of mass assignments ma and mb to singleton sets that will not violate
the constraints imposed by plausibility functions P l1 and P l2 resp. and such
that the sum PX ;X asingleton ma (X )  mb (X ) is maximal.
This is done by the operation of so-called pushing down the plausibilities
to singleton sets. Independently for P l1 and P l2 candidate ma and mb are ob-
taining via pushing-down recursively a singleton  of  . A candidate ma is
obtained if all singletons are pushed down. Dierent candidates are obtained
by dierent sequences of pushing down. It is easy to imagine that the process
is time-consuming and its complexity grows exponentially with the number of
elements of a set. Nonetheless for small domains the operation is feasible.

The idea of the push-down operator  + is as follows: Let P l be a plausibil-
ity function. If A does not contain  , P l+ (A) = min(P l(A), P l(A  {}) 
P l({})), and otherwise P l+ (A) = P l(A).
Under these conditions it is obvious that we do not seek actually the max-
imum product over the whole domain, but rather in some corner points. We
will give a formal proof elsewhere that this check is in fact sucient to establish
the maximum. Here we only want to draw attention to the analogy with linear
programming, where we seek the maximum sub ject to linear constraints. When-
ever we x pushdown of one of the plausibility distributions, we in fact have
a linear optimization case with the other. If found, we can do the same with the
other.
The P l operator is characterized by commutativity and associativity. The
commutativity is easily seen because all the operations are in fact symmetrical
with respect to left and right hand of the operators. The associativity is more
dicult to grasp, and a formal proof will be sub ject of another publication.
Nonetheless we can give here brief common-sense guidelines how it can be es-
tablished. We can essentially concentrate on the associative properties of the
maximum operator. Starting with the expression of combination of all the three
plausibility functions, we can show that we can equivalently denote the same
optimization task when drawing behind braces the rst or the third operand.
In the next section we show some properties of the new operator compared
with Dempster rule of combination for some illustrative examples.

4 Examples

Let us consider the bodies of evidence in the tables 1, 2, 3.

Table 1. mass function for the body of evidence a

m value for the set of elements
0.0 { }
0.25 { red }
0.25 { blue }
0.25 { red , bl');
INSERT INTO posts (postId,userId,title,body) VALUES (5,329,'Transferrable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence (part 5)','ue }
0.25 { green }
0.0 { red , green }
0.0 { blue , green }
0.0 { red , blue , green }

We can check the commutativity and obtain the results as in the table 4.
The associativity has been veried in table 5.
It is worth noting, that the new operator is dierent from Dempster rule,
compare tables 4 and 6

Table 2. mass function for the body of evidence  b

m value for the set of elements
0.0 { }
0.2 { red }
0.4 { blue }
0.1 { red , blue }
0.0 { green }
0.0 { red , green }
0.3 { blue , green }
0.0 { red , blue , green }

Table 3. mass function for the body of evidence  c

m value for the set of elements
0.0 { }
0.0 { red }
0.15 { blue }
0.25 { red , blue }
0.35 { green }
0.25 { red , green }
0.0 { blue , green }
0.0 { red , blue , green }

Table 4. mass function for the body of evidence (a P L  b) =( b P L a)

m value for the set of elements
0.0 { }
0.11111111111111105 { red }
0.4999999999999999 { blue }
0.22222222222222232 { red , blue }
0.0 { green }
0.0 { red , green }
0.16666666666666674 { blue , green }
0.0 { red , blue , green }

Table 5. mass function for the body of evidence ((aP L  b)P L  c) =(aP L ( bP L
 c))

m value for the set of elements
0.0 { }
0.0 { red }
0.4214285714285715 { blue }
0.3214285714285714 { red , blue }
0.014285714285714124 { green }
0.07142857142857151 { red , green }
0.1357142857142858 { blue , green }
0.0357142857142857 { red , blue , green }

Table 6. mass function for the body of evidence (a   b)

m value for the set of elements
0.0 { }
0.20833333333333337 { red }
0.6249999999999999 { blue }
0.04166666666666663 { red , blue }
0.12500000000000003 { green }
0 { red , green }
0 { blue , green }
0 { red , blue , green }

With this and other experiments we see clearly the tendency of Dempster
rule to move mass downwards to singleton sets, whereas the new rule is much
more cautious here and in fact does not introduce the feeling of certainty where
it is not justied.

5 Conclusions

We have introduced in this paper a new DST operator for combining independent
evidence providing a clear probabilistic denition of the plausibility function,
which is preserved under this rule of combination.
We have also provided several toy examples to give an impression what results
are returned by the new operator.
Though the strict theoretical proof of properties like cummutativeness, asso-
ciativeness is still to be provided, the computations for test examples show that
the properties really hold. It is also obvious from the examples that the new rule
diers from the Dempster rule of evidence combination. An interested reader is
invited
to
visit
the
Web
page
http://www.ipipan.waw.pl/klopotek/DSTnew/DSTdemo.html to try out him-
self.

References

1. Cano J., Delgado M., Moral S.: An axiomatic framework for propagating uncer-
tainty in directed acyclic networks, Int. J. of Approximate Reasoning. 1993:8, 253-
280.
2. A.P.Dempster: Upper and lower probabilities induced by a multi-valued mapping.
Ann. Math. Stat. 38 (1967), 325-339
3. R.Fagin, J.Y.Halpern: Uncertainty, belief, and probability, Proc. Int. Joint Conf.
AI, IJCAI89, Detroit, 1989, 1161-1167.
4. R.Fagin, J.Y.Halpern: A new approach to updating beliefs, in:J.F. Lemmer, L.N.
Kanal eds: Uncertainty in Articial Intel ligence 6 (North-Holland Amsterdam,
1991), 347-');
INSERT INTO posts (postId,userId,title,body) VALUES (6,329,'Transferrable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence (part 6)','374.
5. R. Fagin, J.Y. Halpern: Uncertainty, belief, and probability. Comput. Intell. 71991,
160-173.
6. J.Y. Halpern, R. Fagin: Two views of belief: belief as generalized probability and
belief as evidence.Articial Intel ligence 541992, 275-317
7. H.E. Kyburg Jr: Bayesian and non-Bayesian evidential updating.
8. M.A.Kopotek, S.T.Wierzcho: Quest on New Applications for Dempster-Shafer
Theory: Risk Analysis in Pro ject Protability Calculus. In: P. Grzegorzewski,
O.Hryiewicz, M.A.Gil Eds.: Soft Methods in Probability, Statistics and Data Anal-
ysis , Advances in Soft Computing Series, Physica-Verlag/Springer Verlag, 2002,
ISBN 3-7908-1526-8, pp. 302-309
9. Pearl, 1990 J. Pearl: Bayesian and Belief-Function formalisms for evidential rea-
soning:A conceptual analysis. in:G. Shafer, J. Pearl (eds): Readings in Uncertain
Reasoning, (ISBN 1-55860-125-2, Morgan Kaufmann Publishers Inc., San Mateo,
California, 1990), 540-569.

10. Provan, 1990 G.M.Provan: A logic-based analysis of Dempster-Shafer Theory, In-
ternational Journal of Approximate Reasoning 41990, 451-495.
11. Z.W. Ras: Query processing in distributed information systems, Fundamenta In-
formaticae Journal, Special Issue on Logics for Articial Intelligence, IOS Press,
Vol. XV, No. 3/4, 1991, 381-397
12. G.Shafer: A Mathematical Theory of Evidence , Princeton University Press, Prince-
ton, 1976
13. G.Shafer: Perspectives on the theory and practice of belief functions, International
Journal of Approximate Reasoning, 1990:4, 323-362.
14. P.P.Shenoy: Valuation networks and conditional independence. International Jour-
nal of Uncertainty, Fuzziness and Know ledge-Based Systems, Vol.2, No.2, June
1994.
15. A.Skowron, J.W.Grzyma la-Busse: From rough set theory to evidence theory.
in:R.R.Yager, J.Kasprzyk and M.Fedrizzi, eds, Advances in the Dempster-Shafer
Theory of Evidence J. Wiley, New York (1994), 193-236.
16. Smets, 1992 Ph.Smets: Resolving misunderstandings about belief functions, Inter-
national Journal of Approximate Reasoning 1992:6:321-344.
17. Smets, Kenne, 1994 Ph.Smets, R.Kennes: The tranferable belief model, Articial
Intel ligence 66 (1994), 191-234
18. S.T.Wierzcho, M.A.Kopotek: Evidential Reasoning. An Interpretative Investiga-
tion. Wydawnictwo Akademii Podlaskiej, Siedlce, 2002 PL ISSN 0860-2719, 304
pages,

');
INSERT INTO posts (postId,userId,title,body) VALUES (7,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu','Toyota Technological Institute at Chicago
{shtoshni, haotang, llu, klivescu}@ttic.edu
12rA5 L.c 116047:ir
Abstract
End-to-end training of deep learning-based models allows for
implicit learning of intermediate representations based on the
nal task loss. However, the end-to-end approach ignores the
useful domain knowledge encoded in explicit intermediate-level
supervision. We hypothesize that using intermediate representa-
tions as auxiliary supervision at lower levels of deep networks
may be a good way of combining the advantages of end-to-end
training and more traditional pipeline approaches. We present
experiments on conversational speech recognition where we use
lower-level tasks, such as phoneme recognition, in a multitask
training approach with an encoder-decoder model for direct char-
acter transcription. We compare multiple types of lower-level
tasks and analyze the effects of the auxiliary tasks. Our results
on the Switchboard corpus show that this approach improves
recognition accuracy over a standard encoder-decoder model on
the Eval2000 test set.
Index Terms: speech recognition, multitask learning, encoder-
decoder, CTC, LSTM

1. Introduction
Automatic speech recognition (ASR) has historically been ad-
dressed with modular approaches, in which multiple parts of the
system are trained separately. For example, traditional ASR sys-
tems include components like frame classiers, phonetic acoustic
models, lexicons (which may or may not be learned from data),
and language models [1]. These components typically corre-
spond to different levels of representation, such as frame-level
triphone states, phones, and words. Breaking up the task into
such modules makes it easy to train each of them separately, pos-
sibly on different data sets, and to study the effect of modifying
each component separately.
Over time, ASR research has moved increasingly toward
training multiple components of ASR systems jointly. Typically,
such approaches involve training initial separate modules, fol-
lowed by joint ne-tuning using sequence-level losses [2, 3].
Recently, completely integrated end-to-end training approaches,
where all parameters are learned jointly using a loss at the nal
output level, have become viable and popular. End-to-end train-
ing is especially natural for deep neural network-based models,
where the nal loss gradient can be backpropagated through all
layers. Typical end-to-end models are based on recurrent neural
network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist
temporal classication (CTC)-based models [8, 9].
End-to-end training is appealing because it is conceptually
simple and allows all model parameters to contribute to the same
nal goal, and to do so in the context of all other model pa-
rameters. End-to-end approaches have also achieved impressive
results in ASR [4, 9, 10] as well as other domains [11, 12, 13].
On the other hand, end-to-end training has some drawbacks:
Optimization can be challenging; the intermediate learned repre-

sentations are not interpretable, making the system hard to debug;
and the approach ignores potentially useful domain-specic in-
formation about intermediate representations, as well as existing
intermediate levels of supervision.
Prior work on analyzing deep end-to-end models has found
that different layers ');
INSERT INTO posts (postId,userId,title,body) VALUES (8,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu (part 2)','tend to specialize for different sub-tasks,
with lower layers focusing on lower-level tasks and higher ones
on higher-level tasks. This effect has been found in systems for
speech processing [14, 15] as well as computer vision [16, 17].
We propose an approach for deep neural ASR that aims to
maintain the advantages of end-to-end approaches, while also
including the domain knowledge and intermediate supervision
used in modular systems. We use a multitask learning approach
that combines the nal task loss (in our case, log loss on the out-
put labels) with losses corresponding to lower-level tasks (such
as phonetic recognition) applied on lower layers. This approach
is intended to encapsulate the intuitive and empirical observa-
tion that different layers encode different levels of information,
and to encourage this effect more explicitly. In other words,
while we want the end-to-end system to take input acoustics and
produce output text, we also believe that at some appropriate
intermediate layer, the network should do a good job at distin-
guishing more basic units like states or phones. Similarly, while
end-to-end training need not require supervision at intermedi-
ate (state/phone) levels, if they are available then our multitask
approach can take advantage of them.
We demonstrate this approach on a neural attention-based
encoder-decoder character-level ASR model. Our baseline
model is inspired by prior work [18, 8, 19, 4, 7], and our lower-
level auxiliary tasks are based on phonetic recognition and frame-
level state classication. We nd that applying an auxiliary loss
at an appropriate intermediate layer of the encoder improves
performance over the baseline.

2. Related Work
Multitask training has been studied extensively in the machine
learning literature [21]. Its application to deep neural networks
has been successful in a variety of settings in speech and lan-
guage processing [22, 23, 24, 25, 26, 27]. Most prior work
combines multiple losses applied at the nal output layer of the
model, such as joint Mandarin character and phonetic recog-
nition in [26] and joint CTC and attention-based training for
English ASR [25]. Our work differs from this prior work in that
our losses relate to different types of supervision and are applied
different levels of the model.
The idea of using low-level supervision at lower levels was,
to our knowledge, rst introduced by Sgaard '||'&'||' Goldberg [28]
for natural language processing tasks, and has since been ex-
tended by [29]. The closest work to ours is the approach of Rao
and Sak [30] using phoneme labels for training a multi-accent
CTC-based ASR system in a multitask setting. Here we study

the approach in the context of encoder-decoder models, and we
compare a number of low-level auxiliary losses.

Figure 1: Sketch of our training-time model with multiple losses applied at different layers. The encoder is a pyramidal bidirectional
LSTM (our experiments use 4 layers; we show 3 layers for simplicity). Different hidden state layers of this encoder are used for predicting
HMM state label si , phone sequence z (using either CTC or a LSTM decoder), and nally the output character sequence y via a LSTM
decoder. The dotted line in the character decoder denotes the use of (sampled) model predictions [20] during training (for the phone
decoder only the ground-truth prior phone is used in training). At test time, only the character decoder is used for transcription.
 for j = 2, 3, 4
where f (j ) and b(j ) denote the forward and backward running
LSTMs at layer j . Following [4], we use pyramidal layers
to reduces the time resolution of the nal state sequence h(4)
by a factor of 23 = 8. This reduction brings down the input
sequence length, initially T = |x|, where |  | denotes the length
of a sequence of vectors, close to the output sequence length2 ,
K = |y |. For sim');
INSERT INTO posts (postId,userId,title,body) VALUES (9,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu (part 3)','plicity, we will refer to h(4) as h.

3. Models
The multitask approach we propose can in principle be applied to
any type of deep end-to-end model. Here we study the approach
in the context of attention-based deep RNNs. Below we describe
the baseline model, followed by the auxiliary low-level training
tasks.

cessed as follows:

2i1 ; h(j1)
i = f (j ) ([h(j1)
h(j )

2i
i = b(j ) ([h(j1)
2i1 ; h(j1)
h(j )


],
2i
h(j )
h(j )
h(j )
i )i = (

h(j )

i1 )
h(j )
i+1 )

3.1. Baseline Model

],

The model is based on attention-enabled encoder-decoder RNNs,
proposed by [19]. The speech encoder reads in acoustic features
x = (x1 , . . . , xT ) and outputs a sequence of high-level features
(hidden states) h which the character decoder attends to in
generating the output character sequence y = (y1 , . . . , yK ), as
shown in Figure 1 (the attention mechanism and a pyramidal
LSTM layer are not shown in the gure for simplicity).

3.1.1. Speech Encoder

The speech encoder is a deep pyramidal bidirectional Long Short-
Term Memory [31] (BiLSTM) network [4]. In the rst layer,
a BiLSTM reads in acoustic features x and outputs h(1) =
1 , . . . , h(1)
(h(1)
T ) given by:


h(1)
h(1)
i = f (1) (xi ,
i1 )


h(1)
i+1 )


h(1)
i = b(1) (xi ,


h(1)
h(1)i
)

h(1)
i = (

where i  {1, . . . , T } denotes the index of the timestep; f (1) ()
and b(1) () denote the rst layer forward and backward LSTMs
respectively1 .
1 , . . . , h(1)
The rst layer output h(1) = (h(1)
T ) is then pro-

3.1.2. Character Decoder
The character decoder is a single-layer LSTM that predicts a
K(cid:89)
sequence of characters y as follows:
t=1

P (y |x) = P (y |h) =

P (yt |h, y<t ).

The conditional dependence on the encoder state vectors h is
represented by context vector ct , which is a function of the
current decoder hidden state and the encoder state sequence:
(cid:62) tanh(W1hi + W2dt + ba )
|h|(cid:88)
i=1

t = softmax(ut )

uit = v

ithi

ct =

where the vectors v , ba and the matrices W1 , W2 are learnable
parameters; dt is the hidden state of the decoder at time step
t. The time complexity of calculating the context vector ct for
every time step is O(|h|); reducing the resolution on encoder
side is crucial to reducing this runtime.
The hidden state of the decoder, dt , which captures the
previous character context y<t , is given by:

dt = g( y t1 , dt1 , ct1 )

1 For brevity we exclude the LSTM equations. The details can be
found, e.g., in Zaremba et al. [32].

2 For Switchboard, the average of number of frames per character is
about 7.

s8s7s6s5s4s3s2x8x7x6x5x4x3x2x1GOGOy1y2xTsTs1PhoneCTC(LCTCp)State(Ls)CharDec(Lc)PhoneDec(LDecp)z1z2z1where g() is the transformation of the single-layer LSTM, dt1
is the previous hidden state of the decoder, and y t1 is a char-
acter embedding vector for yt1 , as is typical practice in RNN-
based language models. Finally, the posterior distribution of the
output at time step t is given by:
P (yt |h, y<t ) = softmax(Ws [ct ; dt ] + bs ),
and the character decoder loss function is then dened as
Lc =  log P (y |x).

3.2. Low-Level Auxiliary Tasks
As shown in Figure 1, we explore multiple types of auxiliary
tasks in our multitask approach. We explore two types of auxil-
iary labels for multitask learning: phonemes and sub-phonetic
states. We hypothesize that the intermediate representations
needed for sub-phonetic state classication are learned at the
lowest layers of the encoder, while representations for phonetic
prediction may be learned at a somewhat higher level.

3.2.2. State-Level Auxiliary Task
Sub-phonetic state labels provide another type of low-level su-
pervision that can be borrowed from traditional modular HMM-
based approaches. We app');
INSERT INTO posts (postId,userId,title,body) VALUES (10,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu (part 4)','ly this type of supervision at the frame
level, as shown in Figure 1, using state alignments obtained from
a standard HMM-based system. We apply this auxiliary task at
layer 2 of the speech encoder. The probability of a sequence of
M(cid:89)
M(cid:89)
states s is dened as
m=1
m=1

P (sm |h(2)
m ),

P (sm |x) =

P (s|x) =

where P (sm |h(2)
m ) is computed by a softmax function, and M
is the number of frames at layer 2 (in this case (cid:100)T /2(cid:101)). Since we
use this task at layer 2, we subsample the state labels to match
the reduced resolution. The nal state-level loss is
Ls =  log P (s|x).

3.2.1. Phoneme-Based Auxiliary Tasks
We use phoneme-level supervision obtained from the word-level
transcriptions and pronunciation dictionary. We consider two
types of phoneme transcription loss: Phoneme Decoder Loss:
Similar to the character decoder described above, we can attach
a phoneme decoder to the speech encoder as well. The phoneme
decoder has exactly the same mathematical form as the character
decoder, but with a phoneme label vocabulary at the output.
Specically, the phoneme decoder loss is dened as
p =  log P (z |x),
LDec
where z is the target phoneme sequence. Since this decoder
can be attached at any depth of the four-layer encoder described
above, we have four depths to choose from. We attach the
phoneme decoder to layer 3 of the speech encoder, and also
compare this choice to attaching it to layer 4 (the nal layer) for
comparison with a more typical multitask training approach.
CTC Loss: A CTC [33] output layer can also be added to
various layers of the speech encoder [30]. This involves adding
an extra softmax output layer on top of the chosen intermediate
layer of the encoder, and applying the CTC loss to the output
of this softmax layer. Specically, let z be the target phoneme
sequence, and k be the speech encoder layer where the loss is
(cid:88)
(cid:88)
J(cid:89)
applied. The probability of z given the input sequence is
B1 (z)
B1 (z)
j=1
where B() removes repetitive symbols and blank symbols, B1
is Bs pre-image, J is the number of frames at layer k and
P (j |h(k)
) is computed by a softmax function. The nal CTCobjective is
p =  log P (z |x).
LCTC
The CTC objective computation requires the output length to be
less than the input length, i.e., |z | < J . In our case the encoder
reduces the time resolution by a factor of 8 between the input
and the top layer, making the top layer occasionally shorter than
the number of phonemes in an utterance. We therefore cannot
apply this loss to the topmost layer, and use it only at the third
layer.3

P ( |h(k) ) =

P (j |h(k)
P (z |x) =

),

3 In fact, even at the third layer we nd occasional instances (about 10
utterances in our training set) where the hidden state sequence is shorter

L =

3.2.3. Training Loss
The nal loss function that we minimize is the average of the
losses involved. For example, in the case where we use the
character and phoneme decoder losses and the state-level loss,
the loss would be

(Lc + LDec
p + Ls ).
3
4. Experiments
We use the Switchboard corpus (LDC97S62) [34], which con-
tains roughly 300 hours of conversational telephone speech, as
our training set. We reserve the rst 4K utterances as a develop-
ment set. Since the training set has a large number of repetitions
of short utterances (yeah, uh-huh, etc.), we remove dupli-
cates beyond a count threshold of 300. The nal training set
has about 192K utterances. For evaluation, we use the HUB5
Eval2000 data set (LDC2002S09), consisting of two subsets:
Switchboard (SWB), which is similar in style to the training set,
and CallHome (CHE), which contains');
INSERT INTO posts (postId,userId,title,body) VALUES (11,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu (part 5)',' unscripted conversations
between close friends and family.
For input features, we use 40-dimensional log-mel lterbank
features along with their deltas, normalized with per-speaker
mean and variance normalization. The phoneme labels for the
auxiliary task are generated by mapping words to their canoni-
cal pronunciations, using the lexicon in the Kaldi Switchboard
training recipe. The HMM state labels were obtained via forced
alignment using a baseline HMM/DNN hybrid system using
the Kaldi NNet1 recipe. The HMM/DNN has 8396 tied states,
which makes the frame-level softmax costly for multitask learn-
ing. We use the importance sampling technique described in [35]
to reduce this cost.
4.1. Model Details and Inference
The speech encoder is a 4-layer pyramidal bidirectional LSTM,
resulting in a 8-fold reduction in time resolution. We use 256
hidden units in each direction of each layer. The decoder for
all tasks is a single-layer LSTM with 256 hidden units. We
represent the decoders output symbols (both characters and,
at training time, phonemes) using 256-dimensional embedding
vectors. At test time, we use a greedy decoder (beam size =

than the input sequence, due to sequences of phonemes of duration less
than 4 frames each. Anecdotally, these examples appear to correspond
to incorrect training utterance alignments

Table 1: Character error rate (CER, %) and word error rate
(WER, %) results on development data.
Dev CER
Model
14.6
Enc-Dec (baseline)
13.8
Enc-Dec + PhoneDec-3
14.5
Enc-Dec + PhoneDec-4
14.0
Enc-Dec + PhoneCTC-3
Enc-Dec + State-2
13.6
13.4
Enc-Dec + PhoneDec-3 + State-2

Dev WER
26.0
24.9
25.9
25.3
24.1
24.1

Table 2: WER (%) on Eval2000 for different end-to-end models.
PhoneDec-n refers to a phoneme decoder applied at layer n
of the encoder. Similarly, PhoneCTC-3 means phoneme CTC
loss applied at layer 3 and State-2 means state-label supervision
applied at layer 2 of the encoder.
Model
Our models
Enc-Dec (baseline)
Enc-Dec + PhoneDec-3
Enc-Dec + PhoneDec-4
Enc-Dec + PhoneCTC-3
Enc-Dec + State-2
Enc-Dec + PhoneDec-3 + State-2
Lu et al.[7]
Enc-Dec
Enc-Dec (word) + 3-gram
Maas et al. [8]
CTC
CTC + 3-layer RNN LM
Zweig et al. [9]
Iterated CTC
CTC + Char Ngram
CTC + Dictionary + Word Ngram

25.0
24.5
25.4
24.6
24.7
23.1

42.4
40.6
41.9
41.3
42.0
40.8

56.1
40.2

37.1
32.1
25.3

38.0
21.4

24.7
19.8
14.0

48.2
46.0

33.7
32.6
33.7
33.0
33.4
32.0

37.8
36.0

47.1
30.8

27.3
25.8

SWB

CHE

Full





Figure 2: Log-likelihood of train data (per character) for differ-
ent model variations.

1) to generate the character sequence. The character with the
maximum posterior probability is chosen at every time step and
fed as input into the next time step. The decoder stops after
encountering the EOS (end-of-sentence) symbol. We use no
explicit language model.
We train all models using Adam [36] with a minibatch size of
64 utterances. The initial learning rate is 1e-3 and is decayed by
a factor of 0.95, whenever there is an increase in log-likelihood
of the development data, calculated after every 1K updates, over
its previous value. All models are trained for 75K gradient
updates (about 25 epochs) and early stopping. To further control
overtting we: (a) use dropout [37] at a rate of 0.1 on the output
of all LSTM layers (b) sample the previous steps prediction [20]
in the character decoder, with a constant probability of 0.1 as
in [4].
4.2. Results
We evaluate performance using word error rate (WER). We
report results on the combined Eval2000 test set as well as sepa-
rately on the SWB and CHE subsets. We also report character
error rates (CER) on the development set.
Development set results are shown in Table 1. We refer to
the baseline model as Enc-Dec and the models with multi-
task training as Enc-Dec + [auxiliary task]-[layer]. Adding
phoneme recognition as an auxil');
INSERT INTO posts (postId,userId,title,body) VALUES (12,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu (part 6)','iary task at layer 3, either with a
separate LSTM decoder or with CTC, reduces both the character
error rates and the nal word error rates.
In order to determine whether the improved performance is
a basic multitask training effect or is specic to the low-level
application of the loss, we compare these results to those of
adding the phoneme decoder at the topmost layer (Enc-Dec +
PhoneDec-4). The top-layer application of the phoneme loss
produces worse performance than having the supervision at the
lower (third) layer. Finally, we obtain the best results by adding
both phoneme decoder supervision at the third layer and frame-
level state supervision at the second layer (Enc-Dec + PhoneDec-
3 + State-2). The results support the hypothesis that lower-level
supervision is best provided at lower layers. Table 2 provides test
set results, showing the same pattern of improvement on both

the SWB and CHE subsets. For comparison, we also include a
variety of other recent results with neural end-to-end approaches
on this task. Our baseline model has better performance than
the most similar previous encoder-decoder result [7]. With the
addition of the low-level auxiliary task training, our models are
competitive with all of the previous end-to-end systems that do
not use a language model.
Figure 2 shows the training set log-likelihood for the baseline
model and two multitask variants. The plot suggests that mul-
titask training helps with optimization (improving the training
error). Training error is very similar for both multitask models,
while the development set performance is better for one of them
(see Table 1), suggesting that there may also be an improved
generalization effect and not only improved optimization.
5. Conclusion
We have presented a multitask training approach for deep end-
to-end ASR models in which lower-level task losses are applied
at lower levels, and we have explored this approach in the con-
text of attention-based encoder-decoder models. Results on
Switchboard and CallHome show consistent improvements over
baseline attention-based models and support the hypothesis that
lower-level supervision is more effective when applied at lower
layers of the deep model. We have compared several types of
auxiliary tasks, obtaining the best performance with a combina-
tion of a phoneme decoder and frame-level state loss. Analysis
of model training and performance suggests that the addition of
auxiliary tasks can help in either optimization or generalization.
Future work includes studying a broader range of auxiliary
tasks and model congurations. For example, it would be in-
teresting to study even deeper models and word-level output,
which would allow for more options of intermediate tasks and
placements of the auxiliary losses. Viewing the approach more
broadly, it may be fruitful to also consider higher-level task su-
pervision, incorporating syntactic or semantic labels, and to view
the ASR output as an intermediate output in a more general
hierarchy of tasks.
6. Acknowledgements
We are grateful to William Chan for helpful discussions, and to the speech
group at TTIC, especially Shane Settle, Herman Kamper, Qingming
Tang, and Bowen Shi for sharing their data processing code. This
research was supported by a Google faculty research award.

[20] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, Scheduled sam-
pling for sequence prediction with recurrent neural networks, in
Neural Information Processing Systems (NIPS), 2015.
[21] R. Caruana, Multitask learning, Machine Learning, 1997.
[22] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,
and P. Kuksa, Natural language processing (almost) from scratch,
Journal of Machine Learning Research (JMLR), 2011.
[23] M. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser, Multi-
task sequence to sequence learning, in International Conference
on Learning Representations (ICLR), 201');
INSERT INTO posts (postId,userId,title,body) VALUES (13,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu (part 7)','6.
[24] A. Eriguchi, Y. Tsuruoka, and K. Cho, Learning to parse
and translate improves neural machine translation, CoRR, vol.
abs/1702.03525, 2017.
[25] S. Kim, T. Hori, and S. Watanabe, Joint CTC-attention based
end-to-end speech recognition using multi-task learning, CoRR,
vol. abs/1609.06773, 2016.
[26] W. Chan and I. Lane, On online attention-based speech recogni-
tion and joint Mandarin character-Pinyin training, in Interspeech,
2016.
[27] Z. Wu, C. Valentini-Botinhao, O. Watts, and S. King, Deep neural
networks employing multi-task learning and stacked bottleneck
features for speech synthesis, in International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2015.
[28] A. Sgaard and Y. Goldberg, Deep multi-task learning with low
level tasks supervised at lower layers, in Annual Meeting of the
Association for Computational Linguistics (ACL), 2016.
[29] K. Hashimoto, C. Xiong, Y. Tsuruoka, and R. Socher, A joint
many-task model: Growing a neural network for multiple NLP
tasks, CoRR, vol. abs/1611.01587, 2016.
[30] K. Rao and H. Sak, Multi-accent speech recognition with hierar-
chical grapheme based models, in International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), 2017.
[31] S. Hochreiter and J. Schmidhuber, Long short-term memory,
Neural Computation, vol. 9, 1997.
[32] W. Zaremba, I. Sutskever, and O. Vinyals, Recurrent neural net-
work regularization, CoRR, vol. abs/1409.2329, 2014.
[33] A. Graves, S. Fern andez, and F. Gomez, Connectionist temporal
classication: Labelling unsegmented sequence data with recur-
rent neural networks, in International Conference on Machine
Learning (ICML), 2006.
[34] J. J. Godfrey, E. C. Holliman, and J. McDaniel, SWITCHBOARD:
Telephone speech corpus for research and development, in Inter-
national Conference on Acoustics, Speech and Signal Processing
(ICASSP), 1992.
[35] S. Jean, K. Cho, R. Memisevic, and Y. Bengio, On using very
large target vocabulary for neural machine translation, in Annual
Meeting of the Association for Computational Linguistics (ACL),
2015.
[36] J. Duchi, E. Hazan, and Y. Singer, Adaptive subgradient meth-
ods for online learning and stochastic optimization, Journal of
Machine Learning Research (JMLR), vol. 12, 2011.
[37] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, Dropout
improves recurrent neural networks for handwriting recognition,
in International Conference on Frontiers in Handwriting Recogni-
tion (ICFHR), 2014.

7. References
[1] M. Gales and S. Young, The application of hidden markov models
in speech recognition, Foundations and trends in signal process-
ing, vol. 1, 2008.
[2] K. Vesel `y, A. Ghoshal, L. Burget, and D. Povey, Sequence-
discriminative training of deep neural networks. in Interspeech,
2013.
[3] D. Povey and B. Kingsbury, Evaluation of proposed modications
to mpe for large scale discriminative training, in IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2007.
[4] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, Listen, attend and
spell: A neural network for large vocabulary conversational speech
recognition, in International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), 2016.
[5] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-
gio, Attention-based models for speech recognition, in Neural
Information Processing Systems (NIPS), 2015.
[6] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,
End-to-end attention-based large vocabulary speech recognition,
in International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), 2016.
[7] L. Lu, X. Zhang, and S. Renals, On training the recurrent neural
network encoder-decoder for l');
INSERT INTO posts (postId,userId,title,body) VALUES (14,2258,'Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu (part 8)','arge vocabulary end-to-end speech
recognition, in International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), 2016.
[8] A. L. Maas, Z. Xie, D. Jurafsky, and A. Y. Ng, Lexicon-free
conversational speech recognition with neural networks, in North
American Chapter of the Association for Computational Linguistics
on Human Language Technology (NAACL HLT), 2015.
[9] G. Zweig, C. Yu, J. Droppo, and A. Stolcke, Advances in all-
neural speech recognition, CoRR, vol. abs/1609.05935, 2016.
[10] Y. Miao, M. Gowayyed, and F. Metze, EESEN: End-to-end speech
recognition using deep RNN models and WFST-based decoding,
in IEEE Workshop on Automatic Speech Recognition and Under-
standing (ASRU), 2015.
[11] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, Deep
networks with stochastic depth, in European Conference on Com-
puter Vision (ECCV), 2016.
[12] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. E.
Hinton, Grammar as a foreign language, in Neural Information
Processing Systems (NIPS), 2015.
[13] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen,
N. Thorat, F. B. Vi egas, M. Wattenberg, G. Corrado, M. Hughes,
and J. Dean, Googles multilingual neural machine translation sys-
tem: Enabling zero-shot translation, CoRR, vol. abs/1611.04558,
2016.
[14] A.-r. Mohamed, G. E. Hinton, and G. Penn, Understanding how
deep belief networks perform acoustic modelling, in International
Conference on Acoustics, Speech, and Signal Processing (ICASSP),
2012.
[15] T. Nagamine, M. L. Seltzer, and N. Mesgarani, On the role of
nonlinear transformations in deep neural network acoustic models,
Interspeech, 2016.
[16] M. D. Zeiler and R. Fergus, Visualizing and understanding convo-
lutional networks, in European Conference on Computer Vision
(ECCV), 2014.
[17] R. Girshick, J. Donahue, T. Darrell, and J. Malik, Rich feature hi-
erarchies for accurate object detection and semantic segmentation,
in IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2014.
[18] I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence
learning with neural networks, in Neural Information Processing
Systems (NIPS), 2014.
[19] D. Bahdanau, K. Cho, and Y. Bengio, Neural machine transla-
tion by jointly learning to align and translate, in International
Conference on Learning Representations (ICLR), 2015.

');
INSERT INTO posts (postId,userId,title,body) VALUES (15,7688,'A Complexity Trichotomy for the Six-Vertex Model','Jin-Yi Cai
jyc@cs.wisc.edu

Zhiguo Fu
fuzg@jlu.edu.cn

Shuai Shao
sh@cs.wisc.edu

Abstract

We prove a complexity classication theorem that divides the six-vertex model into exactly
three types. For every setting of the parameters of the model, the computation of the partition
function is precisely: (1) Solvable in polynomial time for every graph, or (2) #P-hard for general
graphs but solvable in polynomial time for planar graphs, or (3) #P-hard even for planar
graphs. The classication has an explicit criterion. In addition to matchgates and matchgates-
transformable signatures, we discover previously unknown families of planar-tractable partition
functions by a non-local connection to #CSP, dened in terms of a loop space. For the proof
of #P-hardness, we introduce the use of Mobius transformations as a powerful new tool to prove
that certain complexity reductions succeed in polynomial time.

1 Introduction

Partition functions are Sum-of-Product computations. In physics, one considers a set of particles
connected by some bonds. Then physical considerations impose various local constraints, each with
a suitable weight. Given a conguration satisfying all the local constraints, the product of local
weights is the weight of the conguration, and its sum over all congurations is the value of the
partition function. It encodes much information about the physical system.
This is essentially the same set-up as counting constraint satisfaction problems (#CSP). Take
a set of constraint functions F , the problem #CSP(F ) is as follows: The input is a bipartite graph
G = (U, V , E ), where U are the variables (spins), V is labeled by constraint functions from F ,
and E describes how the constraints are applied on the variables. The output is the sum, over all
assignments to variables in U , of the product of constraint function evaluations in V . Note that
each function in F has a xed arity, and in general takes values in C (not just {0, 1}). A spin
system is the special case of #CSP where the constraints are binary functions (in which case each
v  V has degree 2 and can be replaced by an edge).
By denition, a partition function is an exponential sized sum. But in some cases, clever algo-
rithms exist that can compute it in polynomial time. Well-known examples of partition functions
from physics that have been investigated intensively in complexity theory include the Ising model,
Potts model, hardcore gas and Ice model [14, 10, 11, 18, 25]. Most of these are spin systems. If
particles take (+) or () spins, each can be modeled by a Boolean variable, and local constraints
Department of Computer Sciences, University of Wisconsin-Madison. Supported by NSF CCF-1217549.
School of Mathematics, Jilin University. Supported by NSF CCF-1217549.

are expressed by edge (binary) constraint functions. These are nicely modeled by the #CSP frame-
work. Some physical systems are more naturally described as orientation problems, and these can
be modeled by Holant problems, of which #CSP is a special case. Roughly speaking, Holant prob-
lems [7] (see Section 2 for denitions) are tensor networks where edges of a graph are variables while
vertices are local constraint functions. Spin systems can be simulated easily as Holant problems,
but Freedman, Lovasz and Schrijver proved that simulation in the reverse direction is generally not
possible [8]. In this paper we study a family of partition functions that t the Holant problems
naturally, but not as a spin system. This is the six-vertex model.
In physics, the six-vertex model concerns crystal lattices with hydrogen bonds. Remarkably it
can be expressed perfectly as a family of Holant problems with 6 parameters, although in physics
people are more focused on regular planar s');
INSERT INTO posts (postId,userId,title,body) VALUES (16,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 2)','tructures such as lattice graphs, and asymptotic limit.
Previously, without being able to account for the planar restriction, it has been proved [5] that
there is a complexity dichotomy where computing the partition function ZSix is either in P or
#P-hard. However the more interesting problem is what happens on planar structures where
physicists had discovered some remarkable algorithms, such as Kasteleyns algorithm for planar
perfect matchings [21, 15, 16]. Concomitantly, and also probably because of that, to achieve a
complete complexity classication in the planar case is more challenging. It must isolate precisely
those problems that are #P-hard in general graphs but P-time computable on planar graphs.
In this paper we prove a complexity trichotomy theorem for the six-vertex models: According
to the 6 parameters from C, the partition function ZSix is either computable in P-time, or #P-
hard on general graphs but computable in P-time on planar graphs, or remains #P-hard on planar
graphs. The classication has an explicit criterion. In addition to matchgates and matchgates-
transformable signatures, we discover previously unknown families of planar-tractable ZSix by a
non-local connection to #CSP, dened in terms of a loop space.
Linus Pauling in 1935 rst introduced the six-vertex models to account for the residual entropy
of water ice [20]. We have a large number of oxygen and hydrogen atoms in a 1 to 2 ratio. Each
oxygen atom (O) is connected by a bond to four other neighboring oxygen atoms (O), and each
bond is occupied by one hydrogen atom (H). Physical constraint requires that each (H) is closer to
either one or the other of the two neighboring (O), but never in the middle of the bond. Pauling
argued [20] that, furthermore, the allowed congurations are such that at each oxygen (O) site,
exactly two hydrogen (H) are closer to it, and the other two are farther away. The placement of
oxygen and hydrogen atoms can be naturally represented by vertices and edges of a 4-regular graph.
The constraint on the placement of hydrogen atoms (H) can be represented by an orientation of the
Since there are (cid:0)4
(cid:1) = 6 local valid congurations, this is called the six-vertex model. In addition
edges of the graph, such that at every vertex (O), exactly two edges are oriented toward the vertex,
and exactly two edges are oriented away from it. In other words, this is an Eulerian orientation.to water ice, potassium dihydrogen phosphate KH2PO4 (KDP) also satises this model.
The valid local congurations of the six-vertex model are illustrated in Figure 1. The energy E
Then the partition function is ZSix = (cid:80) eE /kB T , where the sum is over all valid congurations,
of the system is determined by six parameters 1 , 2 , . . . , 6 associated with each type of the local
conguration. If there are ni sites in local congurations of type i, then E = n1 1 + n2 2 + . . . + n6 6 .
kB is Boltzmanns constant, and T is the systems temperature. Mathematically, this is a sum-of-
product computation where the sum is over all Eulerian orientations of the graph, and the product
is over all vertices where each vertex contributes a factor ci = ci if it is in conguration i (1  i  6)
for some constant c.

(cid:78)
(cid:73) (cid:73)
(cid:78)

(cid:72)
(cid:74) (cid:74)
(cid:72)

(cid:72)
(cid:73) (cid:73)
(cid:72)

(cid:78)
(cid:74) (cid:74)
(cid:78)

(cid:78)
(cid:73) (cid:74)
(cid:72)

(cid:72)
(cid:74) (cid:73)
(cid:78)

Figure 1: Valid congurations of the six-vertex model

Some choices of the parameters are well-studied. On the square lattice graph, when modeling
approaches (cid:0) 4
(cid:1)3/');
INSERT INTO posts (postId,userId,title,body) VALUES (17,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 3)','2  1.5396007 . . . (Liebs square ice constant). This matched experimental data
ice one takes 1 = 2 = . . . = 6 = 0.
In 1967, Elliott Lieb [19] famously showed that, as the
number N of vertices approaches , the value of the partition function per vertex W = Z 1/N
1.540  0.001 so well that it is considered a triumph. Other well-known six-vertex models include:the KDP model of a ferroelectric (1 = 2 = 0, and 3 = 4 = 5 = 6 > 0), the Rys F model of an
antiferroelectric (1 = 2 = 3 = 4 > 0, and 5 = 6 = 0). Historically these are widely considered
among the most signicant applications ever made of statistical mechanics to real substances. In
classical statistical mechanics the parameters are all real numbers while in quantum theory the
parameters are complex numbers in general.
Disregarding the planarity restriction, [5] proved that computing the partition function ZSix
is either in P or #P-hard. However known cases of planar P-time computable ZSix (but #P-
hard on general graphs) are all #P-hard in this classication. In this paper we tackle the more
dicult planar case, and prove a complexity trichotomy theorem. The most interesting part is
the classication of those ZSix which are #P-hard in general but P-time computable on planar
structures. The classication is valid for all parameter values c1 , c2 , . . . , c6  C. (To state our
theorem in strict Turing machine model, we take c1 , c2 , . . . , c6 to be algebraic numbers.) The
dependence of this trichotomy on the values c1 , c2 , . . . , c6 is explicit.
are transformable by a holographic transformation to matchgates (denoted by (cid:99)M ) do constitute a
We show that constraints that are expressible as matchgates (denoted by M ) or those that
family of ZSix which are #P-hard in general but P-time computable on planar structures. This is
as expected and is known before (Kasteleyns algorithm for planar perfect matchings, and Valiants
holographic algorithms based on matchgates [23, 24]). However we also discover an additional
family of such ZSix which are not transformable to matchgates. The P-time tractability on planar
graphs is via a non-local transformation to #CSP, where the variables in #CSP correspond to
certain loops in the six-vertex model graph. The fact that the #CSP instance is P-time tractable
depends heavily on the global topological constraint imposed by the planar structure.
After carving out this last tractable family, we set about to prove that everything else is #P-
hard, even for the planar case. A powerful tool in such proofs is the interpolation technique [22].
Typically an interpolation proof can succeed when certain quantities (such as eigenvalues) are not
roots of unity, lest the iteration repeat after a bounded number of steps. A sucient condition is
that these quantities have complex norm (cid:54)= 1. However for some constraint functions, we can show
that all constructions necessarily produce only relevant quantities of unit norm. In this case we
introduce Mobius transformations z (cid:55) az+b
cz+d . It turns out that in this case the constraint function
denes a natural Mobius transformation that maps unit circle to unit circle on C. By exploiting
the conformal mapping property we can obtain a suitable Mobius transformation which generates
a group of innite order. This allows us');
INSERT INTO posts (postId,userId,title,body) VALUES (18,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 4)',' to show that our interpolation proof succeeds.

2 Preliminaries and Notations
1, a square root of 1.

In this paper, i denotes

2.1 Denitions and Notations
A constraint function f of arity k is a map {0, 1}k  C. Fix a set F of constraint functions. A
signature grid  = (G, ) is a tuple, where G = (V , E ) is a graph,  labels each v  V with a
consider all 0-1 edge assignments  , each gives an evaluation (cid:81)
function fv  F of arity deg(v), and the incident edges E (v) at v with input variables of fv . We
fv ( |E (v) ), where  |E (v) denotes
vV
(cid:88)
(cid:89)
the restriction of  to E (v). The counting problem on the instance  is to compute
fv ( |E (v) ).
Holant(; F ) =
vV
 :E{0,1}
The Holant problem parameterized by the set F is denoted by Holant(F ). If F = {f } is a single
set, for simplicity, we write {f } as f directly, and also we write {f , g} as f , g . When G is a planar
graph, the corresponding signature grid is called a planer grid. We use Holant (F | G ) to denote the
Holant problem over signature grids with a bipartite graph H = (U, V , E ), where each vertex in U
or V is assigned a signature in F or G respectively. Signatures in F are considered as row vectors
(or covariant tensors); signatures in G are considered as column vectors (or contravariant tensors).
Similarly, Pl-Holant (F | G ) denotes the Holant problem over signature grids with a planar bipartite
(cid:34) f0000 f0010 f0001 f0011
(cid:35)
graph.
A constraint function is also called a signature. A signature f of arity 4 has the signature matrix
f0100 f0110 f0101 f0111
M (f ) = Mx1 x2 ,x4 x3 (f ) =
. If (i, j, k , (cid:96)) is a permutation of (1, 2, 3, 4), then
f1000 f1010 f1001 f1011
f1100 f1110 f1101 f1111
the 4  4 matrix Mxi xj ,x(cid:96) xk (f ) lists the 16 values with row index xixj  {0, 1}2 and column index
(cid:20) 0 0 0 a
(cid:21)
x(cid:96)xk  {0, 1}2 in lexicographic order. Without other specication, M (f ) denotes Mx1 x2 ,x4 x3 (f ).
(cid:104) M (f )2,2 M (f )2,3
(cid:105)
(cid:3) denoted by MIn (f ) the inner matrix of M (f ), and the submatrix
= (cid:2) b c
The planar six-vertex model is Pl-Holant((cid:54)=2 | f ), where M (f ) =
0 b c 0
. We call the subma-
0 z y 0
(cid:104) M (f )1,1 M (f )1,4
(cid:105)
x 0 0 0
trix
z y
M (f )3,2 M (f )3,3
= [ 0 a
x 0 ] denoted by MOut (f ) the outer matrix of M (f ). A binary signature g
M (f )4,1 M (f )4,4
(cid:20) 0 0 0 1
(cid:21)
has the signature matrix M (g) = Mx1 ,x2 (g) = [ g00 g01
g10 g11 ] . Without other specication, M (g) denotes
Mx1 ,x2 (g). We use ( (cid:54)=2 ) to denote binary Disequality signature (0, 1, 1, 0)T . It has the signature
matrix M ((cid:54)=2 ) = [ 0 1
1 0 ]  [ 0 1
. Note that N is the double Disequality,
0 0 1 0
1 0 ]. Let N = [ 0 1
1 0 ] =
0 1 0 0
which is the function of connecting two pairs of edges by ((cid:54)=2 ). A function is symmetric if its value
1 0 0 0
depends only on the Hamming weight of its input. A symmetric function f on k Boolean variables
can be expressed as [f0 , f1 , . . . , fk ], where fw is the value of f on inputs of Hamming weight w.
For example, (=k ) is the Equality signature [1, 0, . . . , 0, 1] (with k  1 many 0s) of arity k . The
support of a function f is the set of inputs on which f is nonzero.
Counting constraint satisfaction pr');
INSERT INTO posts (postId,userId,title,body) VALUES (19,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 5)','oblems (#CSP) can be dened as a special case of Holant
problems. An instance of #CSP(F ) is presented as a bipartite graph. There is one node for each
variable and for each occurrence of constraint functions respectively. Connect a constraint node to
a variable node if the variable appears in that occurrence of constraint, with a labeling on the edges

for the order of these variables. This bipartite graph is also known as the constraint graph. If we
attach each variable node with an Equality function, and consider every edge as a variable, then
the #CSP is just the Holant problem on this bipartite graph. Thus #CSP(F ) T Holant (E Q | F ),
where E Q = {=1 , =2 , =3 , . . . } is the set of Equality signatures of all arities. By restricting to
planar constraint graphs, we have the planar #CSP framework, which we denote by Pl-#CSP. The
construction above also shows that Pl-#CSP(F ) T Pl-Holant (E Q | F ).

2.2 Gadget Construction

One basic notion used throughout the paper is gadget construction. We say a signature f is
constructible or realizable from a signature set F if there is a gadget with some dangling edges
such that each vertex is assigned a signature from F , and the resulting graph, when viewed as a
black-box signature with inputs on the dangling edges, is exactly f . If f is realizable from a set F ,
then we can freely add f into F while preserving the complexity.

f (y1 , . . . , yn ) =

Figure 2: An F -gate with 5 dangling edges.
Formally, this notion is dened by an F -gate. An F -gate is similar to a signature grid (G, )
for Holant(F ) except that G = (V , E , D) is a graph with some dangling edges D. The dangling
edges dene external variables for the F -gate (See Figure 2 for an example). We denote the regular
edges in E by 1, 2, . . . , m and the dangling edges in D by m + 1, . . . , m + n. Then we can dene a
(cid:88)
function f for this F -gate as
x1 ,...,xm{0,1}
where (y1 , . . . , yn )  {0, 1}n is an assignment on the dangling edges and H (x1 , . . . , xm , y1 , . . . , yn )
is the value of the signature grid on an assignment of all edges in G, which is the product of
evaluations at all vertices in V . We also call this function f the signature of the F -gate.
An F -gate is planar if the underlying graph G is a planar graph, and the dangling edges, ordered
counterclockwise corresponding to the order of the input variables, are in the outer face in a planar
embedding. A planar F -gate can be used in a planar signature grid as if it is just a single vertex
with the particular signature.
Using planar F -gates, we can reduce one planar Holant problem to another. Suppose g is the
signature of some planar F -gate. Then Pl-Holant(F , g) T Pl-Holant(F ). The reduction is simple.
Given an instance of Pl-Holant(F , g), by replacing every occurrence of g by the F -gate, we get an
instance of Pl-Holant(F ). Since the signature of the F -gate is g , the Holant values for these two
signature grids are identical. There are three common gadgets we will use in this paper.

H (x1 , . . . , xm , y1 , . . . , yn ),

Suppose f1 and f2 have signature matrices Mxi xj ,x(cid:96) xk (f1 ) and Mxs xt ,xv xu (f2 ), where (i, j, k , (cid:96))
and (s, t, u, v) are permutations of (1, 2, 3, 4). By connecting x(cid:96) with xs , xk with xt , both using Dise-
quality ( (cid:54)=2 ), we get a signature of arity 4 with the signature matrix Mxi xj ,x(cid:96) xk (f1 )N Mxs xt ,xv xu (f2 )
by matrix product with row index xixj and column index xv xu (See Figure 3). In this paper, we

Figure 3: Connect var');
INSERT INTO posts (postId,userId,title,body) VALUES (20,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 6)','iables x(cid:96) , xk of f1 with variables xs , xt of f2 both using ( (cid:54)=2 ).

Mx4 x1 ,x3 x2 (f ) =

. Note that no matter in which signature matrix, only the pair (c, z ) is

focus on planar graphs, and we assume the edges incident to a vertex are ordered counterclockwise.
When connecting two signatures, we need to keep the counterclockwise order of the edges incident
to each vertex. In order to satisfy this planar property, (i, j, k , (cid:96)) and (s, t, u, v) both have to be
cyclic permutations of (1, 2, 3, 4). In this paper, given a signature f of arity 4 with the signature
(cid:20) 0 0 0 a
(cid:21)
(cid:20) 0 0 0 y
(cid:21)
(cid:20) 0 0 0 x
(cid:21)
matrix Mxi xj ,x(cid:96) xk (f ), we always assume (i, j, k , (cid:96)) is a cyclic permutation of (1, 2, 3, 4). There are
four cyclic permutations of (1, 2, 3, 4), so correspondingly, a signature f has four 4  4 signature
(cid:21)
(cid:20) 0 0 0 b
0 y c 0
0 b c 0
0 a z 0
matrices Mx1 x2 ,x4 x3 (f ) =
, Mx2 x3 ,x1 x4 (f ) =
, Mx3 x4 ,x1 x2 (f ) =
, and
0 z y 0
0 c x 0
0 z b 0
x 0 0 0
b 0 0 0
a 0 0 0
0 x z 0
0 c a 0
y 0 0 0
always in the inner matrix. We call (c, z ) the inner pair, and (a, x), (b, y) outer pairs. On the other
hand, given a signature f (cid:48) of arity 4 with the signature matrix Mx1 x2 ,x4 x3 (f (cid:48) ) = Mxi xj ,xk x(cid:96) (f ), we
may relabel the variables x1 , x2 , x3 , x4 of f (cid:48) by xi , xj , x(cid:96) , xk . Then, the signature f (cid:48) is exactly the
signature f . In fact, f (cid:48) can be viewed as a rotation form of f . The four rotation forms of f are
2 , f  and f
2 . Once we get one form, all the four rotation forms can be freely
denoted by f , f
used. In the proof, after one construction, we may use this property to get a similar construction
and conclusion by quoting this rotational symmetry.
A binary signature g has the signature vector g(x1 , x2 ) = (g00 , g01 , g10 , g11 )T , and also g(x2 , x1 ) =
(g00 , g10 , g01 , g11 )T . Without other specication, g denotes g(x1 , x2 ). Let f be a signature of
arity 4 with the signature matrix Mxi xj ,x(cid:96) xk (f ) and (s, t) be a permutation of (1, 2). By con-
necting x(cid:96) with xs and xk with xt , both using Disequality ( (cid:54)=2 ), we get a binary signature
with the signature matrix Mxi xj ,xk x(cid:96) N g(xs , xt ) by matrix product with index xixj (See Figure 4).
If g00 = g11 , then N (g00 , g01 , g10 , g11 )T = (g11 , g10 , g01 , g00 )T = (g00 , g10 , g01 , g11 )T and similarly,
N (g00 , g10 , g01 , g11 )T = (g00 , g01 , g10 , g11 )T . Therefore, Mxi xj ,x(cid:96) xk N g(xs , xt ) = Mxi xj ,x(cid:96) xk g(xt , xs ),
which means connecting variables x(cid:96) , xk of the signature f with variables xs , xt of the signature
g using double Disequality N are equivalent to connecting variables x(cid:96) , xk of the signature f
with variables xt , xs of the signature g directly. In this paper, we use Mxi xj ,x(cid:96) xk g(xt , xs ) instead of
Mxi xj ,x(cid:96) xk N g(xs , xt ) to represent connecting f and g using double Disequality N when g00 = g11 .
Since g is a binary signature, we can rotate it by 180 degree. Then the variables xs and xt change
their positions with each other, and that rotation do not destroy the planar graph. That is, both
g(xs , xt ) and g(xt , xs ) can be freely used once we get one of them.

Figure 4: Connect variables x(cid:96) , xk of f with variables xs , xt of g both using ( (cid:54)=2 ).
A binary signature g also has the 2  2 signature matrix M (g) = Mx1 ,x2 (g) = [ g00 g01
(cid:105)
(cid:104) f0000 f0010 f0001 f0011 f0100 f0110 f0101 f0111
g10 g11 ]. Without
other specication, M (g)');
INSERT INTO posts (postId,userId,title,body) VALUES (21,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 7)',' denotes Mx1 ,x2 (g). A signature f of arity 4 also has the 2  8 signature
. Suppose the signature matrix of
matrix Mx1 ,x2 x4 x3 (f ) =
f1000 f1010 f1001 f1011 f1100 f1110 f1101 f1111
g is Mxs ,xt (g) and the signature matrix of f is Mxi ,xj x(cid:96) xk (f ). By connecting xt with xi using Dise-
qulity ((cid:54)=2 ), we get a signature h of arity 4 with the signature matrix Mxs ,xt (g)Z T ZMxi ,xj x(cid:96) xk (f )
by matrix product with row index xs and column index xj x(cid:96)xk (See Figure 5). We may change

Figure 5: Connect variable xt of g with variable xi of f using ( (cid:54)=2 ).

=

In particular, if
this form to a signature matrix with row index xsxj and column index x(cid:96)xk .
Mxs ,xt (g) = [ 0 1
t 0 ], then
(cid:21) (cid:20)f0000 f0010 f0001 f0011 f0100 f0110 f0101 f0111
(cid:21)
(cid:21) (cid:20)0 1
(cid:20)0 1
Mxs ,xj x(cid:96) xk (h) = Mxs ,xt (g)M ( (cid:54)=2 )Mxi ,xj x(cid:96) xk (f )
(cid:21)
(cid:20) f0000
1 0
f1000 f1010 f1001 f1011 f1100 f1110 f1101 f1111
t 0
f0111
f0101
f0110
f0100
f0011
f0001
f0010
(cid:34) f0000 f0010 f0001 f0011tf1000
tf1010
tf1001
tf1011
tf1100
tf1110
tf1101
tf1111
f0100 f0110 f0101 f0111
tf1000 tf1010 tf1001 tf1011
tf1100 tf1110 tf1101 tf1111
That is, when connecting the variable xt of signature g with the variable xi of signature f , let the
entries of Mxi xj ,x(cid:96) xk (f ) with index xi = 1 multiply by t and that will give the signature matrix of
the constructed signature h. Similarly, when connecting the variable xs of g with the variable xi of
f , let the entries of Mxi xj ,x(cid:96) xk (f ) with index xi = 0 multiply by t and that will give Mxi xj ,x(cid:96) xk (h).
A constant scalar C (cid:54)= 0 does not change the complexity of a Holant problem. That is,
Holant(F  {f }) T Holant(F  {C f }) for any signature set F , signature f and constant scalar

We change the name of the index xs in Mxs ,xj x(cid:96) xk (h) by xi , then Mxi xj ,x(cid:96) xk (h) =

(cid:35)

7

C (cid:54)= 0. In planar instances, that is Pl-Holant(F  {f }) T Pl-Holant(F  {C f }). In other words,
C f is realizable by f for any C (cid:54)= 0. Hence, we can pick a nonzero entry of the signature ma-
trix M (f ) and divide all entries in M (f ) by the nonzero entry we picked. We call this operation
normalization. By normalization, we may assume the nonzero entry we picked is of value 1.

2.3 Holographic Transformation

To introduce the idea of holographic transformation, it is convenient to consider bipartite graphs.
For a general graph, we can always transform it into a bipartite graph while preserving the Holant
value, as follows. For each edge in the graph, we replace it by a path of length two. (This operation
is called the 2-stretch of the graph and yields the edge-vertex incidence graph.) Each new vertex
is assigned the binary Equality signature (=2 ) = [1, 0, 1].
For an invertible 2-by-2 matrix T  GL2 (C) and a signature f of arity n, written as a column
vector (contravariant tensor) f  C2n , we denote by T 1f = (T 1 )nf the transformed signature.
For a signature set F , dene T 1F = {T 1f | f  F } the set of transformed signatures. For
signatures written as row vectors (covariant tensors) we dene F T similarly. Whenever we write
(cid:2) 1 1
(cid:3), we also dene (cid:98)F = H2F');
INSERT INTO posts (postId,userId,title,body) VALUES (22,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 8)',' . Note that
T 1f or T 1F , we view the signatures as column vectors; similarly for f T or F T as row vectors.
In the special case of the Hadamard matrix H2 = 1
1 1H2 is orthogonal. Since constant factors are immaterial, for convenience we sometime drop the
factor 1
when using H2 .
Let T  GL2 (C). The holographic transformation dened by T is the following operation: given(cid:48) = (H,  (cid:48) ) of Holant (cid:0)F T | T 1G (cid:1) by replacing each signature in F or G with the corresponding
a signature grid  = (H, ) of Holant (F | G ), for the same bipartite graph H , we get a new grid
signature in F T or T 1G .
Theorem 2.1 (Valiants Holant Theorem [25]). For any T  GL2 (C),
Holant(; F | G ) = Holant((cid:48) ; F T | T 1G ).

Therefore, a holographic transformation does not change the complexity of the Holant problem
in the bipartite setting. Clearly, this theorem holds for planar instances.
Denition 2.2. We say a signature set F is C -transformable if there exists a T  GL2 (C) such
that (0, 1, 1, 0)T 2  C and T 1F  C .
This denition is important because if Pl-Holant(C ) is tractable, then Pl-Holant((cid:54)=2 | F ) is
tractable for any C -transformable set F .

2.4 Polynomial Interpolation

Lemma 2.3. Let f be a 4-ary signature with the signature matrix M (f ) =

(cid:21)
(cid:20) 0 0 0 1
Polynomial interpolation is a powerful technique to prove #P-hardness for counting problems. We
introduce this technique with the following lemmas.
(cid:20) 0 0 0 1
(cid:21)
, where b (cid:54)= 0
0 b 0 0
0 0 b 0
1 0 0 0
0 1 0 0
0 0 1 0
1 0 0 0

is not a root of unity. Let 1 be a 4-ary signature with the signature matrix M (1 ) =
Then for any signature set F containing f , we have
Pl-Holant( (cid:54)=2 | F  {1}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).

M (f2s+1 ) = M (f )(N M (f ))2s =

Figure 6: A chain of 2s + 1 many copies of f linked by double Disequality N
Proof. We construct a series of gadgets f2s+1 by a chain of 2s + 1 many copies of f linked by .
double Disequality N (See Figure 6). We know f2s+1 has the signature matrix0 b2s+100
The matrix M (f2s+1 ) has a good form for polynomial interpolation. Suppose 1 appears m times
in an instance  of Pl-Holant( (cid:54)=2 | F  {1}). We replace each appearance of 1 by a copy of the
gadget f2s+1 to get an instance 2s+1 of Pl-Holant( (cid:54)=2 | F  {f2s+1}), which is also an instance of
Pl-Holant( (cid:54)=2 | F ). We divide 2s+1 into two parts. One part consists of m signatures f2s+1 and
its signature is represented by (M (f2s+1 ))m . Here we rewrite (M (f2s+1 ))m as a column vector.
The other part is the rest of 2s+1 and its signature is represented by A which is a tensor expressed
as a row vector. Then, the Holant value of 2s+1 is the dot product (cid:104)A, (M (f2s+1 ))m (cid:105), which is
a summation over 4m bits. That is, the value of the 4m edges connecting the two parts. We can
stratify all 0, 1 assignments of these 4m bits having a nonzero evaluation of a term in Pl-Holant2s+1
into the following categories:
 There are i many copies of f2s+1 receiving inputs 0011 or 1100;
 There are j many copies of f2s+1 receiving inputs 0110 or 1001;
where i + j = m.
For any assignment in the category with parameter (i, j ), the evaluation of (M (f2s+1 ))m is
clearly b(2s+1)j . Let aij be the summation of values of the part A over all assignments in the
category (i, j ). Note that aij is independent from the value of s since we view the gadget f2s+1 as
(cid:88)
a block. Since i + j = m, we can denote aij by ');
INSERT INTO posts (postId,userId,title,body) VALUES (23,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 9)','aj . Then, we rewrite the dot product summation
and get
Pl-Holant2s+1 = (cid:104)A, (M (f2s+1 ))m (cid:105) =
0(cid:54)j(cid:54)m
(cid:88)
Under this stratication, the Holant value of Pl-Holant(, (cid:54)=2 | F  {1}) can be represented as
Pl-Holant = (cid:104)A, (M (1 ))m (cid:105) =
aj .
0(cid:54)j(cid:54)m


Since b (cid:54)= 0 is not a root of unity, the Vandermonde coecients matrix
  
bm
b1
b0
  
(b3 )m
(b3 )1
(b3 )0
...
...
...
...
  
(b2m+1 )m
(b2m+1 )1
(b2m+1 )0
00
b2s+1 00

aj b(2s+1)j .

polynomial time and obtain the value of p(x) = (cid:80)
has full rank. By oracle querying the values of Pl-Holant2s+1 , we can solve the coecients aj in
aj xj for any x. Let x = 1, we get Pl-Holant .
(cid:20) 0 0 0 1
(cid:21)
0(cid:54)j(cid:54)m
Therefore, we have Pl-Holant((cid:54)=2 | F  {1}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).
0 b 0 0
0 0 b 0
1 0 0 0

Corollary 2.4. Let f be a 4-ary signature with the signature matrix M (f ) =

(cid:20) 0 0 0 1
(cid:21)
, where b (cid:54)=
0 1 0 0
0 0 1 0
1 0 0 0

M (f2s+1 ) = M (f )(N M (f ))2s =

0 is not a root of unity. Let 2 be a 4-ary signature with the signature matrix M (2 ) =
Then for any signature set F containing f , we have
Pl-Holant( (cid:54)=2 | F  {2}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).
 .
 0
Proof. We still construct a series of gadgets f2s+1 by a chain of odd copies of f linked by
double Disequality N . We know f2s+1 has the signature matrix0b2s+10b2s+1 0000
Suppose 2 appears m times in an instance  of Pl-Holant((cid:54)=2 | f  2 ). We replace each appearance
of 2 by a copy of the gadget f2s+1 to get an instance 2s+1 of Pl-Holant((cid:54)=2 | F  {f2s+1}).
Same as the proof of Lemma 2.3, we divide 2s+1 into two parts. One part is represented by
(M (f2s+1 ))m and the other part is represented by A. Then, the Holant value of 2s+1 is the dot
product (cid:104)A, (M (f2s+1 ))m (cid:105). We can stratify all 0, 1 assignments of these 4m bits having a nonzero
evaluation of a term in Pl-Holant2s+1 into the following categories:
 There are i many copies of f2s+1 receiving inputs 0011;
 There are j many copies of f2s+1 receiving inputs 0110 or 1001;
 There are k many copies of f2s+1 receiving inputs 1100;
where i + j + k = m.
For any assignment in those categories with parameters (i, j, k) where k  0(mod 2), the eval-
uation of (M (f2s+1 ))m is clearly (1)k b(2s+1)j = b(2s+1)j . And for any assignment in those cat-
egories with parameters (i, j, k) where k  1(mod 2), the evaluation of (M (f2s+1 ))m is clearly
(1)k b(2s+1)j = b(2s+1)j . Since i + j + k = m, the index i is determined by j and k . Let aj 0
be the summation of values of the part A over all assignments in those categories (i, j, k) where
k  0(mod 2), and aj 1 be the summation of values of the part A over all assignments in those
categories (i, j, k) where k  1(mod 2). Note that aj 0 and aj 1 are independent from the value of
(cid:88)
(cid:88)
s. Let aj = aj 0  aj 1 . Then, we rewrite the dot product summation and get
Pl-Holant2s+1 = (cid:104)A, (M (f2s+1 ))m (cid:105) =
(aj 0 b(2s+1)j  aj 1 b(2s+1)j ) =
0(cid:54)j(cid:54)m
0(cid:54)j(cid:54)m
(cid:88)
(cid:88)
Under this stratication, the Holant value of Pl-Hol');
INSERT INTO posts (postId,userId,title,body) VALUES (24,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 10)','ant (; (cid:54)=2 | f  2 ) can be represented as
Pl-Holant = (cid:104)A, (M (2 ))m (cid:105) =
(aj 0  aj 1 ) =
aj .
0(cid:54)j(cid:54)m
0(cid:54)j(cid:54)m
solve the coecients in polynomial time and obtain the value of p(x) = (cid:80)
Since b (cid:54)= 0 is not a root of unity, the Vandermonde coecients matrix has full rank. Hence we can
aj xj for any x. Let
0(cid:54)j(cid:54)m
x = 1, we get Pl-Holant . Therefore, we have Pl-Holant( (cid:54)=2 | F  {2}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).

aj b(2s+1)j .

10

Lemma 2.5. Let g = (0, 1, t, 0)T be a binary signature, where t (cid:54)= 0 is not a root of unity. Then
Pl-Holant (cid:0) (cid:54)=2 | F  {g (cid:48)}(cid:1) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .
for any binary signature g (cid:48) of the form (0, 1, t(cid:48) , 0)T and any signature set F containing g , we have
Inductively, for any nite signature set B consisting of binary signatures of the form (0, 1, t(cid:48) , 0)T
and any signature set F containing g , we have
Pl-Holant ((cid:54)=2 | F  B) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .
Proof. Note that M (g) = [ 0 1
t 0 ]. Connect the variable x2 of a copy of g with the variable x1
(cid:20) 0
(cid:21) (cid:20)0 1
(cid:21) (cid:20)0 1
(cid:20)0 1
(cid:21)
(cid:21)
of another copy of g using ( (cid:54)=2 ). We get a signature g2 with the signature matrix
M (g2 ) = Mx1 ,x2 (g)M ( (cid:54)=2 )Mx1 ,x2 (g) =t2 0
t 0
1 0
t 0
That is, g2 = (0, 1, t2 , 0)T . Recursively, we can construct gs = (0, 1, ts , 0)T for s  N. Here, g1
denotes g . Given an instance (cid:48) of Pl-Holant ((cid:54)=2 | F  {g (cid:48)}), same as the proof in Lemma 2.3, we
can replace each appearance of g (cid:48) by gs and get an instance s of Pl-Holant ((cid:54)=2 | F ). Similarly, the
(cid:88)
Holant value of s can be represented as
0(cid:54)j(cid:54)m
(cid:88)
while the Holant value of (cid:48) can be represented as
0(cid:54)j(cid:54)m
Since t (cid:54)= 0 is not a root of unity, we know all ts are distinct, which means the Vandermonde
the value of p(x) = (cid:80)
coecients matrix has full rank. Hence, we can solve the coecients in polynomial time and obtain
aj xj for any x. Let x = t(cid:48) , we get Pl-Holant(cid:48) . Therefore, we have
0(cid:54)j(cid:54)m
Pl-Holant( (cid:54)=2 | F  {g (cid:48)}) (cid:54)T Pl-Holant( (cid:54)=2 | F ). The second part of this lemma follows directly by
the rst part.

Pl-Holant(cid:48) =

Pl-Holants =

.

aj (ts )j ,

aj (t(cid:48) )j .

Remark: Note that the reason why the interpolation can succeed is that we can construct poly-
nomially many binary signatures gs of the form (0, 1, ts , 0)T , where all ts are distinct such that the
Vandermonde coecients matrix has full rank. According to this, we have the following corollary.
Corollary 2.6. Given a signature set F , if we can use F to construct polynomial ly many distinct
binary signatures gs = (0, 1, ts , 0)T , then for any nite signature set B consisting of binary signatures
of the form (0, 1, t(cid:48) , 0)T , we have
Pl-Holant ((cid:54)=2 | F  B) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .
plane (cid:98)C = C  {}, the complex plane plus a point at innity, is a rational fun');
INSERT INTO posts (postId,userId,title,body) VALUES (25,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 11)','ction of the form
In Lemma 6.4, we will show how to construct polynomially many distinct binary signatures gs =
(0, 1, ts , 0)T using Mobius transformation[1]. A Mobius transformation of the extended complex
z  az+b
cz+d of complex variable z, where the coecients a, b, c, d are complex numbers satisfying

11

det (cid:2) a b
(cid:3) = ad  bc (cid:54)= 0. It is a bijective conformal map. In particular, a Mobius transformation
c d
mapping the unit circle S 1 = {z | |z | = 1} to itself is of the form (z) = ei (z+)
1+ z denoted
by M(, ei ), where || (cid:54)= 1. When || < 1, it maps the interior of S 1 to the interior, while
when || > 1, it maps the interior of S 1 to the exterior. A Mobius transformation is completely
determined by its values on any 3 distinct points.
An interpolation proof based on a lattice structure will be given in Lemma 6.1, where the
following lemma is used.
Lemma 2.7. [5] Suppose ,   C  {0}, and the lattice L = {(j, k)  Z2 | j  k = 1} has the form
(cid:1), then
j,k0, j+km (j  k )(cid:96)xj,k for (cid:96) = 1, 2, . . . (cid:0)m+2
s t = 1. If we are given the values N(cid:96) = (cid:80)
L = {(ns, nt) | n  Z}, where s, t  Z and (s, t) (cid:54)= (0, 0). Let  and  be any numbers satisfying
we can compute (cid:80)j,k0, j+km j k xj,k in polynomial time.
2.5 Tractable Signature Sets

We give some signatures that are known to be computable in polynomial time, called tractable.
There are three families: ane signatures, product-type signatures, and matchgate signatures [4].

Ane Signatures

Denition 2.8. For a signature f of arity n, the support of f is
supp(f ) = {(x1 , x2 , . . . , xn )  Zn
2 | f (x1 , x2 , . . . , xn ) (cid:54)= 0}.

Denition 2.9. A signature f (x1 , . . . , xn ) of arity n is ane if it has the form
  AX=0  iQ(X ) ,
where   C, X = (x1 , x2 , . . . , xn , 1), A is a matrix over Z2 , Q(x1 , x2 , . . . , xn )  Z4 [x1 , x2 , . . . , xn ]
is a quadratic (total degree at most 2) multilinear polynomial with the additional requirement that
(cid:88)
n(cid:88)
the coecients of al l cross terms are even, i.e., Q has the form
1i<jn
k=1

Q(x1 , x2 , . . . , xn ) = a0 +

ak xk +

2bij xixj ,

and  is a 0-1 indicator function such that AX=0 is 1 i AX = 0. We use A to denote the set of
al l ane signatures.
Follows by the denition directly, one can check the following two lemmas.
matrix M (g) = a (cid:2) iq00 iq01
(cid:3), where q00 , q01 , q10 , q11  N and q00 + q01 + q10 + q11  0(mod 2).
Lemma 2.10. Let g be a binary signature with support of size 4. Then, g  A i g has the signature
iq1 (cid:3), where q0 , q1  N.
M (h) = a (cid:2)iq0
iq10 iq11
Lemma 2.11. Let h be a unary signature with support of size 2. Then, h  A i h is of the form

12

Product-Type Signatures

Denition 2.12. A signature on a set of variables X is of product type if it can be expressed as
a product of unary functions, binary equality functions ([1, 0, 1]), and binary disequality functions
([0, 1, 0]), each on one or two variables of X . We use P to denote the set');
INSERT INTO posts (postId,userId,title,body) VALUES (26,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 12)',' of product-type functions.
Theorem 2.13. Let F be any set of complex-valued signatures in Boolean variables. If F  A or
P , then Holant( (cid:54)=2 | F ) is tractable.
Problems dened by A are tractable essentially by Gauss sums [2]. Problems dened by P are
tractable by a propagation algorithm.

Matchgate Signatures Matchgates were introduced by Valiant [23, 24] to give polynomial-time
algorithms for a collection of counting problems over planar graphs. As the name suggests, problems
expressible by matchgates can be reduced to computing a weighted sum of perfect matchings.
The latter problem is tractable over planar graphs by Kasteleyns algorithm [16], a.k.a. the FKT
algorithm [21, 15]. These counting problems are naturally expressed in the Holant framework using
matchgate signatures. We use M to denote the set of all matchgate signatures; thus Pl-Holant(M )
is tractable.
The parity of a signature is even (resp. odd) if its support is on entries of even (resp. odd)
Hamming weight. We say a signature satises the even (resp. odd) Parity Condition if all entries
of odd (resp. even) weight are zero. For signatures of arity no more than 4, the matchgate signatures
are characterized by the following lemma.
Lemma 2.14. (cf. Lemma 2.3, Lemma 2.4 in [3]) If f has arity (cid:54) 3, then f  M i f satises
the Parity Condition.
 ,
f0000
If f has arity 4 and f satises even Parity Condition, i.e.,0
f1100
0
f0110 f0101
f1010 f10010

Mx1 x2 ,x4 x3 (f ) =

f00110
f1111

then f  M i
det MOut (f ) = f0000f1111  f1100f0011 = f0110f1001  f1010f0101 = det MIn (f )

Holographic transformations extend the reach of the FKT algorithm even further, as stated
below.
Theorem 2.15. Let F be any set of complex-valued signatures in Boolean variables.
If F is
(cid:3). Follows by the denition directly,
(cid:2) 1 1
M -transformable, then Pl-Holant( (cid:54)=2 | F ) is tractable.
Recall the signature class (cid:99)M = H2M , where H2 = 1
1 1
(cid:21)
(cid:20) 0 0 0 aOne can check the following lemmas:
f  (cid:99)M .
0 b c 0
Lemma 2.16. A signature f with the signature matrix M (f ) =
0 z y 0
x 0 0 0

is M -transformable i

13

g10 g11 ] is in (cid:99)M i g00 = g11 and
Lemma 2.17. A signature g with the signature matrix M (g) = [ g00 g01
(cid:20) 0 0 0 0
(cid:21)
g01 = g10 , where  = 1.
is in (cid:99)M i b = y and
0 b c 0
Lemma 2.18. A signature f with the signature matrix M (f ) =
0 z y 0
(cid:20) 0 0 0 a
(cid:21)
c = z , where  = 1.
0 0 0 0
, where abxy (cid:54)= 0, then f / (cid:99)M .
0 b 0 0
0 0 y 0
x 0 0 0

Lemma 2.19. If f has the signature matrix M (f ) =

2.6 Known Dichotomies and Hardness Results
Denition 2.20. A 4-ary signature is non-singular redundant i in one of its four 4  4 signature
f0000 f0010 f0011
 (cid:54)= 0.
matrices, the midd le two rows are identical and the midd le two columns are identical, and the
determinant
f0100 f0110 f0111
f1100 f1110 f1111
Theorem 2.21. [6] If f is a non-singular redundant signature, then Pl-Holant( (cid:54)=2 |f ) is #P-hard.
Theorem 2.22. [17] Let G be a connected planar graph and E O(H ) be the set of al l Eulerian
(cid:88)
Orientation of the medial graph H = H (G). Then
OE O(H )

2 (O) = 2T (G; 3, 3),

det

where  (O) is the number of sadd le vertices in orientation O, i.e., vertices in which the edges are
Remark: Note that (cid:80)
oriented in, out, in, out in cyclic order.
(cid:21)
(cid:20) 0 0 0 1
OE O(H ) 2 (O) can be expressed as Pl-Holant( (cid:54)=2 | f ), where f has the
. Therefore, we have Pl-Holant( (cid:54)=2 | f ) is #P-hard.
0 1 2 0
signature matrix M (f ) =
0 2 1 0
1 0 0 0
Pl-#CSP(F ) is #P-hard unless F  A , F  P , or F  (');
INSERT INTO posts (postId,userId,title,body) VALUES (27,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 13)','cid:99)M , in which case the problem is
Theorem 2.23. [4] Let F be any set of complex-valued signatures in Boolean variables. Then
computable in polynomial time. If F  A or F  P , then #CSP(F ) is computable in polynomial
(cid:21)
(cid:20) 0 0 0 a
time without planarity; otherwise #CSP(F ) is #P-hard.
0 b c 0
0 z y 0
x 0 0 0

Theorem 2.24. [5] Let f be a 4-ary signature with the signature matrix M (f ) =
Holant ((cid:54)=2 | f ) is #P-hard except for the fol lowing cases:
 f  P ;
 f  A ;
 there is a zero in each pair (a, x), (b, y), (c, z );
in which cases Holant ((cid:54)=2 | f ) is computable in polynomial time.

, then

14

3 Main Theorem and Proof Outline

(cid:20) 0 0 0 a
(cid:21)
, where a, b, c, x, y , z 
0 b c 0
Theorem 3.1. Let f be a signature with the signature matrix M (f ) =
0 z y 0
C. Then Pl-Holant( (cid:54)=2 | f ) is polynomial time computable in the fol lowing cases, and #P-hard oth-
x 0 0 0
erwise:
1. f  P or A ;
3. f  M or (cid:99)M ;
2. There is a zero in each pair (a, x), (b, y), (c, z );
4. c = z = 0 and
(i). (ax)2 = (by)2 , or


, where ,  ,   N, and    (mod 2);


(ii). x = ai , b = ai
, and y = a
If f satises condition 1 or 2, then Holant( (cid:54)=2 | f ) is computable in polynomial time without the
planarity restriction; otherwise (the non-planar) Holant( (cid:54)=2 | f ) is #P-hard.
Let N be the number of zeros in {a, b, c, x, y , z}. If N (cid:62) 3, then either there is a zero pair, or
we have N = 3 and each pair has exactly one zero. We dene Case I as N = 3 with each pair
having exactly one zero. We dene Case II as having a zero pair, which also includes some cases of
N = 2. For the remaining cases of N = 2 the two zeros are in dierent pairs. In particular there
is an outer pair that has a single zero. We dene Case III to be N = 2 and having no zero pair,
or N = 1 and the zero is in an outer pair. In Case III an outer pair has exactly one zero, and the
other two pairs together have at most one zero. Then we dene Case IV as having N = 1 and the
zero is in an inner pair, or N = 0.
Case I is tractable, even for non-planar Holant((cid:54)=2 | f ) (see [5]).
In Case II, depending on whether the zero pair is inner or outer we have two dierent connections
to #CSP. A previously established connection to #CSP [5] can be adapted in the planar setting
to the case with a zero outer pair. This connection is a local transformation, and we observe that
it preserves planarity. A signicantly more involved non-local connection to #CSP is discovered in
this paper when the inner pair is zero (and no outer pair is zero). We show that by the support
structure of the signature we can dene a set of circuits, which forms a partition of the edge set.
There are exactly two valid congurations along such a circuit, corresponding to its two cyclic
orientations. These circuits may intersect in complicated ways, including self-intersections. But
we can dene a #CSP problem, where the variables are these circuits, and their edge functions
exactly account for the intersections. We show that Pl-Holant( (cid:54)=2 | f ) is');
INSERT INTO posts (postId,userId,title,body) VALUES (28,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 14)',' equivalent to these #CSP
problems, which are non-planar in general. However, crucially, because Pl-Holant( (cid:54)=2 | f ) is planar,
every two such circuits must intersect even times. Due to the planarity of Pl-Holant((cid:54)=2 | f ) we can
exactly carve out a new class of tractable problems via this non-local #CSP connection.
For #P-hardness proofs in this paper, one particularly dicult case is in Lemma 6.4. In this
case, all constructable binary signatures correspond to points on the unit circle S 1 , and any iteration
of the construction amounts to mapping this point by a Mobius transformation which preserves S 1 .
The 4 Cases are dened formally as follows:
I. There is exactly one zero in each pair. In this case, Holant ((cid:54)=2 | f ) is tractable, proved in [5].
II. There is a zero pair:
if f  P , A , M or (cid:99)M , and is #P-hard otherwise.
1. An outer pair (a, x) or (b, y) is a zero pair. We prove that Pl-Holant((cid:54)=2 | f ) is tractable
In this Case II.1, we can rotate the signature f such that the matrix MOut (f ) is the

15

III.

IV.

zero matrix. We reduce Pl-#CSP(MIn (f )) to Pl-Holant ( (cid:54)=2 | f ) via a local replacement
(Lemma 4.2). We apply the dichotomy of Pl-#CSP to get #P-hardness (Theorem 4.3).
Tractability of Pl-Holant ( (cid:54)=2 | f ) follows from known tractable signatures.
2. The inner pair (c, z ) is a zero pair and no outer pair is a zero pair. We prove that
Pl-Holant( (cid:54)=2 | f ) is #P-hard unless f satises condition 4, in which it is tractable.
This is the non-local reduction described above. The tractable condition 4 is previously
unknown. (Curiously, in Case II.2, condition 4 subsumes f  M .)
1. There are exactly two zeros and they are in dierent pairs;
2. There is exactly one zero and it is in an outer pair.
We prove that Pl-Holant( (cid:54)=2 | f ) is #P-hard unless f  M , in which case it is tractable.
(cid:20) 0 0 0 0
(cid:21)
In this case, there is a single zero in an outer pair. By connecting two copies of the signature
f , we can construct a 4-ary signature f1 such that one outer pair is a zero pair. When f / M ,
0 0 1 0
we can realize a signature M (g) =
by interpolation using f1 (Lemma 5.1). This g
0 1 0 0
0 0 0 0
can help us extract MIn (f ). By connecting f and g , we can construct a signature that
belongs to Case II. We then prove #P-hardness using the result of Case II (Theorem 5.2).
1. There is exactly one zero and it is in the inner pair;
2. All values in {a, x, b, y , c, z} are nonzero.
We prove that Pl-Holant( (cid:54)=2 | f ) is #P-hard unless f  M , in which it is tractable.
Assume f (cid:54) M . The main idea is to use Mobius transformations. However, there are some
settings where we cannot do so, either because we dont have the initial signature to start the
process, or the matrix that would dene the Mobius transformation is singular. So we rst
treat the following two special cases.
 If a = x, b = y and c = z , where  = 1, by interpolation based on a lattice structure,
 If det (cid:2) b c
(cid:3) = 0 or det [ a z
either we can realize a non-singular redundant signature or reduce from the evaluation
of the Tutte polynomial at (3, 3), both of which are #P-hard (Lemma 6.1).
c x ] = 0, then either we can realize a non-singular redundant
z y
signature or a signature that is #P-hard by Lemma 6.1 (Lemma 6.2).
If f does not belong to the above two cases, we want to realize binary signatures of the form
(0, 1, t, 0)T , for arbitrary values of t. If this can be done, by carefully choosing the values of t,
we can construct a signature that belongs to Case III and it is #P-hard when f / M (Lemma
6.3). We realize binary signatures by connecting f with ( (cid:54)=2 ). This corresponds naturally to
a Mobius transformation. By discussing the following dierent forms of binary signatures we
get, we can ');
INSERT INTO posts (postId,userId,title,body) VALUES (29,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 15)','either realize arbitrary (0, 1, t, 0)T or a signature belonging to Case II.2 that does
not satisfy condition 4, therefore is #P-hard (Theorem 6.8).
 If we can get a signature of the form g = (0, 1, t, 0)T where t (cid:54)= 0 is not a root of unity,
then by connecting a chain of g , we can get polynomially many distinct binary signatures
gi = (0, 1, ti , 0)T . Then, by interpolation, we can realize arbitrary binary signatures of
the form (0, 1, t(cid:48) , 0)T .
 Suppose we can get a signature of the form (0, 1, t, 0)T , where t (cid:54)= 0 is an n-th primitive
But we can relate f to two Mobius transformations due to det (cid:2) b c
(cid:3) (cid:54)= 0 and det [ a z
root of unity (n (cid:62) 5). Now, we only have n many distinct signatures gi = (0, 1, ti , 0)T .
c x ] (cid:54)= 0.
z y
For each Mobius transformation , we can realize the signatures g = (0, 1, (ti ), 0)T . If
on the extended complex plane (cid:98)C, it can map at most two points of S 1 to 0 or . Hence,
|(ti )| (cid:54)= 0, 1 or  for some i, then this is treated above. Otherwise, since  is a bijection

16

|(ti )| = 1 for at least three ti . But a Mobius transformation is determined by any three
distinct points. This implies that  maps S 1 to itself. Such  have a known special
. By exploiting its property we can construct a signature f (cid:48) such that
form ei z + 
1 + z
its corresponding Mobius transformation (cid:48) denes an innite group. This implies that
(cid:48)k (t) are all distinct. Then, we can get polynomially many distinct binary signatures
(0, 1, (cid:48)k (t), 0), and realize arbitrary binary signatures of the form (0, 1, t(cid:48) , 0)T (Lemma
6.4).
 Suppose we can get a signature of the form (0, 1, t, 0)T where t (cid:54)= 0 is an n-th primitive
root of unity (n = 3, 4). Then we can either relate it to two Mobius transformations
mapping the unit circle to itself, or realize a double pinning (0, 1, 0, 0)T = (1, 0)T  (0, 1)T
(Corollary 6.5).
 Suppose we can get a signature of the form (0, 1, 0, 0)T . By connecting f with it, we can
get new signatures of the form (0, 1, t, 0)T . Similarly, by analyzing the value of t, we can
either realize arbitrary binary signatures of the form (0, 1, s, 0)T , or realize a signature
that belongs to Case II.2, which is #P-hard (Lemma 6.6).
 Suppose we can only get signatures of the form (0, 1, 1, 0). That implies a = x, b = y
and c = z , where  = 1. This has been treated before.
These 4 Cases above cover all possibilities. If N  3, then there is a zero pair or there is exactly
one zero in each pair. This is either in Case I or Case II. If N = 2, it is either in Case II or Case
III.1. If N = 1, it is either in Case III.2 or Case IV.1. If N = 0, it is in Case IV.2.

4 Case II: One Zero Pair

If an outer pair is a zero pair, by rotational symmetry, we may assume (a, x) is a zero pair.
0 0 0 0
 ,
Denition 4.1. Given a 4-ary signature f with the signature matrix
0 b
c 0
M (f ) =
(4.1)
0 z y 0
let fIn denote the binary signature of the form M (fIn ) = MIn (f ) = (cid:2) b c
(cid:3) . Given a set F consisting
0 0 0 0
z y
of signatures of the form (4.1), let FIn denote the signature set {fIn | f  F }.
Lemma 4.2. Let F be a set consisting of signatures of the form (4.1). Then,
Pl-#CSP(FIn ) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .

Proof.[5] We prove this reduction in two steps. In each step, we begin with a signature grid
and end with a new signature grid such that the Holant values of both signature grids are the same.
For step one, let G = (U, V , E ) be a planar bipartite graph representing an instance of Pl-#CSP(FIn ),
where each u  U is a variable, and each v  V has degree two and is labeled by some fIn  FIn . We
dene a cyclic order of the edges incident to each vertex u  U , and decompose u into k = deg(u)
vertices. Then we connect the k edges originally incident to u to these k new vertices so that each
vertex is incident to exactly ');
INSERT INTO posts (postId,userId,title,body) VALUES (30,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 16)','one edge. We also connect these k new vertices in a cycle according to

17

u

u(cid:48)

u(cid:48)

(a)
(b)
(c)
Figure 7: The reduction from #Pl-CSP(fIn ) to Pl-Holant( (cid:54)=2 | f ). The circle vertices are
labeled by (=d ), where d is the degree of the corresponding vertex, the diamond vertices are
labeled by fIn , the triangle vertices are labeled by the corresponding f , and the square vertices
are labeled by ((cid:54)=2 ).

the cyclic order (see Figure 7b). Thus, in eect we have replaced u by a cycle of length k = deg(u).
(If k = 1 there is a self-loop.) Each of k vertices has degree 3, and we label them by (=3 ). Clearly
this preserves planarity and does not change the value of the partition function. The resulting
graph has the following properties: (1) every vertex has either degree 2 or degree 3; (2) each degree
2 vertex is connected to degree 3 vertices; (3) each degree 3 vertex is connected to exactly one
degree 2 vertex.

f

x1 x4
x2 x3
u(cid:48)

f

x1 x4
x2 x3
u(cid:48)

f

x1 x4
x2 x3
u(cid:48)

f

x1 x4
x2 x3
u(cid:48)

(a) fIn00f0110

(b) fIn01f0101

(c) fIn10f1010

(d) fIn11f1001

Figure 8: Assign input variables of f : Suppose the binary signature fIn is applied to (the
ordered pair) (u, u(cid:48) ). The variables u and u(cid:48) have been replaced by cycles of length deg(u) and
deg(u(cid:48) ) respectively. For the cycle Cu representing a variable u, we associate the value u = 0
with a clockwise orientation, and u = 1 with a counter-clockwise orientation. Then by the
support of f , (x1 , x4 ) can only take assignment (0, 1) or (1, 0), and similarly (x2 , x3 ) can only
take assignment (0, 1) or (1, 0).
Now step two. For every v  V , v has degree 2 and is labeled by some fIn . We contract the
two edges incident to v . The resulting graph G(cid:48) = (V (cid:48) , E (cid:48) ) is 4-regular and planar. We put a node
on every edge of G(cid:48) and labet it by ((cid:54)=2 ) (see Figure 7c). Next, we assign the corresponding f to
every v (cid:48)  V (cid:48) after this contraction. The input variables x1 , x2 , x3 , x4 are carefully assigned at each
copy of f (as illustrated in Figure 8) such that there are exactly two congurations to each original
cycle, which correspond to cyclic orientations, due to the ( (cid:54)=2 ) on it and the support set of f . These

18

cyclic orientations correspond to the {0, 1} assignments at the original variable u  U . Under this
one-to-one correspondence, the value of fIn is perfectly mirrored by the value of f . Therefore, we
have Pl-#CSP(FIn ) (cid:54)T Pl-Holant ((cid:54)=2 | F ) .
is #P-hard unless F  P , F  A , or F  (cid:99)M , in which cases the problem is tractable.
Theorem 4.3. Let F be a set consisting of signatures of the form (4.1). Then Pl-Holant( (cid:54)=2 | F )
Tractability follows by Theorems 2.13 and 2.15. For any f  F , notice that the
Proof.
support of f is on x1 (cid:54)=x2 and x3 (cid:54)=x4 , where  is the indicator function. We have
f (x1 , x2 , x3 , x4 ) = fIn (x1 , x4 )  x1 (cid:54)=x2  x3 (cid:54)=x4 .
Thus, FIn  P or A is equivalent to F  P or A . In addition, by Lemmas 2.17 and 2.18, FIn  (cid:99)M
is equivalent to F  (cid:99)M . Therefore, if F (cid:42) P , A or (cid:99)M , then FIn (ci');
INSERT INTO posts (postId,userId,title,body) VALUES (31,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 17)','d:42) P , A or (cid:99)M . By Theorem
2.23, Pl-#CSP(FIn ) is #P-hard, and then by Lemma 4.2, Pl-Holant ( (cid:54)=2 | F ) is #P-hard.
Remark: One may observe that if f  M , then Pl-Holant ( (cid:54)=2 | F ) is also tractable as F and (=2 )
are both realized by matchgates. However, Theorem 4.3 already accounted for this case because
for F consisting of signatures of the form (4.1), F  M implies F  P .
Now, we consider the case that the inner pair is a zero pair and no outer pair is a zero pair.
 ,
0 0 0 a
Denition 4.4. Given a 4-ary signature f with the signature matrix
0 b 0 0
0 0 y 0
x 0 0 0
(cid:20)ak1+(cid:96)1 yk2+(cid:96)2 xk3+(cid:96)3 bk4+(cid:96)4 ak2+(cid:96)4 yk3+(cid:96)1 xk4+(cid:96)2 bk1+(cid:96)3
(cid:21)
where (a, x) (cid:54)= (0, 0) and (b, y) (cid:54)= (0, 0), let Gf denote the set of al l binary signatures gf of the form
M (gf ) =
ak4+(cid:96)2 yk1+(cid:96)3 xk2+(cid:96)4 bk3+(cid:96)1 ak3+(cid:96)3 yk4+(cid:96)4 xk1+(cid:96)1 bk2+(cid:96)2
where k1 , k2 , k3 , k4 , (cid:96)1 , (cid:96)2 , (cid:96)3 , (cid:96)4  N, and k = k1 + k2 + k3 + k4 = (cid:96)1 + (cid:96)2 + (cid:96)3 + (cid:96)4 = (cid:96), and let Hf
M (hf ) = (cid:2)am1 ym2 xm3 bm4 am3 ym4 xm1 bm2 (cid:3) ,
denote the set of al l unary signatures hf of the form
where m1 , m2 , m3 , m4  N.
(cid:104) a2 by
(cid:105)
Let k = k1 = (cid:96)1 = (cid:96) = 1, we get a specic signature in Gf denoted by g1f , where M (g1f ) =
(cid:105)
(cid:104) ax b2
. Let k = k1 = (cid:96)3 = (cid:96) = 1, we get another specic signature in Gf denoted by g2f , where
by x2
M (g2f ) =y2 ax
Remark: For any i, j  {1, 2, 3, 4}, let k = ki = (cid:96)j = (cid:96) = 1, we can get 16 signatures in Gf that
have similar signature matrices to M (g1f ) and M (g2f ). In fact, Gf is the closure of the Hadamard
product of these 16 basic signature matrices.

M (f ) =

(4.2)

19

(4.3)

(4.4)

Lemma 4.5. Let f be a signature of the form (4.2). Then,
If a2 = x2 (cid:54)= 0, b2 = y2 (cid:54)= 0 and (cid:0) b
(cid:1)8 (cid:54)= 1, then
Pl-Holant( (cid:54)=2 | f ) (cid:54)T #CSP(Gf  Hf ),#CSP(g1f , g2f ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).
Proof. We divide the proof into two parts: We show the reduction (4.3) in Part I, and the
reduction (4.4) in Part II.
Part I: Since we are considering planar graph, we can view it in a plane. Given a vertex of arity
4, list the four edges incident to it in counterclockwise order. We say two edges are not adjacent if
there is exactly one other edge between them. Given an instance  = (G, ) of Pl-Holant( (cid:54)=2 | f ),
two edges in G are called 2-ary edge twins if they are incident to a vertex of degree 2, and 4-ary
edge twins if they are incident to a vertex of degree 4 but they are not adjacent. Both 2-ary edge
twins and 4-ary edge twins are called edge twins.
Each edge has an unique 2-ary edge twin at its endpoint of degree 2 and an unique 4-ary edge
twin at its endpoint of degree 4. That is, edge twins induce a binary relation and the transitive
closure of this relation on an edge forms a circuit. Therefore, graph G can be divided into some
circuits C1 , C2 ,    , Ck . Note that Ci may include repeated vertices called self-intersection vertices,
but no repeated edges. We pick an edge ei of Ci to be the leader edge of Ci . Given the leader edge
ei , the direction from its endpoint of degree 2 to its endpoint of degree 4 gives an orientation of
the circuit Ci . In edge twins, depending on the orientation, we can say one edge is the successor
of the other edge. When we list the assignment of edges in a circuit, we start with its leader edge
and follow with the successor of the leader edge, and so forth.
(cid:89');
INSERT INTO posts (postId,userId,title,body) VALUES (32,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 18)',')
(cid:88)
For any nonzero term in the sum
vV
 :E (cid:55){0,1}
the assignment of all edges  : E (cid:55) {0, 1} can be uniquely extended by the assignment of all leader
edges  (cid:48) : {e1 , e2 ,    ek } (cid:55) {0, 1}. This is because at each vertex v , fv ( |E(v) ) (cid:54)= 0 if and only
if each pair of edge twins in E(v) is assigned value (0, 1) or (1, 0). More specically, all edges in
Ci take assignment (0, 1, 0, 1,    , 0, 1) when ei = 0 and (1, 0, 1, 0,    , 1, 0) when ei = 1. In other
(cid:88)
(cid:89)
words, all pairs of 4-ary edge twins in Ci take assignment (0, 1) when ei = 0 and (1, 0) when ei = 1.
Then, we have
fv ( (cid:48) |E(v) ).
vV
 (cid:48) :{e1 , ,ek }(cid:55){0,1}
Let Vi,j = Ci  Cj (i < j ) be the set of all intersection vertices in Ci and Cj . Since G is a planar
graph, |Vi,j |  0 ( mod 2). Let  (cid:48)
(ei ,ej ) denote the restriction of  (cid:48) on edges ei and ej . Dene binary
(cid:89)
(ei ,ej ) : {ei , ej } (cid:55) {0, 1}, we have
function gi,j on ei and ej : Given an assignment  (cid:48)
(ei ,ej ) |E(v) ).
fv ( (cid:48)
vVi,j

fv ( |E(v) ),

Pl-Holant =

Pl-Holant =

gi,j (ei , ej ) =

Since all edges incident to vertices in Vi,j are either in Ci or Cj , the assignment of these edges can
(ei ,ej ) . Hence, gi,j is well-dened. We show gi,j  Gf by induction
be extended by the assignment  (cid:48)
on the number n of self-intersection vertices in Ci .

20

 First, n = 0. That is, Ci is a simple cycle without self-intersection. By Jordan Curve
Theorem, Ci divides the plane into an interior region and an exterior region. According to
the orientation of Ci , we denote the left side of Ci to be the interior region and the right side
to be the exterior region. At a half of intersection vertices in Vi,j , Cj enters the interior of Ci ,
and at the other half of intersection vertices, Cj exits. We call those vertices entry-vertices
and exit-vertices separately (See Figure 9).

Figure 9: Intersection vertices between Ci and Cj

For each vertex in Vi,j , consider the two pairs of edge twins incident to it. We label the
edge twins in Ci by variables (x1 , x3 ) obeying the orientation of Ci . That is, x3 is always the
successor of x1 . Hence, all variables (x1 , x3 ) take the same assignment (0, 1) when ei = 0 and
(1, 0) when ei = 1. Then, the labeling (x2 , x4 ) of edge twins in Cj is uniquely determined by
the counterclockwise order of (x1 , x2 , x3 , x4 ). Moreover, at any vertex in Vi,j , the variable x2
is always in the exterior of Ci (See Figure 9), which means at entry-vertices, x4 is the successor
of x2 , while at exit-vertices, x2 is the successor of x4 . Therefore, at entry-vertices, variables
(x2 , x4 ) take assignment (0, 1) when ej = 0 and (1, 0) when ej = 1, while at exit-vertices,
they take assignment (1, 0) and (0, 1) correspondingly.

(ei , ej )

(0, 0)

(0, 1)

(1, 1)

(1, 0)

entry-vertices

(x1 , x2 , x3 , x4 )f(0, 0, 1, 1)

y

(0, 1, 1, 0)

(1, 1, 0, 0)

(1, 0, 0, 1)

x

a

x

f 
y

b
2

b

y

exit-vertices

(x1 , x2 , x3 , x4 )f(0, 1, 1, 0)

b

(0, 0, 1, 1)

(1, 0, 0, 1)

(1, 1, 0, 0)

y

y

b

f 
x

a
2

x

a

Table 1: The values of f at intersection vertices

According to the dierent assignments of (ei , ej ) as listed in Column 1 of Table 1, Column
2 and Column 7 (indexed by (x1 , x2 , x3 , x4 )) list the assignments of (x1 , x2 , x3 , x4 ) at entry-
(cid:21)
(cid:21)
(cid:20) 0 0 0 b
(cid:20) 0 0 0 x
(cid:20) 0 0 0 y
(cid:20) 0 0 0 a
(cid:21)
(cid:21)
vertices and exit-vertices separately. Note that under this labeling, signature f has four
rotation forms:
0 y 0 0
0 x 0 0
0 b 0 0
0 a 0 0
0 0 a 0
0 0 y 0
0 0 x 0
0 0 b 0
y 0 0 0
a 0 0 0
b 0 0 0
x 0 0 0

, M (f  ) =

and M (f

M (f ) =
2 ) =

, M (f


2 ) =

21

Columns 3, 4, 5, 6 and Columns 8, 9, 1');
INSERT INTO posts (postId,userId,title,body) VALUES (33,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 19)','0, 11 list the corresponding values of signature f in
2 , f  and f
four forms f , f
2 separately.

Suppose there are k1 many entry-vertices assigned f , k2 many entry-vertices assigned f
2 , k3many entry-vertices assigned f  , and k4 many entry-vertices assigned f
2 , while there are

(cid:96)4 many exit-vertices assigned f , (cid:96)1 many exit-vertices assigned f
2 , (cid:96)2 many exit-verticesassigned f  , and (cid:96)3 many exit-vertices assigned f
2 . Then, according to the assignments of
(ei , ej ), the values of gi,j are listed in Table 2 :

(0, 1)

(1, 1)

(1, 0)
2 )(cid:96)3 f (cid:96)4

(ei , ej )
(0, 0)


gi,j (ei , ej ) = f k1 (f
2 )(cid:96)1 (f  )(cid:96)2 (f
2 )k4 (f
2 )k2 (f  )k3 (f
ak1 yk2 xk3 bk4 a(cid:96)1 y (cid:96)2 x(cid:96)3 b(cid:96)4
bk1 ak2 yk3 xk4 y (cid:96)1 x(cid:96)2 b(cid:96)3 a(cid:96)4
xk1 bk2 ak3 yk4 x(cid:96)1 b(cid:96)2 a(cid:96)3 y (cid:96)4
yk1 xk2 bk3 ak4 b(cid:96)1 a(cid:96)2 y (cid:96)3 x(cid:96)4
Table 2: The values of gi,j
(cid:20)ak1+(cid:96)1 yk2+(cid:96)2 xk3+(cid:96)3 bk4+(cid:96)4 ak2+(cid:96)4 yk3+(cid:96)1 xk4+(cid:96)2 bk1+(cid:96)3
ak4+(cid:96)2 yk1+(cid:96)3 xk2+(cid:96)4 bk3+(cid:96)1 ak3+(cid:96)3 yk4+(cid:96)4 xk1+(cid:96)1 bk2+(cid:96)2
Since the number of entry-vertices is equal to the number of exit-vertices, k1 + k2 + k3 + k4 =
(cid:96)1 + (cid:96)2 + (cid:96)3 + (cid:96)4 . Hence, we have gi,j  Gf .
 Then, suppose gi,j  Gf holds for any circuit Ci with at most n many self-intersection vertices.
Ci can be decomposed into two edge-disjoint circuits, each of which has at most n many self-
intersection vertices (See Figure 10).

M (gi,j ) =

That is,

(cid:21)

Figure 10: Decompose Ci into C 1
i and C 2
i .
i  C 2
Now, we have Ci = C 1
i . Each C s
i (s = 1, 2) is a circuit with at most n many self-
intersection vertices. The orientation of Ci induces the orientations of C 1
i and C 2
i . Clearly,
the assignment of edges in C s
i can be uniquely extended by the assignment of ei . We can still
i  Cj be the set of all intersection
i,j = C s
i . Let V s
i and C 2
view ei as the leader edge of both C 1
i,j  V 2
i,j  V 2
i,j = . Same as the denition of
vertices in C s
i,j , where V 1
i and Cj . Then Vi,j = V 1
(cid:89)
gi,j , we dene binary function:
vV s
i,j
i,j is well-dened and by induction hypothesis, it is in Gf . Also,
As we have showed above, g (s)

(ei ,ej ) |E(v) ).
fv ( (cid:48)

g (s)
i,j (ei , ej ) =

22

gi,j =

we have

(ei ,ej ) |E(v) ) =
fv ( (cid:48)

(cid:89)
(cid:89)
(cid:89)
vVi,j
vV 2
vV 1
i,j
i,j
Note that Gf is closed under function multiplication. That is, gi,j  Gf .
Let Vi be the set of all self-intersection vertices in Ci . Let  (cid:48)
(ei ) denote the restriction of  (cid:48) on
(ei ) : ei (cid:55) {0, 1}, we have
ei . Dene unary function hi on ei : Given an assignment  (cid:48)
(cid:89)
(ei ) |E(v) ).
fv ( (cid:48)
vVi

(ei ,ej ) |E(v) ) = g (1)
fv ( (cid:48)
i,j g (2)
i,j .

hi (ei ) =

The assignment of those edges incident to vertices in Vi can be uniquely extended by the assignment
(ei ) . Hence, hi is well-dened. We ');
INSERT INTO posts (postId,userId,title,body) VALUES (34,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 20)','show hi  Hf .
 (cid:48)
For each vertex in Vi , since it is a self-intersection vertex, the two pairs of edge twins incident
to it are both in Ci . We still label each pair of edge twins by a pair of variables (x1 , x3 ) obeying the
orientation of Ci . That is, x3 is always the successor of x1 . Now, at each vertex in Vi , the four edges
incident to it are labeled by (x1 , x1 , x3 , x3 ) listed in counterclockwise order. We pick the proper
pair of variables (x1 , x3 ) and change it to (x2 , x4 ) such that the label of four edges is (x1 , x2 , x3 , x4 )
in counterclockwise order. Clearly, (x2 , x4 ) and (x1 , x3 ) take the same assignment. That is, at each
vertex in Vi , the assignment of (x1 , x2 , x3 , x4 ) is (0, 0, 1, 1) when ei = 0, and (1, 1, 0, 0) when ei = 1.
Under this labeling, signature f still has four rotation forms. The values of signature f in four
forms are listed in Table 3.

ei
1

(x1 , x2 , x3 , x4 )
(0, 0, 1, 1)

(1, 1, 0, 0)

a


f

b

f 
a
2

b

Table 3: The values of f at self-intersection vertices

Suppose there are m1 many vertices assigned f , m2 many vertices assigned fassigned f  and m4 many vertices assigned f
2 . Then, we have


2 , m3 many vertices

Pl-Holant =

M (hi ) = [am1 ym2 xm3 bm4 am3 ym4 xm1 bm2 ].
That is, hi  Hf .
(cid:32) (cid:89)
(cid:33)(cid:32) (cid:89)
For any vertex v  V , it is either in some Vi,j or some Vi . Thus,
(cid:88)
fv ( (cid:48) |E(v) )
(cid:33)(cid:32) (cid:89)
(cid:32) (cid:89)
vVi
vVi,j
 (cid:48) :{e1 , ,ek }(cid:55){0,1}
(cid:88)
1(cid:54)i(cid:54)k
1(cid:54)i<j(cid:54)k
 (cid:48) :{e1 , ,ek }(cid:55){0,1}
1(cid:54)i(cid:54)k
1(cid:54)i<j(cid:54)k
where gi,j  Gf and hi  Hf . Therefore, Pl-Holant((cid:54)=2 | f ) (cid:54)T #CSP(Gf  Hf ).
Here, we give an example for the reduction (4.3).

gi,j (ei , ej )

hi (ei )

fv ( (cid:48) |E(v) )
(cid:33)
(cid:33)

23

Figure 11: An example for the reduction (4.3)

Example. Given an instance  = (G, ) of Pl-Holant ( (cid:54)=2 | f ), there are two circuits C1 (The
Square) and C2 (The Horizontal Eight) in G (See Figure 11). Each circuit Ci has a leader
edge ei . Given the leader, the direction from its endpoint of degree 2 to the endpoint of degree
4 gives a default orientation of the circuit. Given a nonzero term in the sum Pl-Holant , as a
consequence of the support of f , the assignment of edges in each circuit is uniquely determined by
the assignment of its leader. That is, any assignment of the leaders  (cid:48) : {e1 , e2} (cid:55) {0, 1} can be
uniquely extended to an assignment of all edges  : E (cid:55) {0, 1} such that on each circuit, the {0, 1}
assignments alternate.
Consider the signatures f on the intersection vertices between C1 and C2 , (fv1 , fv2 , fv3 and fv4 ).
Due to planarity, there are an even number of intersection vertices. Assume C1 does not have self-
intersection (as is The Square). Otherwise, we will decompose C1 further and reason inductively.
Without self-intersection, C1 has an interior and exterior region (Jordan Curve Theorem) depending
on its default orientation. With respect to C1 , The circuit C2 enters and exits the interior of C1
alternately. Thus, we can divide the intersection vertices into an equal number of entry-vertices
and exit-vertices. In this example, fv1 and fv4 are on entry-vertices, while fv2 and fv3 are on
exit-vertices. By analyzing the values of each f when e1 and e2 take assignment 0 or 1, we can
view each f as a binary constraint on C1 and C2 . Depending on the 4 dierent rotation forms
of f and whether f is on entry-vertices or exit-vertices, the resulting binary constraint has
8 dierent forms (See Table 1). By multiplying these constraints, we get the binary constraint
g1,2 . This can be viewed as a binary edge');
INSERT INTO posts (postId,userId,title,body) VALUES (35,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 21)',' function on the circuits C1 and C2 . The property of
g1,2 crucially depends on there are an even number of intersection vertices. Given a particular
(e1 ,e2 ) : {e1 , e2} (cid:55) {0, 1}2 , we have
assignment  (cid:48)
(cid:89)
1(cid:54)i(cid:54)4

(e1 ,e2 ) |E(v) ).
fvi ( (cid:48)

g1,2 (e1 , e2 ) =


If the placement of fv1 were to be rotated clockwise 
v1 in the above
2 , then fv1 will be changed to f
formula, where Mx1 x2 ,x4 x3 (f
v1 ) = Mx2 x3 ,x1 x4 (fv1 ).For the self-intersection vertex fv5 , the notions of entry-vertex and exit-vertex do not apply.
fv5 gives rise to an unary constraint h2 on e2 . Depending on the 4 dierent rotation forms of f , h2
(e2 ) : e2 (cid:55) {0, 1},
has 4 dierent forms (see Table 3 in full version). Given a particular assignment  (cid:48)
we have
(e2 ) |E(v) ).
h2 (e2 ) = fv5 ( (cid:48)

24

Therefore, we have

Pl-Holant =

=

(cid:89)
vV

(cid:88)
(cid:88)
 :E (cid:55){0,1}
(cid:88)
 (cid:48) :{e1 ,e2 }(cid:55){0,1}
 (cid:48) :{e1 ,e2 }(cid:55){0,1}

fv ( |E(v) )
(cid:32) (cid:89)
1(cid:54)i(cid:54)4

(cid:33)
fv5 ( (cid:48) |E(v) )

fvi ( (cid:48) |E(v) )

g1,2 (e1 , e2 )h2 (e2 ).

Part II: Given an instance I of #CSP(g1f , g2f ). Consider binary constraints on variables xi
and xj (i < j ). Note that g1f is symmetric, that is, g1f (xi , xj ) = g1f (xj , xi ). We assume there
are si,j many constraints g1f (xi , xj ), ti,j many constraints g2f (xi , xj ) and t(cid:48)
i,j many constraints
g2f (xj , xi ). These are all constraints between xi and xj . Let gi,j (xi , xj ) be the function product of
these constraints. That is,

t(cid:48)
i,j
2f

(xj , xi ).

and 2 =

#CSP(I ) =

Then, we have

. i is constructed as

gi,j (xi , xj ) = gsi,j
1f

(xi , xj )g ti,j
(xi , xj )g
(cid:89)
(cid:88)
2f
gi,j (xi , xj ).
1(cid:54)i<j(cid:54)n
 (cid:48) :{x1 , ,xk }(cid:55){0,1}
(cid:21)
(cid:20) 0 0 0 1
(cid:20) 0 0 0 1
(cid:21)
We prove the reduction (4.4) in two steps. We rst reduce #CSP(I ) to an instance i (i = 1, 2)
of Pl-Holant ( (cid:54)=2 | f , i ) repectively, where 1 =
0 1 0 0
0 1 0 0
0 0 1 0
0 0 1 0
1 0 0 0
1 0 0 0
follows:
1. In a plane, draw cycle D1 . Then draw cycle D2 , and let D2 intersect with D1 at least
2(s1,2 + t1,2 + t(cid:48)
1,2 ) many times. This can be done since we can let D2 enter and exit the interior
of D1 alternately and there will be an even number of intersection vertices. Successively draw
cycles Dj until Dk , and let Dj intersect with Di at least 2(si,j + ti,j + t(cid:48)
i,j ) many times
for 1 (cid:54) i < j (cid:54) k . This can be done since we can let Dj enter and exit the interiors of
D1 ,    , Dj1 successively. Note that, when letting Dj intersect with other cycles, it may
intersect with Di again. This is why we say Dj intersects with Di at least 2(si,j + ti,j + t(cid:48)
i,j )
many times. We will deal with those extra intersection vertices later. Moreover, when drawing
these cycles, we can easily make them satisfy the following conditions:
a. There is no self-intersection.
b. There is no more than two cycles that intersect with others at the same vertex. That is,
each intersection vertex is of degree 4.
2. Replace each edge by a path of length two. We nally get a planar bipartite graph G = (V , E ).
On one side, all vertices are of degree 2, and on the other side, all vertices are of degree 4.
We can still dene edge twins as dened in Part I. Moreover, we still divide the graph into
some circuits C1 ,    , Ck . In fact, Ci is just the cycle Di after the replacement of each edge
by a path of length two.
Let Vi,j = Ci  Cj (i < j');
INSERT INTO posts (postId,userId,title,body) VALUES (36,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 22)',' ) be the intersection vertices in Ci and Cj . Clearly, |Vi,j | is even and
no less than 2(si,j + ti,j + t(cid:48)
i,j ). Since there is no self-intersection, each circuit is a simple cycle.
We can dene entry-vertices and exit-vertices as in Part I. Among Vi,j , a half of them
are entry-vertices and the other half are exit-vertices. As we did in Part I, we pick an edge ei

25

as the leader edge of Ci and this gives an orientation of Ci . List the edges in Ci according to
the orientation of Ci . Then, all edges in Ci take assignment (0, 1, 0, 1,    , 0, 1) when ei = 0
and (1, 0, 1, 0,    , 1, 0) when ei = 1.
3. Label the vertex of degree 2 by ( (cid:54)=2 ). For any vertex in Vi,j , as we showed in Part I, we
can label the four edges incident to it by variables (x1 , x2 , x3 , x4 ) in a way such that when
(ei , ej ) = (s, t), we have (x1 , x2 , x3 , x4 ) = (s, t, 1s, 1t) at entry-vertex, and (x1 , x2 , x3 , x4 ) =
(s, 1  t, 1  s, t) at exit-vertex (See Table 1). Note that f has four rotation forms under this

labeling. Label si,j many entry-vertices by f and si,j many exit-vertices by f
2 , ti,j many
2 , and t(cid:48)i,j many entry-vertices by f 
entry-vertices by f  and ti,j many exit-vertices by f
2 . Let V (cid:48)
i,j be the set of these 2(si,j + ti,j + t(cid:48)

and si,j many exit-vertices by f
i,j ) vertices. Refer
to Table 2, we have(cid:89)
(ei ,ej ) |E(v) ) = gsi,j
fv ( (cid:48)
1f
vV (cid:48)
ij

t(cid:48)
i,j
(ei , ej )g
2f

(ei , ej )g ti,j
2f

(ej , ei ) = gi,j (ei , ej ).
(cid:20) 0 0 0 1
(cid:21)
0 1 0 0
0 0 1 0
1 0 0 0

, then refer to

For any vertex in Vi,j \V (cid:48)
i,j , if label it by an auxiliary signature 1 =
Table 2 (Here a = x = b = y = 1), we have(cid:89)
(ei ,ej ) |E(v) )  1.
1 ( (cid:48)
vVi,j \V (cid:48)
ij

(cid:20) 0 0 0 1
(cid:21)
We can also label the vertex in Vi,j \V (cid:48)
0 1 0 0
i,j by auxiliary signature 2 =
. Note that in
(cid:21)
(cid:20) 0 0 0 1
0 0 1 0
1 0 0 0
Vi,j \V (cid:48)
i,j , the number of entry-vertices are equal to the number of exit-vertices. We label all

0 1 0 0
0 0 1 0
2 =
entry-vertices by 2 and label all exit-vertices by its rotation form (cid:89)
Table 2 (Here a = b = y = 1, x = 1, and k = k1 = (cid:96)1 = (cid:96)), we have
1 0 0 0
(ei ,ej ) |E(v) )  1.
2 ( (cid:48)
vVi,j \V (cid:48)
ij
Now, we get an instance i (i = 1, 2) for each problem Pl-Holant ((cid:54)=2 | f , i ) respectively. Note
that i has the same support as f . As we have showed in Part I, for any nonzero term in the sum
Pl-Holanti , the assignment of all edges  : E (cid:55) {0, 1} can be uniquely extended by the assignment
(cid:88)
(cid:89)
of all leader edges  (cid:48) : {e1 , e2 ,    , ek } (cid:55) {0, 1}. Therefore, we have
(cid:33)(cid:32) (cid:89)
(cid:32) (cid:89)
(cid:88)
1(cid:54)i<j(cid:54)n
 (cid:48) :{e1 , ,ek }(cid:55){0,1}
fv ( (cid:48) |E(v) )
 (cid:48) :{e1 , ,ek }(cid:55){0,1}
vV (cid:48)
vVi,j \V (cid:48)
i,j
i,j
1(cid:54)i<j(cid:54)n
1(cid:54)i<j(cid:54)n

i v ( (cid:48) |E(v) )

#CSP(I ) =

gi,j (ei , ej )

. Refer to

(cid:33)

= Pl-Holanti
That is, #CSP(g1f , g2f ) (cid:54)T Pl-Holant ((cid:54)=2 | f , i ).

26

Then, we show

Pl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f )
when a = x, b = y , where  = 1 and
Pl-Holant( (cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f )
when a = x, b = y , where  = 1 by interpolation.
(cid:20) 0 0 0 1
(cid:21)
 If a = x and b = y , since they are all no zeros and ( b
a )8 (cid:54)= 1, by normalization, we may assume
, where b (cid:54)= 0 and b8 (cid:54)= 1.
0 b 0 0
M (f ) =
0 0 b 0
If b is not a root of unity, by Lemma 2.3, we have Pl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl');
INSERT INTO posts (postId,userId,title,body) VALUES (37,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 23)','-Holant( (cid:54)=2 | f ).
1 0 0 0
Otherwise, b is a root of unity. Construct gadget f(cid:2) as showed in Figure 12. Given an

Figure 12: The Square gadget
assignment (x1 , x2 , x3 , x4 ) of f(cid:2) , f(cid:2) (x1 , x2 , x3 , x4 ) (cid:54)= 0 if and only if (x1 , x2 , x3 , x4 ) is equal to
the assignment of fv5 . That is, f(cid:2) has support (0, 0, 1, 1), (1, 1, 0, 0), (0, 1, 1, 0) and (1, 0, 0, 1).
In fact, each Diagonal Line in this gadget is a part of some circuit, which means the edges
in it can only take assignment (0, 1, 0, 1, 0, 1) or (1, 0, 1, 0, 1, 0), otherwise the term is zero.
Also, the Square cycle in this gadget is a circuit itself, which means the edges in it can
only take assignment (0, 1, 0, 1, 0, 1, 0, 1) or (1, 0, 1, 0, 1, 0, 1, 0). We simplify them by (0, 1)
and (1, 0).
For the signature f , if one pair of its edge twins ips its assignment between (0, 1) and (1, 0),
then the value of f changes from 1 to b, or b to 1.
If two pairs of edge twins both ip
their assignments, then the value of f does not change. According to this property, we give
the following Table 4. Here, we label vertices v1 , v2 , v3 , v4 and v5 in a way such that the
values of f on these vertices are all 1 under the assignment (x1 , x2 , x3 , x4 ) = (0, 0, 1, 1) and
Square= (0, 1) (Row 2). When the assignment of Square ips from (0, 1) to (1, 0), one
pair of edge twins of each vertex except v5 ips its assignment. So the values of f on these
vertices except v5 change from 1 to b (Row 3). When (x1 , x3 ) ips its assignment, one pair
of edge twins of v1 , v3 and v5 ip their assignments. When (x2 , x4 ) ips its assignment, one
(cid:35)
(cid:34) 0
pair of edge twins of v2 , v4 and v5 ip their assignments. Using this fact, we get other rows
correspondingly.
0 1+b4 0
. Since |b| = 1 and b4 (cid:54)= 1, we have
. Since b4 (cid:54)= 1, 1 + b4 (cid:54)= 0,
2b3 0
Hence, f(cid:2) has the signature matrix M (f(cid:2) ) =0
0 2b30
1+b4 0000 2b30
1+b4
2b3
1+b4 001

by normalization, we have M (f(cid:2) ) =
0

27

(x1 , x2 , x3 , x4 ) Square fv1(0, 1)
(1, 0)
(0, 0, 1, 1)

fv2b

fv3b

fv4b

fv51

(1, 1, 0, 0)

(0, 1, 1, 0)

(1, 0, 0, 1)

(0, 1)
(1, 0)

(0, 1)
(1, 0)

(0, 1)
(1, 0)
1
b
1
1
1
b
1
b
1
1
1
b
1
b
b

f(cid:2)

1 + b4

1 + b4

2b3

2b3

Table 4: The values of gadget f(cid:2) when a = x = 1 and b = y

1+b4 | > |b3 | = 1, which means 2b3
|1 + b4 | < 2. Then | 2b3
1+b4 is not a root of unity. By Lemma 2.3,
we have Pl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , f(cid:2) ). Since f(cid:2) is constructed by f , we have
(cid:20) 0 0 0 a
(cid:21)
Pl-Holant( (cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).
 If a = x and b = y , then M (f ) =
0 b 0 0
0 0 b 0
. Connect the variable x4 with x3 of f using
a 0 0 0
((cid:54)=2 ), and we get a binary signature g (cid:48) , where
g (cid:48) = Mx1 x2 ,x4 x3 (0, 1, 1, 0)T = (0, b, b, 0)T .
(cid:20) 0 0 0 a
(cid:21)
Since b (cid:54)= 0, g (cid:48) can be normalized as (0, 1, 1, 0)T . Connect the variable x2 of g (cid:48) with the
variable x1 of f , and we get a signature f (cid:48) with the signature matrix M (f (cid:48) ) =
0 b 0 0
. As
0 0 b 0
we have proved above, Pl-Holant((cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , f (cid:48) ). Since f (cid:48) is constructed
a 0 0 0
by f , we have Pl-Holant((cid:54)=2 | f , 1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).
(cid:20) 0 0 0 1
(cid:21)
 If a = x, b = y or a = x, b = y , by normalization and rotational symmetry, we may
, where b (cid:54)= 0 and b8 (cid:54)= 1.
0 b 0 0
assume M (f ) =
0 0 b 0
1 0 0 0
If b is not a root of unity, by Corollary 2.4, we have Pl-Holant((cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).
Otherwise, b is a root of unity. Construct gadget f(cid:2) in the same way as showed above.
Similarly, given an assignment (x1 , x2');
INSERT INTO posts (postId,userId,title,body) VALUES (38,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 24)',' , x3 , x4 ) of f(cid:2) , f(cid:2) (x1 , x2 , x3 , x4 ) (cid:54)= 0 if and only if
(x1 , x2 , x3 , x4 ) is equal to the assignment of fv5 . Also, the edges in Square can only take
assignment (0, 1) or (1, 0).
For the signature f , if one pair of its edge twins ips its assignment between (0, 1) and (1, 0),
then the value of f changes from 1 to b, or b to 1. If two pairs of edge twins both ip
their assignments, then the value of f does not change if the value is b, or change its sign if
the value is 1. According to this property, we have the following Table 5. Here, we label
vertices v1 , v2 , v3 , v4 and v5 in a way such that the values of f on these vertices are all 1
under the assignment (x1 , x2 , x3 , x4 ) = (0, 0, 1, 1) and Square= (0, 1) (Row 2). When the
assignment of Square ips from (0, 1) to (1, 0), one pair of edge twins of each vertex except
v5 ips its assignment. So the values of f on these vertices except v5 change from 1 to b
(Row 3). When (x1 , x3 ) ips its assignment, one pair of edge twins of v1 , v3 and v5 ip their
assignments. When (x2 , x4 ) ips its assignment, one pair of edge twins of v2 , v4 and v5 ip
their assignments. Using this fact, we get other rows correspondingly.

28

(0, 0, 1, 1)

(1, 1, 0, 0)

(0, 1, 1, 0)

f(cid:2)

fv2b

fv3b

(0, 1)
(1, 0)

(x1 , x2 , x3 , x4 ) Square fv1(0, 1)
(1, 0)
fv5
fv41
1 + b4b
b 1 (1 + b4 )b1 1 1 1 11bb 1
b 11b(0, 1)1
b 1(1, 0)Table 5: The values of gadget f(cid:2) when a = x = 1 and b = y
(cid:35)
(cid:34)

(0, 1)
(1, 0)

(1, 0, 0, 1)

2b3

2b3

0 1+b40
. Since |b| = 1 and b8 (cid:54)= 1, we have
2b3 00
Hence, f(cid:2) has the signature matrix
0 2b30
(1+b4 ) 00
1+b4 is not a root of unity. By Corollary 2.4, Pl-Holant((cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , f(cid:2) ),
2b3
and hence Pl-Holant((cid:54)=2 | f , 2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).
In summary, we have

(cid:54)T

#CSP(g1f , g2f )

Pl-Holant ((cid:54)=2 | f , 1 )
a = x, b = y ( = 1)
Pl-Holant ((cid:54)=2 | f )
(cid:54)T
a = x, b = y ( = 1)
Pl-Holant ((cid:54)=2 | f , 2 )
Therefore, we have #CSP(g1f , g2f ) (cid:54)T Pl-Holant ((cid:54)=2 | f ) when a2 = x2 (cid:54)= 0, b2 = y2 (cid:54)= 0 and
a )8 (cid:54)= 1.
( b
Theorem 4.6. Let f be a 4-ary signature of the form (4.2). Then Pl-Holant( (cid:54)=2 | f ) is #P-hard
unless
(i). (ax)2 = (by)2 , or


, where ,  ,   N, and    (mod 2),

(ii). x = ai , b = ai
, y = a
in which cases, the problem is tractable.



Proof of Tractability:
 In case (i), if ax = by = 0, then f has support of size at most 2. So we have f  P ,
and hence Pl-Holant( (cid:54)=2 | f ) is tractable by Theorem 2.13. Otherwise, (ax)2 = (by)2 (cid:54)= 0.
For any signature g in Gf , we have g00  g11 = (ax)k1+(cid:96)1+k3+(cid:96)3 (by)k2+(cid:96)2+k4+(cid:96)4 and g01  g10 =
(ax)k2+(cid:96)2+k4+(cid:96)4 (by)k1+(cid:96)1+k3+(cid:96)3 . Since (k1+(cid:96)1+k3+(cid:96)3 )(k2+(cid:96)2+k4+(cid:96)4 )  k+(cid:96)  0 ( mod 2),
(cid:18) ax
(cid:19) (k1+(cid:96)1+k3+(cid:96)3 )(k2+(cid:96)2+k4+(cid:96)4 )
(cid:19)(k1+(cid:96)1+k3+(cid:96)3 )(k2+(cid:96)2+k4+(cid:96)4 )
(cid:18) (ax)2
we have
g00  g11g01  g10
(by)2
by

= 1.

=

29
)5)5
ak+l

= ak+(cid:96)

p01

p11

p00 
p10 

That is, g  P . Since any signature h in Hf is unary, h  P . Hence, we have Gf  Hf  P .
By Theorem 2.23, #CSP(Gf  Hf ) is tractable. By reduction (4.3) of Lemma 4.5, we have
Pl-Holant( (cid:54)=2 | f ) is tractable.
(cid:34)
(cid:35)
(cid:21)
(cid:20)
 In case (ii), for any signature g in Gf , M (g) is of the form
 (k4+(cid:96)4 )+ (k2+(cid:96)2 )+2(k3+(cid:96)3 ) 
 (k1+(cid:96)3 )+ (k3+(cid:96)1 )+2(k4+(cid:96)2 )

 (k3+(cid:96)1 )+ (k1+(cid:96)3 )+2(k2+(cid:96');
INSERT INTO posts (postId,userId,title,body) VALUES (39,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 25)',')4 ) 
ii
 (k2+(cid:96)2 )+ (k4+(cid:96)4 )+2(k1+(cid:96)1 )ii

i in g correspondingly. Since  
where p00 , p01 , p10 and p11 denote the exponents of
 (mod 2), if they are both even, then p00  p01  p10  p11  0 (mod 2); if they are both
odd, then p00  p11  k2 + (cid:96)2 + k4 + (cid:96)4  k1 + (cid:96)1 + k3 + (cid:96)3  p01  p10 (mod 2). If these

i. Hence, g is of the form a(cid:48) (iq00 , iq01 , iq10 , iq11 )T ,

exponents are all odd, we can take out a
2 or pij 1
i, and for all i, j  {0, 1}, qij = pij
where a(cid:48) = ak+l or ak+l
. Thus,q00 + q01 + q10 + q11  (p00 + p01 + p10 + p11 )/2 (mod 2).
Moreover, since p00 + p01 + p10 + p11 = (k + (cid:96))( +  + 2)  0 ( mod 4), we have q00 + q01 +
q10 + q11  0 (mod 2). Therefore, g  A by Lemma 2.10.
m4+m2+2m1 (cid:105)
am (cid:104)
In this case, for any signature h in Hf , M (h) is of the form
m2+m4+2m3 iSince    (mod 2), we have m2 + m4  m4 + m2 (mod 2). Hence, h is of the form

i. That is, h  A by Lemma 2.11. Hence, Gf  Hf  A .
a(cid:48)(cid:48) (iq0 , iq1 ), where a(cid:48)(cid:48) = am or am
By Theorem 2.23, #CSP(Gf  Hf ) is tractable. By reduction (4.3) of Lemma 4.5, we have
(cid:20) 0 0 0 a
(cid:21)
(cid:20) 0 0 0 x
(cid:21)
Pl-Holant( (cid:54)=2 | f ) is tractable.
0 y 0 0
0 b 0 0
Proof of Hardness: Note that Mx1 x2 ,x4 x3 (f ) =
and Mx3 x4 ,x2 x1 (f ) =
0 0 y 0
0 0 b 0
x 0 0 0
a 0 0 0
Connect variables x4 , x3 of a copy of signature f with variables x1 , x2 of another copy of signature
 .
 0
f both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix0byx2Similarly, connect x4 , x3 of a copy of signature f with x3 , x4 of another copy of signature f both
 .
 0
using ( (cid:54)=2 ). We get a signature f2 with the signature matrix0
b20
M (f2 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =
y20
(cid:34) 0 0 0 0
(cid:34) 0 0 0 0
(cid:35)
(cid:35)
(cid:105)
(cid:104) ax b2
(cid:105)
(cid:104) a2 by0
ax


0 a2 by 0
0 ax b2 0
and M (g2f ) =
, M (g1f ) =
2 ) =
1 ) =, M (f
Notice that M (f2
y2 ax
0 y2 ax 0
by x2
0 by x2 0
0 0 0 0
0 0 0 0
i In . Thus, we have fi (x1 , x2 , x3 , x4 ) = gi (x1 , x2 )  x2 (cid:54)=x3  x1 (cid:54)=x4 . Now, we analyze

That is, gif = fg1f and g2f .

M (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (f ) =

ax0
0
by0

a20
.

30

.

 If {g1f , g2f }  P , then either (ax)2 = (by)2 due to degeneracy, or g1f and g2f are generalized
Equality or generalized Disequality respectively. In the later case, since (a, x) (cid:54)= (0, 0)
and (b, y) (cid:54)= (0, 0), it forces that ax = by = 0. So we still have (ax)2 = (by)2 . That is,
{a, b, x, y} belongs to case (i).
 If {g1f , g2f }  A , there are two subcases.
 If both g1f and g2f have support of size at most 2, then we have ax = by = 0 due to
(a, x) (cid:54)= (0, 0) and (b, y) (cid:54)= (0, 0). This belongs to case (i).
 Otherwise, both g1f and g2f have support of size 4, which means abxy (cid:54)= 0. Let x(cid:48) =
(cid:20) 1
(cid:21)
a and y (cid:48) = y
a , b(cid:48) = ba . By normalization, we have
b(cid:48)y (cid:48)
M (g1f ) = a2
b(cid:48)y (cid:48) x(cid:48)2
Since g1f  A , by Lemma 2.10, x(cid:48)2 and b(cid:48)y (cid:48) are both powers of i, and the sum of all
exponents is even. It forces that x(cid:48)2 = i2 for some   N. Then, we can choose  such
(cid:21)
(cid:20) x(cid:48)
x(cid:48) = i . Also, we have
b(cid:48)2
M (g2f ) = a2
y (cid:48)2 x(cid:48)Since g2f  A and x(cid:48) is already a power of i, y (cid:48)2 and b(cid:48)2 are both powers of i. That is,



. Also, since g1f  A , b(cid:48)y (cid:48) =
b(cid:48) =
and y (cid:48) =

is a power of i, which meansi If {g1f , g2f }  (cid:99)M , then by Lemma 2.17, we have a2 = x2 and b2 = y2 , denoted by case (iii).
   (mod 2). That is, {a, b, x, y} belongs to case (ii).
or (cid:99)M . By Theorem 2.23, we ha');
INSERT INTO posts (postId,userId,title,body) VALUES (40,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 26)','ve Pl-#CSP(g1f , g2f ) is #P-hard. Recall g1f = f
Therefore, if {a, b, x, y} does not belong to case (i), case (ii) or case (iii), then {g1f , g2f } (cid:42) P , A


1 In and g2f = f
2 In .2
By Lemma 4.2, Pl-Holant( (cid:54)=2 | f
2 ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.


1 , f2
Otherwise, {a, b, x, y} does not belong to case (i) or case (ii), but belongs to case (iii). Then

a2 = x2 (cid:54)= 0, b2 = y2 (cid:54)= 0 and b
a )8 (cid:54)= 1. By reduction (4.4)
i, that is ( b
a is not a power of
of lemma 4.6, we have #CSP(g1f , g2f ) (cid:54)T Pl-Holant( (cid:54)=2 | f ). Moreover, since {a, b, x, y} does not
belong to case (i) or case (ii), we have {g1f , g2f } (cid:42) P or A . By Theorem 2.23, #CSP(g1f , g2f ) is
#P-hard. Therefore, we have Pl-Holant( (cid:54)=2 | f ) is #P-hard.

5 Case III: Exactly Two Zeros and They Are in Dierent Pairs or
Exactly One Zero and It Is in an Outer Pair

If there are exactly two zeros and they are in dierent pairs, there must be a zero in an outer pair.
(cid:21)
(cid:20) 0 0 0 0
By rotational symmetry, we may assume a is zero and we prove this case in Theorem 5.2. We rst
give the following lemma.
(cid:20) 0 0 0 0
(cid:21)
0 b c 0
, where
Lemma 5.1. Let f be a 4-ary signature with the signature matrix M (f ) =
0 z y 0
0 0 0 0
det MIn (f ) = by  cz (cid:54)= 0. Let g be a 4-ary signature with the signature matrix M (g) =
0 0 1 0
0 1 0 0
Then for any signature set F containing f , we have
0 0 0 0
Pl-Holant( (cid:54)=2 | F  {g}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).

31

= n
0

0

P =

and s =

(cid:35)

and M (fn ) =

(cid:34) 0 0
0 0
0 n
1 0 0
0 0 n
1 0
0 0
0 0

M (fs ) = M (f )(N M (f ))s1 = N (N M (f ))s = N

(cid:34) 0 0
(cid:35)
(cid:21)
(cid:20) 0 0 0 0
1. If  = 0, and 2
is a root of unity. Suppose ( 21
0 0
0 0 n
1 0
0 0 1 0
0 n
0 1 0 0
1 0 0
0 0 0 0
0 0
0 0

Proof. We construct a series of gadgets fs by a chain of s copies of f linked by double
 .
 0
Disequality N . fs has the signature matrix
(cid:21)s
(cid:20) z y0b0Consider the inner matrix NInMIn (f ) of N M (f ). Suppose NInMIn (f ) = Q1Q, where  = [ 1 is the Jordan Canonical Form. Note that 12 = det  = det(NInMIn (f )) (cid:54)= 0. We have M (fs ) =
0 2
 .
 0
N P 1sP , where
 1 0 0

(cid:21)s
(cid:20) 1 0 Q 000
(cid:34) 0 0
0 0 100 0
0 n
1 0 0
)n = 1. Then n =
0 0 n
2 0
0 0
0 0
(cid:34) 0 0 0 0
(cid:35)
. After normalization, we can realize the signature g .
0 s
1 0 0
2. If  = 0, and 2
has a good form for
is not a root of unity. The matrix s =
0 0 s
2 0interpolation. Suppose g appears m times in an instance  of Pl-Holant( (cid:54)=2 | F {g}). Replace
0 0 0 0
each appearance of g by a copy of gadget fs to get an instance s of Pl-Holant( (cid:54)=2 | F  {fs}),
which is also an instance of Pl-Holant((cid:54)=2 | F ). We can treat each of the m appearances of fs
as a new gadget composed of four functions in sequence N , P 1 , s and P , and denote this
s into two parts. One part consists of m signatures m
s . We divide (cid:48)
new instance by (cid:48)s
is expressed as a column vector. The other part is the rest of (c');
INSERT INTO posts (postId,userId,title,body) VALUES (41,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 27)','id:48)
Here m
s and its signatureis represented by A which is a tensor expressed as a row vector. Then the Holant value of
(cid:105), which is a summation over 4m bits. That is, the value of
s is the dot product (cid:104)A, m
(cid:48)the 4m edges connecting the two parts. We can stratify all 0, 1 assignments of these 4m bits
having a nonzero evaluation of a term in Pl-Holant(cid:48)
s into the following categories:
 There are i many copies of s receiving inputs 0110;
 There are j many copies of s receiving inputs 1001;
2 = sm (cid:16) 2
(cid:17)sj
where i + j = m.
For any assignment in the category with parameter (i, j ), the evaluation of m1 sj
si
. Let aij be the summation of values of the part A over all assignments inthe category (i, j ). Note that aij is independent from the value of s since we view the gadget
s as a block. Since i + j = m, we can denote aij by aj . Then we rewrite the dot product
(cid:88)
summation and get
0(cid:54)j(cid:54)m

s = (cid:104)A, m
Pl-Holants = Pl-Holant(cid:48)
(cid:105) =

smaj (

)sj .

(cid:35)

is clearly
1

32

(m+1)m

aj .

2m

2m

2m

(cid:19)m
(cid:19)2m
(cid:19)(m+1)m

(cid:20) 0 0 0 0
(cid:21)
Note that M (g) = N P 1 (N M (g))P , where N M (g) =
0 1 0 0
. Similarly, divide  into two
(cid:88)
0 0 1 0
0 0 0 0
parts. Under this stratication, we have
Pl-Holant = (cid:104)A, (N M (g))m (cid:105) =
0(cid:54)j(cid:54)m


(cid:18) 2
(cid:19)0
(cid:18) 2
(cid:19)1
(cid:18) 2
Since 2
is not a root of unity, the Vandermonde coecients matrix(cid:19)21
(cid:19)20
(cid:18) 2
(cid:18) 2
(cid:18) 2
  1  
(cid:18) 2
(cid:19)(m+1)1
(cid:18) 2
(cid:19)(m+1)0
(cid:18) 21...
...
...
...
   (m+1)m
(m+1)m1(cid:20) 0 0
(cid:21)
has full rank. Hence, by oracle querying the values of Pl-Holants , we can solve coecients
aj and obtain the value of Pl-Holant in polynomial time.0
0 s ss1 0
. We use this form to give
3. If  = 1, and 1 = 2 denoted by . Then s =0 0a polynomial interpolation. As in the case above, we can stratify the assignments of m
0 00of these 4m bits having a nonzero evaluation of a term in Pl-Holant(cid:48)
s into the following
categories:
 There are i many copies of s receiving inputs 0110 or 1001;
 There are j many copies of s receiving inputs 0101;
where i + j = m.
For any assignment in the category with parameter (i, j ), the evaluation of m
is clearlysi (ss1 )j = sm ( s
 )j . Let aij be the summation of values of the part A over all assignments
in the category (i, j ). aij is independent from s. Since i + j = m, we can denote aij by aj .
(cid:16) s
(cid:17)j
(cid:105) = sm (cid:88)
Then, we rewrite the dot product summation and get
s = (cid:104)A, m
Pl-Holants = Pl-Holant(cid:48)
0(cid:54)j(cid:54)m
Similarly, divide  into two parts. Under this stratication, we have
Pl-Holant = (cid:104)A, (N M (g))m (cid:105) = a0 .


(cid:18) 1
(cid:18) 1
(cid:19)1
(cid:19)0
The Vandermonde coecients matrix
(cid:19)1
(cid:18) 2
(cid:19)0
(cid:18) 2


(cid:18) m + 1
(cid:18) m + 1


...
...
(m+1)m


has full rank. Hence, we can solve a0 in polynomial time and it is the value of Pl-Holant .

  
  
...
   (m+1)m

(cid:18) 1
(cid:19)m
(cid:19)m
(cid:18) 2

(cid:18) m + 1

...


2m

2m

2m

(cid:19)m

aj

(cid:19)0

(m+1)m

(cid:19)1

33

where (cid:2)');
INSERT INTO posts (postId,userId,title,body) VALUES (42,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 28)',' b1 c1
z1 y1

Therefore, we have Pl-Holant((cid:54)=2 | F  {g}) (cid:54)T Pl-Holant( (cid:54)=2 | F ).
0 0 0 0Theorem 5.2. Let f be a 4-ary signature with the signature matrix
0 b
c 0
0 z y 0
x 0 0 0
where x (cid:54)= 0 and there is at most one number in {b, c, y , z} that is 0. Then Pl-Holant( (cid:54)=2 | f ) is
#P-hard unless f  M , in which case the problem is tractable.
Suppose f / M . By Lemma 2.14, det MIn (f ) (cid:54)= det MOut (f ), that is det (cid:2) b c
(cid:3) = by  cz (cid:54)= 0.
(cid:20) 0 0 0 0
(cid:20) 0 0 0 x
(cid:20) 0 0 0 y
(cid:21)
(cid:21)
(cid:21)
Proof. Tractability follows by Theorem 2.15.
z y
0 y c 0
0 b c 0
0 0 z 0, and Mx2 x3 ,x1 x4 (f ) =
, Mx3 x4 ,x2 x1 (f ) =
Note that Mx1 x2 ,x4 x3 (f ) =
0 z y 0
0 c x 0
0 z b 0
b 0 0 0
x 0 0 0
0 0 0 0
Connect variables x4 , x3 of a copy of signature f with variables x3 , x4 of another copy of signature ,
f both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix0c1 0
0 b1
M (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =
0 z1 y1 0
(cid:3) = (by  cz )2 (cid:54)= 0. By Lemma 5.1, we have
(cid:3) . Here, det (cid:2) b1 c1
(cid:3)  (cid:2) z b
(cid:3) = (cid:2) b c00
z y
y c
z1 y1
(cid:21)
(cid:20) 0 0 0 0
Pl-Holant( (cid:54)=2 | f , g) (cid:54)T Pl-Holant( (cid:54)=2 | f , f1 ),
0 0 1 0
where g has the signature matrix M (g) =0 1 0 0
 If bcyz (cid:54)= 0, connect variables x1 , x4 of signature f with variables x1 , x2 of signature g both
0 0 0 0
 .
0 0 0 0
using ( (cid:54)=2 ). We get a signature f2 with the signature matrix
0 0 z 0
0 c x 0
0 0 0 0
 Otherwise, connect variables x4 , x3 of signature f with variables x1 , x2 of signature g both
0 0 0 0
 ,
using ( (cid:54)=2 ). We get a signature f2 with the signature matrix
0 b
c 0
M (f2 ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (g) =
0 z y 0
0 0 0 0
In both cases, the support of f2 has size 3, which means f2 / P , A or (cid:99)M . By Theorem 4.3,
and there is exactly one in {b, c, y , z} that is zero.
Pl-Holant( (cid:54)=2 | f2 ) is #P-hard. Since
Pl-Holant( (cid:54)=2 | f2 ) (cid:54)T Pl-Holant( (cid:54)=2 | f , g) (cid:54)T Pl-Holant( (cid:54)=2 | f , f1 ) (cid:54)T Pl-Holant( (cid:54)=2 | f ),
we have Pl-Holant((cid:54)=2 | f ) is #P-hard.

M (f2 ) = Mx2 x3 ,x1 x4 (f )N Mx1 x2 ,x4 x3 (g) =

M (f ) =

34

6 Case IV: Exactly One Zero and It Is in the Inner Pair or All
Values Are Nonzero
By rotational symmetry, if there is one zero in the inner pair, we may assume c = 0. We rst
consider the case that a = x, b = y and c = z , where  = 1.
,  = 1 and abc (cid:54)= 0.
 0
Lemma 6.1. Let f be a 4-ary signature with the signature matrix0cbb 0
c00
a
Then Pl-Holant( (cid:54)=2 | f ) is #P-hard if f / M .
If  = 1. Connect the variable x4 with x3 of f using ((cid:54)=2 ), and we get a binary
Proof.
signature g1 , where

M (f ) =

g1 = Mx1 x2 ,x4 x3 (f )(0, 1, 1, 0)T = (0, b + c, (b + c), 0)T .
Also connect the variable x1 with x2 of f using ( (cid:54)=2 ), and we get a binary signature g2 , where
g2 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 (f ))T = (0, b  c, (b  c), 0)T .
Since bc (cid:54)= 0, b + c and b  c can not be both zero. Without loss of generality, assume b + c (cid:54)= 0. By
(cid:20) 0 0 0 a
(cid:21)
normalization, we have g1 = (0, 1, 1, 0)T . Then, connect the variable x2 of g1 with the variable x1
of f using ( (cid:54)=2 ), and we get a signature with the signature matrix
0 b c 0
. Therefore, it suces
0 c b 0
a 0 0 0
to show #P-hardness for the case that  = 1.
Since f / M , by Lemma 2.14, c2  b2 (cid:54)= a2 . We prove #P-hardness in three cases depending
on the values of a, b and c.
Case 1: If c2  b2 (cid:54)= 0 and |c + b| (cid:54)= |c  b|, or c2  a2 (cid:54)= 0 and |c + a| (cid:54)= |c  a|. By rotational
(cid:21)
(cid:20) 0 0 0 1
symmetry, we may assume c2  b2 (cid:54)= 0 and |c + b| (cid:54)= |c  b|');
INSERT INTO posts (postId,userId,title,body) VALUES (43,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 29)','. Normalizing f by assuming a = 1, we
, where c2  b2 (cid:54)= 0 or 1.
0 b c 0
have M (f ) =
0 c b 0
1 0 0 0
We construct a series of gadgets fs by a chain of s copies of f linked by double Disequality
 .
 1
N . fs has the signature matrix
(cid:21)s
(cid:20) c0
M (fs ) = M (f )(N M (f ))s1 = N (N M (f ))s = N0b(cid:3) (note that H 1 = H ), and get M (fs ) = N P sP , where
(cid:2) 1 1
(cid:3)s using H = 1
We diagonalize (cid:2) c b0 .
 1
1 1
 ,
 1 0 0
b c00
0 (c + b)s0
(c  b)s 0
0 H 00
0 0 100

and s =

P =

35

(cid:35)
(cid:34) 0 0 0 1
The the signature matrix s has a good form for polynomial interpolation. Suppose we have a
problem Pl-Holant((cid:54)=2 | f ), where M ( f ) =
to be reduced to Pl-Holant( (cid:54)=2 | f ). Suppose
0 b c 0
0 c b 0
1 0 0 0
f appears m times in an instance  of Pl-Holant((cid:54)=2 | f ). We replace each appearance of f by
a copy of gadget fs to get an instance s of Pl-Holant( (cid:54)=2 | f , fs ), which is also an instance of
Pl-Holant( (cid:54)=2 | f ). We can treat each of the m appearances of fs as a new gadget composed of four
functions in sequence N , P , s and P , and denote this new instance by (cid:48)
s . We divide (cid:48)
s into
two parts. One part consists of m signatures m
. Here, m
is expressed as a column vector.s
The other part is the rest of (cid:48)
s and its signature is represented by A which is a tensor expressed
s is the dot product (cid:104)A, m
(cid:105), which is a summation
as a row vector. Then the Holant value of (cid:48)over 4m bits. That is, the value of the 4m edges connecting the two parts. We can stratify all
0, 1 assignments of these 4m bits having a nonzero evaluation of a term in Pl-Holant(cid:48)
s into the
following categories:
 There are i many copies of s receiving inputs 0000 or 1111;
 There are j many copies of s receiving inputs 0110;
 There are k many copies of s receiving inputs 1001;
where i + j + k = m.
For any assignment in the category with parameter (i, j, k), the evaluation of m
is clearly(c + b)sj (c  b)sk . Let aij k be the summation of values of the part A over all assignments in the
category (i, j, k). Note that aij k is independent on the value of s. Since i + j + k = m, we can
(cid:88)
denote aij k by aj k . Then we rewrite the dot product summation and get
aj k (c + b)sj (c  b)sk .
(cid:105) =
s = (cid:104)A, m
Pl-Holants = Pl-Holant(cid:48)0(cid:54)j+k(cid:54)m
(cid:88)
Under this stratication, correspondingly we can dene (cid:48) and . Then we have
aj k (c + b)j (c  b)k .
Pl-Holant  = Pl-Holant (cid:48) = (cid:104)A, m (cid:105) =
Let  = c + b and  = c  b. If we can obtain the value of p(, ) = (cid:80)
0(cid:54)j+k(cid:54)m
0(cid:54)j+k(cid:54)m
time, then we will have
Pl-Holant( (cid:54)=2 | f ) (cid:54)T Pl-Holant( (cid:54)=2 | f ).
Let  = c + b and  = c  b. Since c2  b2 (cid:54)= 0 or 1, we have  (cid:54)= 0,  (cid:54)= 0 and  (cid:54)= 1. Also,
by assumption |c + b| (cid:54)= |c  b|, we have || (cid:54)= | |. Dene L = {(j, k)  Z2 | j  k = 1}. This is a
sublattice of Z2 . Every lattice has a basis. There are 3 cases depending on the rank of L.
 L = {(0, 0)}. All j  k are distinct. It is an interpolation reduction in full power. Th');
INSERT INTO posts (postId,userId,title,body) VALUES (44,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 30)','at is,
(cid:20) 0 0 0 1
(cid:21)
we can interpolate p(, ) for any  and  in polynomial time. Let  = 4 and  = 0, that
is b = 2 and c = 2, and hence M ( f ) =
. That is, f is non-singular redundant. By
0 2 2 0
0 2 2 0
1 0 0 0
Theorem 2.21, Pl-Holant((cid:54)=2 | f ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.
 L contains two independent vectors (j1 , k1 ) and (j2 , k2 ) over Q. Then the nonzero vectors
j2 (j1 , k1 )  j1 (j2 , k2 ) = (0, j2k1  j1k2 ) and k2 (j1 , k1 )  k1 (j2 , k2 ) = (k2 j1  k1 j2 , 0) are in L.
Hence, both  and  are roots of unity. That is || = | | = 1. Contradiction.

aj k j k in polynomial

36

 L = {(ns, nt) | n  Z}, where s, t  Z and (s, t) (cid:54)= (0, 0). Without loss of generality, we
may assume t (cid:62) 0, and s > 0 when t = 0. Also, we have s + t (cid:54)= 0, otherwise || = | |.
 . There are three cases depending on the values of s and t.
 0
Contradiction. By Lemma 2.7, for any numbers  and  satisfying s t = 1, we can obtain
p(, ) in polynomial time. Since  = c + b and  = c  b, we have b = 
and c = +
2 .01
0 0
That is M ( f ) =2

0 +200 If s (cid:62) 0 and s + t (cid:62) 2. Consider the function q(x) = (2  x)sxt  1. Since s (cid:62) 0 and
t (cid:62) 0, it is a polynomial. Clearly, 1 is one of its roots and 0 is not its root. If q(x) has
no other roots, then for some constant  (cid:54)= 0,
q(x) = (2  x)sxt  1 = (x  1)s+t = (1)s+t((2  x)  1)s+t .
Notice that xt |q(x) + 1, while xt (cid:45) (x  1)s+t + 1 for t (cid:62) 2. Also, notice that (2 
x)s |q(x) + 1, while (2  x)s (cid:45) (1)s+t((2  x)  1)s+t for s (cid:62) 2. Hence, t = s = 1, which
(cid:21)
(cid:20) 0
(cid:21)
(cid:20) 0
means  = 1. Contradiction.
Therefore, q(x) has a root x0 , where x0 (cid:54)= 1 or 0. Let  = x0 and  = 2  x0 . Then
0 0 1x010 1x0
s t = 1 and M ( f ) =
. Note that Mx2 x3 ,x1 x4 ( f ) =01 11x0 0
. Since00
1 1
1x0 0 0101  x0 (cid:54)= 0, f is non-singular redudant. By Theorem 2.21, Pl-Holant((cid:54)=2 | f ) is #P-hard
and hence Pl-Holant ( (cid:54)=2 | f ) is #P-hard.
 If s < 0 and t > 0. Consider the function q(x) = xt  (2  x)s . Since t > 0 and s > 0,
it is a polynomial. Clearly, 1 is one of its roots and 0 is not its root. Since t + s (cid:54)= 0,
the highest order term of q(x) is either xt or (x)s , which means the coecient of
the highest order term is 1. While the constant term of q(x) is 2s (cid:54)= 1. Hence,
q(x) can not be of the form (x  1)max(t,s) for some constant  (cid:54)= 0. Moreover, since
t + s (cid:54)= 0, max(t, s) (cid:62) 2, which means q(x) has a root x0 , where x0 (cid:54)= 1 or 0. Similarly,
let  = x0 and  = 2  x0 , and we have Pl-Holant ((cid:54)=2 | f ) is #P-hard.
 If s (cid:62) 0 and s + t = 1. In this case, we have s = 0, t = 1 or s = 1, t = 0 due to t (cid:62) 0.
(cid:34) 0 0 0 1
(cid:35)
(cid:20) 0 0 0 1
(cid:21)
 s = 1, t = 0. Let  = 1 and  = 1
2 . Then we have 10 = 1 and M ( f ) =
. Clearly, Pl-Holant( (cid:54)=2 | f (cid:48) ) (cid:54)T
. Let M (f (cid:48) ) = 4Mx2 x3 ,x1 x4 ( f ) =
0 14 0
0 4 3 00 3 4 00 3
4 01 0 0 0
Pl-Holant ((cid:54)=2 | f ). For M (f (cid:48) ), correspondingly we dene (cid:48) = 3 + 4 = 7 and  (cid:48) =
1 0 0 0
3  4 = 1. Obviously, (cid:48)
(cid:54)= 0,  (cid:48)
(cid:54)= 0, (cid:48) (cid:48)
(cid:54)= 1, and |(cid:48) | (cid:54)= | (cid:48) |. Let L(cid:48) =
{(j, k)  Z2 | (cid:48)j  (cid:48)k = 1}. Then we have L(cid:48) = {(ns(cid:48) , nt(cid:48) ) | n  Z}, where s(cid:48) = 0
(cid:20) 0 0 0 1
(cid:21)
and t(cid:48) = 2. Therefore, s(cid:48) (cid:62) 0 and s(cid:48) + t(cid:48) (cid:62) 2. As we have showed above, we have
Pl-Holant ((cid:54)=2 | f (cid:48) ) is #P-hard, and hence Pl-Holant ( (cid:54)=2 | f ) is #P-hard.
 s = 0, t = 1. Let  = 3 and  = 1. Then we have 01 = 1 and M ( f ) =
0 1 2 00 2 1 0
1 0 0 0
By Theorem 2.22, Pl-Holant((cid:54)=2 | f ) is #P-hard, and hence Pl-Holan');
INSERT INTO posts (postId,userId,title,body) VALUES (45,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 31)','t ((cid:54)=2 | f ) is
#P-hard.
Case 2: If c2  b2 (cid:54)= 0 and |c + b| = |c  b|, or c2  a2 (cid:54)= 0 and |c + a| = |c  a|. By rotational
(cid:20) 0 0 0 a
(cid:21)
symmetry, we may assume c2  b2 (cid:54)= 0 and |c + b| = |c  b|. Normalizing f by assuming c = 1, we
, where 12  b2 (cid:54)= 0 and 12  b2 (cid:54)= a2 due to f / M . Since |1 + b| = |1  b|,
0 b 1 0
have M (f ) =
0 1 b 0
a 0 0 0
b is a pure imaginary number.

37

M (f2 ) = Mx1 x2 ,x4 x3 (f1 )N Mx1 x2 ,x4 x3 (f1 ) =

Connect variables x4 , x3 of a copy of signature f with variables x1 , x2 of another copy of
 0
 .
signature f both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix0
b2 + 12b
b2 + 1
2b(cid:35)
(cid:34) 0
a201a. If c2  a2 = 0, that is a2 = 1, and then M (f1 ) =
b2+1 0
0 2b
. Since b2 < 0, we have
(cid:20) 0 0
(cid:21)
0 b2+1 2b 000
(b2 + 1)2  (2b)2 = (b2  1)2 > 1 = (a2 )2 , which means f1 / M .
0 1
 If b2 = 1, then M (f1 ) =
. By Theorem 4.6, Pl-Holant((cid:54)=2 | f1 ) is #P-hard,
0 2i 0 0
(cid:34) 0
(cid:35)
0 0 2i 0
and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.
1 0
0 0
0 If b2 = 2, then M (f1 ) =
0 2
2i 1
0 1 2
2i 001
signature f2 with the signature matrix

. Connect two copies of f1 , and we have a
 .
000 4
2i72i 000
It is easy to check f2 / M . Then, f2 belongs to Case 1. Therefore, Pl-Holant((cid:54)=2 | f2 ) is
#P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.
 If b2 (cid:54)= 1 or 2, then b2 + 1 (cid:54)= 1, and hence 12  (b2 + 1)2 (cid:54)= 0 due to b (cid:54)= 0. Also,
(cid:34) 0 0 0 a1
(cid:35)
since b2 + 1 is a real number and b2 + 1 (cid:54)= 0, we have |(b2 + 1) + 1| (cid:54)= |(b2 + 1)  1|. Then,
f1 / M has the signature matrix of form
1  a2
, where a1 b1 c1 (cid:54)= 0, c2
1 (cid:54)= 0
0 b1 c1 0
0 c1 b1 0
a1 0 0 0
and |c1 + a1 | (cid:54)= |c1  a1 |. That is, f1 belongs to Case 1. Therefore, Pl-Holant( (cid:54)=2 | f1 ) is
#P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.
b. If c2  a2 (cid:54)= 0 and |c + a| = |c  a|, then a is also a pure imaginary number. Connect variables
 0
 .
x1 , x4 of a copy of signature f with variables x2 , x3 of another copy of signature f . We get
a signature f3 with the signature matrix
b20
a2 + 12aa2 + 12ab20Note that f3  M implies (a2  1)2 = (b2 )2 . Since f / M , 1  a2 (cid:54)= b2 . Hence, f3  M
implies a2  1 = b2 . Similarly, f1  M implies b2  1 = a2 . Clearly, f1 and f3 can not be
both in M . Without loss of generality, we may assume f3 / M .
 If a2 (cid:54)= 1. There are two subcases.
(cid:34) 0 0 0 a3
(cid:35)
 (a2 + 1)2  (b2 )2 = 0. Since a is a pure imagianry number, |a2 + 1 + 2a| = |a + 1|2 =
|a  1|2 = |a2 + 1  2a|. Then f3 has the signature matrix of form
0 b3 c3 0
0 c3 b3 0
a3 0 0 0

M (f3 ) = Mx2 x3 ,x1 x4 (f )N Mx2 x3 ,x1 x4 (f ) =

M (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (f ) =

a20
38

2i

. Note that Mx2 x3 ,x1 x4 (f1 ) =

where a3 b3 c3 (cid:54)= 0, c2
3  b2
3 (cid:54)= 0, |c3 + b3 | = |c3  b3 | and c2
3  a2
3 = 0. That is, f3 belongs
to Case 2.a. Therefore, Pl-Holant((cid:54)=2 | f3 ) is #P-hard, and he');
INSERT INTO posts (postId,userId,title,body) VALUES (46,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 32)','nce Pl-Holant((cid:54)=2 | f )
is #P-hard.
 (a2 + 1)2  (b2 )2 (cid:54)= 0. Since a2 + 1 and b2 are both nonzero real numbers due to a
(cid:35)
(cid:34) 0 0 0 a3
and b are both pure imaginary numbers, we have |a2 + 1 + b2 | (cid:54)= |a2 + 1  b2 |. Then
3  a2
, where a3 b3 c3 (cid:54)= 0, c2
3 (cid:54)= 0 and
0 b3 c3 0
f3 has the signature matrix of form
0 c3 b3 0
a3 0 0 0
|c3 + a3 | (cid:54)= |c3  a3 |. That is, f3 belongs to Case 1. Therefore, Pl-Holant( (cid:54)=2 | f3 ) is
(cid:20) 0 0 0 b2
(cid:21)
#P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.
 If a2 = 1 and b2 (cid:54)= 2, then M (f3 ) =
, where |2a| = 2 (cid:54)= |b2 |. By Theorem
0 2a 0 0
(cid:34) 0
(cid:35)
0 0 2a 0
4.6, Pl-Holant( (cid:54)=2 | f3 ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard.
b2 0 0 0
0(cid:34) 0
(cid:35)
 If a2 = 1 and b2 = 2, then M (f1 ) =
0 2
2i 1
1 22i 0
00
0 2. We have f1 is non-singular redudant. Therefore, Pl-Holant((cid:54)=2 | f1 )
1 10
1 1
0is #P-hard, and hence Pl-Holant( (cid:54)=2 | f ) is #P-hard.
2i 00
c. If c2  a2 (cid:54)= 0 and |c + a| (cid:54)= |c  a|. This is Case 1. Done.
Case 3: c2  b2 = 0 and c2  a2 = 0. If c = b or c = a, then f is non-singular redudant,
(cid:21)
(cid:20) 0 0
(cid:21)
(cid:20) 0
and hence Pl-Holant((cid:54)=2 | f ) is #P-hard. Otherwise, a = b = c. By normalization, we have
0 10 1
. Notice that 22  12 (cid:54)= 0 and |2+ 1| (cid:54)= |2 1|.
0 1 1
0 2 2 00 2 2 0
1 1 0
, and then M (f1 ) =
M (f ) =1 0
That is, f1 belongs to Case 1. Therefore, Pl-Holant( (cid:54)=2 | f1 ) is #P-hard, and hence Pl-Holant( (cid:54)=2 | f )
0 0
1 00
is #P-hard.
, abcxyz (cid:54)= 0.
0 0 0 a
Lemma 6.2. Let f be a 4-ary signature with the signature matrix
0 b
c 0
0 z y 0
x 0 0 0
If by  cz = 0 or ax  cz = 0, then Pl-Holant( (cid:54)=2 | f ) is #P-hard.
(cid:20) 0 0 0 a
(cid:21)
Proof. By rotational symmetry, we assume by  cz = 0. By normalization, we assume b = 1,
0 1 c 0
and then y = cz . That is, Mx1 x2 ,x4 x3 (f ) =0 z cz 0
x 0 0 0
 If 1 + c (cid:54)= 0. Connect the variables x4 with x3 of f using ((cid:54)=2 ), and we get a binary signature
g1 , where

M (f ) =

g1 = Mx1 x2 ,x4 x3 (f )(0, 1, 1, 0)T = (0, 1 + c, (1 + c)z , 0)T .
(cid:21)
(cid:20) 0
Note that g1 (x1 , x2 ) can be normalized as (0, z1 , 1, 0)T . That is g(x2 , x1 ) = (0, 1, z1 , 0)T .
Connect the variable x1 of g1 with the variable x1 of f , and we get a signature f1 with the
0 0 a
. Connect the variable x1 with x2 of f1 using ( (cid:54)=2 ), and we get a1 c 01 c 0
xz1 0 0 0
binary signature g2 , where
g2 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 (f ))T = (0, 2, 2c, 0)T .

signature matrix

39

Note that g2 (x1 , x2 ) can be normalized as (0, c1 , 1, 0)T , That is g2 (x2 , x1 ) = (0, 1, c1 , 0)T .
(cid:20) 0
(cid:21)
Connect the variable x1 of g2 with the variable x3 of f1 , and we get a signature f2 with
0 0 ac10
1 1
. It is non-singular redudant. By Lemma 2.21, we have
the signature matrix0
1 1
xz1 0 0
Pl-Holant( (cid:54)=2 | f2 ) is #P-hard, and hence Pl-Holant((cid:54)=2 | f ) is #P-hard. If 1 + z (cid:54)= 0, then connect the variable x1 with x2 of f using ((cid:54)=2 ), and we get a binary
signature g (cid:48)
1 , where

and Mx3 x4 ,x2 x1 (f ) =

M (f3 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =

g (cid:48)
1 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 )T = (0, 1 + z , (1 + z )c, 0)T .
1 (x1 , x2 ) can be normalized as (0, c1 , 1, 0)T . Same as the analysis of the case
Note that g (cid:48)
(cid:20) 0 0
(cid:21)
1 + c (cid:54)= 0, we still have Pl-Holant((cid:54)=2 | f ) is #P');
INSERT INTO posts (postId,userId,title,body) VALUES (47,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 33)','-hard.
(cid:21)
(cid:20) 0 0
0 a
 Otherwise, 1 + c = 1 and 1 + z = 1, that is c = z = 1. Then Mx1 x2 ,x4 x3 (f ) =
0 1 1 0
0 1 1 0
x 0
0 0
0 x
0 1 1 0
0 1 1 0
. Connect variables x4 , x3 of a copy of signature f with
 ,
 0
a 0
0 0
variables x3 , x4 of another copy of signature f , and we get a signature f3 with the signature
matrix
ax0
0 22
2 20ax0
Clearly, ax (cid:54)= 0 and f3 / M . By Lemma 6.1, Pl-Holant ((cid:54)=2 | f3 ) is #P-hard and hence
Pl-Holant ((cid:54)=2 | f ) is #P-hard.
In the following Lemmas 6.3, 6.4, 6.6 and Corollaries 6.5, 6.7, let f be a 4-ary signature with0 0 0 a
the signature matrix
0 b
c 0
M (f ) =
0 z y 0
where abxyz (cid:54)= 0, det (cid:2) b c
(cid:3) = by  cz (cid:54)= 0 and det [ a z
x 0 0 0
c x ] = ax  cz (cid:54)= 0. Moreover f / M , that is
z y
cz  by (cid:54)= ax.
Lemma 6.3. Let g = (0, 1, t, 0)T be a binary signature, where t (cid:54)= 0 is not a root of unity. Then
Pl-Holant( (cid:54)=2 | f , g) is #P-hard.
Proof. Let B = {g1 , g2 , g3} be a set of three binary signatures gi = (0, 1, ti , 0)T . By Lemma
2.5, we have Pl-Holant ( (cid:54)=2 | {f }  B) (cid:54) Pl-Holant ((cid:54)=2 | f , g) . We will show Pl-Holant ((cid:54)=2 | {f }  B)
is #P-hard and it follows Pl-Holant ( (cid:54)=2 | f , g) is #P-hard.
(cid:20) 0
(cid:21)
Connect the variable x2 of gi (i = 1, 2) with the variable x1 of f using ((cid:54)=2 ) separately. We0 ab
c 0
. Note that det MIn (fti ) =
get two signatures fti with the signature matrix M (fti ) =
0 ti z ti y 0
ti x 0
0 0
ti det MIn (f ) and det MOut (fti ) = ti det MOut (f ). Connect variables x4 , x3 of f with variables x1 ,

40

M (f1 ) =

M (f2 ) =

 0
 0
 = M (f )N M (ft1 ) =
 .
x2 of ft1 both using ( (cid:54)=2 ). We get a signature f1 with the signature matrix
a20
a10
t1 by + c2b1
c10
t1 bz + bct1z 2 + ybz1 y10
t1yz + yct1x20x10We rst show that there is a t1 (cid:54)= 0 such that b1y1 c1z1 (cid:54)= 0 and (b1z )(y1 c)  (c1 b)(z1y) (cid:54)= 0.
Consider the quadratic function p(t) = (tbz + bc)(tyz + yc)cz  (tby + c2 )(tz 2 + yb)by . Then, we
have p(t1 ) = (b1z )(y1 c)  (c1 b)(z1y). Notice that the coecient of the quadratic term in p(t) is
byz 2 (cz  by). It is not equal to zero since byz 2 (cid:54)= 0 and cz  by (cid:54)= 0. That is, p(t) has degree 2, and
hence it has at most two roots. Also, b1y1 = 0 implies t1 =  c
z , c1 = 0 implies t1 =  c2
by , and z1 = 0
implies t1 =  yb
z ,  c2
z2 . Therefore we can choose such a t1 that does not take these values 0,  c
by
and  yb
z2 , and t1 is not a root of p(t). That is, t1 (cid:54)= 0, b1y1 c1z1 (cid:54)= 0 and (b1z )(y1 c)  (c1 b)(z1y) (cid:54)= 0.
Connect variables x4 , x3 of f1 with variables x1 , x2 of ft2 both using ( (cid:54)=2 ). We get a signature

 0
 = M (f1 )N M (ft2 ) =
 0
f2 with the signature matrix0
a2
a1a00c2
b2t2 b1z + c1 b
t2 b1y + c1 c0z2 y2t2z1z + y1 b
t2z1y + y1 cx2
t2x1x00Since b1z (cid:54)= 0 and c1 b (cid:54)= 0, we can let t2 =  c1 b
b1 z and t2 (cid:54)= 0. Then b2 = t2 b1z + c1 b = 0. Since
(b1z )(y1 c)  (c1 b)(z1y) (cid:54)= 0, we have y2 = t2z1y + y');
INSERT INTO posts (postId,userId,title,body) VALUES (48,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 34)','1 c (cid:54)= 0. Notice that
det MIn (f2 ) = det MIn (f1 )  (1)  det MIn (ft2 )
= det MIn (f )  (1)  det MIn (ft1 )  (1)  det MIn (ft2 )
= t1 t2 det MIn (f )3
(cid:54)= 0.
(cid:34) 0 0 0 a2
(cid:35)
We have det MIn (f2 ) = b2y2  c2z2 = c2z2 (cid:54)= 0. Similarly, we have det MOut (f2 ) = a2x2 =
t1 t2 det MOut (f )3 (cid:54)= 0. Therefore, M (f2 ) is of the form
, where a2x2y2 c2z2 (cid:54)= 0. That
0 0 c2 0
0 z2 y2 0
x2 0 0 0
is, f2 is a signature in Case II. If f2 / M , then Pl-Holant ((cid:54)=2 | f2 ) is #P-hard by Theorem 5.2, and
hence Pl-Holant ( (cid:54)=2 | {f }  B) is #P-hard.
det MIn (f )3
det MOut (f )3 = 1. Since f / M ,
Otherwise, f2  M , which means
det MIn (f2 )
= 1. That is
det MOut (f2 )
det MIn (f )7
det MOut (f )7 (cid:54)= 1. Similar to the construction of f1 , we construct f3 .
(cid:54)= 1, and hence
det MIn (f )
(cid:35)
(cid:34) 0
det MOut (f )
First, connect the variable x2 of g3 with the variable x1 of f1 using ( (cid:54)=2 ). We get a signature f1t3
0 a10
b1
c1. Note that det MIn (f1t3 ) = t3 det MIn (f1 )
with the signature matrix M (f1t3 ) =
t3 z1 t3 y1 0t3 x10and det MOut (f1t3 ) = t3 det MOut (f1 ). Then connect variables x4 , x3 of f1 with variables x1 , x2 of

41

M (f3 ) =

 0
 0
 = M (f1 )N M (f1t3 ) =
 .
f1t3 both using ( (cid:54)=2 ). We get a signature f3 with the signature matrix
a20
a30
t3 b1y1 + c2b3
c30
t3 b1z1 + b1 c11
t3z 2z3 y30
t3y1z1 + y1 c1
1 + y1 b1t3x20x301
and t3 (cid:54)= 0. Then b3 = b1 (t3z1 + c1 ) = 0 and
Since c1 (cid:54)= 0 and z1 (cid:54)= 0, we can dene t3 =  c1
z1
y3 = y1 (t3z1 + c1 ) = 0. Notice that
det MIn (f3 ) = det MIn (f1 )  (1)  det MIn (f1t3 )
=  det MIn (f1 )  t3 det MIn (f1 )
= t3 (det MIn (f )  (1)  det MIn (ft1 ))2
= t3 t2
1 det MIn (f )4
(cid:54)= 0
(cid:34) 0 0 0 a3
(cid:35)
We have det MIn (f3 ) = c3z3 (cid:54)= 0 and similarly, det MOut (f3 ) = a3x3 = t3 t2
1 det MOut (f )4 (cid:54)= 0.
where a3x3 c3z3 (cid:54)= 0.
0 0 c3 0
That is, M (f3 ) is of the form
0 z3 0 0
x3 0 0 0
Connect variables x4 , x3 of f2 with variables x1 , x2 of f3 both using ( (cid:54)=2 ). We get a signature
 .
 0
 0
 = M (f2 )N M (f3 ) =
f4 with the signature matrix00
x4
x2x3
Clearly, f4 is a signature in Case II. Also, notice that
det MIn (f4 ) = det MIn (f2 )  (1)  det MIn (f3 )
= t1 t2 det MIn (f )3  t3 t2
1 det MIn (f )4
= t3 t2 t3
1 det MIn (f )7 .
0
c2 c3z2z3 y2 c30

a2a30
M (f4 ) =
0
b4
c4
z4 y40

a40
and

det MOut (f4 ) = t3 t2 t3
1 det MOut (f )7 .

We have

det MIn (f )7
det MOut (f )7 (cid:54)= 1,
det MIn (f4 )det MOut (f4 )
which means f4 / M . By Theorem 5.2, Pl-Holant ( (cid:54)=2 | f4 ) is #P-hard, and hence Pl-Holant ((cid:54)=2 | {f }  B)
is #P-hard.

Lemma 6.4. Let g = (0, 1, t, 0)T be a binary signature where t is an n-th primitive root of unity,
and n  5. Then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.

42

=

Proof. Note that Mx1 ,x2 (g) = [ 0 1
t 0 ]. Connect the variable x2 of a copy of signature g with the
(cid:21) (cid:20)0 1
(cid:21) (cid:20)0 1
(cid:20)0 1
(cid:21)
(cid:20) 0
(cid:21)
variable x1 of another copy of signature g using ( (cid:54)=2 ). We get a signature g2 with the signature
matrixMx1 ,x2 (g2 ) =
t2 0
t 0
1 0
t 0
That is, g2 = (0, 1, t2 , 0)T . Similarly, we can construct gi = (0, 1, ti , 0)T for 1 (cid:54) i (cid:54) 5. Here, g1
denotes g . Since the order n (cid:62) 5, gi are all distinct.
Connect variables x4 , x3 of signature f with variables x1 , x2 of gi for 1 (cid:54) i (cid:54) 5 respectively.
 0
 =
 0

 .
0 0 0 a
We get binary signatures hi , where
b + cti0 b
c 0
hi = Mx1 x2 ,x4 x3 (f )gi =
ti
z + yti
0 z y 0
(cid:3) = by  cz (cid:54)= 0, (z) is a Mobius transformation of the extended
. Since det (cid:2) b c
x 0 0 00
z + yz
complex plane (cid:98)C. We rewrite hi in the form of (b + cti )(0, 1, (ti ), 0)T , with the understanding that
Let (z) =
z y
b + cz
if b + cti = 0, then (ti ) = , and we dene (b + cti )(0, 1, (ti ), 0)T to be (0, 1, z + yti , 0)T . If there
is a ti such that (ti ) is not a root of unity, and (ti ) (cid:54');
INSERT INTO posts (postId,userId,title,body) VALUES (49,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 35)',')= 0 and (ti ) (cid:54)= , by Lemma 6.3, we have
0,  or a root of unity for 1 (cid:54) i (cid:54) 5. Since (z) is a bijection of (cid:98)C, there is at most one ti such
Pl-Holant ((cid:54)=2 | f , hi ) is #P-hard, and hence Pl-Holant ( (cid:54)=2 | f , g1 ) is #P-hard. Otherwise, (ti ) is
that (ti ) = 0 and at most one ti such that (ti ) = . That means, there are at least three ti such
that |(ti )| = 1. Since a Mobius transformation is determined by any 3 distinct points, mapping
3 distinct points from S 1 to S 1 implies that this (z) maps S 1 homeomorphically onto S 1 . Such a
Mobius transformation has a special form: M(, ei ) = ei (z + )
, where || (cid:54)= 1.
(cid:20) 0 0 0 y
(cid:21)
1 + z
By normalization in signature f , we may assume b = 1. Compare the coecients, we have
c = , y = ei and z = ei . Here  (cid:54)= 0 due to z (cid:54)= 0. Also, since Mx2 x3 ,x1 x4 (f ) =
0 a z 0
0 c x 0
b 0 0 0
c x ] = ax  cz (cid:54)= 0, we have another Mobius transformation (z) =
c + xz
det [ a z
. Plug in c =  and
a + z z
z = ei , we have

a + x

a z
 + xz
a + ei z
1 + ei
a z
By the same proof for (z), we get Pl-Holant ( (cid:54)=2 | f , g) is #P-hard, unless (z) also maps S 1 to
S 1 . Hence, we can assume (z) has the form M( , ei (cid:48)
, where | | (cid:54)= 1. Compare the
) = ei (cid:48) (z +  )

1 +  z
coecients, we have
ei
= = ei(cid:48)
= ei(cid:48)a
 . Let  = 
ei and x = 
Solving this equation, we get a = 
, and we have a =  ei and


x =  , where | | (cid:54)= || since | | (cid:54)= 1 and  (cid:54)= 0 since x (cid:54)= 0. Then, we have signature matrices

(z) =

and

.



43

(cid:35)

(cid:35)

(cid:35)

0

1

0

0

0

M (f1 ) =

Mx1 x2 ,x4 x3 (f ) =

, Mx3 x4 ,x2 x1 (f ) =

(1 + 2 )ei
( + )
0
(1 + 2 )ei
( + )

(cid:34) 0
(cid:34) 0
(cid:34) 00 
0  ei
ei0(cid:35)
(cid:34) 0 0
ei  0
0  ei ei 00
0
, Mx2 x3 ,x1 x4 (f ) =
0 ei 1 0
0 ei ei20

 ei
00
0 0000
0  ei 0
. Connect variables x4 , x3 of a copy of signature f with variables
and Mx4 x1 ,x3 x2 (f ) =
0   ei 0
 .
 0
ei 0
x3 , x4 of another copy of signature f using ( (cid:54)=2 ). We get a signature f1 with the signature matrix0
  ei0
( + )ei
1 + 20
M (f1 ) = Mx1 x2 ,x4 x3 (f )N Mx3 x4 ,x2 x1 (f ) =
( + )ei
(1 + 2 )ei20
  ei0
 .
 If  +  (cid:54)= 0, normalizing Mx1 x2 ,x4 x3 (f1 ) by dividing by ( + )ei , we have( + )( + )
(1 + 2 )ei
(1 + 2 )ei
(1 + 2 )ei
, and then  =
are conjugates. Let  =
and
Note that
( + )
( + )
( + )
(1 + 2 )ei
(cid:3) . Notice
due to ||2 (cid:54)= 1. Consider the inner matrix of M (f1 ), we have MIn (f1 ) = (cid:2) 1 
(1 + 2 )(1 + 2 )
(cid:54)= 1 due to det MIn (f1 ) (cid:54)= 0, and  (cid:54)= 0
. We have | |2 =   =
(cid:12)(cid:12)(cid:12) 1| |
(cid:12)(cid:12)(cid:12) (cid:54)= 1, which
( + )2
( + )
 1
that the two eigenvalues of MIn (f1 ) are 1 + | | and 1  | |, and obviously
1+| |
means there is no integer n and complex number C such that M n
In (f1 ) = C I . Note that
is a Mobius transformation of the form M(, 1) mapping S 1 to S 1 .
 + z
1 + z
Connect variables x4 , x3 of signature f1 with variables x1 , x2 of signatures gi . We get binary
 .
 0
 = (1 +  ti )
 0
 =
 0
0 0 0 

signatures g(i,1 ) , where
0 1  0
1 +  ti1
ti
 + ti
1 (ti )
0  1 0
 0 0 00Since 1 is a Mobius transformation mapping S 1 to S 1 and |ti | = 1, we have |1 (ti )| = 1,
which means 1 +  ti (cid:54)= 0. Hence, g(i,1 ) can be normalized as (0, 1, 1 (ti ), 1)T . Successively
1 ) by connecting f1 with g(i,n1
) . We have
construct binary signatures g(i,n(cid:0)1 + k
1 (ti )(cid:1). We know C(i,n) (cid:54)= 0, beca');
INSERT INTO posts (postId,userId,title,body) VALUES (50,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 36)','use for any k , 1+ k1
where C(i,n) = (cid:81)
) = M n (f1 )gi = C(i,n) (0, 1, n
1 (ti ), 1)T ,
1 ) = M (f1 )g(i,n1
g(i,n1
0(cid:54)k(cid:54)n1
| + k1
(ti )|
due to |k
1 (ti )| =1 ) can be normalized as (0, 1, n
1 (ti ), 1)T .
(ti )| = 1. Hence, g(i,n
|1 + k1
g(i,1 ) = Mx1 x2 ,x4 x3 (f1 )gi =

(ti ) (cid:54)= 0

1 (z) =

44

.

(cid:18) 1
(cid:18) 1
(cid:19)
(cid:19)
Notice that the nonzero entries (1, n
1 (ti ))T of g(i,n
1 ) are completely decided by the inner
matrix MIn (f1 ). That is
M n
In (f1 )
= C(i,n)
ti
1 (ti )If for each i  {1, 2, 3}, there is some ni (cid:62) 1 such that (1, ni
1 (ti ))T = (1, ti )T , then n0
1 (ti ) = ti ,
where n0 = n1n2n3 for 1 (cid:54) i (cid:54) 3, i.e., the Mobius transformation n0
1 xes three distinct
complex numbers t, t2 , t3 . So the Mobius transformation is the identity map, i.e., n0
1 (z) = z
for all z  C. This implies that M n0
In (f1 ) = C [ 1 0
0 1 ] for some constant C . This contradicts the
fact that the ratio of the eigenvalues of MIn is not a root of unity. Therefore, there is an i such
1 (ti ))T are all distinct for n  N. Then, we can realize polynomially many distinct
that (1, n
1 (ti ), 1)T . By Lemma 2.6, we have Pl-Holant((cid:54)=2 | f , g) is
binary signatures of the form (0, 1, n
#P-hard.
 Otherwise  +  = 0, which means  is a pure imaginary number. Suppose  = mi, where
m  R and |m| (cid:54)= 0 or 1. Connect variables x1 , x4 of a copy of signature f with variables x4 ,
x1 of another copy of signature f , we get a signature f2 with the signature matrix

 0

0 0 0 1
M (f2 ) = Mx2 x3 ,x1 x4 (f )N Mx4 x1 ,x3 x2 (f )
ei0 ei miei
0 0 1 00
0 mi
0 mi
 0
 .
0 1 0 0
ei
1 0 0 000ei0
( 2  m2 )ei2
( +  )miei0
( +  )miei
 2  m20
ei0
 .
 If  +  (cid:54)= 0, normalizing M (f2 ) by dividing by ( +  )miei , we have
( 2  m2 )ei
( +  )mi( 2  m2 )ei
( +  )mi( 2  m2 )ei
( 2  m2 )ei
( 2  m2 )ei
( +  )mi
( +  )mi
( +  )mi
Note that
, and
are conjugates. Let  =
and
then | | (cid:54)= 1 due to det MIn (f2 ) (cid:54)= 0, and  (cid:54)= 0 due to | | (cid:54)= |m|. Same as the analysis
of MIn (f1 ), the ratio of the two eigenvalues of MIn (f2 ) is also not equal to 1, which
means there is no integer n and complex number C such that M n
In (f2 ) = C I . Notice
is also a Mobius transformation of the form M( , 1) mapping S 1
 + w
1 +  w
to S 1 . Similarly, we can realize polynomially many distinct binary signatures, and hence
Pl-Holant( (cid:54)=2 | f , g) is #P-hard.
 Otherwise,  +  = 0, which means  is a real number. Suppose  = n, where n  R
and |n| (cid:54)= 0 or |m|. Connect variables x4 , x3 of a copy of signature f with variables x1 ,
1
miei 0
 ei0
that 2 (w) =

=

MIn (f2 ) =

45

=

M (f (cid:48) ) =

nei0
0
mi
ei
x2 of another copy of signature f , we get a signature f (cid:48) with the signature matrix

 0

0 0 0 1

 0
M (f (cid:48) ) = Mx1 x2 ,x4 x3 (f )N Mx1 x2 ,x4 x3 (f )
nei0mi
0 0 1 011 0
 .
0 miei
ei
0 miei
0 1 0 01 0 0 0000
n2ei20
ei  m2
(ei  1)mi0
(ei2  ei )mi
ei  ei2m20
(cid:20) 0 0 0 
(cid:21)
n20 If ei = 1, then M (f ) =
 1 ] . Since || (cid:54)= 1, same as the
, and MIn (f ) = [ 1 
0 1  0
0  1 0
 0 0 0
analysis of MIn (f1 ), we can realize polynomially many binary signatures, and hence
Pl-Holant( (cid:54)=2 | f , g) is #P-hard.
 .

 Otherwise ei (cid:54');
INSERT INTO posts (postId,userId,title,body) VALUES (51,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 37)',')= 1, normalizing M (f (cid:48) ) by dividing by (ei  1)mi, we have
n2
(ei  1)mi
ei  m2
1  eim2
n2ei
n2
(ei  1)mi
(ei  1)mi
(ei  1)mi
(ei  1)mi
are conjugates, and
Note that
1  eim2
(cid:34) 0
(cid:35)
n2ei
are conjugates. Let (cid:48) =
. Then M (f (cid:48) ) =
(ei  1)mi
(ei  1)mi
0  (cid:48) ei. Notice that M (f (cid:48) ) and M (f ) have the same forms. Similar to
(cid:48)00 (cid:48) ei ei (cid:48)0the construction of f2 , we can construct a signature f (cid:48)
2 using f (cid:48) instead of f . Since
 (cid:48) +  (cid:48) =  n2ei
=  n2
n2
(cid:54)= 0, by the analysis of f2 , we can
(ei  1)mi
(ei  1)mimi
still realize polynomially many binary signatures and hence Pl-Holant((cid:54)=2 | f , g) is
#P-hard.
Remark: The order n (cid:62) 5 promises that there are at least three points mapped to points on S 1 ,
since at most one point can be mapped to 0 and at most one can be mapped to . When the
order n is 3 or 4, if no point is mapped to 0 or , then there are still at least three points mapped
to points on S 1 . So, we have the following corollary.
ei  m2
(ei  1)mi
ei
1  eim2
(ei  1)mi
n2ei
(ei  1)mi
and  (cid:48) =

0

and

 ei

0

and

 ei

Corollary 6.5. Let g = (0, 1, t, 0)T be a binary signature where t is an n-th primitive root of unity,
and n = 3 or 4. Let gm denote (0, 1, tm , 0)T . For any (i, j, k , (cid:96)) which is a cyclic permutation

46

of (1, 2, 3, 4), if there is no gm such that Mxi xj ,x(cid:96) xk (f )gm = d1 (0, 1, 0, 0)T or d2 (0, 0, 1, 0)T , where
d1 , d2  C , then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.
Lemma 6.6. Let g = (0, 1, 0, 0)T be a binary signature. Then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.

Proof. Connect variables x4 , x3 of the signature f with variables x2 and x1 of g both using
((cid:54)=2 ). We get a binary signature g1 , where

g1 = Mx1 x2 ,x4 x3 (f )(0, 1, 0, 0)T = (0, 1, z , 0)T .
Note that g1 (x1 , x2 ) can be normalized as (0, z1 , 1, 0)T since z (cid:54)= 0. That is, g1 (x2 , x1 ) =
(cid:34) 0
(cid:35)
(cid:20) 0 0 0 a
(cid:21)
(0, 1, z1 , 0). Then connect the variable x1 of g1 with the variable x1 of f . We get a signature0, where x1y1 (cid:54)= 0.10
0 1 c 0
1 yz1 0
f1 with the signature matrix M (f1 ) =
denoted by
0 1 y1 0xz1 0
x1 0 0 00
 If c = 0, connect variables x4 , x3 of f1 with variables x1 , x2 of g both using ( (cid:54)=2 ). We get a
binary signature h1 , where

h1 = Mx1 x2 ,x4 x3 (f1 )(0, 0, 1, 0)T = (0, 1, y1 , 0)T .
Also, connect the variable x4 with x3 of f1 using ( (cid:54)=2 ). We get a binary signature h2 , where

h2 = Mx1 x2 ,x4 x3 (f1 )(0, 1, 1, 0)T = (0, 2, y1 , 0)T .
2 , 0)T . Clearly, |y1 | (cid:54)= | y1
2 |, so they can not both be
Note that h2 can be normalized as (0, 1, y1
roots of unity. By Lemma 6.3, Pl-Holant ( (cid:54)=2 | f , h1 , h2 ) is #P-hard, and hence Pl-Holant ((cid:54)=2 | f , g)
is #P-hard.
 Otherwise c (cid:54)= 0. Connect variables x2 , x1 of g with variables x1 , x2 of f both using ((cid:54)=2 ).
We get a binary signature g2 , where

g2 = ((0, 1, 0, 0)Mx1 x2 ,x4 x3 (f1 ))T = (0, 1, c, 0)T .
Note that g2 (x1 , x2 ) can be normalized as (0, c1 , 1, 0)T since c (cid:54)= 0. That is, g2 (x2 , x1 ) =
(cid:34) 0
(cid:35)
(cid:20) 0 0 0 a2
(cid:21)
(0, 1, c1 , 0)T . Then connect the variable x1 of g2 with the variable x3 of f . We get a sign');
INSERT INTO posts (postId,userId,title,body) VALUES (52,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 38)','ature
ac10
(cid:20) 0 0 0 y2
(cid:21)01
0 1 1 0
1 yz1 c1
0 1 y2 00
xz1 0
x2 0 0 00
a2x2y2 (cid:54)= 0. Notice that Mx2 x3 ,x1 x4 (f2 ) =
0 a2 1 0
. Connect variables x1 , x4 of signature
0 1 x2 0
f2 with variables x2 , x1 of g both using ( (cid:54)=2 ). We get a binary signature h3 , where
1 0 0 0

f2 with the signature matrix M (f2 ) =

denoted by

, where

h3 = Mx2 x3 ,x1 x4 (f2 )(0, 1, 0, 0)T = (0, a2 , 1, 0)T .

h3 can be normalized as (0, 1, 1
, 0)T . Also connect variables x1 , x4 of signature f2 with
variables x1 , x2 of g both using ( (cid:54)=2 ). We get a binary signature h4 , where
a2

h4 = Mx2 x3 ,x1 x4 (f2 )(0, 0, 1, 0)T = (0, 1, x2 , 0)T .
If |a2 | (cid:54)= 1 or |x2 | (cid:54)= 1, then a2 or x2 is not a root of unity. By Lemma 6.3, Pl-Holant ((cid:54)=2 | f , h3 , h4 )
is #P-hard, and hence Pl-Holant ( (cid:54)=2 | f , g) is #P-hard. Otherwise, |a2 | = |x2 | = 1. Same as

47

1 and h(cid:48)
the construction of h1 and h2 , construct binary signatures h(cid:48)
2 using f2 instead of f1 .
We get
h(cid:48)
1 = Mx1 x2 ,x4 x3 (f2 )(0, 0, 1, 0)T = (0, 1, y2 , 0)T ,

and

h(cid:48)
2 = Mx1 x2 ,x4 x3 (f2 )(0, 1, 1, 0)T = (0, 2, 1 + y2 , 0)T .
Note that h(cid:48)
2 can be normalized as (0, 1, 1+y2
, 0)T .
 If y2 is not a root of unity, then by Lemma 6.3, Pl-Holant ( (cid:54)=2 | f , h(cid:48)1 ) is #P-hard, and
hence Pl-Holant ( (cid:54)=2 | f , g) is #P-hard.
 If y2 is an n-th primitive root of unity and n (cid:62) 5, then by Lemma 6.4, Pl-Holant ((cid:54)=2 | f , h(cid:48)
1 )
is #P-hard, and hence Pl-Holant ((cid:54)=2 | f , g) is #P-hard.

), 2 or i, then 0 < | 1+y2
| < 1, which means it is not zero
 If y2 = ( = 1+
3i
neither a root of unity. By Lemma 6.3, Pl-Holant ( (cid:54)=2 | f , h(cid:48)2
2 ) is #P-hard, and hence
Pl-Holant ((cid:54)=2 | f , g) is #P-hard.
 If y2 = 1, then f2 is non-singular redudant and hence Pl-Holant ( (cid:54)=2 | f , g) is #P-hard.
 0
 .
 If y2 = 1. Connect two copies of f2 , we get a signature f3 with the signature matrix
0 0 a202 00 2x2
2 0 02 | = 1 (cid:54)= 4. Therefore, {a2
Since |a2 | = |x2 | = 1, |a2
2 , 2} does not belong to
2x2
2 , 2, x2
case (i) or case (ii) in Theorem 4.6. Hence, Pl-Holant ((cid:54)=2 | f3 ) is #P-hard, and hence
Pl-Holant ((cid:54)=2 | f , g) is #P-hard.
Combine Lemma 6.4, Corollary 6.5 and Lemma 6.6. We have the following corollary.

M (f3 ) = Mx1 x2 ,x4 x3 (f2 )N Mx1 x2 ,x4 x3 (f2 ) =

Corollary 6.7. Let g = (0, 1, t, 0)T be a binary signature where t is an n-th primitive root of unity,
and n (cid:62) 3. Then Pl-Holant( (cid:54)=2 | f , g) is #P-hard.
Now, we are able to prove the following theorem for Case IV.
0 0 0 aTheorem 6.8. Let f be a 4-ary signature with the signature matrix
0 b
c 0
0 z y 0
x 0 0 0
where abxyz (cid:54)= 0. Pl-Holant( (cid:54)=2 | f ) is #P-hard unless f  M , in which case, Pl-Holant( (cid:54)=2 | f ) is
tractable.

M (f ) =

Proof. Tractability follows by 2.15.
Now suppose f / M . Connect the variable x4 with x3 of f using ((cid:54)=2 ), and we get a binary
signature g1 , where

g1 = Mx1 x2 ,x4 x3 (0, 1, 1, 0)T = (0, b + c, z + y , 0)T .
Connect the variable x1 with x2 of f using ( (cid:54)=2 ), and we get a binary signature g2 , where
g2 = ((0, 1, 1, 0)Mx1 x2 ,x4 x3 )T = (0, b + z , c + y , 0)T .

48

 If one of g1 and g2 is of the form (0, 0, 0, 0), then by = (c)(z ) = cz . That is by  cz = 0.
Here c (cid:54)= 0 due to by (cid:54)= 0. By Lemma 6.2, Pl-Holant( (cid:54)=2 | f ) is #P-hard.
 If one of g1 and g2 can be normalized as (0, 1, 0');
INSERT INTO posts (postId,userId,title,body) VALUES (53,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 39)',', 0) or (0, 0, 1, 0). By Lemma 6.6, Pl-Holant( (cid:54)=2 |
f ) is #P-hard.
 If one of g1 and g2 can be normalized as (0, 1, t, 0)T , where t (cid:54)= 0 is not a root of unity, then
by Lemma 6.3, Pl-Holant( (cid:54)=2 | f ) is #P-hard.
 If one of g1 and g2 can be normalized as (0, 1, t, 0)T , where t is an n-th primitive root of unity
and n (cid:62) 3, then by Corollary 6.7, Pl-Holant((cid:54)=2 | f ) is #P-hard.
 Otherwise, g1 and g2 do not belong to those cases above, which means both g1 and g2 both
can be normalized as (0, 1, 1 , 0) and (0, 1, 2 , 0), where 1 = 1 and 2 = 1. That is,
b + c = 1 (z + y) (cid:54)= 0 and b + z = 2 (c + y) (cid:54)= 0.
 If b + c = z + y and b + z = c + y , then b = y and c = z . To be proved below.
 If b + c = (z + y) and b + z = c + y , then b + z = c + y = 0. Contradiction.
 If b + c = z + y and b + z = (c + y), then b + c = z + y = 0. Contradiction.
 If b + c = (z + y) and b + z = (c + y), we have g1 = (0, 1, 1, 0)T . Connect the
(cid:21)
(cid:20) 0
variable x2 of g1 with the variable x1 of f , and we get a signature f (cid:48) with the signature
0 a. Connect the variable x1 with x2 of f (cid:48) using ((cid:54)=2 ), and we
matrix M (f (cid:48) ) =b
c 0
0 z y 0
x 0
0 0
get a binary signature g (cid:48) = (0, b  z , c  y , 0)T . Same as the analysis of g1 and g2 above,
we have Pl-Holant((cid:54)=2 | f (cid:48) ) is #P-hard unless g (cid:48) can be normalized as (0, 1, 3 , 0), where
3 = 1. That is, b  z = 3 (c  y) (cid:54)= 0,
 If b  z = c  y , combine with b + c = (z + y). We have b = y and c = z . To
be proved below.
 If b  z = (c  y), combine with b + c = (z + y). We have b + c = z + y = 0.
Contradiction.
Therefore, Pl-Holant( (cid:54)=2 | f (cid:48) ) is #P-hard and hence Pl-Holant((cid:54)=2 | f ) is #P-hard
So far, we have Pl-Holant( (cid:54)=2 | f ) is #P-hard unless b = y and c = z , where  = 1. Similarly,
connect the variable x2 with x3 of f using ((cid:54)=2 ), and we get a binary signature g3 = (0, a +
c, z + x, 0)T . Connect the variable x1 with x4 of f using ( (cid:54)=2 ), and we get a binary signature
g4 = (0, a + z , c + x, 0)T . Same as the analysis of g1 and g2 , we have Pl-Holant((cid:54)=2 | f ) is #P-hard
unless a = (cid:48)x and c = (cid:48)z , where (cid:48) = 1. Therefore, Pl-Holant( (cid:54)=2 | f ) is #P-hard unless a = x,
b = y and c = z , where  = 1. In this case, since z (cid:54)= 0, we have abc (cid:54)= 0. By Lemma 6.1,
Pl-Holant( (cid:54)=2 | f ) is #P-hard.

49

References

[1] Lars Ahlfors. Complex Analysis, 3 ed, McGraw-Hill, 1979.
[2] Jin-Yi Cai, Xi Chen, Richard J. Lipton, and Pinyan Lu. On tractable exponential sums, in
proceedings of FAW 2010, 148-159.
[3] Jin-Yi Cai and Zhiguo Fu. A collapse theorem for holographic algorithms with matchgates on
domain size at most 4, Inf. Comput. 239 (2014), 149-169.
[4] Jin-Yi Cai and Zhiguo Fu. Holographic algorithm with matchgates is universal for planar
#CSP over boolean domain, CoRR abs/1603.07046 (2016), to appear in STOC 2017.
[5] Jin-Yi Cai, Zhiguo Fu and Mingji Xia. Complexity classication of the Six-Vertex Model,
CoRR abs/1702.02863 (2017).
[6] Jin-Yi Cai, Heng Guo and Tyson Williams. A complete dichotomy rises from the capture of
vanishing signatures, SIAM J. Comput., 45(5) (2016), 1671-1728.
[7] Jin-yi Cai, Pinyan Lu and Mingji Xia. Holant problems and counting CSP, in proceedings of
STOC 2009, 715-724.
[8] Michael Freedman, Laszlo Lovasz and Alexander Schrijver. Reection positivity, rank con-
nectivity, and homomorphism of graphs. Journal of the American Mathematical Society 20.1
(2007), 37-51.
[9] Jin-Yi Cai, Pinyan Lu and Mingji Xia. The complexity of complex weighted Boolean #CSP,
J. Comput. Syst. Sci. 80(1) (2014), 217-236.
[10] Leslie Ann Goldberg, Mark Jerrum and Mike Paterson. The computational complexity of
two-state spin systems, Random Struct. Algorithms 23(2) (2003), 133-154.
[11] Leslie Ann Goldberg and Mark Jerrum. Approximating the partition function');
INSERT INTO posts (postId,userId,title,body) VALUES (54,7688,'A Complexity Trichotomy for the Six-Vertex Model (part 40)',' of the ferro-
magnetic potts model, J. ACM 59(5) (2012), 25.
[12] Heng Guo and Tyson Williams. The complexity of planar boolean #CSP with complex
weights, in proceedings of ICALP (1) 2013, 516-527.
[13] Sangxia Huang and Pinyan Lu. A dichotomy for real weighted Holant problems, Computa-
tional Complexity 25(1) (2016), 255-304.
[14] Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the Ising
Model, SIAM J. Comput. 22(5) (1993), 1087-1116.
[15] Pieter W. Kasteleyn. The statistics of Dimers on a lattice, Physica 27 (1961), 1209-1225.
[16] Pieter W. Kasteleyn. Graph theory and crystal physics, in Graph Theory and Theoretical
Physics, (F. Harary, ed.), Academic Press, London (1967), 43-110.
[17] Michel Las Vergnas. On the evaluation at (3, 3) of the Tutte polynomial of a graph, J. Comb.
Theory, Ser. B 45(3) (1988), 367-372.
[18] Liang Li, Pinyan Lu and Yitong Yin. Correlation decay up to uniqueness in Spin systems, in
proceedings of SODA 2013, 67-84.
[19] Elliott H. Lieb. Residual entropy of square ice, Physical Review 162 (1) (1967), 162-172.
[20] Linus C. Pauling. The structure and entropy of ice and of other crystals with some randomness
of atomic arrangement, Journal of the American Chemical Society 57 (12) (1935), 2680-2684.
[21] Harold N. V. Temperley and Michael E. Fisher. Dimer problem in statistical mechanics - an
exact result, Philosophical Magazine 6 (1961), 1061- 1063.
[22] Leslie G. Valiant. The Complexity of computing the permanent, Theor. Comput. Sci. 8 (1979),
189-201.

50

[23] Leslie G. Valiant. Quantum circuits that can be simulated classically in polynomial time,
SIAM J. Comput. 31(4) (2002), 1229-1254.
[24] Leslie G. Valiant. Expressiveness of matchgates. Theor. Comput, Sci. 289(1) (2002), 457-471.
[25] Leslie G. Valiant. Holographic algorithms, SIAM J. Comput. 37(5) (2008), 1565-1594.

51

');
INSERT INTO posts (postId,userId,title,body) VALUES (55,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction','Hao Gao

School of Mathematics and Statistics, University of Glasgow, UK

Liuyang Feng

School of Mathematics and Statistics, University of Glasgow, UK

Nan Qi

School of Mathematics and Statistics, University of Glasgow, UK

Colin Berry

Institute of Cardiovascular and Medical Science, University of Glasgow, UK

Boyce Grith

Departments of Mathematics and Biomedical Engineering and McAl lister Heart Institute,
University of North Carolina, Chapel Hil l, NC, USA

School of Mathematics and Statistics, University of Glasgow, UK

Xiaoyu Luo

Abstract

Understanding the interaction between the valves and walls of the heart is im-
portant in assessing and subsequently treating heart dysfunction. With ad-
vancements in cardiac imaging, nonlinear mechanics and computational tech-
niques, it is now possible to explore the mechanics of valve-heart interactions
using anatomically and physiologically realistic models. This study presents an
integrated model of the mitral valve (MV) coupled to the left ventricle (LV),
with the geometry derived from in vivo clinical magnetic resonance images.
Numerical simulations using this coupled MV-LV model are developed using an
immersed boundary/nite element method. The model incorporates detailed
valvular features, left ventricular contraction, nonlinear soft tissue mechanics,
and uid-mediated interactions between the MV and LV wall. We use the model
to simulate the cardiac function from diastole to systole, and investigate how
myocardial active relaxation function aects the LV pump function. The results
of the new model agree with in vivo measurements, and demonstrate that the
diastolic lling pressure increases signicantly with impaired myocardial active
relaxation to maintain the normal cardiac output. The coupled model has the

Preprint submitted to Journal of Medical Engineering '||'&'||' Physics

April 7, 2017

potential to advance fundamental knowledge of mechanisms underlying MV-LV
interaction, and help in risk stratication and optimization of therapies for heart
diseases.

Keywords: mitral valve, left ventricle, uid structure interaction, immersed
boundary method, nite element method, soft tissue mechanics

1. Introduction

The mitral valve (MV) has a complex structure that includes two distinct
asymmetric leaets, a mitral annulus, and chordal tendinae that connect the
leaets to papillary muscles that attach to the wall of the left ventricle (LV).
MV dysfunction remains a ma jor medical problem because of its close link to
cardiac dysfunctions leading to morbidity and premature mortality [1].
Computational modelling for understanding the MV mechanics promises
more eective MV repairs and replacement [2, 3, 4, 5]. Biomechanical MV
models have been developed for several decades, starting from the simplied two-
dimensional approximation to three-dimensional models, and to multi-physics/-
scale models [6, 7, 8, 9, 10, 11, 12]. Most of previous studies were based on
structural and quasi-static analysis applicable to a closed valve [13]; however,
MV function during the cardiac cycle cannot be fully assessed without modelling
the ventricular dynamics and the uid-structure interaction (FSI) between the
MV, ventricles, and the blood ow [13, 14].
Because of the complex interactions among the MV, the sub-mitral ap-
paratus, the heart walls, and the associated blood ow, few modelling stud-
ies have been carried out that integrate the MV and ventricles in a single
model [15, 16, 17]. Kunzelman, Einstein, and co-workers rst simulated normal
and pathological mitral function [18, 19, 20] with FSI using LS-DYNA (Liver-
more Software Technology Corporation, Livermore,');
INSERT INTO posts (postId,userId,title,body) VALUES (56,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 2)',' CA, USA) by putting the
MV into a straight tube. Using similar modelling approach, Lau et al. [21] com-
pared MV dynamics with and without FSI, and they found that valvular closure
conguration is dierent when using the FSI MV model. Similar ndings are
reported by Toma et al [22]. Over the last few years, there have also been a
number of FSI valvular models using the immersed boundary (IB) method to
study the ow across the MV [23, 24, 25]. In a series of studies, Toma [26, 22, 27]
developed a FSI MV model based on in vitro MV experimental system to study
the function of the chordal structure, and good agreement was found between
the computational model and in vitro experimental measurements. However,
none of the aforementioned MV models accounted for the MV interaction with
the LV dynamics. Indeed, Lau et al. [21] found that even with a xed U-shaped
ventricle, the ow pattern is substantially dierent from that estimated using
a tubular geometry. Despite the advancements in computational modelling of
individual MV [13, 12] and LV models [28, 29, 30], it remains challenging to de-
velop an integrated MV-LV model which includes the strong coupling between
the valvular deformation and the blood ow. Reasons for this include limited
data for model construction, dicult choices of boundary conditions, and large
computational resources required by these simulations.
Wenk et al.
[15] reported a structure-only MV-LV model using LS-DYNA
that included the LV, MV, and chordae tendineae. This model was later ex-
tended to study MV stress distributions using a saddle shaped and asymmetric
mitral annuloplasty ring [16]. A more complete whole-heart model was recently
developed using a human cardiac function simulator in the Dassault Systemess
Living Heart pro ject [17], which includes four ventricular chambers, cardiac

valves, electrophysiology, and detailed myobre and collagen architecture. Us-
ing the same simulator, eects of dierent mitral annulus ring were studied by
Rausch et al. [31]. However, this simulator does not yet account for detailed
FSI.
The earliest valve-heart coupling model that includes FSI is credited to Pe-
skin and McQueens pioneering work in the 1970s [32, 33, 34] using the classical
IB approach [35]. Using this same method, Yin et al. [36] investigated uid vor-
tices associated with the LV motion as a prescribed moving boundary. Recently,
Chandran and Kim [37] reported a prototype FSI MV dynamics in a simplied
LV chamber model during diastolic lling using an immersed interface-like ap-
proach. One of the key limitations of these coupled models is the simplied
representation of the biomechanics of the LV wall. To date, there has been no
work reported a coupled MV-LV model which has full FSI and based on realistic
geometry and experimentally-based models of soft tissue mechanics.
This study reports an integrated MV-LV model with FSI derived from in vivo
images of a healthy volunteer. Although some simplications are made, this is
the rst three-dimensional FSI MV-LV model that includes MV dynamics, LV
contraction, and experimentally constrained descriptions of nonlinear soft tissue
mechanics. This work is built on our previous models of the MV [24, 25] and
LV [38, 29]. The model is implemented using a hybrid immersed boundary
method with nite element elasticity (IB/FE) [39].

2. Methodology

2.1. IB/FE Framework

The coupled MV-LV model employs an Eulerian description for the bl');
INSERT INTO posts (postId,userId,title,body) VALUES (57,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 3)','ood,
which is modelled as a viscous incompressible uid, along with a Lagrangian
description for the structure immersed in the uid. The xed physical coordi-
nates are x = (x1 , x2 , x3 )  , and the Lagrangian reference coordinate system
is X = (X1 , X2 , X3 )  U . The exterior unit normal along U is N(X). Let
(X, t) denote the physical position of any material point X at time t, so that
(U, t) = s (t) is the physical region occupied by the immersed structure. The
(cid:19)
(cid:18) u
IB/FE formulation of the FSI system reads
(x, t) + u(x, t)  u(x, t)
= p(x, t) + 2u(x, t) + f s (x, t),
(cid:90)
 t
  u(x, t) = 0,
(cid:90)
(cid:90)

Ps (X, t) N(X) (x  (X, t)) dA(X),
(3)

  Ps (X, t) (x  (X, t)) dX

u(x, t) (x  (X, t)) dx,


 t

(X, t) =

(1)

(2)

(4)



f s (x, t) =

where  is the uid density,  is the uid viscosity, u is the Eulerian velocity, p is
the Eulerian pressure, and f s is the Eulerian elastic force density. Dierent from
the classical IB approach [35], here the elastic force density f s is determined from
the rst Piola-Kircho stress tensor of the immersed structure Ps as in Eq. 3.
This allows the solid deformations to be described using nonlinear soft tissue
constitutive laws. Interactions between the Lagrangian and Eulerian elds are
achieved by integral transforms with a Dirac delta function kernel (x) [35] in
Eqs. 3 4. For more details of the hybrid IB/FE framework, please refer to [39].

2.2. MV-LV Model Construction
A cardiac magnetic resonance (CMR) study was performed on a healthy
volunteer (male, age 28). The study was approved by the local NHS Research
Ethics Committee, and written informed consent was obtained before the CMR
scan. Twelve imaging planes along the LV outow tract (LVOT) view were
imaged to cover the whole MV region shown in Fig. 1(a). LV geometry and
function was imaged with conventional short-axis and long-axis cine images.
The parameters for the LVOT MV cine images were: slice thickness: 3 mm with
0 gap, in-plane pixel size: 0.70.7 mm2 , eld of view: 302  400 mm2 , frame
rate: 25 per cardiac cycle. Short-axis cine images covered the LV region from
the basal plane to the apex, with slice thickness: 7 mm with 3 mm gap, in-plane
pixel size: 1.3  1.3 mm2 , and frame rate: 25 per cardiac cycle.
The MV geometry was reconstructed from LVOT MV cine images at early-
diastole, just after the MV opens. The leaet boundaries were manually delin-
eated from MR images, as shown in Fig. 1(a), in which the heads of papillary
muscle and the annulus ring were identied as shown in Fig. 1(b). The MV
geometry and its sub-valvular apparatus were reconstructed using SolidWorks
(Dassault Systmes SolidWorks Corporation, Waltham, MA, USA). Because it
is dicult to see the chordal structural in the CMR, we modelled the chordae
structure using sixteen evenly distributed chordae tendineae running through
the leaet free edges to the annulus ring, as shown in Fig. 1(c), following prior
studies [25, 24]. In a similar approach to the MV reconstruction, the LV geom-
etry was reconstructed from the same volunteer at early-diastole by using both
the short-axis and long-axis cine images [40, 29]. Fig. 1(d) shows the inow and
outow tracts from one MR image. The LV wall was assembled from the short
and long axis MR images (Fig. 1(e)) to form the three dimensional reconstruc-
tion (Fig. 1(f )). The LV model was divided into four regions: the LV and the
valvular region and the inow and the outow tracts, as shown in Fig. 1(g).
The MV model was mounted into the inow tract of the LV model according
to the relative p');
INSERT INTO posts (postId,userId,title,body) VALUES (58,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 4)','ositions derived from the MR images in Fig. 1(g). The left
atrium was not reconstructed but modelled as a tubular structure, the gap
between the MV annulus ring and the LV model was lled using a housing
disc structure. A three-element Windkessel model was attached to the outow
tract of the LV model to provide physiological pressure boundary conditions
when the LV is in systolic ejection [40]. The chordae were not directly attached
to the LV wall since the papillary muscles were not modelled, similar to [25].
The myocardium has a highly layered myobre architecture, which is usually

described using a bre-sheet-normal (f , s, n) system. A rule-based method was
used to construct the myobre orientation within the LV wall. The myobre
angle was assumed to rotate from -60o to 60o from endocardium to epicardium,
represented by the red arrows in Fig. 1(h). In a similar way, the collagen bres
in the MV leaets were assumed to be circumferentially distributed, parallel
along the annulus ring, represented by the yellow arrows in Fig. 1(h).

2.3. Soft Tissue Mechanics

(cid:40)
The total Cauchy stress () in the coupled MV-LV system is
for x  s ,
otherwise,

 f (x, t) +  s (x, t)
 f (x, t)

(x, t) =

where  f is the uid-like stress tensor, dened as
 f (x, t) = pI + [u + (u)T ].

(5)

(6)

Ps =

 s is the solid stress tensor obtained from the nonlinear soft tissue consitutive
laws. The rst Piola-Kirchho stress tensor Ps in Eq. 3 is related to  s through
Ps = J  sFT ,
in which F = /X is the deformation gradient and J = det(F).
In the MV-LV model, we assume the structure below the LV base is contrac-
tile (Fig 1(g)), the regions above the LV basal plane, including the MV and its
(cid:40)Pp + Pa below the basal plane,
apparatuses, are passive. Namely,
Pp
above the basal plane,
where Pa and Pp are the active and passive Piola-Kirchho stress tensors, re-
spectively. The MV leaets are modelled as an incompressible bre-reinforced
material with the strain energy function
WMV = C1 (I1  3) +

av
2bv
in which I1 = trace(C) is the rst invariant of the right Cauchy-Green deforma-
0  (Cf c
tion tensor C = FT F, I c
f = f c
0 ) is the squared stretch along the collagen
bre direction, and f c
0 denotes the collagen bre orientation in the reference
conguration. The max() function ensures the embedded collagen network only
bears the loads when stretched, but not in compression. C1 , av , and bv are
material parameters adopted from a prior study [25] and listed in Table 1. The
passive stress tensor Pp in the MV leaets is
F  C1FT + s log(I3 )FT ,
WMV

f , 1)  1)2 ]  1),
(exp[bv (max(I c

Pp =

(10)

(7)

(8)

(9)

where I3 = det(C), and s is the bulk modulus for ensuring the incompressibility
of immersed solid, so that the pressure-like term C1FT ensures the elastic stress
response is zero when F = I.
We model the chordae tendineae as the Neo-Hookean material,
Wchordae = C (I1  3),

(11)

where C is the shear modulus. We further assume C is much larger in systole
when the MV is closed than in diastole when the valve is opened. The much
larger value of C models the eects of papillary muscle contraction.');
INSERT INTO posts (postId,userId,title,body) VALUES (59,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 5)',' Values of
C are listed in Table 1. Pp for the chordae tendineae is similarly derived as in
Eq. 10.
The passive response of the LV myocardium is described using the Holzapfel-
(cid:88)
Ogden model [41],
exp[b(I1  3)] +
ai
2bi
i=f ,s
{exp[bfs (I8fs )2 ]  1}

{exp[bi (max(I4i , 1)  1)2 ]  1}

Wmyo =

(12)
2b

afs
2bfs

in which a, b, af , bf , as , bs , afs , bfs are the material parameters, I4f , I4s and I8fs
are the strain invariants related to the the myobre orientations. Denoting the
myobre direction in the reference state is f 0 and the sheet direction is s0 , we
have
I4f = f 0  (Cf 0 ), I4s = s0  (Cs0 ), and I8fs = f 0  (Cs0 ).
The myocardial active stress is dened as
Pa = J T F f 0  f 0

(14)

(13)

where T is the active tension described by the myolament model of Niederer et
al. [42], using a set of ordinary dierential equations involving the intracellular
calcium transient (Ca2+ ), sarcomere length and the active tension at the resting
sarcomere length (T req ). In our simulations, we use the same parameters as in
ref. [42], except that T req is adjusted to yield realistic contraction as the imaged
volunteer.
All the constitutive parameters in Eqs.9, 11, 12 are summarized in Table 1.

2.4. Boundary Conditions and Model Implementation

Because only the myocardium below the LV basal plane contracts, we x the
LV basal plane along the circumferential and longitudinal displacements, but al-
low the radial expansion. The myocardium below the LV basal plane is left free
to move. The valvular region is assumed to be much softer than the LV region.
In diastole, a maximum displacement of 6 mm is allowed in the valvular region
using a tethering force. In systole, the valve region is gradually pulled back to
the original position. The inow and outow tracts are xed. Because the MV
annulus ring are attached to a housing structure which is xed, no additional

boundary conditions are applied to the MV annulus ring. Fluid boundary condi-
tions are applied to the top planes of the inow and outow tracts. The function
of the aortic valve is modelled simply: the aortic valve is either fully opened
or fully closed, determined by the pressure dierence between the values inside
the LV chamber and the aorta. After end-diastole, the LV region will contract
simultaneously triggered by a spatially homogeneously prescribed intracellular
Ca2+ transient [29], as shown in Fig. 3. The ow boundary conditions in a
cardiac cycle are summarized below.
 Diastolic lling: A linearly ramped pressure from 0 to a population-
based end-diastolic pressure (EDP=8 mmHg) is applied to the inow tract
over 0.8 s, which is slightly longer than the actual diastolic duration of
the imaged volunteer (0.6 s).
In diastole about 80% of diastolic lling
volume is due to the sucking eect of the left ventricle in early-diastole [43].
This negative pressure eld inside the LV cavity is due to the myocardial
relaxation. We model this sucking eect using an additional pressure
loading applied to the endocardial surface, denoted as Pendo , which is
linearly ramped from 0 to 12 mmHg over 0.4 s, and then linearly decreased
to zero at end-diast');
INSERT INTO posts (postId,userId,title,body) VALUES (60,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 6)','ole. The value of Pendo is chosen by matching the
simulated end-diastolic volume to the measured data from CMR images.
Blood ow is not allowed to move out of the LV cavity through the inow
tract in diastole. Zero ow boundary conditions are applied to the top
plane of the outow tract.
 Iso-volumetric contraction: Along the top plane of the inow tract, the
EDP loading is maintained, but we allow free uid ow in and out of the
inow tract. Zero ow boundary conditions are retained for the outow
tract. The duration of the iso-volumetric contraction is determined by
the myocardial contraction and ends when the aortic valve opens. The
aortic valve opens when the LV pressure is higher than the pressure in the
aorta, which is initially set to be the cu-measured diastolic pressure in
the brachial artery, 85 mmHg.
 Systolic ejection: When the aortic valve opens, a three-element Wind-
kessl model is coupled to the top plane of the outow tract to provide
afterload. The volumetric ow rates across the top plane of the outow
tract is calculated from the three-dimensional MV-LV model, and fed into
the Windkessel model [44], which returns an updated pressure for the out-
ow tract in the next time step. The systolic ejection phase ends when
the left ventricle cannot pump any ow through the outow tract, and the
Windkessel model is detached.
 Iso-volumetric relaxation: Zero ow boundary conditions are applied
to both the top planes of the outow and inow tracts until the total cycle
ends at 1.2 seconds.
The coupled MV-LV model is immersed in a 17cm  16cm  16cm uid
box. A basic time step size t0 = 1.22  104 s is used in the diastolic and

relaxation phases, a reduced time step size (0.25 t0 ) is used in the early systole
with a duration of 0.1 s, and an even smaller time step of 0.125 t0 is used in
the remainder of the systolic phase. Because explicit time stepping is used in
the numerical simulations [39], we need to use a time step size small enough to
avoid numerical instabilities, particularly during the systolic phase to resolve the
highly dynamic LV deformation. The MV-LV model is implemented using the
open-source IBAMR software framework (https://github.com/IBAMR/IBAMR),
which provides an adaptive and distributed-memory parallel implementation of
the IB methods.

3. Results

Fig. 2 shows the computed volumetric ow rates across the MV and the
AV from beginning of diastole to end-systole. In diastole, the volumetric ow
rate across the MV linearly increases with Pendo , with a maximum value of
210 mL/s at 0.4 s. Diastolic lling is maintained by the increased pressure in
the inow tract, but with decreased ow rates until end of diastole at 0.8 s. The
negative ow rate in Fig. 2 indicates the ow is entering the LV chamber. After
end-diastole, the myocardium starts to contract, and the central LV pressure
increases until it exceeds the aortic pressure (initially set to be 85 mmHg) at
0.857 s. During iso-volumetric contraction, the MV closes with a total closure
regurgitation ow of 7.2 mL, around 10% of the total lling volume, which is
comparable to the value reported by Laniado et al. [45]. There is only minor
regurgitation across the MV during systolic ejection after the iso-volumetric
contraction phase. Blood is then ejected out of the ventricle through the AV,
and the ow rate across the AV during systole reaches a peak value of 468 mL/s
(Fig. 2). Th');
INSERT INTO posts (postId,userId,title,body) VALUES (61,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 7)','e total ejection duration is 243 ms with a stroke volume of 63.2 mL.
The total blood ejected out of the LV chamber, including the regurgitation
across the MV, is 72.1 mL, which corresponds to an ejection fraction of 51%.
Fig. 3 shows the proles of the normalized intracellular Ca2+ , LV cavity
volume, central LV pressure, and the average myocardial active tension from
diastole to systole. Until mid-diastole (0 s to 0.56 s), the central LV pressure
is negative, and the associated diastolic lling volume is around 65 mL, which
is 90% of the total diastolic lling volume.
In late-diastole, the LV pressure
becomes positive. There is a delay between the myocardial active tension and
the intracellular Ca2+ prole, but the central LV pressure follows the active
tension closely throughout the cycle as shown in Fig 3.
Fig. 4 shows the deformed MV leaets along with the corresponding CMR
cine images at early-diastole (the reference state), end-diastole, and mid-systole.
In general, the in vivo MV and LV dynamics from diastole to systole are qual-
itatively captured well by the coupled MV-LV model. However, a discrepancy
is observed during the diastolic lling, when the MV orice in the model is
not opened as widely as in the CMR cine image (Fig. 4(b)). In addition, the
modelled MV leaets have small gaps near the commissure areas even in the
fully closure state. This is partially caused by the nite size of the regularized

delta function at the interface and uncertainties in MV geometry reconstruction
using CMR images.
Figs. 5(a, b, c, d) show the streamlines at early-diastolic lling, late-diastolic
lling, when the MV is closing (iso-volumtric contraction), and mid-systolic
ejection when the left ventricle is ejecting. During the diastolic lling (Fig. 5(a)),
the blood ows directly through the MV into the LV chamber towards the LV
apex, in late-diastole in Fig. 5(b), the ow pattern becomes highly complex.
When iso-volmeric contraction ends, the MV is pushed back towards the left
atrium. In mid-systole, the blood is pumped out of the LV chamber through
the aortic valve into the systemic circulation, forming a strong jet as shown in
Fig. 5 (d).
The LV systolic strain related to end-diastole is shown in Fig. 6 (a), which
is negative throughout most of the region except near the basal plane, where
the LV motion is articially constrained in the model. The average myocardial
strain along myobre direction is -0.1620.05. Fig. 6(b) is the bre strain in the
MV leaets at end-diastole, the leaets are mostly slightly stretched during the
diastolic lling. In systole, because of the much higher pressure in the LV, the
leaets are pushed towards the left atrium side as shown in Fig. 6(c). Near the
leaet tip and the commissiour areas, the leaets are highly compressed, while
in the trigons near the annulus ring, the leaet is stretched.
From Fig. 3, one can see that the applied endocardial pressure (Pendo ) cre-
ates a negative pressure inside the LV chamber, similar to the eects of the
myocardial active relaxation. We further investigate how Pendo aects the MV-
LV dynamics by varying its');
INSERT INTO posts (postId,userId,title,body) VALUES (62,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 8)',' value from 8 mmHg to 16 mmHg, and the eects
without Pendo but with an increased EDP from 8 mmHg to 20 mmHg. We ob-
serve that with an increased Pendo , the peak ow rate across the MV during the
lling phase becomes higher with more ejected volume through the aortic valve.
We also have a longer ejection duration, shorter iso-volumetric contraction time,
and higher ejection fraction as a result of increasing Pendo . On the other hand,
if we dont apply Pendo , a much greater and nonphysiological EDP is needed for
the required ejection fraction. For example, with EDP=8 mmHg, the ejection
fraction is only 29%. Only when EDP=20 mmHg, the pump function is compa-
rable to the case with EDP=8 mmHg and Pendo = 16 mmHg. These results are
summarized in Table 2.

4. Discussion

This study demonstrates the feasibility of integrating a MV model with a
LV model from a healthy volunteer based on in vivo CMR images. This is the
rst physiologically based MV-LV model with uid structure interaction that
includes nonlinear hyperelastic constitutive modelling of the soft tissue. The
coupled MV-LV model is used to simulate MV dynamics, LV wall deformation,
myocardial active contraction, as well as intraventricular ow. The modelling
results are in reasonable quantitative agreement with in vivo measurements
and clinical observations. For example, the peak aortic ow rate is 468 mL/s,
close to the measured peak value (498 mL/s); the ejection duration is 243 ms,

10

and the measured value is around 300 ms; the peak LV pressure is 162 mmHg,
comparable to the cu-measured peak blood pressure 150 mmHg; the average
LV systolic strain is around -0.16, which also lies in the normal range of healthy
sub jects [46].
Diastolic heart failure is usually associated with impaired myocardial relax-
ation and increased lling pressure [47, 48]. In this study, we model the eects of
myocardial relaxation by applying an endocardial surface pressure Pendo . Specif-
ically we can enhance or suppress the myocardial relaxation by adjusting Pendo .
Our results in Table 2) show that, with an enhanced myocardial relaxation,
say, when Pendo  12 mmHg, there is more lling during diastole, compared to
the cases when Pendo < 12 mmHg under the same EDP. This in turn gives rise
to higher ejection fraction and stroke volume. However, if myocardial relax-
ation is suppressed, diastolic lling is less ecient, with subsequently smaller
ejection fraction and stroke volume.
In the extreme case, when the myocar-
dial relaxation is entirely absent, chamber volume increases by only 29.5 mL,
and ejection fraction decreases to 29%. To maintain stroke volume obtained for
Pendo=12 mmHg, EDP needs to be as high as 20 mmHg. Indeed, increased EDP
due to an impaired myocardial relaxation has been reported in a clinical study
by Zile et al. [48]. A higher EDP indicates the elevated lling pressure through-
out the relling phase. Increased lling pressure can help to maintain a normal
lling volume and ejection fraction, but runs the risks of ventricular dysfunction
in the longer term, because pump failure will occur if no other compensation
mechanism exists.
During diastole, the MV-LV model seems to yield a smaller orice compared
to the corresponding CMR images.
In our previous study [25], the MV was
mounted in a rigid straight tube, the peak diastolic lling pressure is around
10 mmHg, and the peak ow rate across the MV is comparable to the measured
value (600 mL/s). While in this coupled MV-LV model, even though with ad-
ditional Pendo , the peak ow rate (200 mL/s) is much less than the measured
value. One reason is because of the extra resistance from the LV wall, which
is absent in the MV-tube model [25]. The diastolic phase can be divided into
three phasse [43]: the rapid lling');
INSERT INTO posts (postId,userId,title,body) VALUES (63,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 9)',', slow lling, and atrial contraction. During
rapid lling, the transvalvular ow is resulted from myocardial relaxation (the
sucking eect), which contributes to 80% of the total transvalvular ow volume.
During slow lling and atrial contraction, the left atrium needs to generate a
higher pressure to provide additional lling.
In the coupled MV-LV model,
the ramped pressure in the top plane of the inow tract during late-diastole is
related to the atrial contraction, and during this time, only 10% of the total
transvalvular ow occurs. However, the peak ow rate in rapid lling phase
is much lower compared to the measured value, which suggests the myocardial
relaxation would be much stronger.
In a series of studies based on in vitro CT experiments, Toma [26, 27, 22]
suggested that MV models with simplied chordal structure would not compare
well with experimental data, and that a sub ject-specic 3D chordal structure
is necessary. This may explain some of the discrepancies we observed here. A
simplied chordal structure is used in this study because we are unable to re-

11

construct the chordal structure from the CMR data. CT imaging may allow the
chordae reconstruction but it comes with radiation risk. Patient-specic chordal
structure in the coupled MV-LV model would require further improvements of
in vivo imaging techniques.
Several other limitations in the model may also contribute to the discrepan-
cies. These include the uncertainty of patient-specic parameter identication,
uncertainties in MV geometry reconstruction from CMR images, the passive
response assumption around the annulus ring and the valvular region of the LV
model, and the lack of pre-strain eects. Studies addressing these issues are
already under way. We expect that further improvement in personalized mod-
elling and more ecient high performance computing would make the modelling
more physiologically detailed yet fast enough for applications in risk stratica-
tion and optimization of therapies in heart diseases.

5. Conclusion

We have developed a rst fully coupled MV-LV model that includes uid-
structure interaction as well as experimentally constrained descriptions of the
soft tissue mechanics. The model geometry is derived from in vivo magnetic res-
onance images of a healthy volunteer. It incorporates three-dimensional nite
element representations of the MV leaets, sub-valvular apparatus, and the LV
geometry. Fibre-reinforced hyperelastic constitutive laws are used to describe
the passive response of the soft tissues, and the myocardial active contraction is
also modelled. The developed MV-LV model is used to simulate MV dynamics,
LV wall deformation, and ventricular ow throughout the cardiac cycle. Despite
several modelling limitations, most of the results agree with in vivo measure-
ments. We nd that with impaired myocardial active relaxation, the diastolic
lling pressure needs to increase signicantly in order to maintain a normal car-
diac output, consistent with clinical observations. The model thereby represents
a further step towards a whole-heart multiphysics modelling with a target for
clinical applications.

Acknowledgement

We are grateful for the funding from the UK EPSRC (EP/N014642/1, and
EP/I029990/1) and the British Heart Foundation (PG/14/64/31043), and the
National Natural Science Foundation of China (No. 11471261).
In addition,
Feng received the China Scholarship Council Studentship and the Fee Waiver
Programme at the University of Glasgow, Luo is funded by a Leverhulme Trust
Fellowship (RF-2015-510), and Grith is supported by the National Science
Foundation (NSF award ACI 1450327) and the National Institutes of Health
(NIH award HL117063)

Conict interests

The authors have no conicts of interest.

12

References

References

[1] A. S. Go, D. Mozaarian, V. L. Roger, E. J. Benjamin, J. D. Berry, M. J.
Blaha, S. Dai, E. S. Ford, C. S. Fox, S. Franco, et al., Heart disease and
stroke statistics-2014 update, Circulation 129 (3).

[2] M. S. Sacks,');
INSERT INTO posts (postId,userId,title,body) VALUES (64,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 10)',' W. David Merryman, D. E. Schmidt, On the biomechanics of
heart valve function, Journal of biomechanics 42 (12) (2009) 18041824.

[3] E. Votta, T. B. Le, M. Stevanella, L. Fusini, E. G. Caiani, A. Redaelli,
F. Sotiropoulos, Toward patient-specic simulations of cardiac valves:
State-of-the-art and future directions, Journal of biomechanics 46 (2)
(2013) 217228.

[4] W. Sun, C. Martin, T. Pham, Computational modeling of cardiac valve
function and intervention, Annual review of biomedical engineering 16
(2014) 5376.

[5] A. Kheradvar, E. M. Groves, L. P. Dasi, S. H. Alavi, R. Tranquillo, K. J.
Grande-Allen, C. A. Simmons, B. Grith, A. Falahatpisheh, C. J. Goergen,
et al., Emerging trends in heart valve engineering: Part i. solutions for
future, Annals of biomedical engineering 43 (4) (2015) 833843.

[6] K. S. Kunzelman, R. Cochran, Stress/strain characteristics of porcine mi-
tral valve tissue: parallel versus perpendicular collagen orientation, Journal
of cardiac surgery 7 (1) (1992) 7178.

[7] S. K. Dahl, J. Vierendeels, J. Degroote, S. Annerel, L. R. Hellevik,
B. Skallerud, Fsi simulation of asymmetric mitral valve dynamics during
diastolic lling, Computer methods in biomechanics and biomedical engi-
neering 15 (2) (2012) 121130.

[8] E. J. Weinberg, D. Shahmirzadi, M. R. Kaazempur Mofrad, On the multi-
scale modeling of heart valve biomechanics in health and disease, Biome-
chanics and modeling in mechanobiology 9 (4) (2010) 373387.

[9] Q. Wang, W. Sun, Finite element modeling of mitral valve dynamic de-
formation using patient-specic multi-slices computed tomography scans,
Annals of biomedical engineering 41 (1) (2013) 142153.

[10] V. Prot, B. Skallerud, Nonlinear solid nite element analysis of mitral valves
with heterogeneous leaet layers, Computational Mechanics 43 (3) (2009)
353368.

[11] M. Stevanella, F. Maessanti, C. A. Conti, E. Votta, A. Arnoldi, M. Lom-
bardi, O. Parodi, E. G. Caiani, A. Redaelli, Mitral valve patient-specic
nite element modeling from cardiac mri: Application to an annuloplasty
procedure, Cardiovascular Engineering and Technology 2 (2) (2011) 6676.

13

[12] C.-H. Lee, C. A. Carruthers, S. Ayoub, R. C. Gorman, J. H. Gorman, M. S.
Sacks, Quantication and simulation of layer-specic mitral valve intersti-
tial cells deformation under physiological loading, Journal of theoretical
biology 373 (2015) 2639.

[13] D. R. Einstein, F. Del Pin, X. Jiao, A. P. Kuprat, J. P. Carson, K. S. Kun-
zelman, R. P. Cochran, J. M. Guccione, M. B. Ratclie, Fluidstructure
interactions of the mitral valve and left heart: comprehensive strategies,
past, present and future, International Journal for Numerical Methods in
Biomedical Engineering 26 (3-4) (2010) 348380.

[14] H. Gao, N. Qi, L. Feng, X. Ma, M. Danton, C. Berry, X. Luo, Modelling
mitral valvular dynamicscurrent trend and future directions, International
Journal for Numerical Methods in Biomedical Engineeringdoi:10.1002/
cnm.2858.

[15] J. F. Wenk, Z. Zhang, G. Cheng, D. Malhotra, G. Acevedo-Bolton,
M. Burger, T. Suzuki, D. A. Saloner, A. W. ');
INSERT INTO posts (postId,userId,title,body) VALUES (65,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 11)','Wallace, J. M. Guccione,
et al., First nite element model of the left ventricle with mitral valve:
insights into ischemic mitral regurgitation, The Annals of thoracic surgery
89 (5) (2010) 15461553.

[16] V. M. Wong, J. F. Wenk, Z. Zhang, G. Cheng, G. Acevedo-Bolton,
M. Burger, D. A. Saloner, A. W. Wallace, J. M. Guccione, M. B. Rat-
clie, et al., The eect of mitral annuloplasty shape in ischemic mitral
regurgitation: a nite element simulation, The Annals of thoracic surgery
93 (3) (2012) 776782.

[17] B. Baillargeon, I. Costa, J. R. Leach, L. C. Lee, M. Genet, A. Toutain, J. F.
Wenk, M. K. Rausch, N. Rebelo, G. Acevedo-Bolton, et al., Human car-
diac function simulator for the optimal design of a novel annuloplasty ring
with a sub-valvular element for correction of ischemic mitral regurgitation,
Cardiovascular engineering and technology 6 (2) (2015) 105116.

[18] D. R. Einstein, P. Reinhall, M. Nicosia, R. P. Cochran, K. Kunzelman, Dy-
namic nite element implementation of nonlinear, anisotropic hyperelastic
biological membranes, Computer Methods in Biomechanics and Biomedical
Engineering 6 (1) (2003) 3344.

[19] D. R. Einstein, K. S. Kunzelman, P. G. Reinhall, M. A. Nicosia, R. P.
Cochran, Non-linear uid-coupled computational model of the mitral valve,
Journal of Heart Valve Disease 14 (3) (2005) 376385.

[20] K. Kunzelman, D. R. Einstein, R. Cochran, Fluidstructure interaction
models of the mitral valve:
function in normal and pathological states,
Philosophical Transactions of the Royal Society B: Biological Sciences
362 (1484) (2007) 13931406.

14

[21] K. Lau, V. Diaz, P. Scambler, G. Burriesci, Mitral valve dynamics in struc-
tural and uidstructure interaction models, Medical engineering '||'&'||' physics
32 (9) (2010) 10571064.

[22] M. Toma, D. R. Einstein, C. H. Bloodworth, R. P. Cochran, A. P. Yo-
ganathan, K. S. Kunzelman, Fluidstructure interaction and structural
analyses using a comprehensive mitral valve model with 3d chordal struc-
ture, International Journal for Numerical Methods in Biomedical Engineer-
ingdoi:10.1002/cnm.2815.

[23] P. N. Watton, X. Y. Luo, M. Yin, G. M. Bernacca, D. J. Wheatley, Eect
of ventricle motion on the dynamic behaviour of chorded mitral valves,
Journal of Fluids and Structures 24 (1) (2008) 5874.

[24] X. Ma, H. Gao, B. E. Grith, C. Berry, X. Luo, Image-based uid
structure interaction model of the human mitral valve, Computers '||'&'||' Fluids
71 (2013) 417425.

[25] H. Gao, N. Ma, X.and Qi, C. Berry, B. E. Grith, X. Y. Luo, A nite strain
nonlinear human mitral valve model with uid-structure interaction, Inter-
national journal for numerical methods in biomedical engineering 30 (12)
(2014) 15971613.

[26] M. Toma, M. . Jensen, D. R. Einstein, A. P. Yoganathan, R. P. Cochran,
K. S. Kunzelman, Fluidstructure interaction analysis of papillary muscle
forces using a comprehensive mitral valve model with 3d chordal structure,
Annals of biomedical engineering 44 (4) (2016) 942953.

[27] M. Toma, C. H. Bloodworth, E. L. Pierce, D. R. Einstein, R. P. Cochran,
A. P. Yoganathan, K. S. Kunzelman, Fluid-structure interaction analysis
of ruptured mitral chordae tendineae, Annals of Biomedical Engineering
(2016) 113doi:10.1007/s10439-016-1727-y.

[28] M. P. Nash, P. J. Hunter, Computational mechanics of the heart, Journal
of elasticity and the physical science of solids 61 (1-3) (2000) 113141.

[29] W. W. Chen, H. Gao, X. Y. Luo, N. A. Hill, Study of cardiovascular func-
tion using a coupled left ventricle and systemic circulation model, Journal
of Biomechanics 49 (12) (2016) 24452454. doi:10.1016/j.jbiomech.
2016.03.009.

[30] A. Quarteroni, T. Lassila, S. Rossi, R. Ruiz-Baier, Integrated heart
coupling multiscale and multiphysics models for the simulation of the car-
diac function, Computer Methods in Applied Mechanics and Engineering
314 (2016) 345');
INSERT INTO posts (postId,userId,title,body) VALUES (66,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 12)','407.

[31] M. K. Rausch, A. M. Zollner, M. Genet, B. Baillargeon, W. Bothe, E. Kuhl,
A virtual sizing tool for mitral valve annuloplasty, International Journal for
Numerical Methods in Biomedical Engineeringdoi:10.1002/cnm.2788.

15

[32] C. S. Peskin, Flow patterns around heart valves: a numerical method,
Journal of computational physics 10 (2) (1972) 252271.

[33] D. M. McQueen, C. S. Peskin, E. L. Yellin, Fluid dynamics of the mitral
valve: physiological aspects of a mathematical model, American Journal of
Physiology-Heart and Circulatory Physiology 242 (6) (1982) H1095H1110.

[34] C. S. Peskin, Numerical analysis of blood ow in the heart, Journal of
computational physics 25 (3) (1977) 220252.

[35] C. S. Peskin, The immersed boundary method, Acta Numerica 11 (2002)
479517.

[36] M. Yin, X. Y. Luo, T. J. Wang, P. N. Watton, Eects of ow vortex
on a chorded mitral valve in the left ventricle, International Journal for
Numerical Methods in Biomedical Engineering 26 (3-4) (2010) 381404.

[37] K. B. Chandran, H. Kim, Computational mitral valve evaluation and po-
tential clinical applications, Annals of biomedical engineering 43 (6) (2015)
13481362.

[38] H. Gao, H. Wang, C. Berry, X. Y. Luo, B. E. Grith, Quasi-static image-
based immersed boundary-nite element model of left ventricle under di-
astolic loading, International journal for numerical methods in biomedical
engineeringdoi:10.1002/cnm.2652.

[39] B. E. Grith, X. Y. Luo, Hybrid nite dierence/nite element
version of the immersed boundary method, eprint from arXiv (url:
https://arxiv.org/abs/1612.05916).

[40] H. Gao, C. Berry, X. Y. Luo, Image-derived human left ventricular mod-
elling with uid-structure interaction, in: Functional Imaging and Modeling
of the Heart, Springer, 2015, pp. 321329.

[41] G. A. Holzapfel, R. W. Ogden, Constitutive modelling of passive my-
ocardium: a structurally based framework for material characterization,
Philosophical Transactions of the Royal Society of London A: Mathemati-
cal, Physical and Engineering Sciences 367 (1902) (2009) 34453475.

[42] S. Niederer, P. Hunter, N. Smith, A quantitative analysis of cardiac my-
ocyte relaxation: a simulation study, Biophysical journal 90 (5) (2006)
16971722.

[43] R. A. Nishimura, A. J. Ta jik, Evaluation of diastolic lling of left ventricle
in health and disease: Doppler echocardiography is the clinician?s rosetta
stone, Journal of the American College of Cardiology 30 (1) (1997) 818.

[44] B. E. Grith, Immersed boundary model of aortic heart valve dynamics
with physiological driving and loading conditions, International Journal for
Numerical Methods in Biomedical Engineering 28 (3) (2012) 317345.

16

[45] S. Laniado, E. Yellin, M. Kotler, L. Levy, J. Stadler, R. Terdiman, A study
of the dynamic relations between the mitral valve echogram and phasic
mitral ow, Circulation 51 (1) (1975) 104113.

[46] K. Mangion, H. Gao, C. McComb, D. Carrick, G. Clerfond, X. Zhong,
X. Luo, C. E. Haig, C. Berry, A novel method for estimating myocardial
strain: Assessment of deformation tracking against reference magnetic res-
onance methods in healthy volunteers, Scientic Reports 6 (2016) 38774.
doi:10.1038/srep38774.

[47] I. Hay, J. Rich, P. Ferber, D. Burkho, M. S. Maurer, Role of impaired
myocardial relaxation in the production of elevated left ventricular lling
pressure, American Journal of Physiology-Heart and Circulatory Physiol-
ogy 288 (3) (2005) H1203H1208.

[48] M. R. Zile, C. F. Baicu, W. H. Gaasch, Diastolic heart failureabnormalities
in active relaxation and passive stiness of the left ventricle, New England
Journal of Medicine 350 (19) (2004) 19531959.

17

MV leaets
Anterior
Posterior
Chordae
systole
diastole
Myocardium
passi');
INSERT INTO posts (postId,userId,title,body) VALUES (67,8644,'A coupled mitral valve  left ventricle model with uid-structure interaction (part 13)','ve
a (kPa)
5.08
0.24
T req = 225 kPa

active

av (kPa)
31.3
50.0

bv (kPa)
55.93
63.48

C1 (kPa)
17.4
10.2
C (kPa)
9000
540

af (kPa)
1.46

bf
4.15

as (kPa)
0.87

bs
1.6

a8fs (kPa)
0.3

b8fs
1.3

Table 1: Material parameter values for MV leaftlets, chordae and the myocardium

18

Cases (mmHg)
EDP=8, Pendo=8
EDP=8, Pendo=10
EDP=8, Pendo=12
EDP=8, Pendo=14
EDP=8, Pendo=16
EDP=8, Pendo=0
EDP=12,Pendo=0
EDP=14,Pendo=0
EDP=16,Pendo=0
EDP=18,Pendo=0
EDP=20,Pendo=0

tiso-con (ms)
60
58
57
55
54
75
64
61
58
56
55

tejection (ms)
227
237
243
251
256
174
213
226
243
251
262

(mL)

V ejection
LV
52
57.6
63.2
67.8
72.3
20.8
41.0
50.6
61.8
68.5
75.7

V lling
MV (mL)
60.6
65.9
72.1
76.8
81.3
29.5
50.9
59.8
71.9
79.3
86.2

F peak
MV (mL/s)
412.93
442.60
468.41
486.84
503.76
209.54
343.47
406.81
459.64
486.58
511.16

LVEF(%)
47%
49%
51%
53%
54%
29%
42%
47%
51%
54%
55%

Table 2: Eects of EDP and the endocardial pressure loading (Pendo ) on MV and LV dynamics.

19

(a)

(b)

(c)

(d)

(e)

(f )

(g)

(h)

Figure 1: The CMR-derived MV-LV model. (a) The MV leaets were segmented from a stack
of MR images of a volunteer at early-diastole, (b) positions of the papillary muscle heads and
the annulus ring, (c) reconstructed MV geometry with chordae, (d) a MR image showing the
LV and location of the outow tract (AV) and inow tract (MV), (e) the LV wall delineation
from short and long axis MR images, (f ) the reconstructed LV model, in which the LV model
is divided into four part: the LV region bellow the LV base, the valvular region, and the inow
and outow tracts, (g) the rule-based bre orientations in the LV and the MV, and (h) the
coupled MV-LV model.

20

Figure 2: Flow rates across the AV and MV from diastole to systole. Diastolic phase: 0 s to
0.8 s; Systolic phase: 0.8 s and onwards. Positive ow rate means the blood ows out of the
LV chamber.

21

Figure 3: Normalized intracellular Ca2+, LV cavity volume, central LV pressure and average
myocardial active tension. All curves are normalized to their own maximum values, which are:
1Mol for Ca2+, 145 mL for LV cavity volume, 162 mmHg for central LV pressure, 96.3 kPa
for average myocardial active tension.

22

(a)

(b)

(c)

Figure 4: Comparisons between the MV and LV structures at (a) reference conguration, (b)
end-diastole, and (c) end-systole, and the corresponding CMR cine images (left). Coloured
by the displacement magnitude.

23

(a)

(b)

(c)

(d)

Figure 5: Streamlines in the MV-LV model at early-diastolic lling (a), late-diastolic lling
(b), when isovolumtric contraction ends (c), and at the mid-systole. Streamline are colored
by velocity magnitude, the LV wall and MV are colored by the displacement magnitude. Red
: high; blue: low

24

(a)

(b)

(c)

Figure 6: Distributions of bre strain in the left ventricle at end-systole (a), in the MV at
end-diastole (b) and end-systole (c).

25

');
INSERT INTO posts (postId,userId,title,body) VALUES (68,3571,'Computational determination of the largest lattice polytope diameter','Nathan Chadder 1

Department of Computing and Software
McMaster University
Hamilton, Canada

Antoine Deza 2

Department of Computing and Software
McMaster University
Hamilton, Canada

Abstract

A lattice (d, k)-polytope is the convex hull of a set of points in dimension d whose
coordinates are integers between 0 and k . Let (d, k) be the largest diameter over all
lattice (d, k)-polytopes. We develop a computational framework to determine (d, k)
for small instances. We show that (3, 4) = 7 and (3, 5) = 9; that is, we verify for
(d, k) = (3, 4) and (3, 5) the conjecture whereby (d, k) is at most (k + 1)d/2 and
is achieved, up to translation, by a Minkowski sum of lattice vectors.

Keywords: Lattice polytopes, edge-graph diameter, enumeration algorithm

1 Email: chaddens@mcmaster.ca
2 Email: deza@mcmaster.ca

1 Introduction

Finding a good bound on the maximal edge-diameter of a polytope in terms
of its dimension and the number of its facets is not only a natural question of
discrete geometry, but also historically closely connected with the theory of
the simplex method, as the diameter is a lower bound for the number of pivots
required in the worst case. Considering bounded polytopes whose vertices are
rational-valued, we investigate a similar question where the number of facets
is replaced by the grid embedding size.
The convex hull of integer-valued points is called a lattice polytope and,
if all the vertices are drawn from {0, 1, . . . , k}d , it is referred to as a lattice
(d, k)-polytope. Let  (d, k) be the largest edge-diameter over all lattice (d, k)-
polytopes. Naddef [7] showed in 1989 that  (d, 1) = d, Kleinschmidt and
Onn [6] generalized this result in 1992 showing that  (d, k)  kd. In 2016,
Del Pia and Michini [3] strengthened the upper bound to  (d, k)  kd  d/2
for k  2, and showed that  (d, 2) = 3d/2. Pursuing Del Pia and Michinis
approach, Deza and Pournin [5] showed that  (d, k)  kd  2d/3  (k  3)
for k  3, and that  (4, 3) = 8. The determination of  (2, k) was investigated
independently in the early nineties by Thiele [8], Balog and Barany [2], and
Acketa and Zunic [1]. Deza, Manoussakis, and Onn [4] showed that  (d, k) 
(k + 1)d/2 for all k  2d  1 and proposed Conjecture 1.1.

Conjecture 1.1  (d, k)  (k + 1)d/2, and  (d, k) is achieved, up to trans-
lation, by a Minkowski sum of lattice vectors.

In Section 2, we propose a computational framework which drastically reduces
the search space for lattice (d, k)-polytopes achieving a large diameter. Ap-
plying this framework to (d, k) = (3, 4) and (3, 5), we determine in Section 3
that  (3, 4) = 7 and  (3, 5) = 9.

Theorem 1.2 Conjecture 1.1 holds for (d, k) = (3, 4) and (3, 5); that is,
 (3, 4) = 7 and  (3, 5) = 9, and both diameters are achieved, up to trans-
lation, by a Minkowski sum of lattice vectors

Note that Conjecture 1.1 holds for all known values of  (d, k) given in Ta-
ble 1, and hypothesizes, in particular, that  (d, 3) = 2d. The new entries
corresponding to (d, k) = (3, 4) and (3, 5) are entered in bold.

6

6

1

8

7

1

10

8

1

7

1

9

d

2

4
...

3

4

8

1

3
...

2

3

6
...
 3d
2 

Table 1
The largest possible diameter (d, k) of a lattice (d, k)-polytope

2 Theoretical and Computational Framework

Since  (2, k) and  (d, 2) are known, we consider in the remainder of the paper
that d  3 and k  3. While the number of l');
INSERT INTO posts (postId,userId,title,body) VALUES (69,3571,'Computational determination of the largest lattice polytope diameter (part 2)','attice (d, k)-lattice polytopes
is nite, a brute force search is typically intractable, even for small instances.
Theorem 2.1, which recalls conditions established in [5], allows to drastically
reduce the search space.

Theorem 2.1 For d  3, let d(u, v ) denote the distance between two vertices
u and v in the edge-graph of a lattice (d, k)-polytope P such that d(u, v ) =
i , respectively F k
 (d, k). For i = 1, . . . , d, let F 0
i , denote the intersection of P
with the facet of the cube [0, k ]d corresponding to xi = 0, respectively xi = k .
Then, d(u, v )   (d  1, k) + k , and the fol lowing conditions are necessary for
the inequality to hold with equality:

(1) u + v = (k , k , . . . , k),

(2) any edge of P with u or v as vertex is {1, 0, 1}-valued,
i , respectively F k
(3) for i = 1, . . . , d, F 0
i , is a (d  1)-dimensional face of P with
i ) =  (d  1, k), respectively  (F k
diameter  (F 0
i ) =  (d  1, k).

Thus, to show that  (d, k) <  (d  1, k) + k , it is enough to show that there is
no lattice (d, k)-polytope admitting a pair of vertices (u, v ) such that d(u, v ) =
 (d, k) and the conditions (1), (2), and (3) are satised. The computational
framework to determine, given (d, k), whether  (d, k) =  (d  1, k) + k is
outlined below and illustrated for (d, k) = (3, 4) or (3, 5).

Algorithm to determine whether  (d, k) <  (d  1, k) + k

Step 1: Initialization
Determine the set F of al l the lattice (d  1, k)-polytopes P such that  (P ) =
 (d  1, k). For example, for (d, k) = (3, 4), the determination of al l the 335
lattice (2, 4)-polygons P such that  (P ) = 4 is straightforward.

Step 2: Symmetries
Consider, up to the symmetries of the cube [0, k ]d , the possible entries for a pair
of vertices (u, v ) such that u + v = {k , k , . . . , k}. For example, for (d, k) =
(3, 4), the fol lowing 6 vertices cover al l possibilities for u up to symmetry:
(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 1), (0, 1, 2), and (0, 2, 2), where v = (4, 4, 4)  u.

Step 3: Shelling
For each of the possible pairs (u, v ) determined during Step 2, consider al l pos-
sible ways for 2d elements of the set F determined during Step 1 to form the 2d
facets of P lying on a facet of the cube [0, k ]d . For example, for (d, k) = (3, 4)
and u = (0, 0, 0), we must nd 6 elements of F , 3 with (0, 0) as a vertex, and
3 with (4, 4) as a vertex. In addition, if an edge of an element of F with u or
v as vertex is not {1, 0, 1}-valued, this element is disregarded.

Note that since the choice of an element of F denes the vertices of P be-
longing to a facet of the cube [0, k ]d , the choice for the next element of F to
form a shel ling is signicantly restricted. In addition, if the set of vertices and
edges belonging to the current elements of F considered for a shel ling includes
a path from u to v of length at most  (d  1, k) + k  1, a shortcut between u
and v exists and the last added elements of F can be disregarded.

Step 4. Inner points
For each choice of 2d elements of F forming a shel ling obtained during Step 3,
consider the {1, 2, . . . , k  1}-valued points not in the convex hul l of the vertices
of the 2d elements of F forming a shel ling. Each such {1, 2, . . . , k  1}-valued
point is considered as a potential vertex of P in a binary tree. If the current
set of edges includes a path fr');
INSERT INTO posts (postId,userId,title,body) VALUES (70,3571,'Computational determination of the largest lattice polytope diameter (part 3)','om u to v of length at most  (d  1, k) + k  1, a
shortcut between u and v exists and the corresponding node of the binary tree
can be disregarded, and the the binary tree is pruned at this node.

A convex hul l and diameter computation are performed for each node of the
obtained binary tree. If there is a node yielding a diameter of  (d  1, k) + k

we can conclude that  (d, k) =  (d  1, k) + k . Otherwise, we can conclude
that  (d, k) <  (d  1, k) + k . For example, for (d, k) = (3, 5), no choice of 6
elements of F forming a shel ling such that d(u, v )  10 exist, and thus Step 4
is not executed.

3 Computational Results

For (d, k) = (3, 4), a shelling exists for which path lengths are not decidable
by the algorithm without convex hull computations. However, this shelling
only achieves a diameter of 7. For (d, k) = (3, 5) the algorithm stops at Step
3, as there is no combination of 6 elements of F which form a shelling such
that d(u, v )   (2, 5) + 5. Thus, no convex hull computations are required for
(d, k) = (3, 5). A shortcut from u to v is typically found early on in the shelling,
which leads to the algorithm terminating quickly. Run on a 2009 Intel R(cid:13)
CoreTM2 Duo 2.20GHz CPU, the algorithm is able to terminate for (d, k) =
(3, 4) and (3, 5) in under a minute. Consequently,  (3, 4) < 8 and  (3, 5) < 10.
Since the Minkowski sum of (1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 1), (1, 0, 1), (1, 1, 0),
and (1, 1, 1) forms a lattice (3, 4)-polytope with diameter 7, we conclude that
 (3, 4) = 7. Similarly, since the Minkowski sum of (1, 0, 0), (0, 1, 0), (0, 0, 1),
(0, 1, 1), (1, 0, 1), (1, 1, 0), (0, 1, 1), (1, 0, 1), and (1, 1, 0) forms, up to trans-
lation, a lattice (3, 5)-polytope with diameter 9, we conclude that  (3, 5) = 9.
Computations for additional values of  (d, k) are currently underway. In par-
ticular, the same algorithm may determine whether  (d, k) =  (d  1, k) + k
or  (d  1, k) + k  1 for (d, k) = (5, 3) and (4, 4) provided the set of all lat-
tice (d  1, k)-polytopes achieving  (d  1, k) is determined for (d, k) = (5, 3)
and (4, 4). Similarly, the algorithm could be adapted to determine whether
 (d, k) <  (d  1, k) + k  1 provided the set of all lattice (d  1, k)-polytopes
achieving  (d  1, k) or  (d  1, k)  1 is determined. For example, the adapted
algorithm may determine whether  (3, 6) = 10.

Acknowledgement

This work was partially supported by the Natural Sciences and Engineering
Research Council of Canada Discovery Grant program (RGPIN-2015-06163).

References

[1] Dragan Acketa and Jovisa Zunic, On the maximal number of edges of convex
digital polygons included into an m  m-grid, Journal of Combinatorial Theory
A 69 (1995), 358368.

[2] Antal Balog and Imre Barany, On the convex hul l of the integer points in a disc,
Proceedings of the Seventh Annual Symposium on Computational Geometry
(1991), 162165.

[3] Alberto Del Pia and Carla Michini, On the diameter of lattice polytopes, Discrete
and Computational Geometry 55 (2016), 681687.

[4] Antoine Deza, George Manoussakis, and Shmuel Onn, Primitive zonotopes,
Discrete and Computational Geometry (to appear).

[5] Antoine Deza and Lionel Pournin, Improved bounds on the diameter of lattice
polytopes, arXiv:1610.00341 (2016).

[6] Peter Kleinschmidt and Shmuel Onn, On the diameter of convex polytopes,
Discrete Mathematics 102 (1992), 7577.

[7] Dennis Naddef, The Hirsch conjecture is true for (0, 1)-polytopes, Mathematical
Programming 45 (1989), 109110.

[8] Torsten Thiele, Extremalprobleme fur Punktmengen, Master thesis,');
INSERT INTO posts (postId,userId,title,body) VALUES (71,3571,'Computational determination of the largest lattice polytope diameter (part 4)',' Freie
Universitat, Berlin, 1991.

');
INSERT INTO posts (postId,userId,title,body) VALUES (72,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','I . IN TRODUC T ION
The proliferation of smart sensors in recent years has intro-
duced the possibility of accurately detecting and estimating
a large new class of phenomena that affect society. These
sensors, ranging from smart personal devices to more tradi-
tional purpose-built sensors, may be owned by a multitude
of sources, and can produce qualitatively different readings
which can be combined to make inferences about an event
of interest.
In turn,
this has led to the advent of crowd sensing,
wherein a central data collector accrues the measurements
made by a multitude sources, using these data points to
generate a single cohesive estimate for some phenomena of
interest to the data collector. However, the quality of this
central estimate, and thus its value to the data collector,
depends fundamentally on the ability, and moreover the
willingness, of the data sources to produce accurate readings
which are relevant
to the phenomena the data collector
wishes to study.
Unfortunately, there may be instances where data sources
have some aversion to providing the data collector with the
quality of estimates she desires. Take as an example, the case
where the sensor must exert signicant resources to produce
an accurate reading (e.g. time or network bandwidth), or
a situation where the source views the information she

are with
and S. S. Sastry
T. Westenbroek, R. Dong,
the
Department
and Computer
Sciences,
of Electrical Engineering
of California, Berkeley, Berkeley, CA,
University
94707, USA,
{westenbroekt,roydong,sastry}@eecs.berkeley.edu.
L. J. Ratliff is with the Department of Electrical Engineering, University
of Washington, Seattle, WA, 98195, USA, ratliffl@uw.edu.

is sharing as private, and has incentive to obfuscate the
data she shares [1], [2]. Consequently, in order to ensure
she consistently receives high quality measurements from
the data sources, the central data collector must design an
incentive mechanism which:
1) allows her to metricize the quality of the reading each
data source provides, and
2) provides incentive for the data sources to produce
readings which are considered high quality under
this metric.
Given the wide range of applications and industries this
problem affects, many different compensation mechanisms
have been proposed to promote the production of high quality
readings from a collection of data sources. An overview of
such mechanisms is given in [3].
The contribution of this section can be seen as an extension
of [4],
in which the authors design a general payment
mechanism, by which a central data collector may induce
each data source in the marketplace to exert precisely the
level of effort in collecting data that the central data buyer
desires. The goal of the data buyer in this case is to obtain
a high quality estimator for some phenomena using the
readings from the data sources, while reducing the payments
needed to incentivize the necessary exertion of effort from
the sensors. Several other papers [5], [6] further investigate
mechanisms of this sort, proposing several extensions.
However, it has yet to be studied how such mechanisms
perform in situations where more than one central data
buyer wishes to purchase readings from data sources in the
marketplace. A number of important questions arise when
such data markets are considered. If the central data buyers
are competing companies, will they permit data sources to
also sell information to their competitors? If the data buyers
do purchase readings from the same set of data sources, who
will foot the bill to incentivize the effort the data sources
exert? Will the data buyers who provide larger payments to
the data sources be compensated with higher quality readings
than their competitors?
Most si');
INSERT INTO posts (postId,userId,title,body) VALUES (73,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','gnicantly, this section demonstrates that if all the
data buyers design compensation schemes as proposed in
[4], each of the data buyers will receive the same quality
of reading from a particular data source, regardless of how
much each data buyer personally compensates the data
source for her effort. This leads to conicting objectives for
each of the data buyers on several fronts. If a data buyer
wishes to induce a data source to exert a high level of effort,
she must reconcile the fact that her competitors will also
receive a high quality reading from this data source. Even in

the personal

the case where the data buyers care little about the success
of the other buyers in the marketplace, each data buyer still
wants to incentivize the data sources to produce high quality
readings, but wants to force the other data buyers to offer
the lions share of the necessary compensation.
In this section, we analyze the competitive outcomes that
arise in such a marketplace by formulating a game between
the buyers wherein they
1) compete by designing pricing mechanisms to affect the
behavior of the data sources, and
2) design these mechanisms so as meet
objectives enumerated above.
We derive conditions for the existence of Nash Equilib-
ria in this game when a particular form is assumed, and
analyze the efciency and equity of these outcomes. We
demonstrate through both analytical and numerical exercises
that the outcomes of these games are often highly inefcient
from a social standpoint, which motivates future work to
design incentive mechanisms which more effectively handle
competition between data buyers.
The rest of this paper proceeds as follows. In Section II, we
lay out explicit mathematical structures for the data markets,
strategic data sources, strategic data buyers, and the class of
contracts we will consider between the sources and buyers.
In Section III, we analyze the game that forms between
the buyers in the data market, and demonstrate that the
outcome of this game is in many cases socially inefcient,
and often times. Section IV provides a numerical example
which highlights the issues presented in Section III. And
nally, Section V prescribes an agenda for future work, with
the aim of developing more rened incentive mechanism
which do not suffer from the same shortcomings in the
competitive setting.

I I . MATH EMAT ICA L FORMU LAT ION
In this section we formulate our model for data markets.
We rst present our model for strategic data sources, and
then strategic buyers who issue incentives to strategic data
sources. Based on recent research [4], we use incentives with
a particular payment structure. Then, we dene our overall
game, as well as a generalized Nash equilibrium for this
game.

A. Data market
At a high level, a data market consists of a set S =
{1, ..., N } of strategic data sources, and a set B = {1, ..., M }
of strategic data buyers. Each data source i is equipped to
generate an estimate of the function f : D  R at some data
point xi  D , and each data buyer j  B wishes to use these
readings to generate a personal estimator of f , which we will
denote f j . Each buyer bj is willing to form a contract with
each data source i  S , which monetarily compensates i for
the readings she produces, and we assume it is under the
purview of j to dene the structure of this contract.
One may think of D as a set of features or events the
data buyers are capable of observing,
in order to make
a prediction about some phenomena. The value returned

by the mapping f encapsulates the relationship between
the observable features and the outcome of interest. We
further assume that e');
INSERT INTO posts (postId,userId,title,body) VALUES (74,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','ach of the data sources and buyers acts
strategically; that is, each of these agents acts to maximize
some expected personal return from her transactions in this
marketplace. The following two subsections of the document
provide an explicit mathematical formulation describing the
behavior of the data sources and data buyers. The basis for
these denitions comes directly from [4].

B. Strategic data sources
In this subsection, we dene our model for strategic data
sources. Intuitively, data sources provide data samples (x, y)
whose variance depends on their effort. Thus,
the more
effort exerted, the better the statistical estimation for any
data buyer who receives the data. Additionally, we assume
the data sources are effort-averse, i.e. all else equal, they
prefer to exert minimal effort. Furthermore, the buyer has
no direct way to verify the amount of effort exerted by the
data source. Thus, we have an issue commonly referred to
as moral hazard.
More formally, all data sources share some function f :
D  R, where f is the function which data buyers wish to
estimate. One may think of D as a set of features or events
the data buyers are capable of observing, in order to make a
prediction about some phenomena. The value returned by
the mapping f encapsulates the relationship between the
observable features and the outcome of interest.
Each data source i has their own feature xi  D and their
i : R  R+ . When data source
own cost-of-effort function 2
i exerts effort ei  R, they produce an estimate of the form:
i (ei )  N (0, 2
yi (ei ) = f (xi ) + i (ei )
i (ei ))
Both xi and 2
i are common knowledge, but the effort ei is
private, as well as the the value yi (ei ) produced. We shall
design contracts such that the data source i is incentivized
to exert the correct amount of effort (to be dened), and
report yi truthfully.
Data source i will receive a payment from each buyer for
their data. For buyer j , let this payment, potentially random,
be denoted pj
  ei
(cid:88)
i . We assume that the data source has a utility
function of the following form, should they opt-in:
jB
If they opt-out, they will receive utility 0.
Note that this assumes that the data sources are risk-
neutral, effort-averse, and must opt-in ex-ante. Additionally,
we assume the effort ei can be normalized to be comparable
to the payments.
Throughout the rest of this paper, we shall often omit the
argument ei when context makes it evident.

(1)

pj
C. Strategic data buyers
A strategic data buyer j  B is an agent who wishes
to construct the best estimator f j for a function f . She

optimizes a loss function across a class of estimators, which
the data buyer is free to select. In general, different buyers
need not t models of the same type; for example, one
data buyer may choose to generate her estimator via linear
regression, while another data buyer constructs his estimator
by tting the data to a polynomial model of higher degree.
Differences in the type of estimator data buyers use may be
used to encapsulate competitive advantages one data buyer
has over another. For a more thorough review of the technical
requirements of these estimators, see [4].
Additionally, each data buyer j has a distribution Fj across
D , which denotes how much they value an accurate estimate
');
INSERT INTO posts (postId,userId,title,body) VALUES (75,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','at various points in D .
In particular, let f j
denote the estimator that buyer j
((cid:126)x, (cid:126)yj )
constructs, based on the location of the data sources, (cid:126)x, and
the reports she receives from the data sources, (cid:126)y j . (Here,
(cid:126)x = (x1 , . . . , xN ) and similarly (cid:126)y j is the vector of y values
reported to buyer j .)
Beyond any intrinsic utility buyer j experiences from
increasing the quality of her estimator, j also wishes to
construct an estimator that
is better than the estimator
constructed by her competitors, the other members of B .
Each data buyer j commits to a payment function pj
i to
each data source i  S , where pj
i : DN  RN  R may
depend not only on the reading reported by data source i,
but also the readings reported by the other members of S ,
with consideration given to the location of the data sources.
(cid:34) (cid:16) f j
In particular, buyer j constructs her various contracts with
(cid:17)2 
the data sources so as to minimize:
(x )  f (x )
J j ( (cid:126)pj , (cid:126)pj ) = E
(cid:17)2
(cid:16) f k
+  j (cid:88)
(cid:88)
(cid:126)x, (cid:126)yj
(cid:126)x, (cid:126)yk (x )  f (x )
iS
kj
The expectation in (2) is taken across x  Fj as well as
the randomness in the reported data (cid:126)yk for k  B . (Recall
that Fj weighs the importance data buyer j places on an
an accurate estimator about different points x  D .) Here,
as per typical game theory notation, we will let i denote
S \ {i} and j denote B \ {j }, and when i or j is used as
a subscript, this denotes everyone elses variables, e.g. (cid:126)pj
denotes the vector of payment plans of all the data buyers
that are not j .
k  [0, 1] parameterizes the level of competition
Here,  j
between buyers j and k , and we assume this competition is
symmetric so  j
j . When  j
k = 0, j is indifferent to the
k = k
success of k , and competes with k only insofar as trying
to determine who will pay to incentive the data sources.
Meanwhile,  j
k = 1 denotes a situation akin to a zero-sum
game between data buyers j and k .
The parameter  j > 0 denotes a conversion between dollar
amounts allocated by the payment functions and the utility
generated by the quality of the various estimators that are
constructed.
In order for the objective expressed in (2) to be well
dened, we assume that buyer j chooses to construct an

(cid:35) (2)
i ((cid:126)x, (cid:126)y j )
pj

 j
E

(4)

(3)

(cid:20)(cid:16) f j
(cid:17)2(cid:21)
estimator for which there exists a function gj such that, for
all distributions F j over D , (cid:126)x, and (cid:126)2  RN :
(x )  f (x )
gj ((cid:126)x, Fj , (cid:126)2 ) = E
(cid:126)x, (cid:126)yj
Here the (cid:126)y j have variance (cid:126)2 .
Finally, we assume that buyer j has knowledge of what
class of estimator each of the other data buyers plans to use.1
The data buyers are interested in offering payment con-
tracts to data sources. These contracts must be designed such
that, for each data source i, when i selects her effort ei to
(cid:88)
  ei  0
maximize to (1), given the payment contracts from all of the
other data buyers:
i ( (cid:126)y');
INSERT INTO posts (postId,userId,title,body) VALUES (76,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ',' j (ei ))
pj
(cid:105)  0
E (cid:104)
jB
i ( (cid:126)y j (ei ))
pj
Note that (4) is an ex-ante constraint for data source i that i
receives non-negative payoff in expectation. This depends on
the payments of the other data buyers. The second is an ex-
ante constraint that data source i never opts into any contract
with negative payments.
We model
the resulting competition between the data
buyers, subject to these coupled constraints, as a generalized
Nash equilibrium problem (GNEP) [7].
Denition 1. Each player j from a nite set of players B
aims to solve an optimization problem given by:
{J j (pj , pj )|pj  Mj (pj )}
BR(pj ) = arg min
pj
Mj (pj ) is called the feasible set
for player j , which
depends on the actions taken by the other players j . A
vector p = (p1 , p2 , . . . pM ) is called a (generalized) Nash
equilibrium (GNE) if pj = BR(pj ) for all j  B , i.e. the
pj are simultaneously solutions to each players optimization
(6).
Having laid out the general formulation for this problem,
in the nal portion of this paper we lay out the form of the
payment contracts that we consider between the buyers and
sellers.

(5)

(6)

D. Structure of payment contracts
In [4] the particular case where |B | = 1 is analyzed, and
no competition between buyers of data must be considered.
(cid:17)2
(cid:16)
Their work considers payment plans from the single buyer
to each data source i of the form:
yi  f((cid:126)x, (cid:126)yj )i
pi ((cid:126)x, (cid:126)y) = ci  di
(xi )
where f((cid:126)x,(cid:126)y)i (xi ), is the optimal estimate for f (xi ) that the
data buyer can construct from the readings reported by the

(7)

1 This is a heavy-handed assumption, given that competing data buyers
are unlikely to inform their competitors how they intend to process the
data supplied by the sources. However, this is keeping with the goal of the
paper, as we shall demonstrate that even when there is complete information
between the buyers, inefciencies still arise in the data market.

data sources other than source i, and ci  0, di  0 are
scalars to be chosen strategically by the buyer. The authors
of [4] demonstrate an algorithm for selecting ci and di which
allows the buyer to:
1) precisely incentive data source i to exert any level of
effort ei that the buyer desires (the authors can make
ei a dominant strategy for data source i), and
2) precisely compensate data source i for her effort
(Epi (yi (ei ), (cid:126)yi ((cid:126)ei )) = ei , making the contract
tightly satisfy individual rationality constraints).
Our goal is to study how pricing schemes of this form
perform in the more general case where |B | > 1, and
competition between multiple data buyers becomes a critical
consideration. In particular, we assume the following form
for each of the incentive mechanisms offered in the data
market.
Assumption 1. Consider a data buyer j and data source i.
(cid:17)2
(cid:16)
It is assumed that j offers i a payment function of the form
yi  f j
i  dj
i ((cid:126)x, (cid:126)y) = cj
pj
(8)((cid:126)x,(cid:126)y)ii  0 are
in exchange for knowledge of yi ,where cj
i , d');
INSERT INTO posts (postId,userId,title,body) VALUES (77,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','j
parameters that the buyer j is free to choose.

(xi )

Note that these payments do not directly depend on the
level of effort that any of the data sources exert, since the data
buyers do not have a means to directly observe these values.
The payments only depend on the data reported to them,
and can be calculated by data buyers. Having dened the
necessary structures for the data markets we wish to study,
we are now ready to study the competitive equilibria that
arise in these marketplaces.
First, we note that for any data source i, due to the form
of the payment contract, they will report the same value to
all data buyers.
Proposition 1. Fix any data source i. Pick any vector of
variances (cid:126)2 (one variance for each data buyer), and let e =
max {e : 2
i (e) = ( (cid:126)2 )j }, i.e. e is the minimum amount of
effort for data source i to generate measurements of variance
(cid:126)2 . Then, data source i has higher payoff, dened by (1), by
choosing variances 2
i (e) for all j , than the payoff earned
from providing each buyer j with data of variance ( (cid:126)2 )j .
In other words, since the payment contract from each data
buyer j is increasing (in expectation) with respect to effort,
data source i will never have incentive to add noise to a
measurement once the effort has been exerted. Thus, for the
rest of this paper, we shall write (cid:126)y to denote the measurement
reported to all data sources j .

I I I . R E SU LT S
In this section, we analyze the behavior we can expect
from each of the agents in the market place, by considering
the game that forms between the members of B as they select
the parameters in the contracts they offer to the data sources.
Adopting standard game-theoretic short-hand notion, we
denote the set of pricing parameters buyer k selects by

dj
=

dj
(9)

(ck , dk ) , and we denote the choice of the pricing parameters
of the other members of B by (ck , dk ). From now on, we
use the index k to single out a specic buyer, the index q to
single out a data source, the index j to sum over a collection
of buyers, and the index i (and sometimes l) to sum over a
collection of sources.
We begin our analysis by determining under what condi-
tions the data sources will accept the collection of contracts
offered to them by the data buyers. Recall that data source q
will accept all of the contracts offered by the data sources if
and only if the ex-ante total payments are non-negative (4)
and each data buyers payment is non-negative ex-ante (5).
Let x denote the probability measure that puts mass 1 at
(cid:88)
 =
point x. Then, we may simplify (4) for a xed q by noting
(cid:17)2
(cid:16)
(cid:88)
qE (cid:88)
that:
yq  f j
cj
pj
(cid:16)
(cid:17)
(xq )
q ((cid:126)x, (cid:126)y)
(cid:88)
q  (cid:88)
(cid:126)xq ,(cid:126)yq
jB
jB
jB
q (eq ) + gj ( xq , xq , (cid:126)2q )
cj
djq
jB
jB
(cid:16)
(cid:17)  eq
(cid:88)
q  (cid:88)
Then, (4) holds if and only if:
q (eq ) + gj ( xq , xq , (cid:126)2q )
cjjB
jB
(cid:16)
(cid:17)
Similarly, (5) holds if and only if:
q  dj
q (eq ) + gj ( xq , xq , (cid:126)2q )
cjq
As our goal is to nd situations where the buyers receive
data from each of the data sources, we shall include equations
(9) and (10) as constraints in the game between data buyers.
Indeed, given a choice of (ck , dk ), the objective of buyer
k is to optimize the following problem:
E (cid:104)(cid:80)
(cid:105)  e
J k ((ck , dk ), (ck , dk ))
E (cid:104)(cid:80)
(cid:105)  ei
i  0
i ((cid:126)x, (cid:126)y( (cid:126)e ))
jB pj
i ((cid:126)x, (cid:126)y((cid:126)e))(cid:3)  0
E (cid:2)pki ((cid:126)x, (cid:126)y( (cid:126)e ))
jB pj
(13)
i = arg maxei
(14)
i  0, dk
i  0
(15)
ck
Each constraint ho');
INSERT INTO posts (postId,userId,title,body) VALUES (78,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','lds for all i  S . Recall that J k was
dened in (2). Note that [4] showed that the payments induce
dominant strategies, so (13) is an optimization that does not
depend on ei .
In general, this may be a computationally difcult problem
for bk to solve. For illustrative purposes, for the rest of this
paper, we will assume specic forms for the estimators the
buyers employ and the  functions which dene the data
sources. We rst assume:
Assumption 2. For each data source i, i (ei ) is character-
ized by the the constant i > 0 and of the form:
i (ei ) = exp(i ei )
Note that this implies that  is convex, strictly decreasing
and always positive, which are all desirable properties in

min
ck ,dk
s.t.

(10)

(11)

(12)

(16)

2

our context. Furthermore, note that this is the form of the
standard deviation, not the variance.
We next determine the level of effort data sources will
exert given the pricing parameters set by the data buyers.
Fix a data source q and taking the derivative of (1) with
 i (eq )
(cid:88)
respect to eq , we obtain:
q (eq )  1 =djdeq
jB
(cid:88)
 q exp(2q eq )  1
jB
(cid:17)
(cid:16)(cid:80)
(cid:16)
(cid:17)
Setting this derivative equal to 0 yields:
jB dj2q
q =

(17)

dj
q

ln

2

2

(18)
q =


 dj
q (eq ) =

hk (xi , (cid:126)x, F )2
gk ((cid:126)x, F , (cid:126)2 ) =

This is the optimum effort selection for data source q . We
can also compute how this optimal point varies with dj
i :
(cid:17)
(cid:16)(cid:80)jB djAlso we can easily calculate the optimum variance:
(cid:17)
(cid:16)(cid:80)jB djq
Assumption 3. (Separable estimators) For each buyer k 
B , the estimator for f that buyer k employs, f k , is separable.
(cid:88)
In other words, there exists a function hk such that:
iS
Furthermore, we assume that h  0.
Note that
linear regression, polynomial regression and
nite-kernel regression all produce separable estimators. Ap-
plying Assumption 3 for the estimators, we may rewrite the
(cid:88)
loss function for buyer k as:
J k ((ck , dk ), (ck , dk )) =
(cid:88)
(cid:88)
iS
(cid:34)
(cid:32)k (cid:88)
(cid:88)iS
jk
i  dk
i (eck
i ) +iS
li
Recall that each xi is xed and common knowledge; thus,
we can replace each of the above evaluations of the h
functions with constants. Dene  j
i = hj (xi , (cid:126)x, Fj ),  j
i,l =
hj (xl , (cid:126)xi , xi ) for i (cid:54)= l and  j
i,i = 1. Note that   0.

i ) 
i (e
hk (xi , (cid:126)x, Fk )2
i (e
(cid:35)(cid:33)
hj (xi , (cid:126)x, Fj )2
i ) +
l (e
hk (xl , (cid:126)xi , xi )2
l )

(cid:88)
(cid:88)
iS
iS

2

+

 kjB dj
j  j(cid:88)li

k (cid:88)
iS

Then, this becomes:
i ) 
J k ((ck , dk ), (ck , dk )) =
i (e
(cid:88)
i 2
 k
i (e
(cid:32)
(cid:34)
(cid:35)(cid:33)
 ji 2
i ) +
(cid:88)
k (cid:88)jk
i  dk
i (e
l (e
 2
 kck
i,l2
 k
i ) +
l )
i  (cid:88)
(cid:88)iS
li
i (e
(cid:34)
(cid:35)(cid:33)
(cid:32)
i ) +
k (cid:88)
iS
jk
i  dk
i (e
l (eck
i,l2
 k
i ) +
l )
i  (cid:80)iS
j  j
In efforts towards succinctness, let  k
i .
jk k
i =  k
i (e
i ) in (18),
We will now plug in the expression for 2
(cid:88)
yielding:
i (e
J k ((ck , dk ), (ck , dk )) =
(cid:35)(cid:33)
(cid:32)
(cid:34)
i 2
 k
i ) +
k (cid:88)
(cid:88)
iS
i  dk
i (e
l (e
i,l2
 kck
(cid:88)
i ) +
l )(cid:16)(cid:80)
(cid:17)
iS
li
(cid:32)
(cid:34)
iS(cid:17)
(cid:16)(cid:80)
i  dk(cid:35)(cid:33)
ck
(cid:88)jB dj2
(cid:16)(cid:80)
(cid:17) k
i,l(cid:88)
jB dj
lil
(cid:16)(cid:80)
(cid:17) k

(cid:88)
ck+
k (cid:88)
jB dj
iSi(cid:16)(cid:80)
(cid:17)
 k
i  dk
i,ljB dj
iS
lS(Note here we joyfully take advantage of our convention that
i,i = 1.)
 k
Finally, similar reasoning lets us write for any data source
(cid:16)
(cid:17)
q ((cid:126)x, (cid:126)y)(cid:3) = ck
E (cid:2)pk
q and data buyer k :
q  dk =
q (eq ) + gk ( xq , xq , (cid:126)2q )(cid:88)q  dk
ck
hk (xi , (cid:126)xq , xi )2
i (ei )
q (eq ) +
(cid:33)
(cid:32)(cid:88)iq
q  dk
q ,i2
 k
ck
i (ei )iS
(cid:32)(cid:88)
q ((cid:126)x, (cid:126)y)(cid:3) = ck
E (cid:2)pk
At optimum effort l');
INSERT INTO posts (postId,userId,title,body) VALUES (79,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','evels, this becomes:
q  dkiS

i (e
q ,i2
 k
i )

(cid:33)

+

=

i

(22)

(21)

jB

 0

 0 (20)

q  dk
ck
k
i,l
(cid:17)
2dtotall

lS
(cid:16)
2dtotal2i

(cid:88)

(cid:16)(cid:80)
(cid:17)
 k
q ,i
jB dj
iSiAlso using the expression for e
i given in (17), buyer k
(cid:35)(cid:33)
(cid:32)
(cid:34)(cid:80)
has the following optimization problem:
+ k (cid:80)
(cid:80)
i  dk(cid:32)(cid:80)
(cid:34)
(cid:33)(cid:35)
ck
(19)min
iS
iS
s.t. (cid:80)2dtotalck ,dkj
i  dj
 ln

(cid:32)(cid:80)
(cid:33)
cj
i,l
lS
2dtotallk
i  dk
= (cid:80)
i,l
ck
lS2dtotall
jB dj
dtotali
i  0, dk
i  0
ck
(23)
Every constraint above holds for all i  S . Here, (22)
is a denitional, rather than binding, constraint. Also, note
that without loss of generality, we can take k = 1, by
(cid:19)
(cid:18)(cid:80)
i accordingly. Additionally, we can remove
normalizing the  k
i  0, as it is redundant in light of the
the constraint ck
 0, since   0 and
i  dkconstraint ck
i,l
lS2dtotal
d  0.This leads to the following result.
Theorem 1. Consider the game where each buyers objective
i  0 for
is to solve the optimization in (19), and assume  j
all i  S , j  B . Then there are either an innite number of
generalized Nash equilibria, or there is no generalized Nash
equilibrium.
Furthermore,
in the case where there are an innite
number of generalized Nash equilibria, there is a unique
collection of d parameters, in the sense that if ((cid:126)c, (cid:126)d) and
((cid:126)c(cid:48) , (cid:126)d(cid:48) ) are both generalized Nash equilibria, then (cid:126)d = (cid:126)d(cid:48) .
(cid:33)
(cid:32)(cid:88)
Additionally, the c parameters lie in the convex polytope
ln (cid:0)2dtotal
(cid:1)
(cid:88)
(cid:88)
dened by the following constraints:
 j
i,l
(cid:33)
(cid:32)(cid:88)2dtotal
2i
jB
jB
lS k
i  dk
i,l
ck2dtotal
lSThe effort exerted by each data source is the same in each
generalized Nash equilibrium.

cj
i =

dj
i

l

l

Before proving this theorem, we discuss the assumption
i  0. This implies that, for each data buyer, the
that  j
penalty for other data buyers successful estimation does
not outweigh the benet of having a good estimator. This
assumption means that no data buyer will have incentive to
drive the variance of one data source up towards innity.
We prove the following useful lemma, and then prove our
theorem.
Lemma 1. Suppose ((cid:126)c, (cid:126)d) is a GNE for the game dened by
(cid:33)
(cid:32)(cid:88)
ln (cid:0)2dtotal
(cid:1)
 (cid:88)
(cid:88)
(19). The following equality holds for all i and k:
 j
i,l2dtotal
2i
jB
jk
lSIn other words, (20) is always tight in equilibrium.

ck
i =

dj
cj
i

+

l

cj
+

dj
= 0

(cid:1)

ck
i >

i  dk
ck
 k
i,l
2dtotal
(cid:88)
jB

 j
i,l
2dtotal
Proof. To prove this, note that, by the cost function of buyer
k , ck
i will always be chosen such that at least one of (20)
(cid:32)(cid:88)
(cid:33)
and (21) is tight. Suppose (21) is exclusively active, i.e.
 k
i  dk
i,l
ck
= 0
(cid:33)
(cid:32)(cid:88)
ln (cid:0)2dtotal2dtotallSi
2i
lS

 (cid:88)
jk
(24)
Note that (24) is the same constraint for every data buyer.
In other words, if it is loose for k , it is loose for all other
j . Thus, some other buyer j can reduce their cj
i and lower
their cost, and thus ((cid:126)c, (cid:126)d) cannot be an equilibrium.
This argument does fall apart in one situation, however.
No buyer can reduce their cost just by modifying c if (21)
(cid:33)
(cid:32)(cid:88)
is tight for all buyers k , i.e. for all k :
lS
In this case, (20), which we assumed held loosely, becomes
(cid:18)(cid:80)
(cid:19)
i such that 2dtotal
i < 1. ');
INSERT INTO posts (postId,userId,title,body) VALUES (80,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','Let buyer k increase dk
2dtotal
i =i
1, and then choose a new ck such that (21) holds tightly, i.e.. Note that this decreases their
ck = dk
(cid:88)
(cid:88)
i,l
lS2dtotalcost:
iS
iS
(This uses the fact that, since (21) holds for all buyers j , the
second term disappears.) Additionally, all the constraints of
the original optimization are still satised, so (ck
i ) was
i , dk
not an optimizer for buyer k .
This concludes our proof.
Proof.
(Theorem 1) We invoke Lemma 1 and substitute this
(cid:32)
(cid:16)
(cid:17)
(cid:88)
into the objective function, (19), for buyer k . Let:
 k
(cid:126)c, (cid:126)dJ(cid:32)(cid:88)
(cid:32)
(cid:33)
(cid:33)
ln (cid:0)2dtotal
2dtotaliSi
2i
lS

 cji
(cid:16)
(cid:17)
(cid:18)(cid:80)
(cid:126)c, (cid:126)d= (cid:80)
i  dkck
i,l
lS2dtotall
jB dj
dtotali
i  0
dk
(cid:32)
We quickly manipulate the cost function a little to a more
desirable form: (cid:88)
iS

(cid:88)
jk

 j
i,l
2dtotal
 k2dtotal
 k2dtotal
This yields:

subject to

min
ck ,dk

(cid:33)

(cid:19)

 0

(cid:1)

 k
+

>

=

dj
i

(cid:32)
dj
(cid:88)
jk

(cid:33)

(cid:1)

=

+

l

(cid:33)

 j
i,l
2dtotal
(cid:33)
ln (cid:0)2dtotal
 cj+
(cid:33)
ln (cid:0)2dtotal
(cid:1)2ii
2i
(cid:88)
 (cid:88)
i  j
dj
i,l
cj
i =
(cid:33)
ln (cid:0)2dtotal
(cid:1)
2dtotaliS
jki2i
(cid:88)
 (cid:88)
iS
jk

(cid:32)(cid:88)
(cid:32)
(cid:88)
lS
 k2dtotal
(cid:88)
(cid:88)
(cid:88)
iS(cid:32)
(cid:88)
iS
jk
lS
 k2dtotal
(cid:88)
(cid:88)
(cid:88)
iSl  j
dj
l,i
(cid:32)
2dtotal(cid:88)
iS
lS
jk(cid:32)(cid:88)
iS
lS

 k(cid:33)
2dtotali

dj
l  j
l,i
2dtotali

 cj
cj
i =

+
ln (cid:0)2dtotal2i

(cid:33)

(cid:1)

+

(cid:88)
jk

(cid:1)

cj
i +

min
ck ,dk

subject to

 kln (cid:0)2dtotal
(cid:1)2dtotaliiln (cid:0)2dtotal
2i2i

Note the index swap on the  terms in the second equality.
Then dene:
i , ck , dk ) =
(cid:32)(cid:88)
(cid:33)
i (dk
J k
(cid:88)
dj
l  j
 cj
l,ii + (cid:80)
(cid:80)2dtotal (cid:88)
lS
jklS dj
l  j
 k
jk
l,i
2dtotaljk(cid:80)
Thus, the overall optimization can again be re-written:
(cid:19)
(cid:18)(cid:80)
i , ck , dk )
iS J k
i (dk
= (cid:80)
 0
i  dkck
i,l
lS2dtotall
jB dj
dtotali
i  0
dk
(cid:88)
We differentiate the cost with respect to dk
q :
i , ck , dk ) =
q , ck , dk ) =
J k
i (dk
q (dk
J k
q + (cid:80)
(cid:80)
iS
lS dj
l  j
  k
jk
l,q
q  (cid:80)
(cid:80)
2(dtotal
)2q k
lS dj
l  j
l,q + dtotal
jkq  (cid:80)
(cid:80)
2(dtotal
)2q k
l  j
lq dj
l,q + dk
jk)2q
2(dtotal
1
2dtotal

 dk

 dk
q

=

k )

k , d


 dk
q (dk
J k
q , c

q =  k
dk
q +

Note that we use the fact that  j
q ,q = 1 for all j . It is easy
< 0 if 0  dk
q + (cid:80)
(cid:80)
to see that:
q + (cid:80)
(cid:80)
lq dj
l j
q <  k
q + (cid:80)
(cid:80)
jk
l,q
lq dj
l j
= 0 if dk
q =  k
jk
l,q
l j
lq dj
> 0 if dk
q >  k
jk
l,q
(cid:88)
(cid:88)
Thus, the maximizing dk
q is given by:
l  j
dj
l,q
jk
lq
Performing this analysis for all combinations of q  S and
k  B yields a system of M  N equations with M  N
unknowns, of the form (25).
As we have before, let (cid:126)d denote a column vector with
i for each i  S and j  B . Similarly, let (cid:126) denote
entries dj
a column vector containing all the terms of the form  j
i .
Then, we may represent this system of equations with the
following matrix equation:
(cid:126)d = A (');
INSERT INTO posts (postId,userId,title,body) VALUES (81,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','cid:126)d + (cid:126)

(25)

(26)

Here, A is a non-negative matrix whose entries are the values
of the various  parameters at the appropriate places, such
that (26) expresses the set of equality constraints dened by
(25) for all q  S and k  B . To nd an GNE of this game,
i  0 for all
it sufces to nd a solution to (26) such that dj
i and j .
Systems of equations of this form are well studied in the
economics literature, as they are of the form specied by the
celebrated Leontief input-output model. It has been shown
that such systems of equations have a non-negative solution
if and only if (A) < 1, where (A) is the spectral radius of
A [8]. Moreover, if such a solution exists, it must be unique.
Thus, if (A) < 1, inversion of this A matrix yields the
(cid:33)
(cid:32)(cid:88)
equilibrium (cid:126)d, and, by Lemma 1, we can pick any (cid:126)c that
ln (cid:0)2dtotal
(cid:1)
satises:(cid:88)
(cid:88)
 j
i,l(cid:33)
(cid:32)(cid:88)2dtotal
2i
jB
jB
lS k
i  dk
i,l
ck2dtotallSIf (A)  1, there will not exist a non-negative solution
and there is no point ((cid:126)c, (cid:126)d) that simultaneously optimizes
(19) for all k . It follows that there is either a unique (cid:126)d that
will constitute a Nash solution for the game, which produces
a convex polytope of potential GNE, or there this no solution
to the game, as desired.

cj
i =

dj
i

It is interesting to note that the existence of GNE depends
solely on the value of the  parameters; it does not depend
on the magnitude of the  parameters. This implies that the
existence or non-existence of GNE in this game is simply
an artifact of the incentive mechanisms we have chosen to
analyze, and does not depend on whether or not there are
solutions that are benecial to all parties involved. Note
that we chose this incentive mechanism based on several
desirable properties in the single-buyer case; whether or not
there exist mechanisms that extend to multi-buyer games in

a fashion that provides good efciency properties is an open
problem that we are currently investigating. In Section IV,
we calculate the  parameters for a specic example, and see
how equilibrium solutions in these marketplaces collapse as
the characteristics are varied.
Additionally, note that,
in the case where there is a
continuum of GNE, the effort exerted by data sources and (cid:126)d
parameters are the same across all equilibria. The ambiguity
arises in the (cid:126)c parameters. In other words, the ambiguity
arises in determining which data buyers will pay to ensure
that each data sources total compensation covers the cost
of their effort. In the extreme case, it is possible for one
on expectation. That is, for some k  B , (cid:80)
rm to pay for the entirety of the expected compensation
(cid:80)
i , and for all j (cid:54)= k , (cid:80)
offered to the data sources, while the the rms pay nothing
iS pk
i ((cid:126)x, (cid:126)y) =
iS e
iS pj
i ((cid:126)x, (cid:126)y) = 0. In Section V
we discuss possible mechanisms to alleviate the disparity that
may arise in these situations.
We next turn to analyzing the total utility experienced in
the marketplace for a given outcome of the');
INSERT INTO posts (postId,userId,title,body) VALUES (82,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ',' game. We begin
with the following denition.
Denition 2. (Ex-ante social loss of the data market) Sup-
pose that  j = 1 for all buyers j . Let (cid:126)e be the vector denoting
the level of effort the data sources exert. Then, we dene the
ex-ante social loss the marketplace to be the sum of the utility
(cid:34) (cid:16) f j
(cid:32)
(cid:17)2 
(cid:88)
functions of all the data buyers and data sources:
(x )  f (x )
L((cid:126)e) =(cid:17)2 (cid:35)(cid:33)
(cid:16) f k
(cid:126)x, (cid:126)yj
(cid:88)
(cid:88)
jB
(cid:126)x, (cid:126)yk (x )  f (x )
iS
kj
Note that this sum does not include any of the payments
made in the marketplace, as they are simply lossless transfers
of wealth. We require the additionally assumption that  j = 1
for all buyers j to ensure that these transfers of wealth are
lossless from a utility perspective, i.e. the buyers and sources
value the payment equally. This assumption allows us to
isolate the social loss due to the mechanism, and ignore any
losses due to differential preferences in payment currency.
Theorem 2. Suppose that Assumptions 2 and 3 hold. Fur-
i > 0, for all i  S and j  B , and that
ther, assume that  j
i,l > 0 for some i, l  S , j  B . Finally, suppose GNE
 j
solutions exist for the game, and let (cid:126)e denote the unique
exists (cid:126)e  RN such that L (cid:16)(cid:126)e
(cid:17)
< L (cid:0) (cid:126)e (cid:1). Furthermore, the
level of effort exerted by the data sources across each of
these GNE solutions, as stipulated by Theorem 1. Then, there
socially optimal levels of effort, (cid:126)e, are always less than the
induced levels of effort at equilibrium (cid:126)e .
which minimizes the value of L (cid:16)(cid:126)e
(cid:17)
Proof. We begin by calculating solving for the value of (cid:126)e
. Invoking Assumption 3
L (cid:16)(cid:126)e
(cid:17)
(cid:88)
(cid:88)
(cid:88)
and our denition of  , we may write:
 j
i 2
i ( ei ) +
iS
jB
iS

(27)

 j
ei

+

ei

 j
2q

Taking the derivative with respect to eq and repeating
our analysis with Assumption 2, and setting the resulting
(cid:88)
 exp(2q eq ) + 1 = 0
equation to zero we obtain:
jB
(cid:80)
We can re-arrange this to yield:
jB  j
q )
ln(2q
eq =
2q
produce the unique minimizer of L (cid:16)(cid:126)e
(cid:17)
Note, that L is strictly convex with respect to eq , so
choosing the entries of (cid:126)e according to equation (28) mustNext we compare this to the level of effort the sources
produce in the GNE of the game between the buyers. By
(cid:88)
(cid:88)
(25), we obtain that in the GNE for all i  S and k  B :
l,i   k
l  j
dj
i > 0
jk
li

i =  k
dk
i +

(28)

dj
q >

Furthermore, since there exists at least one  > 0, we know
q for some data source q and buyer k . It follows
that dk
q >  k
that, for this particular q :(cid:88)
(cid:88)
jB
jB

 j(cid:80)
(cid:80)
Thus, by (17), we see that
jB  j
jB dj
q )
ln(2q
q )
ln(2q(29)
= eq .q =
2q
2q
L (cid:16)(cid:126)e
(cid:17)
< L (cid:0) (cid:126)e (cid:1) since we chose (cid:126)e to be the unique minimizer
Thus the theorem is proved, since it must be the case that
of L.

Theorem 2 shows that there is always some social loss, ex-
ante, from a Nash solution compared to the social optimum.
Furthermore, the proof provides a way to identify where
this loss is incurred, and how to calculate how much is
lost. Note that the social welfare is always lost because
the effort induced in equilibrium is higher than is socially
optimal. This captures the intuition that each data buyer has
a negative externality: they wish to improve their estimates
without considering how their improved estim');
INSERT INTO posts (postId,userId,title,body) VALUES (83,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','ates hurt other
data buyers.
The proof itself also provides strong intuition on the
 parameters between buyers. Loosely speaking,
these 
parameters can be thought of as a measure of each buyers
market power, in the sense that it quanties how much one
buyer can inuence the payment contracts of other buyers
in the data market to his advantage. As an extremal case,
i,l = 0 for all i, l  S and j  B , there is no
when  j
coupling between the payments the buyers make, and the
social optimum coincides with the Nash solution.

IV. EXAM PL E : B E TW E EN TWO FIRM S
In this section, we present an example which demonstrates
how a data market may collapse as the parameters of the
system are varied. This example will also demonstrate how
the efciency of the data market, in terms of the ex-ante
social loss function L, changes as the market approaches
this collapse. In particular, we consider the case where there
are two data sources (s1 and s2 ) and two rms acting as data
buyers (b1 and b2 ). Each of the data sources is capable of
estimating the function
f : [1, 1]  R.
(30)
Let x1 , x2  [1, 1] denote the locations where s1 and s2
sample f , respectively. Assume that each of the data sources
are as dened in Assumption 2, with the characteristic
parameters 1 = 2 = 1.
Next, we assume that each of the data buyers is performing
(cid:34) (cid:20)x
linear regression on f , using the samples reported by the data
(cid:21)T
source. In this case [4]:
gj ((cid:126)x, Fj , 2 ((cid:126)e)) = ExFj

2 (e2 ))  X (X T X )1
1 (e1 ), 2
diag(2

(X T X )1X T 
(cid:21) (cid:35)
(cid:20)x
=

i =  2
 1
i =

2,1 = g(x1 , x2 , 2
2,1 =  2
 1
1 (e1 )) =

1,2 = g(x2 , x1 , 2
1,2 =  2
 1
2 (e2 )) =

1 (e1 ) +  j
 j
2 2
1 2
2 (e2 )
In this example, we assume F1 = F2 as the uniform
distribution on the domain of f , [1, 1]. Thus, for i  {1, 2}:
(x1  x2 )2/3 + (x2
i  x1x2 )2
2  2x1x2 )2
(x2
1 + x2
Note that, by these assumptions, g1 = g2 , and furthermore:
(x1x2 + 1)2
(x2
2 + 1)2
(x1x2 + 1)2
(x2
1 + 1)2
For illustrative purposes, we x x2 = 1, and see what
happens to the data market as we vary x1 along the interval
[1, 1].
Note that, when x1 = x2 = 1, it is no longer possible to
construct a linear estimator of f because there is insufcient
data. Thus, the example shows how the game between buyers
behaves as it becomes increasingly difcult to construct good
estimators. The (cid:126)d parameters of any Nash solution can be
d1


 1
 =
 1
found by solving:1
d2
 21(cid:124)
(cid:123)(cid:122)
(cid:125)
d1
 1 12
d2
 22
1,2Note that this B matrix is equal to I  A as dened in the
proof. We numerically solve this system of equations for
varying values of x1  [1, 1], and the results are shown in
Figures 1 through 4.

 2
2,10
0 2
1,2
0
 1
2,10

(31)

Figures 1 and 2, demonstrate how the  and  parameters
of the game change as a function of x1 . Figure 3 demon-
strates the d parameters that the buyers will offer the data
sources as x1 varies. And nally, Figure demonstrates the
L (cid:0) (cid:126)e (cid:1)
price of anarchy in the data market, as a function of x1
L (cid:16)(cid:126)e
(cid:17)
which is given by:
Here, (cid:126)e is the induced effort of the sensors in the Nash
solution of the game between data buyers, and (cid:126)e is the
socially optimal effort for data sources to exert. Further
comments in the captions of Figures 3 and 4 demonstrate
the inefciencies that arise in this example');
INSERT INTO posts (postId,userId,title,body) VALUES (84,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ','.

Fig. 1.
This gure depicts how the various  parameters of the system
vary as a function of x1 . Note that as x1  1,  diverges to innity, which
reects the fact that as x1 and x2 become increasingly close it becomes
more difcult to generate a linear estimator from samples at these data
points.

Fig. 2. This gure depicts how the various  parameters vary as a function
of x1 . As all of the  parameters converge to a value of 1 as x1  1, the
matrix B in equation (31) becomes singular, causing the breakdown of
solutions for the d parameters, as is depicted in Figure 3.

V. C LO S ING REMARK S
Weve analyzed the game that forms between a set of
data buyers when they wish to communally incentivize a
collection of strategic data sources, using a mechanism
that has been proposed in the literature. We derived, for a
particular form of the game, conditions for the existence of
GNE, and demonstrated that these solutions are frequently
socially inefcient. This motivates future work to develop a

1.00.50.00.51.0x110-1100101102103104105ji Parameters vs. x111,2112,221.00.50.00.51.0x10.00.20.40.60.81.01.21.41.6ji,l parameters vs. x111,2,21,212,1,22,1[6] F. Farokhi, I. Shames, and M. Cantoni, Budget-constrained contract
design for effort-averse sensors in averaging based estimation, arXiv
preprint arXiv:1509.08193, 2015.
[7] D. Dorsch, H. T. Jongen, and V. Shikhman, On structure and compu-
tation of generalized nash equilibria, SIAM J. Optimization, vol. 23,
no. 1, pp. 452474, 2013.
[8] S. Sta nczak, M. Wiczanowski, and H. Boche, Chapter 2: On the Positive
Solution to a Linear System with Nonnegative Coefcients. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2006, pp. 5168.

This gure depicts the Nash equilibrium (cid:126)d parameters for the
Fig. 3.
game between the buyers as a function of x1 . Note that, as x1  1,
the (cid:126)d parameters go off to innity, and the Nash equilibria between the
buyers breaks down. Comparing these results to Figure 1, we see that the
(cid:126)d parameters diverge much more quickly than the  parameters, meaning
that in the Nash equilibria to the game between the two buyers becomes
increasingly inefcient as x1  1.

Fig. 4.
This gure depicts the price of anarchy for the marketplace as
a function of x1 . When x1 = 1, the payments in the marketplace are
decoupled and the  parameters are all zero; in this instance the price of
anarchy is 1, and the market is perfectly efcient. However, as x1  1,
the price of anarchy diverges asymptotically to innity, and the marketplace
becomes increasingly inefcient as it becomes more difcult for the buyers
to construct the estimators they desire.

richer class of incentive mechanisms which alleviate these
issues. Possible solutions include more complex pricing
mechanisms, or perhaps the addition of a trusted third party
market-maker to mediate socially benecial transactions in
these data markets.

R E FER ENC E S

[1] D. E. Bakken, R. Rarameswaran, D. M. Blough, A. A. Franz, and T. J.
Palmer, Data obfuscation: anonymity and desensitization of usable data
sets, IEEE Security Privacy, vol. 2, no. 6, pp. 3441, Nov 2004.
[2] C. Dwork and A. Roth, The Algorithmic Foundations of Differential
Privacy.
Foundations and Trends in Theoretical Computer Science,
2014.
[3] H. Gao, C. H. Liu, W. Wang, J. Zhao, Z. Song, X. Su, J. Crowcroft,
and K. K. Leung, A survey of incentive mechanisms for participatory
sensing, IEEE Communications Surveys '||'&'||' Tutorials, vol. 17, no. 2,
pp. 918943, 2015.
[4] Y. Cai, C. Daskalakis, and C. Papdimitriou, Optimum statistical
estimation with strategic data sources, in JMLR: Workship and Conf.
Proc., vol.');
INSERT INTO posts (postId,userId,title,body) VALUES (85,9991,'Tyler Westenbroek, Roy Dong, Lillian J. Ratliff, and S. Shankar Sastry 12rA4 T.c 151047:ir Abstract In this paper, we introduce ',' 40, 2015, pp. 117.
[5] D. G. Dobakhshari, N. Li, and V. Gupta, An incentive-based approach
to distributed estimation with strategic sensors, in Decision and Control
(CDC), 2016 IEEE 55th Conference on.
IEEE, 2016, pp. 61416146.

1.00.50.00.51.0x110-110110310510710910111013101510171019102110231025djid parameters vs. x1d11,d21d12,d221.00.50.00.51.0x105101520253035L(~e)L(~e)Price of Anarchy vs. x1');
INSERT INTO posts (postId,userId,title,body) VALUES (86,7664,'Swami Sankaranarayanan ','Yogesh Balaji 
Rama Chellappa
UMIACS, University of Maryland, College Park

Carlos D. Castillo
12rA6 V.c 157047:ir
Abstract

Visual Domain adaptation is an actively researched
problem in Computer Vision.
In this work, we propose
an approach that leverages unsupervised data to bring the
source and target distributions closer in a learned joint fea-
ture space. We accomplish this by inducing a symbiotic re-
lationship between the learned embedding and a generative
adversarial framework. This is in contrast to methods which
use an adversarial framework for realistic data generation
and retraining deep models with such data. We show the
strength and generality of our method by performing ex-
periments on three different tasks: (1) Digit classication
(MNIST, SVHN and USPS datasets) (2) Object recognition
using OFFICE dataset and (3) Face recognition using the
Celebrity Frontal Prole (CFP) dataset.

1. Introduction
The development of powerful learning algorithms such
as Convolutional Neural Networks (CNNs) have provided
a classic pipeline for solving many classication problems
[28]. The abundance of labeled data has resulted in re-
markable improvements for tasks such as the Imagenet chal-
lenge: beginning with the CNN framework of Krizhevsky et
al [13] and more recently Resnet [9] and its variants. An-
other example is the steady improvements in performance
on the LFW dataset [26]. The common theme across all
these approaches is the dependence on lot of labeled data.
While labeled data is available and getting labeled data has
been easier over the years, the lack of uniformity of label
distributions across different domains results in suboptimal
performance of even the powerful CNN-based algorithms
on realistic unseen test data. This is abundantly clear in the
example of faces where most available labeled data tends to
be frontal. Hence the learning algorithm, in general, does
not perform equally well across viewpoints. The use of un-
labeled target data to mitigate the shift between source and
target distributions is the most promising direction domain

 First two authors contributed equally

adaptation. Hence this paper focuses on the topic of unsu-
pervised domain adaptation.

Figure 1. The top half provided an illustration of our image gen-
eration pipeline and a visual demonstration of how the proposed
approach is able to reconstruct source and target images. The tar-
get labels were not used during training. In the bottom half, we
show a t-SNE visualization of the unadapted and adapted encoder
representations demonstrating how our approach is able to adapt
the source and target distributions. Blue dots represent source data
and Red dots represent the target data.

In this work, we propose a way to learn an embedding
that is robust to the shift between source and target distri-
butions. We intend to achieve this by using unsupervised
data sampled from the target distribution to guide the su-
pervised learning procedure that uses data sampled from
the source distribution. The main contribution of this work
is that we propose an adversarial image generation frame-
work to directly learn the shared feature embedding using
labeled data from source and unlabeled data from the tar-
get.
It should be noted that while several methods have

Random noiseEmbeddingSource: MNISTTarget: USPSOriginal ImagesReconstructed ImagesImage generation pipelineAdaptedLearning to adapt between source and targetNon-adaptedused an adversarial framework for solving the domain adap-
tation problem, the novelty of the proposed approach is in
using a joint generative discriminative method where the
generation is performed using a variant of Generative Ad-
versarial Network (GAN) [7]. During training, the source
images are passed through the encoder to obtain an em-
bedding which is then used by the classier for ');
INSERT INTO posts (postId,userId,title,body) VALUES (87,7664,'Swami Sankaranarayanan  (part 2)','predict-
ing the source label and also used by the generator to gen-
erate a realistic source image. The realistic nature of the
images from the generator is controlled by the discrimina-
tor. The embedding is updated based on the discrimina-
tive gradients from the classier and generative gradients
from the adversarial framework. Given unlabeled target im-
ages, the embedding is updated using only gradients from
the adversarial part, since the labels are unavailable. Thus,
the embedding learns to discriminate better even in the tar-
get domain using the knowledge imparted by the generator-
discriminator pair. We would like to point out that although
the proposed approach uses an image generation procedure
for learning the domain shift, the quality of the image re-
construction is not our focus. By using the discriminator
as a multi-class classier, we ensure that the gradient sig-
nals backpropagated by the discriminator for the unlabeled
target images belong to the feature space of the respective
classes. By sampling from the distribution of the generator
after training, we show that the network has indeed learnt
to bring the source and target distributions closer. The bot-
tom half of gure 1 shows a t-SNE [19] visualization of the
embeddings for the MNISTUSPS setting for two cases:
(1) Non-adapted: Encoder trained with images from source
only (2) Adapted: Encoder trained with the proposed ap-
proach. It can be observed that the proposed approach re-
sults in a closer match between the source and target distri-
butions. We show examples of such reconstructions in Sec-
tion 4. To summarize, the major contribution of this work
is to provide an adversarial image generation framework for
unsupervised domain adaptation that directly learns a joint
feature space in which the distance between source and tar-
get distributions is minimized. Our experiments show that
the proposed framework yields superior results compared to
similar approaches which update the embedding based on
auto-encoders [4] or disentangling the domain information
from the embedding by learning a separate domain classi-
er [3]. This paper is organized as follows: Section 2 de-
scribes contemporary approaches for the unsupervised do-
main adaptation problem and places our work among them.
In section 3, we describe in detail the formulation of our ap-
proach and the iterative training procedure. In section 4, we
describe the experimental setups and discuss the results us-
ing both quantitative and qualitative experiments. We con-
clude this paper in section 5.

2. Related Work
Domain adaptation is an actively researched topic in
many areas including Machine Learning, Natural Language
Processing and Computer Vision. In this section, we focus
on visual domain adaptation since it is more relevant to our
work. Earlier approaches to domain adaptation focussed on
building feature representations that are invariant across do-
mains. This was accomplished either by feature reweighting
and selection mechanisms[10] [2], or by learning an explicit
feature transformation that aligns source distribution to the
target[8] [22] [6].
Recently, Deep Neural Networks have been shown to be
successful in learning complex feature representations that
enable them to achieve state-of-the-art performance in most
machine learning tasks [13] [9]. This ability to learn power-
ful representations has been harnessed to perform unsuper-
vised domain adaptation in [3][31][16] [18][30]. The un-
derlying idea behind such methods is to minimize a suitable
lo');
INSERT INTO posts (postId,userId,title,body) VALUES (88,7664,'Swami Sankaranarayanan  (part 3)','ss function that captures domain discrepancy, in addition
to the task being solved.
Deep learning methods for visual domain adaptation can
be broadly grouped into few major categories. One line of
work uses Maximum Mean Discrepancy(MMD) as a metric
to measure the shift across domains. Deep Domain Con-
fusion (DDC) [31] jointly minimizes the classication loss
and MMD loss of the last fully connected layer. Deep Adap-
tation Networks (DAN) [16] extends this idea by embed-
ding all task specic layers in a reproducing kernel Hilbert
space and minimizing the MMD in the projected space. In
addition to MMD, Residual Transfer Networks (RTN) [18]
uses a gated residual layer for classier adaptation.
Another class of methods uses adversarial losses to per-
form domain adaptation. Revgrad [3] poses the domain
adaptation problem as a minimax game between a domain
classier and a feature extractor. The goal of the feature
extractor is to produce embeddings that fool the domain
classier, while at the same time minimize the classica-
tion loss. Adversarial Discriminative Domain Adaptation
(ADDA) [30] on the other hand learns separate feature ex-
traction networks for source and target domains, the target
CNN is trained so that a domain classier cannot distinguish
the embeddings produced by the source or target CNN.
Adversarial networks have been successfully applied for
image generation tasks where the generator network G
and the discriminator network D compete in a 2-player
game [7]. G models the data distribution while D dis-
tinguishes the distribution produced by G from the true
data distribution. Following the success of GAN, several
methods have tried to use GAN based approaches for un-
supervised domain adaptation. Taigman et al. [29] train a
cross-domain generative model that maps samples from the
source domain to the target domain without utilizing any
source-target correspondence. Domain adaptation is then

performed by learning a classier on the transferred images.
Coupled GAN (CoGAN) [15] on the other hand trains a
coupled generative model that learns the joint data distribu-
tion across the two domains. A domain invariant classier
is learnt by sharing weights with the discriminator of the
CoGAN network.
Unlike the methods discussed above, we use image gen-
eration as a sub-task for domain adaptation. Related works
that use a similar approach are Deep Reconstruction Classi-
cation Networks(DRCN) [4] and Domain Separation Net-
works(DSN) [1]. DRCN uses feature embedding to mul-
titask source label prediction and target image reconstruc-
tion. DSN explicitly models the private and shared compo-
nents of source and target feature representations, and learns
such representations using a combination of feature similar-
ity loss and image reconstruction loss. Unlike these meth-
ods, we enforce the domain alignment constraint strongly
by training a generative model for the source data, and forc-
ing the encoder to produce the embeddings for the target
data, which when fed to the generative model produces
good source-like images.

3. Method
In this section, we provide a formal treatment of the pro-
posed approach and explain in detail our iterative optimiza-
tion procedure. Let X = {xi}N
i=1 be an input space of im-
ages and Y = {yi }N
i=1 be the label space. We assume there
exists a source distribution, S (x, y) and target distribution
T (x, y) over the samples in X.
In unsupervised domain
adaptation, we have access to the source distribution using
labeled data from X and the target distribution via unlabeled
dictor that is optimal in the joint distribution space S (cid:78) T
data sampled from X. Operationally, the problem of unsu-
pervised domain adaptation can be stated as learning a pre-
by using labeled source data and unlabeled target data sam-
pled from X. We consider problems where the data from X
have discrete labels from the set L = {1, 2, 3, ...Nc }, whe');
INSERT INTO posts (postId,userId,title,body) VALUES (89,7664,'Swami Sankaranarayanan  (part 4)','re
Nc is the total number of classes. Our objective is to learn
an embedding map F : X (cid:55) Rd that is used by a prediction
function C : Rd (cid:55) L. The predictor has access only to the
labels for the data sampled from the source distribution and
not from the target distribution. By extracting information
from the target data during training, F implicitly learns the
domain shift between S and T . In the rest of this section,
we use the terms source (target) distribution and source (tar-
get) domain interchangeably.
As explained in section 2, several approaches including
learning entropy-based metrics, learning a domain classier
based on a embedding network or denoising autoencoders
have been used to transfer information between the source
and target distributions. In this work, we use a GAN to help
the embedding bridge the gap between source and target
domains, since this enables a rich information transfer by

In
using both a generative and a discriminative process.
a traditional GAN, two competing mappings are learned:
the discriminator D and the generator G, both of which are
modeled as deep neural networks. G and D play a minimax
game where D tries to classify the generated samples as
fake and G tries to fool D by producing examples that are
as realistic as possible. More formally, to train a GAN, the
following optimization problem is solved in an alternative
manner:

(1)

min
max
Expdata (log(D(x))
+ Ezpnoise log(1  D  G(z ))
As an extension to traditional GANs, conditional GANs
[20] enable conditioning the generator and discriminator
mappings on additional data such as a class label or an em-
bedding. They have been shown to generate images of digits
and faces conditioned on the class label or the embedding
respectively [29]. Training a conditional GAN involves op-
timizing the following minimax objective:

(2)

min
max
Expdata (log(D(x))
+ E{zpnoise ,y} log(1  D  G(y , z ))
In this work, we employ a conditional GAN by condi-
tioning the generator using the embedding. Specically,
the input to the generator is a concatenated version of the
embedding and a random noise vector :
[F (x), z ] where
z  N (0, 1); F is the encoder mapping and [...] repre-
sents vector concatenation. The dimensionality of z is a
hyperparameter of our method; however for all our exper-
iments we have achieved consistent performance by set-
ting it equal to the dimensionality of F . The intuition be-
hind using a random noise vector is to give the generator
some extra degrees of freedom to model external variations
that are absent in the source data. The discriminator map-
ping D is a (Nc+1)-way classier taking labels from the set
LD = {L  {Nc + 1}}, with Nc real classes and an extra
class being the fake class. We denote the set {1, 2, . . . , Nc }
as real labels and Nc + 1 as the fake label. The inputs to
D can be real images from the source domain or generated
fake images from the source or target domains. To jointly
learn the embedding and the generator-discriminator pair,
we employ an alternating optimization procedure:

1. Given labeled source images as input, D classies the
real images into one of the real classes and classies
the generated fake images into the fake class.

2. Using the gradients from D , G is updated to produce
realistic class consistent source images.

3. F and C are updated based on the source images and
real labels in a traditional supervised manner.

Figure 2. The data ow during forward pass (solid lines) and gra-
dients during backward pass (dashed lines) are shown. A dashed
boundary for a block implies that block is being updated and solid
boundary implies it is held xed.

4. In the nal step, given the target images, we update
F by minimizing the probability of D to classify the
generated target images as fake.

For ease of implementation, steps 2 and 3 are combined
into a single update. Figure 2 shows th');
INSERT INTO posts (postId,userId,title,body) VALUES (90,7664,'Swami Sankaranarayanan  (part 5)','e direction of data
and gradient ow through our setup in each optimization
step.

(3)

log(1  D  G([F (xi ), zi ]))

Use of target data: The main strength of our approach is
how the target images are used to update the embedding.
Given a batch of target images [xi ]N
i=1 as input, we update
the embedding F by reversing the gradients of the following
N(cid:88)
loss function:
i=1
where zi a random noise vector sampled from N (0, 1).
The loss in (3) encourages D to classify the target images as
fake. Our objective during the target update step is that: For
the target domain, the embedding should be learnt so that
G, conditioned on the embedding, produces source-like im-
ages that fool D .
If this is achieved optimally, then one
can infer that the embedding has fully learnt to map the tar-
get distribution to the source distribution. To enable this
behavior, the loss function in (3) is used to update the em-
bedding by reversing the gradient of the discriminator cor-
responding to the fake label. This update will enable F to
preserve the class information for the target domain due to
the following reason: During the source update step, both
F and D are learned in a class consistent manner using la-
bels from the source domain. As training progresses, the
reversed gradients that are used to update the embedding
become well conditioned on the actual class of the target
image even though target label information is never pro-
vided. This symbiotic relationship between the embedding

Figure 3. Visualization of digit datasets

and the adversarial framework contributes to the success of
the proposed approach. It should be noted that the gradient
signals from (3) are used to update F only and reversal of
the gradient is performed as shown in gure 2.
Our iterative optimization procedure can be summarized
as follows:
 For source images, we update the mappings D , G, C and
F using the gradient of the loss function:

Lsrc = Ladv + Lcls where,
ExS log(D(x) +
Ladv = min
min
maxDE{zpnoise ,xS } log(1  D  G([F (x), z ]))
ExS log(C  F (x)
min
Lcls = minC

(8)
 is the coefcient that trades off between the classica-
tion loss and the adversarial loss.
 For the target images, the loss function involves updating
only the embedding F and does not involve the classier,
since no target labels are observed during training.
ExT ,zpnoise  log(1  D  G([F (x), z ]))
(9)

Ltgt = max
The pseudocode for this iterative procedure is given in
Algorithm 1. We nd that our approach is not overly sen-
sitive to the cost coefcient . However, the value of the
parameter is dependent on the application and size of the
dataset. Such specications are mentioned in section 4.

4. Experiments and Results
This section reports the experimental validation of our
method. To demonstrate the versatility of our approach,
we perform experiments on three different tasks that span
across multiple domains - digit recognition, object recog-
nition and face recognition.
In the process, we also test
the sensitivity of our approach to the size of dataset. Each
dataset we experimented on, varies greatly in size, with
some containing just a few hundred images to others hav-
ing tens of thousands of images.

    G   D   FSource Input    G   D   FSource InputStep 1: Update DStep 2: Update G, F, C      G   D   FTarget InputStep 3:  Update F C C CReverse gradient7:

8:

max
Algorithm 1 Iterative training procedure of our approach
1: training iterations');
INSERT INTO posts (postId,userId,title,body) VALUES (91,7664,'Swami Sankaranarayanan  (part 6)',' = N
2: for t in 1:N do
Sample k images from source domain S : {si , yi }k
3:
i=1
Let fi = F (si ) be the embeddings computed for the source images.
4:
i=1  N (0, 1),where dim(zi ) = dim(F (si ))
Sample k random noise samples {zi }k
5:
Update discriminator to classify real/fake samples using the adversarial loss.
k(cid:88)
6:
(cid:2) log(D(si )) + log(1  D  G([fi : zi ]))(cid:3)
i=1
Update the generator through the discriminator gradients computed using real labels.
k(cid:88)
(cid:2) log(1  D  G([fi : zi ]))(cid:3)minG
i=1
Update the embedding using a linear combination of the adversarial loss and classication loss.
k(cid:88)
(cid:2) log(C (fi )) +  log(1  D  G([fi : zi ]))(cid:3)minF
i=1
Sample k images from target domain T : {ti }k
i=1
Let fi = F (ti ) be the embeddings computed for the target images.
Sample k random noise samples {zi}k
i=1  N (0, 1), where dim(zi ) = dim(F (ti ))
Update the embedding by minimizing the likelihood (or maximizing the negative log likelihood) of the target images
being classied as fake by the discriminator.
k(cid:88)
(cid:2)  log(1  D  G([fi : zi ]))(cid:3)
i=1

9:
10:
11:
12:

max
1
(7)
k

(4)

(5)

(6)

13: end for
Table 1. Accuracy (mean  std%) values for cross-domain recognition tasks over ve independent runs on the digits based datasets. The
best numbers are indicated in bold and the second best are underlined.  denotes unreported results. MN: MNIST, US: USPS, SV: SVHN
SV  MN MN  SV
US  MN
MN  US
Method
26.0  1.2
60.3  1.5
75.2  1.6
57.1  1.7
Source only
77.1  1.8
73.0  2.0
RevGrad [3]
73.982.0  0.16
73.7  0.04
91.8  0.09
40.1  0.07
DRCN [4]
89.1  0.8
91.2  0.8CoGAN [15]76.0  1.8
89.4  0.2
90.1  0.8
ADDA [30]36.4  1.2
92.5  0.7
90.8  1.3
84.7  0.9
Ours
 DRCN approach uses more convolutional lters and cross-validates the number of neurons in fully connected layers

4.1. Digit Experiments
In the rst set of experiments, we validate our method
by performing domain adaptation on three standard digit
datasets - MNIST [14], USPS [11] and SVHN [21]. Each
dataset contains digits belonging to 10 classes (0-9), each
captured under different conditions. We test
the four
common domain adaptation settings: SVHN  MNIST,

MNIST  SVHN, MNIST  USPS and USPS  MNIST.
In each setting, we use the label information only from the
source domain, thus following the unsupervised protocol.
MNIST and USPS are large datasets of handwritten dig-
its captured under constrained conditions. Both these do-
mains are visually very similar and this makes adaptation
relatively easy. SVHN dataset, on the other hand was ob-

Figure 4. In each set of images, the top row shows the original images and the bottom row shows the reconstructed images from the
respective dataset. The top half shows the reconstructions for SVHN  MNIST task and the bottom half for MNIST  USPS task.

tained by cropping house numbers in Google Street View
images and hence captures much more diversity. As can
be seen from Figure 3, in the SVHN dataset, there is sig-
nicantly more domain shift with respect to the other two
datasets which makes adaptation hard.
Architecture and Preprocessing For all digit experi-
ments, following other recent works [3][30], we use a mod-
ied version of Lenet architecture as our encoder. The en-
coder pipeline has three 5  5 convolution layers containing
64, 64 and 128 lters respectively, followed by ReLU and
pooling. This maps a 32  32 image to a 128 dimensional
embedding. The label predictor containing two FC layers
(128  128  10) maps this 128 dimensional embed-
ding to a 10 dimensional vector. The generator architecture
was adopted from DCGAN [23] - it contains 4 full convo-
lution layers with 512, 256, 128 and 1 lters respectively,
each followed by ReLU and batch normalization except the
last layer. The output of the last layer is the generated im-
age. The discriminator architecture contains three convolu-
tional layers with 64, 128 and 256 lters, followed by two
fully c');
INSERT INTO posts (postId,userId,title,body) VALUES (92,7664,'Swami Sankaranarayanan  (part 7)','onnected layers of size 128 and 11. We used Adam
solver [12] with base learning rate of 0.0002 to train our
models. The cost coefcient  is set to 0.1. We resize all
input images to 32  32 and scale their values to the range
[0, 1]. No data augmentation was performed.
(a) MNIST  USPS
We start with the easy case of adaptation involving MNIST
and USPS. The MNIST dataset is split into 60000 train-
ing and 10000 test images, while the USPS contains 7291
training and 2007 test images. In our experiments, we fol-

low the protocol established in [17], sampling 2000 images
from MNIST and 1800 images from USPS. Since random
sampling is prone to high variance in performance, we con-
duct ve independent runs, sampling data randomly in each
run and report the average performance. We observe that
our method performs well in both directions, MNIST 
SVHN and SVHN  MNIST. Specically, we achieve the
best performance of 92.5% in MNIST  USPS and 90.8%
in USPS  MNIST among the compared methods.
(b) SVHN  MNIST
Compared to the previous experiment, SVHN  MNIST
presents a harder case of domain adaptation owing to larger
domain gap. Following other works [3]
[30], we use the
entire training set (73257 SVHN images and 60000 MNIST
images) to train our model, and evaluate on the test set of
target domain. Adaptation is much harder in MNIST 
SVHN direction compared to SVHN  MNIST, as SVHN
is more diverse than the constrained MNIST dataset. In fact,
most methods fail to adapt [3], or did not report results in
this setting. However, our method achieves 36.4% accuracy,
which is 10% higher than the baseline. We would like to
point out that the best performing method DRCN uses data
augmentation and denoising to improve their performance,
none of which we perform. In addition, they use more lters
in the convolutional layers and select their model architec-
ture by cross-validating the number of neurons in the fully
connected layers. On the other hand, we use a xed standard
architecture for all our experiments. In SVHN  MNIST,
our method performs considerably better than other meth-
ods achieving state-of-the-art accuracy of 84.7%. A sam-

SVHN-sourceMNIST-targetSVHN to MNISTMNIST to USPSMNIST-sourceUSPS-targetFigure 5. Visualization of ofce datasets

ple of reconstructions obtained from our approach for two
tasks, SVHN  MNIST and MNIST  USPS are visual-
ized in Figure 4.
4.2. OFFICE dataset
The next set of experiments involve the OFFICE dataset,
which is a small scale dataset containing images belong-
ing to 31 classes from three domains - Amazon, Webcam
and DSLR, each containing 2817, 795 and 498 images re-
spectively. The small dataset size poses a challenge to our
approach since we rely on GAN which demands more data
for better image generation. Nevertheless, we perform ex-
periments on OFFICE dataset since our interest is not in
generating good images, rather in utilizing the generative
process to obtain domain invariant feature representations.
Architecture and Preprocessing Training deep net-
works from scratch on small datasets give poor perfor-
mance. So, an effective technique used in practice is to
ne-tune networks trained on a related task having large
data [32]. Following other domain adaptation works [3] [4]
[18], we use a pre-trained Alexnet model trained on Ima-
genet as our encoder. We plug in an FC layer to the encoder
to produce a 256 dimensional vector output, which we use
as our feature embedding. The generator contains 5 full
convolution layers with 1024, 512, 256, 128 and 1 lters re-
spectively, each followed by ReLU and batch normalization
except the last layer. Each layer upsamples its input to twice
the size. The output of the last layer is the generated image.
The discriminator architecture contains four convolutional
layers with 128, 128 and 128 lters each of size 55 ex-
cept the last layer whose lters are 55. This is followed
by three fully connected layers of size 500, 500 and 32. We
initialize all newly added layers u');
INSERT INTO posts (postId,userId,title,body) VALUES (93,7664,'Swami Sankaranarayanan  (part 8)','sing the method of Glorot
et al. [5] and learn them from scratch. It should be noted
that even though the inputs are 224  224, the generator is
made to reconstruct a downsampled version of size 64  64.
We use Adam solver for optimization with a base learning
rate of 0.0002 and momentum 0.8. The dimension of the
random noise vector is set to the dimension as the encoder
representation, which in this case is 256. The cost coef-
cient  is set as 0.1. We rst resize all images to 256  256
and then randomly select 224  224 crops as input images.
In addition to random cropping, we also perform random

Figure 6. Visualization of CFP dataset

mirroring. We would like to point out that these are the
standard augmentation techniques performed in recent do-
main adaptation works [3][18][31]
In all our experiments, we follow the standard unsuper-
vised protocol - using the entire labeled data in the source
domain and unlabeled data in the target domain. Table 2 re-
ports the performance of our method in comparison to other
methods. From these results, we can make the following
observation - Our method performs the best when we have
more data in the source domain. In particular, we observe
good performance improvement with Amazon as source:
0.8% and 3.3% in A  W and A  D respectively. Our
method performs on par with the best performing method
on the other settings too.

4.3. CFP dataset
In this experiment, we evaluate our approach on the do-
main of faces. The Celebrities in Frontal-Prole (CFP)
dataset [27] was curated to evaluate the strength of face ver-
ication approaches across pose, more specically, between
frontal pose (yaw < 10  ) and prole poses (yaw > 60
 ). They argue that commonly used CNN-based approaches
that perform well on the frontal pose setting perform poorly
on prole pose setting. The dataset contains 500 individu-
als in total with 10 frontal images and 4 prole images per
individual. The CFP dataset denes a verication protocol
by providing frontal-frontal and frontal-prole pairs with a
1 or 0 label indicating same/different pair. This protocol
does not provide access to class labels during the training
phase. Since the focus of the current work is improving
classication performance for domain adaptation, we create
a modied version of the CFP protocol for face recognition
similar to the OFFICE protocol: We treat all frontal images
as source and all prole images as target. We then train the
compared approaches using labeled source images and un-
labeled target images and test them on the target images.
We do not consider the adaptation in the opposite direction
since it is not a realistic situation. We will make the codes
and models publicly available.
Architecture Training deep networks for face recogni-
tion from scratch using a small dataset leads to severe over-
tting. Based on our preliminary experiment done by ne-
tuning Alexnet [13] on this dataset, we achieved very low
Rank-1 accuracy on the target images. Hence, we test

Sample Frontal ImagesSample Profile ImagesTable 2. Accuracy (mean  std%) values on the OFFICE dataset for the standard protocol for unsupervised domain adaptation [6]. Results
are reported as an average over 5 independent runs. The best numbers are indicated in bold and the second best are underlined.  denotes
unreported results. A: Amazon, W: Webcam, D: DSLR
W  A
A  W
Method
61.1  0.5
48.6  0.4
Alexnet - Source only
61.0  0.5');
INSERT INTO posts (postId,userId,title,body) VALUES (94,7664,'Swami Sankaranarayanan  (part 9)','
49.4  0.6
DDC [31]
49.8  0.3
68.5  0.3
DAN [16]
73.0  0.6
RevGrad [3]73.3  0.3
51.1  0.5
RTN [18]
54.9  0.5
68.7  0.3
DRCN [4]
53.5  0.8
74.1  0.5
Ours

D  W Average
95.6  0.3
69.1
95.0  0.3
69.3
96.0  0.1
71.7
96.4  0.496.8  0.2
73.7
96.4  0.3
73.6
96.6  0.2
74.7

A  D
64.4  0.3
64.9  0.4
66.8  0.271.0  0.2
66.8  0.5
74.3  0.6

D  A
46.1  0.6
47.2  0.5
50.0  0.450.5  0.3
56.0  0.5
50.6  0.7

W  D
99.1  0.2
98.5  0.3
99.0  0.1
99.2  0.3
99.6  0.1
99.0  0.2
99.3  0.3

the domain adaptation approaches by initializing our en-
coder with the weights of a pretrained face recognition net-
work [25]. We add a fully connected layer with 256 neurons
which is used as our feature embedding. The same architec-
tures as the OFFICE experiment are used for the classier
and discriminator networks. We nd that the generator is
unable to reconstruct the full input image of size 224x224,
hence it is made to reconstruct only a 32x32 downsampled
version of the input. For the generator, we use four full con-
volution layers. We initialize all newly added layers using
the method of Glorot et al. [5] and learn them from scratch.
We use Adam solver for optimization with a base learning
rate of 0.0002 and momentum 0.8. The dimension of the
random noise vector is set to the same dimension as the fea-
ture embedding. The cost coefcient  is set as 0.05.
Preprocessing: We align all the face images from the
CFP dataset using a similarity transformation as required by
the pretrained model [25]. The landmarks used for align-
ment were obtained using the HyperFace approach [24].
The aligned images were then resized to a 256x256 frame.
During training, we applied data augmentation in the form
of random cropping and mirroring as in the previous exper-
iments. Please note that the same preprocessing and data
augmentation techniques were applied to all the approaches
compared in Table 3. The inputs to the encoder are sub-
tracted using the mean provided with [25] and hence the
discriminator inputs are scaled to be in the range [1, 1].

Rank-5
Rank-1
Method
78.1  0.4
57.8  0.5
Source-only
78.7  0.2
58.5  0.3
RevGrad [3]
79.1  0.3
60.1  0.4
RevGrad++
83.2  0.2
62.3  0.5
Ours
Table 3. Domain adaptation performance on CFP dataset. Rank-1
and Rank-5 accuracies (mean  std%) are reported for all the ap-
proaches as an average over 5 independent runs. The CFP dataset
has 2000 prole images in total which is used as the target data.

As this is a new dataset for general domain adaptation
practitioners, we compare our method with two baselines:

(1) Source only, where we directly ne-tune the encoder and
classier layers in a supervised manner to predict 1 of 500
identities using only source data (2) RevGrad, which corre-
sponds to the original architecture from the gradient reversal
work [3] (3) RevGrad++, where we make the domain classi-
er stronger by adding more neurons to the fully connected
layers. The original RevGrad approach has a (10241024)
architecture for the domain classier while the stronger ar-
chitecture we use has a (30722048) architecture. We used
this stronger architecture for gradient reversal in order to
provide a ');
INSERT INTO posts (postId,userId,title,body) VALUES (95,7664,'Swami Sankaranarayanan  (part 10)','fair comparison. Except the source only baseline,
other methods are trained using labeled source data and un-
labeled target data. The results of the CFP experiment is
shown in Table 3. The reported numbers are Rank-1 and
Rank-5 accuracies of the compared methods. Note that a
strong pretrained model used as our baseline network, still
yields only with 57.8% Rank-1 accuracy even after netun-
ing on this dataset using labeled source data. The RevGrad
and RevGrad++ approaches yield only moderate improve-
ments over the source only baseline.
In comparison, our
approach yields a signicant improvement (4.5%) over
the source-only baseline on Rank-1 accuracy and outper-
forms the stronger RevGrad++ baseline by 2.2%. We
show a signicant improvement over the compared methods
on Rank-5 accuracy by outperforming the closest method
by 4.1%. This shows that our method is able to leverage
target data even in a difcult case such as frontal to prole
comparisons.
5. Conclusion and Future Work
In this paper, we have addressed the problem of unsuper-
vised domain adaptation. We proposed a joint adversarial-
discriminative approach that transfers the information of
the target distribution to the learned embedding using a
generator-discriminator pair. We have shown the superi-
ority of our approach over existing methods that address
this problem using experiments on three different tasks, thus
making our approach more generally applicable and versa-
tile. Some avenues for future work include using stronger
encoder architectures and applications of our approach to
more domain adaptation problems such as RGB-D object

recognition and medical imaging.

Acknowledgement
This research is based upon work supported by the Of-
ce of the Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity (IARPA),
via IARPA R'||'&'||'D Contract No. 2014-14071600012. The
views and conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily represent-
ing the ofcial policies or endorsements, either expressed
or implied, of the ODNI, IARPA, or the U.S. Government.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon.

References
[1] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In Advances in Neu-
ral Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016,, 2016. 3
[2] H. Daume III. Frustratingly easy domain adaptation. In Pro-
ceedings of the 45th Annual Meeting of the Association of
Computational Linguistics, June 2007. 2
[3] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
by backpropagation. arXiv preprint arXiv:1409.7495, 2014.
2, 5, 6, 7, 8
[4] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and
W. Li. Deep reconstruction-classication networks for un-
supervised domain adaptation. In European Conference on
Computer Vision. Springer, 2016. 2, 3, 5, 7, 8
[5] X. Glorot and Y. Bengio. Understanding the difculty of
training deep feedforward neural networks. In Aistats, vol-
ume 9, 2010. 7, 8
[6] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ow ker-
nel for unsupervised domain adaptation. In 2012 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
20662073, 2012. 2, 8
[7] I. Goodfellow,
J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 2
[8] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation
for object recognition: An unsupervised approach. In Pro-
ceedings of the 2011 International Conference on Computer
Vision, ICCV 11, 2011. 2
[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pa');
INSERT INTO posts (postId,userId,title,body) VALUES (96,7664,'Swami Sankaranarayanan  (part 11)','ttern Recognition, 2016. 1,[10] J. Huang, A. J. Smola, A. Gretton, K. M. Borgwardt, and
B. Scholkopf. Correcting sample selection bias by unlabeled
data. In Proceedings of the 19th International Conference on
Neural Information Processing Systems, NIPS06, 2006. 2
[11] J. J. Hull. A database for handwritten text recognition re-
search. IEEE Transactions on pattern analysis and machine
intelligence, 16(5):550554, 1994. 5

[12] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. CoRR, abs/1412.6980, 2014. 6
Imagenet
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
classication with deep convolutional neural networks.
In
Advances in neural information processing systems, 2012.
1, 2, 7
[14] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11), 1998. 5
[15] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-
works. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett, editors, Advances in Neural Information Pro-
cessing Systems 29, pages 469477. 2016. 3, 5
[16] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning
transferable features with deep adaptation networks. In Pro-
ceedings of the 32nd International Conference on Machine
Learning, pages 97105, 2015. 2, 8
[17] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
feature learning with joint distribution adaptation. In IEEE
International Conference on Computer Vision, ICCV 2013.
IEEE Computer Society, 2013. 6
[18] M. Long, J. Wang, and M. I. Jordan. Unsupervised do-
main adaptation with residual transfer networks. CoRR,
abs/1602.04433, 2016. 2, 7, 8
[19] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne.
Journal of Machine Learning Research, 9, 2008. 2
[20] M. Mirza and S. Osindero. Conditional generative adversar-
ial nets. arXiv preprint arXiv:1411.1784, 2014. 3
[21] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, volume 2011, page 5, 2011. 5
[22] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
adaptation via transfer component analysis. In Proceedings
of the 21st International Jont Conference on Artical Intelli-
gence, IJCAI09, 2009. 2
[23] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 6
[24] R. Ranjan, V. M. Patel, and R. Chellappa. Hyperface: A deep
multi-task learning framework for face detection, landmark
localization, pose estimation, and gender recognition. arXiv
preprint arXiv:1603.01249, 2016. 8
[25] S. Sankaranarayanan, A. Alavi, C. D. Castillo, and R. Chel-
lappa. Triplet probabilistic embedding for face verication
and clustering. In Biometrics Theory, Applications and Sys-
tems (BTAS), 2016 IEEE 8th International Conference on,
pages 18. IEEE, 2016. 8
[26] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ed embedding for face recognition and clustering. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2015. 1
[27] S. Sengupta, J. C. Chen, C. Castillo, V. M. Patel, R. Chel-
lappa, and D. W. Jacobs. Frontal to prole face verication
in the wild. In 2016 IEEE Winter Conference on Applica-
tions of Computer Vision (WACV), pages 19, March 2016.
[28] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. Cnn features off-the-shelf: an astounding baseline for
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, 2014. 1
[29] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-
domain image generation. CoRR, abs/1611.02200, 2016. 2,[30] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. CoRR, abs/1702.05464,
2017. 2, 5, 6
[31] E. Tzen');
INSERT INTO posts (postId,userId,title,body) VALUES (97,7664,'Swami Sankaranarayanan  (part 12)','g, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
CoRR, abs/1412.3474, 2014. 2, 7, 8
[32] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances in
Neural Information Processing Systems 27: Annual Confer-
ence on Neural Information Processing Systems 2014, 2014.
');
INSERT INTO posts (postId,userId,title,body) VALUES (98,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1','1 Rensselaer Polytechnic Institute, Troy, NY, U.S.A.
2 Universidade de Fortaleza, Fortaleza, CE, Brazil
3 Fundacao de Ciencia, Tecnologia e Inovacao de Fortaleza, Fortaleza, CE, Brazil

Abstract. As part of Smart Cities initiatives, national, regional and lo-
cal governments all over the globe are under the mandate of being more
open regarding how they share their data. Under this mandate, many of
these governments are publishing data under the umbrella of open gov-
ernment data, which includes measurement data from city-wide sensor
networks. Furthermore, many of these data are published in so-called
data portals as documents that may be spreadsheets, comma-separated
value (CSV) data les, or plain documents in PDF or Word documents.
The sharing of these documents may be a convenient way for the data
provider to convey and publish data but it is not the ideal way for data
consumers to reuse the data. For example, the problems of reusing the
data may range from diculty opening a document that is provided in
any format that is not plain text, to the actual problem of understand-
ing the meaning of each piece of knowledge inside of the document. Our
proposal tackles those challenges by identifying metadata that has been
regarded to be relevant for measurement data and providing a schema
for this metadata. We further leverage the Human-Aware Sensor Net-
work Ontology (HASNetO) to build an architecture for data collected in
urban environments. We discuss the use of HASNetO and the supporting
infrastructure to manage both data and metadata in support of the City
of Fortaleza, a large metropolitan area in Brazil.

Keywords: smart cities; sensor network; data quality

Introduction

Smart Cities should be sustainable, safe, inclusive, walkable, creative and in-
novative. In one way or another, the availability of each one of these features
characterize a desirable place to live. Obtaining any of them, however, requires
two key components: access to and understanding of city data. Consequently, a
citys ability to produce and share relevant data that can be understood is crit-
ical and can be viewed as key indicator of a Smart City. Further a citys ability
to derive knowledge from this data and further, to use it to power innovation is
an even better indicator of a smart city.
Two complementary trends are particularly relevant in this context: the Inter-
net of Things (IoT) and the Open Government Data approach (OGD). The IoT
12rA6 I.c 128047:ir
2

Henrique Santos et al.

is already a reality in many large metropolitan areas around the world. A myr-
iad of sensors operating at dierent levels of autonomy are deployed throughout
cities collecting data from every aspect of these cities rich urban environments.
Governments are increasingly sharing their data, often with the goal of promot-
ing innovation via societal participation with the use of the data. In the context
of data sharing, dierent categories of stakeholders may be identied: designers
and software developers may use data to produce public services through the
use of web and mobile applications; scientists may produce elaborate analyses
and studies about the cities; public ocers may use the data to improve city ad-
ministration through the use of eective data-based decision-making techniques;
journalists may use open data to produce more reliable, factually-based and at-
tractive news. Briey, IoT and OGD are enabling knowledge production that is
fundamental for enhancing city smartness. While foundational elements exist
with IoT and OGD, many challenges remain to truly realize the potential of
widespread knowledge generation from the raw data sources increasingly avail-
able in many cities.

One key challenge that we address in this paper is the need for cities to
provide metadata that enables data consumers to understand publicly shared
data, including the context within which data was collected. The lack of proper
metadata signicantly impacts the extraction of knowledge from d');
INSERT INTO posts (postId,userId,title,body) VALUES (99,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 2)','ata for any of
stakeholders identied above. The reliability of data collected by extensive sensor
networks, for example, relies heavily on knowledge about sensor deployment,
sensor calibration, measurement units, measurement accuracy, and many other
types of knowledge that is often lost at measurement time.

Open government data (which includes monitored data from city sensors) is
typically published in data portals that provide access to the contents of datasets.
It is true that datasets are a convenient way to convey and publish data but,
while that holds true on a technological perspective, they are not always the best
suited for nal users to use as the boundaries of a dataset may be more driven
by technical considerations of sensor collection rather than boundaries more
natural for users. Further the context for the boundaries is often not captured
or communicated. The datasets are merely enclosures for data, which may be
serialized inside them in a number of formats using dierent domain vocabularies.
The datasets hopefully come with metadata that attempt to explain the formats
used to serialize the data and also to give meaning to the domain vocabularies
used. Accurately interpreting the contents of the data in one data portal even
with a metadata le is often a challenge and greater challenges exist when users
attempt to integrate data from multiple portals.

Our work addresses the lack of a vocabulary to describe the knowledge about
sensor network measurements in the context of Smart Cities. We introduce the
Human-Aware Sensor Network Ontology for Smart Cities (HASNetO-SC) used
to describe knowledge associated with the collection of city empirical data, focus-
ing in particular on the collection of city empirical data coming from city-wide
sensor networks. The rest of the paper is organized as it follows: In Section
2, we discuss work related to the handling of urban stream data and to the

Contextual Data Collection for Smart Cities

development of ontologies describing empirical data collection. In Section 3, we
present and discuss our research choices regarding HASNetO-SC and the process
of collecting urban data. In Section 4, we describe the use of a HASNetO-SC-
based tool when applied to a network of sensors deployed throughout a large
metropolitan area. In Section 5, we discuss our ndings and future work.

2 Related work

2.1 Open Data Portals

Open government data initiatives around the world are often very large pro jects
in terms of complexity and data volume [9] [17]. Goals behind these initiatives
range from government transparency to fostering societal engagement in govern-
ment decisions. Typically open government data eorts for disseminating data
are centered around the development and publication of online data portals. For
example, many of these portals are built on top of CKAN4 , which operates as a
comprehensive dataset repository with data coming from a plethora of distinct
government agencies. Many times, CKAN-hosted datasets convey data gathered
empirically by city sensors. The data often are consolidated into datasets be-
cause of technology restrictions. This approach, although convenient for data
producers, is often a hindrance for data ');
INSERT INTO posts (postId,userId,title,body) VALUES (100,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 3)','consumers.
Much work exists on the integration and publication of urban data collec-
tions. QuerioCity [11] describes a Linked Data platform to publish, search and
link city data from static datasets or stream data coming from deployed sensors.
This platform is able to create a semantic catalog that describes the content of
the datasets that reuses standard vocabularies to improve interoperability and
discoverability. AECIS [6] describes an automated discovery and integration sys-
tem for urban data streams. Both works, although capable of dealing with urban
data streams, are not concerned with data quality, in terms of comprehensive
metadata and the sensor network and context where the data was collected.

2.2 Semantic sensor network and Observational data

The concept of Observational data is treated in the literature [15] [18] [14] as
data that is obtained while sensing some property of an entity from the real
world. The result of an observation is a value for that property [20]. City-wide
sensors are also performing observations, making content annotation crucial.
Annotations enable interoperability and discoverability, making data easier to
be understood and thus (re)used. To leverage the potential benets of data reuse,
several approaches exist to model the infrastructure that generates the data and
to describe data content and context.
Semantic Sensor Networks make use of the description of instruments and
detectors (many times referred to as sensors in the literature) to leverage and

4 http://ckan.org

4

Henrique Santos et al.

maintain complex networks of sensors, while providing integration of the col-
lected data. In [2], twelve dierent sensor network ontologies are studied and
compared. This work preceded the W3Cs Semantic Sensor Network Ontology
(SSN) [1]. SSN is an ontology that aims to describe sensors, observations and
its related concepts, like sensor capabilities, measurement processes and deploy-
ments. SSN is able to annotate data in a manner that makes it possible to to tell
if it is coming from a certain sensor, using some measurement process to mea-
sure a certain property of an entity of interest. BOnSAI [19] and the SESAME
Meter Data Ontology (SMDO) [4] are other sensor network ontologies that are
focused on smart buildings. Although they are all able to describe the sensor
network behind the collected data, SSN does not rely on standard provenance
approaches, like the W3Cs PROV-O. Besides that, SSN is an ontology, and is
not concerned about how the data is conveyed from its collection in terms of
format and datasets. BOnSAI and SMDO are not scientic-centric ontologies,
not containing key concepts like scientic activities.
O'||'&'||'M [3] is an XML implementation from the Open Geospatial Consor-
tium(OGC) that denes a schema for modeling observations and their results.
In [10], an observation and measurement ontology is proposed that makes use of
O'||'&'||'Ms denitions. OBOE (The Extensible Observation Ontology) [12] is an on-
tology for ecological observational data. It provides a data model that captures
measurement semantics and that can be used to streamline data integration. To
achieve this goal, the OBOE ontology contains concepts and relationships for
describing observational datasets.
The Human-Aware Sensor Network Ontology (HASNetO) [13] is an ontology
for describing the scientic activities involved with the data collection of observa-
tional data, i.e., data that is collected while sensing the environment. HASNetO
reuses three ontologies to achieve this task: W3Cs PROV-O, the Virtual Solar-
Terrestrial Observatory - Instrument module (VSTOI) [5] and OBOE. By using
PROV-O, HASNetO is capable of asserting the provenan');
INSERT INTO posts (postId,userId,title,body) VALUES (101,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 4)','ce of the measured data
using PROV terms: activities, entities and agents. HASNetO links VSTOI and
OBOE concepts to PROV concepts: (i) vstoi:Deployment and oboe:Observation
are subclasses of prov:Activity; (ii) vstoi:Dataset is a prov:Entity and (iii) vs-
toi:Instrument becomes a prov:Agent. By doing this, HASNetO enables prove-
nance tracking of data collection activities using W3Cs recommended standard.
Collected data can be encoded in many distinct formats including CSV, XML,
and NetCDF [16]. In many cases, CSV is a format of choice because of its ease
of use by either computers or people. People often manually enter collected data
in a spreadsheet application (like MS Excel or LibreOce Calc). Spreadsheets
are also capable of exporting content in CSV format. Basically, the CSV format
can be seen as a minimalist enabling approach for data interoperability.
Regardless of the format, no single encoding provides eective mechanisms
for annotating observational data in a way that supports observation as a contex-
tualized measurement collection. For instance, CSV lacks features for expressing
the semantics associated with the data contained in it, so it is challenging to
know, in an automated and interoperable way, the meaning of the data enclosed

Contextual Data Collection for Smart Cities

inside a CSV le. For example, it can be dicult to determine if two entries are
observationally equivalent (measured under the same conditions, using the same
units, in the same area, etc.). Nonetheless, dierent agents may generate data
in dierent formats and standards, making CSV even more dicult to process
automatically.
Although there are existing approaches for accessing CSV metadata and also
for providing a metadata vocabulary for CSV data, they are typically more con-
cerned with content restrictions, rather than the context in which the CSV data
was collected. W3Cs drafts from the CSV on the Web Working Group5 elabo-
rate on techniques for enabling the access of CSV metadata by describing the
content metadata in a separate JSON le that makes use of RDF vocabulary. To
bridge this gap, we propose Contextualized CSV (CCSV)6 as a format that deals
with both content and context restrictions of the observational data enclosed in
it. The CCSV dataset is basically a regular CSV le with a Turtle preamble
on top of it. The Turtle is used to assert that a certain dataset comes from a
particular instrument during an specic deployment, as context metadata. It is
also used to assert content, by stating the property being measured and which
column contains the measured value.

3 A streamlined process to collect and publish data for
Smart Cities

Our approach relies on the HASNetO ontology. HASNetO was primarily con-
ceived to be used in scientic environments by scientists who conduct observation
and data collection activities on entities of interest, such as a physical, chemical,
biological, social or cultural feature. While comprehensive enough to provide
what scientists need to keep track of their activities, HASNetO faces challenges
when dealing with data collected in large complex settings with broad user bases,
such as urban environments. One of those challenges is that the collected data
must be used not only by the city administration or people involved with their
associated measurements (people who have some knowledge about the data at
some level), but, most importantly, b');
INSERT INTO posts (postId,userId,title,body) VALUES (102,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 5)','y citizens who have, in general, little to
no knowledge about the collection environment and the collected data. For in-
stance, the accuracy or resolution of an instrument used to collect data may not
be as important for a regular citizen as it is for a scientist. To address these
challenges, we propose a modied version of HASNetO called the Human-Aware
Data Collection ontology for Smart Cities (HASNetO-SC). Our main goal with
HASNetO-SC is to provide smart city stakeholders with a streamlined way of
collecting, preserving and disseminating urban data with an appropriate level of
contextual metadata for data understanding. The following subsections discuss
the aspects of this process.

5 http://www.w3.org/2013/csvw/wiki/Main Page
6 http://tw.rpi.edu/web/pro ject/JeersonPro jectAtLakeGeorge/download/ccsv

6

Henrique Santos et al.

3.1 HASNetO-SC

In 2007, a report on the ranking of the smartest medium-sized European cities
was published [7]. This report aimed to, among other goals, dene the general
concept of a Smart City by specifying a set of factors and indicators that a
city should be pursuing in order to increase its level of smartness. The work
presented six key areas (characteristics) that were gathered from a literature
search on previous denitions of the term Smart City: smart economy, smart
people, smart governance, smart mobility, smart environment and smart living.
Inside each characteristic, a number of factors were also compiled to support it
and, in turn, each factor consisted of a set of indicators that can identify well a
city is doing with respect to that factor. We developed an extension to HASNetO,
called HASNetO-SC, to describe concepts that exist in urban environments.
HASNetO-SC has been motivated by the smart cities characteristics identied
in the report (not to be mistaken with characteristics of physical entities), while
focusing on the four that had higher potential to produce data that can be
collected empirically and managed using a streamlined process:

 Smart people
 Smart mobility
 Smart environment
 Smart living

The extension basically renes VSTOI concepts that are integrated into HAS-
NetO to better suit urban data collections. The following subsections further
detail our modeling and discuss how we are using HASNetO concepts to cope
with smart cities.

People People are a central component of cities. Cities are made for the citizens,
as the government is intended to foster its society as a whole. Citizens, use city
facilities and participate in the government discussions. In this scenario, people
are capable of providing valuable information to the city. In previous work, [8]
has studied the view of a citizen as a sensor. Our approach is compatible with
this perspective and enables people to either deploy instruments or conduct data
collection activities. By the means of prov:Person, a person is a prov:Agent , as
asserted in the PROV ontology, and the person can have an associated list of
prov:Activity. In HASNetO, prov:Activity is extended to both vstoi:Deployment
and hasneto:DataCollection.

Mobility Mobility is one of the most discussed and explored aspects of smart
cities, due to its need of near-real time raw data (i.e., data from sensors) and
derived data (i.e., results of d');
INSERT INTO posts (postId,userId,title,body) VALUES (103,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 6)','ata analysis). Mobility is central to many key city
goals including accessibility, safe and sustainable transportation etc. To achieve
those goals, many city governments are deploying sensors to monitor a number of
urban mobility indicators. HASNetO enables provenance capture by describing

Contextual Data Collection for Smart Cities

how sensors are deployed, in particular how agents perform instrument deploy-
ments at platforms. In VSTOI, a Platform is a surface where Instruments and
Detectors may be attached to measure, in the context above, characteristics of
urban mobility. As a mobility example, we can identify some platforms and in-
struments that are known to be related to urban mobility. We have extended both
vstoi:Platform and vstoi:Instrument to describe these more specialized mobility-
related classes, as seen in Fig. 1. The gure shows that vstoi:Platform includes
three classes: subclasses hasneto-sc:Bus, hasneto-sc:RoadSegment, and hasneto-
sc:LampPost. In a city, we dened buses, roads and lamp posts as being capable
of hosting mobility-related instruments. Furthermore, vstoi:Instrument has been
extended to a series of generic instruments that we propose should be used as a
basis from which to derive specialized instruments, as each city will have distinct
manufacturers, suppliers and vendors of instruments and platforms. The generic
modeling approach described above makes it possible to further connect data and
metadata, making them meaningful to data consumers (e.g., citizens) who are
not necessarily involved with the specication of specialized instruments, even
though they may know consumer instruments including cameras and GPSs.

Fig. 1. HASNetO-SC classes.

Environment Another well-discussed smart city aspect is environment. Ex-
amples of smart environment entities are air, water, and their associated
monitoring and governance eorts such as sustainable resource management,
pollution control, and environmental protection. Forward-thinking cities are
often concerned with citizen well being and thus are critically interested in envi-

8

Henrique Santos et al.

ronmental aspects. Signals of ecient use of water, electricity, and green space
are some examples of indicators that a city that is doing well in the environment
characteristic. To evaluate those indicators, many instruments are already in
use in big cities that enable the collection of data about air quality, noise levels,
water conditions and so forth. Fig. 1 shows more specialized platforms and in-
struments. Again, those concepts are generic concepts that should be specialized
to fully describe the actual city sensor network.

Living Smart living includes the notions of public health, public safety, edu-
cation and cultural facilities. Smart buildings are becoming a reality with more
sensors being deployed to control access and gather the people ow information.
Public services addressing public health and safety considerations often are plac-
ing sensors including cameras to help monitor potentially dangerous situations.
Fig. 1 also depicts some of the most common platforms and instruments relevant
to smart city living.

3.2 Contextualized Comma Separated Values

As mentioned before, we proposed CCSV as an extension to CSV to address
both content and context restrictions when dealing with observational data. In
our appro');
INSERT INTO posts (postId,userId,title,body) VALUES (104,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 7)','ach, both content and context metadata are needed in order to provide
the connection between data and metadata. From the Smart City perspective,
we needed to align the Turtle preamble of a CCSV dataset with the ontology
concepts introduced by HASNetO-SC. Further in this subsection, the displayed
code is an example of a CCSV preamble of a city dataset.
The preamble contains the following descriptions:

 Knowledge base: In order to provide the possibility of multi-contextual data
collections and also to make the solution more scalable, we make it possible
for the dataset to state which knowledge base should be used for validation
and persistence.
 Deployment: Deployment information makes it possible to link the data the
CCSV dataset conveys to metadata information: (i) instrument and detec-
tor used; (ii) platform and (iii) all attached information to those, including
accuracy, precision, platform location etc.
 Data collection: Data collection is a prov:Activity as asserted in HASNetO.
The use of Data collection information in the CCSV preamble enables the
architecture to have knowledge about this dataset: it is able to know if
distinct datasets were produced under the same context. In other words, the
architecture is able to provide the user enough context information for him
to decide if the data within dierent datasets can be comparable, joined,
analyzed together, based on his needs.
 Dataset: As previously discussed, datasets are not scientic boundaries for
data, but data collection activities are. In this description, we link datasets
to their corresponding data collections.

Contextual Data Collection for Smart Cities

 Measurement(s): Here all measured characteristics are described. For each
measurement type we link its description to a unit and a measured charac-
teristic. Well discuss the need for a domain ontology in the next section.

Example of a CCSV Preamble
< c i t y kb>
a c c s v : K n ow l e d g eB a s e ;
c c s v : h a sC o n n e c t i o nUR L [ s o m e u r l ]   x s d : anyURI

<d e p l o ym e n t>
a v s t o i : D e p l o ym e n t ;
p r o v : s t a r t e d A t T i m e [ s om e t i m e s t am p ]   x s d : d a t eT im e ;
h a s n e t o : h a s D a t a C o l l e c t i o n <d a t a c o l l e c t i o n 001> .
<d a t a c o l l e c t i o n >
a h a s n e t o : D a t a C o l l e c t i o n ;
a t i m e : I n t e r v a l ;
p r o v : s t a r t e d A t T i m e [ s om e t i m e s t am p ]   x s d : d a t eT im e .

<d a t a s e t >
a v s t o i : D a t a s e t ;
p r o v : w a sG e n e r a t e dB y <d a t a c o l l e c t i o n > ;
h a s n e t o : h a sM e a s u r em e n tT y p e <mt0> .

<mt0>
a o b o e : M e a s u r em e n t ;
a t i m e : I n s t a n t ;
t i m e : i nD a t eT im e < t s 0 > ;
c c s v : a tC o lumn 1 ;
[ s o m e c h a r a c t e r i s t i c ] ;
o b o e : o f C h a r a c t e r i s t i co b o e : u s e s S t a n d a r d [ s o m e s t a n d a r d ]

< t s 0 >
a t i m e : I n s t a n t ;
c c s v : a tC o lumn 0 .

3.3 Domain ontology

An Observation from the scientic point of view is dened as the act of observ-
ing some entitys property and obtaining a measured value. ');
INSERT INTO posts (postId,userId,title,body) VALUES (105,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 8)','Measurements are
expressed using a specic standard (or unit). To have measurements properly
annotated, we need a hierarchy of entities of interest with relevant properties,
a hierarchy of entity characteristics with relevant properties, and a hierarchy
of standards to be used in in measurements. The OBOE ontology already pro-
vides a hierarchy of a number of entities, characteristics and measurement units,

10

Henrique Santos et al.

although it is focused on ecological studies. A domain ontology enables deep
linking of the collected data to its metadata. For cities, Section 4 discusses our
initial approach to this ontology in the context of a public transportation system.

4 Use case: Fortaleza urban transportation system

Our primary use cases focus on the city of Fortaleza, Brazil. The city of Fortaleza
is the capital of the Ceara state in the Northeast region of Brazil with approx-
imately 2.5 million inhabitants. Recently, the City Hall launched its smart city
program called Fortaleza Inteligente. This pro ject, coordinated by the Foun-
dation for Science, Technology and Innovation of Fortaleza (CITINOVA), has as
one of its main components the city open data portal, which contains a plethora
of dierent datasets from a variety of public segments. We are collaborating with
CITINOVA to use our approach in the city open data portal, in particular to
represent data about the segment of urban mobility.
The rst initiative was to model public transportation data. Fortaleza has
deployed, on each of its running buses, GPS instruments capable of transmitting
the current geographical position of the bus. GPS data is transmitted to a central
server where they are stored in datasets that follow an ad hoc logic. The les
should refer to a GPS position of a bus, for a given bus line and during a certain
day. In other words GPS data is represented by a triplet bus-id, bus-line, date.
However, this structure is not always followed. There are situations where the
le becomes too large, and is then divided into two datasets. In addition, when
a bus happens to be used in more than one line, then the same GPS receiver is
used to collect more than one bus lines position. This can all be represented in
the same dataset, which again breaks the informal rule (bus-id, bus-line, date).
Other variations exist, but the decisive factor is that none of these informal
rules used for the construction of datasets were specied or clearly documented.
They were discovered in several rounds of interviews with providers of data only
when other users decided to explore the data. This makes the process not easily
reusable or scalable.

4.1 Building the Semantic Sensor Network

In order to develop a proof of concept, we gathered three datasets from the For-
taleza Dados Abertos7 portal that related to the public transportation system:

 Bus checkpoints: A list with all the bus checkpoints around the city. Each
checkpoint is composed of its code, a name, a lat/long position and a radius
around that lat/long to indicate the area the checkpoint that it covers.
 Bus companies: A list with all the bus companies on contract with City Hall
to provide that public service. Each company has a company id, a name and
other information.

7 http://dados.fortaleza.gov.br

Contextual Data Collection for Smart Cities

11

 Bus eet: A list with all the buses running in the city. Each bus has an
id, a model, a maker, which company it belongs to and other information
including plates, serial numbers and so forth.
 GPS bus information for 2015/02: A report including when a specic bus
entered and left a bus checkpoint.

We note that rst three datasets can be viewed as metadata that plays a
support role for the fourth');
INSERT INTO posts (postId,userId,title,body) VALUES (106,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 9)',' dataset. This last dataset is the actual observed data
that was collected during a data collection activity. We also note that information
such as bus company and bus checkpoint provide contextual metadata. We have
mapped the checkpoints and eet datasets using the HASNetO-SC ontology:

 Checkpoints: By analysing the GPS dataset, we can infer that all the mea-
surements contained inside it are coming from the checkpoints themselves.
To reect this, we then mapped all checkpoints to individual instances of
the vstoi:Instrument as they are able to measure when a particular bus is
entering or leaving its coverage area.
 Checkpoints location: We have also inferred that the actual instruments iden-
tied in the previous item were deployed at the place described in it. For the
purpose of this proof of concept, we dened that the place of deployment is
an individual instance of hasneto-sc:RoadSegment, as all checkpoints lat/-
long locations are on roads.

The following is part of the Turtle describing our sensor network using
HASNetO-SC.

Serialized RDF Model of the Sensor Network
< c h e c k p o i n t 1>
a v s t o i : I n s t r u m e n tr d f s : l a b e l  D a l l a s / S o b r a d i n h o /T F P a i v a  .
< c h e c k p o i n t 2>
a v s t o i : I n s t r u m e n tr d f s : l a b e l  P e d r o P e r e i r a / I m p e r a d o r /T G o n c a l v e s  .
. . .
< c h e c k p o i n t p l a t f o r m 1>
a h a s n e t o s c : RoadS egm en tr d f s : l a b e l  D a l l a s / S o b r a d i n h o /T F P a i v a  .
g e o : l a t  3 . 7 9 4 8 6 6 0 0 ;
g e o : l o n g  3 8 . 6 1 6 2 5 7 0 0 .
< c h e c k p o i n t p l a t f o r m 2>
a h a s n e t o s c : RoadS egm en tr d f s : l a b e l  P e d r o P e r e i r a / I m p e r a d o r /T G o n c a l v e s  .
g e o : l a t  3 . 7 2 7 9 1 2 0 0 ;
g e o : l o n g  3 8 . 5 3 4 0 5 2 0 0 .

. . .

12

Henrique Santos et al.

4.2 Annotating the CSV dataset

Once the semantic sensor network was ready, the next step was to annotate the
CSV dataset containing the GPS bus information. The former dataset was a
combined collection of measurements from dierent checkpoint instruments. To
fully integrate this dataset with our process, we split it so that each resulting
dataset contains information collected by a single instrument. To annotate each
dataset, as mentioned before, we need a domain ontology that includes a hi-
erarchy of entities, characteristics and units. For the purpose of the use case,
we have developed a small ontology capable of describing the identied con-
cepts, i.e., to state that we are measuring the occurrence (unit) of an arrival
or departure (characteristic) of a bus (entity). As shown in Fig. 2, the ontology
denes two classes under the namespace pmf (acronym for Prefeitura Municipal
de Fortaleza - Fortaleza City Hall): pmf:BusArrivalDeparture that specializes
oboe:Characteristic and pmf:Binary that specializes oboe:BaseUnit. Also, note
that pmf:ArrivalDeparture characteristic has an associated entity pmf:Bus.
Given this, we annotated each dataset to build the following CCSV preamble.

GPS bus dataset CCSV preamble
<pmfkb>
a c c s v : K n ow l e d g eB a s e ;
c c s v : h a sC o n n e c t i o nUR L  h t t p . . .    x s d : anyURI
<d e p l o ym e n tc h e c k p o i n t 1>
a v s t o i : D e p l o ym e n t ;
p r o v : s t a r t e d A t T i m e 2015 0201T00 : 0 0 : 0 0 Z    x s d : d a t eT im e ;
h a s n e t o : h a s D a t a C o l l e c t i o n < d a t a c o l l e c t i o n c h e c k p o i n t 1> .
< d a t a c o l l e c t i o n c h e c k p o i n t 1>
a h a s n e t o : D a t a C o l l e c t i o n ; a t i m e : I n t e r v a l ;
p r o v : s t a r t e d A t T i m e 2015 0201T00 : 0 0 : 0 0 Z   ');
INSERT INTO posts (postId,userId,title,body) VALUES (107,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 10)',' x s d : d a t eT im e .
<g p sb u si n f o r m a t i o n c h e c k p o i n t 1>
a v s t o i : D a t a s e t ;
p r o v : w a sG e n e r a t e dB y < d a t a c o l l e c t i o n c h e c k p o i n t 1> ;
h a s n e t o : h a sM e a s u r em e n tT y p e <mt0> .

<mt0>
a o b o e : M e a s u r em e n t ; a t i m e : I n s t a n t ;
t i m e : i nD a t eT im e < t s 0 > ;
c c s v : a tC o lumn 1 ;
o b o e : o f C h a r a c t e r i s t i c pmf : A r r i v a l D e p a r t u r e ;
o b o e : u s e s S t a n d a r d pmf : B i n a r y .

< t s 0 >
a t i m e : I n s t a n t ; c c s v : a tC o lumn 0 .

Contextual Data Collection for Smart Cities

13

Fig. 2. Domain ontology for the bus use case.

The dataset was annotated to contain deployment information for that single
instrument and also to specify that a new data collection activity was being
initiated. It is also shown that this dataset has a measurement of characteristic
pmf:ArrivalDepartureEvent in the pmf:Binary unit located on column number
1 that was taken on the timestamp specied at column 0.

4.3 CCSV parsing and indexing

In order to complete the connection between data and metadata described pre-
viously including the full integration with the semantic sensor network, we have
deployed an instance of Apache SOLR8 to index and store both data and meta-
data. SOLR is a NoSQL database capable of indexing documents in a number of
dierent formats including CSV, XML and JSON. SOLR document collections
should include eld denitions, including data types, for content that is to be
indexed and stored. Inside SOLR, we have created two collections: (i) a meta-
data collection for storing the semantic sensor network, the domain ontology and
the deployment metadata, and (ii) a measurement collection for storing the data
itself.
The metadata collection is basically a triple store where the sensor network
Turtle was loaded along with the domain ontology described. The measurement
collection is a regular SOLR collection with the following elds being indexed
and stored:

 Entity, characteristic and unit
 Location and timestamp
 Measured value
 Instrument
 Data collection to which the measurement belongs

To automate the process of storing this information, we have developed an
application called the CCSV-Loader9 . This loader performs, in order, the follow-
ing tasks:

1. Extract the Turtle preamble of the CCSV dataset
2. Get instrument information based on deployment

8 http://lucene.apache.org/solr/
9 http://tw.rpi.edu/web/pro ject/JeersonPro jectAtLakeGeorge/download/ccsv

14

Henrique Santos et al.

3. Get platform information based on deployment
4. Generate a normalized CSV le with the extra columns containing metadata
information
5. Index the normalized CSV le in the measurement collection

Fig. 3. Faceted-browsing of Fortaleza city transportation data.

Once the data is indexed, we make use of SOLR faceted-search capabilities
to present a navigation UI to the user. Fig. 3 shows an example of that UI.
On the right side, measurements are shown, based on the user current search
parameters. On the left side, we have dened some elds to be faceted by SOLR
to provide easy access to commonly requested content. For this use case, we have
dened checkpoints as a eld to be face');
INSERT INTO posts (postId,userId,title,body) VALUES (108,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 11)','ted so a user can select measurements
coming from a specic checkpoint. Also, we added buses and companies to the
facet eld list, giving the ability for users to rene the search results by a bus or
a company.

Contextual Data Collection for Smart Cities

15

5 Discussion

This paper presented a streamlined process to collect, store and disseminate mon-
itored data in an urban environment. The work was motivated by the challenges
related to understanding, using, and integrating data from dataset portals based
on current monitoring and publishing approaches. One issue we introduced is the
boundary of a dataset as a natural context boundary vs. a technologically con-
venient boundary. Our Fortaleza public transportation system use case showed
that our approach enables users to have access not only to the sensor network
metadata but also to metadata driven navigation of the data. This combination
we believe helps circumvent many of the challenges with understanding, using,
and integrating data, even in settings where datasets have varied and potentially
arbitrary boundaries and also in settings where metadata is limited.
In future work, we will articulate and implement additional use cases. We
are also working on tighter integration with the real time sensor network content
in order to augment it using our metadata scheme thus further evaluating our
HASNetO-SC ontology. Additionally, we plan to apply our approach in other
Smart City initiatives. Our vision is to deploy our approach alongside open data
portals, providing end users with a better understanding of complex monitored
urban environment data.

Acknowledgements. The rst author is supported by CNPq - Brazil.

References

1. Compton, M., Barnaghi, P., Bermudez, L., Garca-Castro, R., Corcho, O., Cox,
S., Graybeal, J., Hauswirth, M., Henson, C., Herzog, A., Huang, V., Janowicz,
K., Kelsey, W.D., Le Phuoc, D., Lefort, L., Leggieri, M., Neuhaus, H., Nikolov,
A., Page, K., Passant, A., Sheth, A., Taylor, K.: The SSN ontology of the W3c
semantic sensor network incubator group. Web Semantics: Science, Services and
Agents on the World Wide Web 17, 2532 (Dec 2012)
2. Compton, M., Henson, C., Lefort, L., Neuhaus, H., Sheth, A.: A Survey of the
Semantic Specication of Sensors. CEUR Workshop Proceedings pp. 1732 (Oct
2009)
3. Cox, S.: Observations and Measurements - XML Implementation (Mar 2011)
4. Fensel, A., Tomic, S., Kumar, V., Stefanovic, M., Aleshin, S.V., Novikov, D.O.:
SESAME-S: Semantic Smart Home System for Energy Eciency. Informatik-
Spektrum 36(1), 4657 (Dec 2012)
5. Fox, P., McGuinness, D.L., Cinquini, L., West, P., Garcia, J., Benedict, J.L.,
Middleton, D.: Ontology-supported scientic data frameworks: The Virtual Solar-
Terrestrial Observatory experience. Computers '||'&'||' Geosciences 35(4), 724738 (Apr
2009)
6. Gao, F., Ali, M.I., Mileo, A.: Semantic Discovery and Integration of Urban Data
Streams. In: Proceedings of the Fifth Workshop on Semantics for Smarter Cities.
pp. 1530. Riva del Garda, Italy (Oct 2014)

16

Henrique Santos et al.

7. Ginger, R., Fertner, C., Kramar, H., Kalasek, R., Pichler-Milanovic');
INSERT INTO posts (postId,userId,title,body) VALUES (109,9406,'Henrique Santos1,2 , Vasco Furtado2,3 , Paulo Pinheiro1 , and Deborah L. McGuinness1 (part 12)',', N., Mei-
jers, E.: Smart cities-Ranking of European medium-sized cities. Tech. rep., Vienna
University of Technology (2007)
8. Goodchild, M.F.: Citizens as sensors: the world of volunteered geography. Geo-
Journal 69(4), 211221 (Nov 2007)
9. Hendler, J., Holm, J., Musialek, C., Thomas, G.: US Government Linked Open
Data: Semantic.data.gov. IEEE Intelligent Systems 27(3), 2531 (May 2012)
10. Kuhn, W.: A Functional Ontology of Observation and Measurement. In: Janowicz,
K., Raubal, M., Levashkin, S. (eds.) GeoSpatial Semantics, pp. 2643. No. 5892 in
Lecture Notes in Computer Science, Springer Berlin Heidelberg (2009)
11. Lopez, V., Kotoulas, S., Sbodio, M.L., Stephenson, M., Gkoulalas-Divanis, A.,
Aonghusa, P.M.: QuerioCity: A Linked Data Platform for Urban Information Man-
agement. In: Cudre-Mauroux, P., Hein, J., Sirin, E., Tudorache, T., Euzenat, J.,
Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G., Bernstein, A., Blomqvist,
E. (eds.) The Semantic Web  ISWC 2012, pp. 148163. No. 7650 in Lecture Notes
in Computer Science, Springer Berlin Heidelberg (Jan 2012)
12. Madin, J., Bowers, S., Schildhauer, M., Krivov, S., Pennington, D., Villa, F.: An
ontology for describing and synthesizing ecological observation data. Ecological
Informatics 2(3), 279296 (Oct 2007)
13. Pinheiro, P., McGuinness, D.L., Santos, H.: Human-Aware Sensor Network Ontol-
ogy: Semantic Support for Empirical Data Collection. In: Proceedings of the 5th
Workshop on Linked Science. Bethlehem, PA, USA (2015)
14. Probst, F.: Ontological Analysis of Observations and Measurements. In: Raubal,
M., Miller, H.J., Frank, A.U., Goodchild, M.F. (eds.) Geographic Information Sci-
ence, pp. 304320. No. 4197 in Lecture Notes in Computer Science, Springer Berlin
Heidelberg (2006)
15. Quine, W.V.O.: From Stimulus to Science. Harvard University Press (1995)
16. Rew, R., Davis, G.: NetCDF: an interface for scientic data access. IEEE Computer
Graphics and Applications 10(4), 7682 (Jul 1990)
17. Shadbolt, N., OHara, K., Berners-Lee, T., Gibbins, N., Glaser, H., Hall, W.,
schraefel, m.: Linked Open Government Data: Lessons from Data.gov.uk. IEEE
Intelligent Systems 27(3), 1624 (May 2012)
18. Stasch, C., Janowicz, K., Broring, A., Reis, I., Kuhn, W.: A Stimulus-Centric
Algebraic Approach to Sensors and Observations. In: Trigoni, N., Markham, A.,
Nawaz, S. (eds.) GeoSensor Networks, pp. 169179. No. 5659 in Lecture Notes in
Computer Science, Springer Berlin Heidelberg (2009)
19. Stavropoulos, T.G., Vrakas, D., Vlachava, D., Bassiliades, N.: BOnSAI: A Smart
Building Ontology for Ambient Intelligence. In: Proceedings of the 2Nd Inter-
national Conference on Web Intelligence, Mining and Semantics. pp. 30:130:12.
WIMS 12, ACM, New York, NY, USA (2012)
20. Usbeck, R.: Combining Linked Data and Statistical Information Retrieval. In: Pre-
sutti, V., dAmato, C., Gandon, F., dAquin, M., Staab, S., Tordai, A. (eds.) The
Semantic Web: Trends and Challenges, pp. 845854. No. 8465 in Lecture Notes in
Computer Science, Springer International Publishing (Jan 2014)

');
INSERT INTO posts (postId,userId,title,body) VALUES (110,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470','1 Introduction
Recent years have witnessed several demonstrations of ma-
chine learning vulnerabilities in adversarial settings [Dalvi
et al., 2004; Lowd and Meek, 2005; Barreno et al., 2006;
Rubinstein et al., 2009; Br uckner and Scheffer, 2011; Big-
gio et al., 2012; Goodfellow et al., 2014; Alfeld et al., 2016;
Li et al., 2016]. Due to its approximation of best-response
and its effective simplicity, gradient descent [Biggio et al.,
2013] has emerged as a leading approach to evasion attacks.
We refer to the carefully crafted inputs that resemble le-
gitimate instances but cause misclassication, as adversar-
ial samples, and the malicious behaviours that generate them
as evasion attacks [Russu et al., 2016]. Figure 1 illus-
trates the attacks effect in the previously-explored vision
domain [Szegedy et al., 2013; Goodfellow et al., 2014;
Papernot et al., 2016c]: Figures 1a and 1b present original im-
ages from [Samaria and Harter, 1994] for Adam and Lu-
cas, who are correctly identied by a face recogniser. How-
ever, after indiscernible changes are applied to Figure 1a the
model mistakenly identies Figure 1c as Lucas.
Can these attacks be thwarted, are there effective attack al-
ternatives? This paper addresses these questions, with a case
study on the support vector machine with radial basis func-
tion (RBF) kernel. Our main contributions include:
 An analysis of kernel precision parameters impact on
the success rate of evasion attacks, concluding that larger
precision (less smooth kernels) achieves robustness to
gradient-descent attack;

(a) Adam

(b) Lucas

(c) Adam or Lucas?

Figure 1: An example evasion attack against a learning
model: Image (c) is misclassied as Lucas.

 An novel geometric classier parameter related to mar-
gin that strongly correlates with model vulnerability,
providing an avenue to predict (unseen) attack vulner-
ability; and
 A new approach for generating adversarial samples in
multiclass scenarios, with results demonstrating signi-
cantly higher effectiveness in manipulating test data and
fooling the target model.
The remainder of this paper is organised as follows: Sec-
tion 2 overviews previous work on evasion attacks; Section 3
presents our research problem; we present a detailed exam-
ple of how gradient-descent can fail in Section 4; Section 5
presents the gradient-quotient approach for constructing ad-
versarial samples; experimental results are presented in Sec-
tions 6 and 7; and Section 8 concludes the paper.

2 Related Work
Barreno et al. [2006] categorise how an adversary can tamper
with a classier based on whether they have (partial) control
over the training data: in causative attacks, the adversary can
modify the training data to manipulate the learned model; in
exploratory attacks, the attacker does not poison training, but
carefully alters target test instances to ip classications. See
also [Barreno et al., 2010; Huang et al., 2011]. This paper fo-
cuses on the targeted exploratory case, also known as evasion
attacks [Biggio et al., 2013].
Generalising results on efcient evasion of linear classi-
ers via reverse engineering [Lowd and Meek, 2005], Nelson
et al. [2012] consider families of convex-inducing classiers,
and propose query algorithms that require polynomially-
many queries and achieve near-optimal modication cost.
Szegedy et al. [2013] demonstrate changes imperceptible
to humans that cause DNNs to misclassify images. Addi-
tionally, they offer a linear explanation of adversarial sam-
ples and design a fast gradient sign method for generat-
ing such');
INSERT INTO posts (postId,userId,title,body) VALUES (111,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470',' samples [Goodfellow et al., 2014].
In a similar
vein, Nguyen et al. [2015] propose an approach for producing

DNN-adversarial samples unrecognisable as such to humans.
Papernot et al. published a series of further works in this
area: (1) introducing an algorithm that searches for mini-
mal regions of inputs to perturb [Papernot et al., 2016c];
(2) demonstrating effectiveness of attacking target models via
surrogateswith over 80% of adversarial samples launched
fooling the victim in one instance [Papernot et al., 2016b];
(3) improved approaches for tting surrogates, with further
investigation of intra- and cross-technique transferability be-
tween DNNs, logistic regression, SVMs, decision trees and
k-nearest neighbours [Papernot et al., 2016a].
Moosavi-Dezfooli et al. [2016b] propose algorithm D E E P -
FOO L for generating adversarial samples against DNNs,
which leads samples along trajectories orthogonal to the de-
cision boundary. A similar approach against linear SVM is
proposed in [Papernot et al., 2016a]. Based on D E E PFOOL,
Moosavi-Dezfooli et al. [2016a] design a method for comput-
ing universal perturbations that fool multiple DNNs.
to this paper is the work by Russu et
Most relevant
al. [2016], which analyses the robustness of SVMs against
evasion attacks, including the selection of the regularisation
term, kernel function, classication costs and kernel parame-
ters. Our work delivers a much more detailed analysis of ex-
actly how the kernel parameters impact vulnerability of RBF
SVM, and explanations of why.
3 Preliminaries '||'&'||' Problem Statement
This section recalls evasion attacks,
the gradient-descent
method, the RBF SVM, and summarises the research prob-
lem addressed by this paper.
Evasion Attacks. For target classier f : Rd  {1, 1},
the purpose of an evasion attack is to apply minimum change
 to a target input x, so that the perturbed point is misclassi-
ed, i.e., f (x) (cid:54)= f (x+). The magnitude of adversarial per-
turbation  is commonly quantied in terms of L1 distance.
Formally, evasion attacks are framed as optimisation:
(cid:107)(cid:107)1
f (x) (cid:54)= f (x + ) .
s.t.
arg min
Rd
Note that we permit attackers that can modify all fea-
tures of the input, arbitrarily, but that aim to minimise the
magnitude of changes. Both binary and multiclass scenarios
fall into the evasion problem as described; we consider both
learning tasks in this paper. In multiclass settings, attacks in-
tending to cause specic misclassication of the test sample
are known as mimicry attacks.

Gradient-Descent Method. The gradient descent method1
has been widely used for generating adversarial samples for
evasion attacks [Biggio et al., 2013; Goodfellow et al., 2014;
Moosavi-Dezfooli et al., 2016b; Papernot et al., 2016c], when
f outputs condence scores in R and classications are ob-
tained by thresholding at  = 0. The approach applies gradi-
ent descent to f directly, initialised at the target instance. For-
mally given target instance x0  Rd evaluating f (x0 ) >  ,
xt+1 = xt  t  x f (xt ) ,
1Not to be confused with gradient descent for local optimisation.

where t follows an appropriately-selected step size schedule,
and the iteration is terminated when f (xt ) <  .

The Support Vector Machine. Recall the dual program of
the soft-margin SVM classier learner, with hinge-loss

1(cid:48)  1
(cid:48)G s.t. (cid:48)y = 0, 0 (cid:22)  (cid:22) C 1 (1)
arg max
Rnwhere {(xi , yi ), i = 1, . . . , n} is the t');
INSERT INTO posts (postId,userId,title,body) VALUES (112,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470','raining data with
xi  Rn and yi  {1, 1}n ,  are Lagrange multipliers, 0,
1 the all zeros, ones vectors, C > 0 the regularisation penalty
parameter on misclassied samples, and G the n  n Gram
 > 0 that controls kernel width (cid:112)2/ . By the Representer
matrix with entries Gij = yi yj k(xi , xj ). The RBF kernel
k(xi , xj ) = exp( (cid:107)xi  xj (cid:107)2
2 ) has precision parameter
n(cid:88)
Theorem, the learned classier
i=1

i yik(xi , x) + b .

f (x) =

(2)

Adequacy '||'&'||' Improvements to Gradient-Descent Method.
While the gradient-descent method has been effective against
a number of machine learning models, e.g., DNNs, linear
SVMs, logistic regression [Biggio et al., 2013; Goodfellow
et al., 2014; Moosavi-Dezfooli et al., 2016b; Nguyen et al.,
2015; Papernot et al., 2016b], there is no general guaran-
tee that gradient descent converges to a global minimum of
f () or even converges to local optima quicklyrelevant to
computational complexity (a measure of hardness) of eva-
sion. Under linear modelsa major focus of past work
gradient descent quickly nds global optima. While DNNs
have been argued to exhibit local linearity. The existing body
of evidence is insufcient to properly assess the effectiveness
of the attack approach. As we argue in the next section, the
approach is in fact unlikely to be successful against certain
SVMs with RBF kernels, when kernel parameter  is chosen
appropriately.
Problem 1 What limitations of the gradient-descent method
are to be expected when applied to evasion attacks?
Problem 2 Are there simple defences to the gradient-descent
method for popular learners?
Problem 3 Are there more effective alternative approaches
to generating adversarial samples?

We address each of these problems in this paper, with spe-
cial focus on the RBF SVM as a case study and important
example of where the most popular approach to evasion at-
tack generation can predictably fail, and be improved upon.

4 Gradient-Descent Method Failure Modes
In this section, we explore how the gradient-descent method
can fail against RBF SVMs with small kernel widths.

(a)  = 102 model: two black curves display attack paths under gradi-
ent descent, of two target points reaching the decision boundary.

Figure 3: Gradient-descent method failure modes.

 =

sign of k can be ipped by various choices of  . Suppose
2 > x1 , then solving for 1 = 0, we obtain the point at
x1
1 , x1
which this phase transition occurs as:
1  x1 )  log 2 (x1
2  x1 )
log 1 (x1
2  (cid:107)x  x2(cid:107)2
(cid:107)x  x1(cid:107)2The failure modes hold true in multiclass scenarios. For
test sample x the classier evaluates an fi (x) per class i and
selects the maximiser (a one-vs-all reduction). Suppose that
f1 (x) and f2 (x) are the highest class scores. If  is chosen
appropriately as above, then gradient descent reduces both
f1 (x) and f2 (x) without ever reranking the two classes.
Figure 3 presents a geometric explanation, where distance
between support vectors of opposite classes exceeding kernel
width results in gradient-descent method iterates becoming
trapped in the gap between. This section partially addresses
Problem 1 through the discussed limitations, while setting 
can provide a level of defence per Problem 2.

5 The Gradient-Quotient Method
The previous section motivates Problem 3s search for effec-
tive alternatives to decreasing cur');
INSERT INTO posts (postId,userId,title,body) VALUES (113,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470','rent class i fi (x) while in-
creasing desired class j fj (x). Rather than moving in the
direction fi (x) as the gradient-descent method does (not-
ing this is in the subgradient for the one-vs-all reduction), we
propose following the gradient of the quotient fi (x)/fj (x):
xt+1 = xt  t  (fi (x)/fj (x)) .
Remark 1 Employing fi (x)fj (x) in place of fi (x)/fj (x)
does not achieve the desired result by the same aws suffered
by the gradient-descent method: fi (x), fj (x) and fi (x) 
fj (x) are decreased simultaneously, while fi (x) can remain
larger than fj (x), with no misclassication occurring.
Note that while in the above, i is taken as the current (max-
imising) class index, taking j as the next highest-scoring class
corresponds to evasion attacks while taking j as any xed
target class corresponds to a mimicry attack. The results of
Section 7 establish that this method can be more effective for
manipulating test data in multiclass settings. However, it is
not appropriate to binary-class cases as f1 (x)/f2 (x) = 1.

Step Size. The step size t is important to select carefully:
too small and convergence slows; too large and the attack
incurs excessive L1 change, potentially exposing the attack.

(b)  = 104 model: two magenta curves display attack paths under
gradient descent, of the two target points now move away from the
boundary or take signicantly more steps.

Figure 2: Heatmaps visualising decision boundaries of RBF
SVMs trained on a sample dataset.

Illustrative Example. To demonstrate our key observation,
we trained two RBF SVMs on a toy two-class dataset com-
prising two features [Chang and Lin, 2011; Chang and Lin,
2016], using two distinct values for  : 102 and 104 . Figure 2
displays the heatmaps of the two models decision functions.
As can be seen, for the larger  case, additional regions result
with at, approximately-zero, decision values. Since the gra-
dients in these regions are vanishingly small, it is signicantly
more likely that an iterate in the gradient-descent methods at-
tack trajectory will become trapped, or even move towards a
direction away from the decision boundary altogether. No-
tably, both models achieve test accuracies of 100%.
In Figure 2(a), the two black curves marked with crosses
demonstrate how two initial target points (0.55, 0.1) and
(0.75, 0.5), move towards the decision boundary follow-
ing the gradient-descent method. However, the two magenta
curves marked with squares in Figure 2(b) demonstrate how
the same two points either move away from the boundary or
take signicantly more steps to reach it, following the same
algorithm but under a different model with a much larger  .
This example illustrates that although the gradient-descent
method makes the test sample less similar to the original
class, it does not necessarily become similar to the other class.

Discussion. We employ Figure 3 to further explain pos-
sible failure modes of the gradient-descent method: points
may get stuck or even move in the wrong direction. In this
(cid:80)2
given 2D case, x1 belongs to Class 1, x2 to Class 2. At in-
stance x, the k th component of the gradient is k f (x) =
i  xk )  exp( (cid:107)x  xi (cid:107)2
2 ). Clearly, the
i=1 2i yi (xk

Feature 1-1-0.500.5Feature 2-1-0.500.5-0.02-0.0100.010.02Feature 1-1-0.500.5Feature 2-1-0.500.510-3-2-1012x1x2Class 1 (+)Class 2 ()tSmaller widthLarger gap+width = (2/) gapAlgorithm 1: Gradient-quotient step size.
: Iterate xt ; Current quotient gradient   Rd ;
Input
Parameter  > 0
Output: Step size t
1 Select i  arg maxj[d] |j |.
2 Select t > 0 such that t  |i |  

t[5 , 10 ].

In our experiments, we limit the largest change made to a
single feature per iteration, and determine the step size ac-
cordingly as described in Algorithm 1. Here  is a domain-
specic value corresponding to a unit change in a feature, e.g.,
for a grayscale image  = 1 corresponds to a unit change
intensity level. The select rules [5 , 10');
INSERT INTO posts (postId,userId,title,body) VALUES (114,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470',' ] is motivated by
round-off practicalities in steps: if the largest gradient com-
ponent were smaller than 5 , it is likely that most other com-
ponents would be 0, making convergence extremely slow.
Since [5 , 10 ] is a relatively conservative start, we increase
it gradually; as explained next, the maximum step number in
our experiments is 30. This increasing step size corresponds
to a variant of guess-then-double.

6 Experiments: Gradient-Descent Method
Section 4 demonstrates that RBF SVM is less vulnerable to
evasion attacks when  is set appropriately. In this section,
we present a more detailed analysis of the impact of  on the
attacks success rate, further addressing Problems 1 and 2.
6.1 Dataset
MNIST is a dataset of handwritten digits [LeCun et al.,
1998a; LeCun et al., 1998b]. We choose this dataset as it
facilitates comparison of our results with past work. MNIST
contains a training set of 6104 samples (only the rst 5104
are used in our experiments), and a test set of 104 samples.
Following the approach of [Papernot et al., 2016a], we divide
the training set into 5 subsets of 104 samples, D1  D5 . Each
subset is used to train models separately. Specically, in the
experiments that study binary-class settings, only the data in
D1 (or the test dataset) that belong to the two classes are used
for training (or testing respectively).
6.2
Impact of  on Vulnerability (Binary Class)
We begin with the binary scenario and investigate how  im-
pacts the success rate of causing SVMs to misclassify three
pairs of digits1 '||'&'||' 2, 3 '||'&'||' 4, 5 '||'&'||' 7. Note that all models
discussed in this subsection are trained on D1 . An attack is
considered successful if the perturbed test sample is misclas-
sied within 30 steps. The reason why we choose 30 is that
although larger values will increase the attacks success rate,
the changes made to the original samples are so obvious they
would be easily detected by manual audit. Tables 1a1c il-
lustrate a phase transition in each of the three cases: a small
decrease of  causes a signicant jump in success rate.2

2Testing for effect of C revealed much less impact on success
rate, especially when  = 0.5 in these cases, a smaller C = 10 is
chosen.

Figure 4: Minimum distance between each support vector of
class 3 and all support vectors of class 4.

Figure 5: Minimum distance between each support vector of
class 4 and all support vectors of class 3.

Inter-Class-SV Distance. Since  controls how quickly the
RBF kernel vanishes, we are motivated to compare minimum
(Euclidean) distance between each support vector of one class
(sv1i ) and all support vectors of the opposite class (sv2j ),
i.e., M inDist(sv1i ) = arg minj Distance(sv1i , sv2j ). Our
intuition is that a larger  suggests (1) a quicker drop of values
for both the kernel function and the gradient; (2) a wider gap
between the two classes. Both observations contribute to the
lower success rate of the evasion attack.
Figures 4 and 5 present the minimum distance between
support vectors of classes 3 and 4 (due to space limi-
tations, we omit the similar two sets of results). Observe that
when  rst decreases from 0.5, the support vectors of the
opposite class move further awayhere the corresponding
model is still less vulnerable to evasion attack. As  co');
INSERT INTO posts (postId,userId,title,body) VALUES (115,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470','ntin-
ues to decrease the trend reverses, i.e., the support vectors of
opposite class move closer to each other. A smaller  already
means the RBF kernel vanishes more slowly, and the closer
distance between the two classes makes it even easier for a
test sample to cross the decision boundary. Consequently, the
corresponding model becomes much more vulnerable.
This prompts the question: Given a model with a  , is there
a way to determine whether the model is robust? The results

in Tables 1a1c witness a strong correlation between success
  minimum distance
rate and the percentage of 2/
the lower the percentage the less vulnerable the model.

Margin Explanation. We have observed a positive corre-
lation between margin per support vector and this minimum
distance. These ndings suggest that separated inter-class
support vectors leads to more secure models, lending experi-
mental support to the geometric argument (Section 4).

Minimum distance between support vectors56789101112Cumulative percentage(%)0255075100 = 0.01 = 0.025 = 0.035 = 0.05 = 0.125 = 0.5Minimum distance between support vectors56789101112Cumulative percentage(%)0255075100 = 0.01 = 0.025 = 0.035 = 0.05 = 0.125 = 0.5Table 1: Success rate and average L1 change for gradient-descent method evasion attack (binary class).
(a) RBF SVM: digits 1 and 2.
0.05
0.025
0.02
5  104
5  104
5  104
99.5
99.6
99.5
100
100
100
100
100
100
17.0
54.8
68.9
100
100
59.3

Accuracy(%)
Succ rate (%)

  M inDist) (%)
Succ rate (%)

  M inDist) (%)

0.01
5  104
99.4
100
100
94.1
100

0.1
5  104
99.8
90.1
68.4
11.3
7.5

0.11
5  104
99.7
68.9
55.0
13.3
4.8

0.125
5  104
99.5
36.3
35.0
15.2
2.8

P (2/

P (2/

1  2

2  1

0.5
10
98.6
1.5
0.1
12.9
0.1

P (2/

P (2/

(b) RBF SVM: digits 3 and 4.
0.035
0.025
0.01
104
104
104
99.6
99.7
99.5
100
94.5
76.0
96.7
100
100
98.1
100
100
100
100
99.2

Accuracy(%)
Succ rate (%)

  M inDist) (%)
Succ rate (%)

  M inDist) (%)
(c) RBF SVM: digits 5 and 7.
0.035
0.025
0.01
104
104
104
99.6
99.5
99.3
100
97.0
87.4
98.7
100
100
99.8
100
100
100
100
100

Accuracy(%)
Succ rate (%)

  M inDist) (%)
P (2/
Succ rate (%)

  M inDist) (%)
P (2/
(d) Linear SVM.

3  4

4  3

5  7

7  5

0.05
104
99.5
53.4
61.7
82.5
79.7

0.05
104
99.6
71.1
75.6
91.1
87.2

0.125
104
99.5
14.8
0.29
11.8
0.4

0.1
104
99.1
41.8
0.2
15.5
0.2

0.5
10
99.8
5.3
0.1
1.6
0.1

0.5
10
99.7
6.2
0.1
2.3
0.1

C = 1000
Succ rate (%) Ave L1 change Accuracy (%)
5812
100
100
9575
10085
100
8372
100
7748
100
100
7408

99.3

99.0

99.7

C = 5000
Succ rate (%) Ave L1 change Accuracy (%)
4685
100
100
8883
9520
100
6593
100
7169
100
100
6024

99.6

99.2

99.0

1  2
2  1
3  4
4  3
5  7
7  5

Linear SVM. The experiments were performed for linear
SVMs, with results serving as baseline. Table 1d demon-
strates that success rates under linear models are 100%, as
expected. However a larger C requires smaller changes to the
target sample, as larger C leads to smaller margin.
6.3
Impact of  on Vulnerability (Multiclass)
This section further investigates the impact of  on success
rate of evasion attacks, in multiclass scenarios. Two RBF
SVMs with  as 0.05 and 0.5, are trained on both D1 and
D2 , respectively. For comparison, four linear SVMs are also
trained on the two datasets with different values of C . An
attack is considered successful if the perturbed test sample is
misclassied within 30 steps. As can be seen from Table 2:
(1) for RBF SVMs, the success rates under the models with
larger  are much lower; (2) for linear SVMs the success rates
are always 100%, but the average L1 change is smaller as C
increasesobservations consistent with previous results.

Table 2: Success rate and average L1 change of gradient-
descent method evasion attack, in multiclass scenarios.a
AveL1
Accuracy
Succ rate
change
(%)
(%)
RBF ( =
91.8
87.2
Model 1
10037b
0.05, C = 103 )
92.8
87.7
Model 2
10323b
94.8
24.4
Model 1
RBF ( =
4509b
');
INSERT INTO posts (postId,userId,title,body) VALUES (116,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470','0.5, C = 10)
23.7
94.8
Model 2
4637b
Linear
7259
100
89.0
Model 1
(C = 103 )
6837
100
89.2
Model 2
Linear
91.4
100
4766
Model 1
(C = 2  104 )
4604
100
91.7
Model 2
a Since it takes more than an order of magnitude longer to run experi-
ments using RBF SVM than linear kernel, each result regarding
RBF SVM is based on 1000 test samples, while each linear SVM
result is based on 5000 test samples.
b Only the successful cases are counted.

7 Experiments: Gradient-Quotient Method
In this section we present experimental results establishing
the effectiveness of our proposed method for generating ad-
versarial samples.
7.1 Attacking the Model Directly
Recall that based on our new approach, a test sample x is up-
dated as xt+1 = xt  t  (f1 (x)/f2 (x)), where f1 (x) and
f2 (x) are the scores for the top two scoring classes for x. In
order to test whether this method is more effective than the
popular gradient-descent method, we run similar experiments
to Section 6.3, where one RBF SVM ( = 0.5, C = 10) and
one linear SVM (C = 1000) are trained on D1 , . . . , D5 , re-
spectively. Comparing the results in Table 2 and Table 3, we
observe that: (1) for the RBF SVMs with  = 0.5, the suc-
cess rates increase from around 24% to a resounding 100%.
Moreover we have tested a wide range of values for  from
0.01 through 10, with resulting success rates always 100%
under our new approach; (2) the required L1 perturbation
also decreases. These two observations establish that the new
approach is more effective in crafting adversarial samples.
Table 3: Success rate and average L1 change of the evasion
attack (the gradient quotient method, multiclass scenarios).a
AveL1
Accuracy
Succ rate
change
(%)
(%)
4532
100
94.8
Model 1
4571
100
94.8
Model 2
4429
100
95.0
Model 3
95.2
100
4475
Model 4
4610
100
95.0
Model 5
4927
100
89.0
Model 1
5251
100
89.2
Model 2
89.1
100
5317
Model 3
5311
100
89.2
Model 4
88.9
100
5066
Model 5
a Each result regarding RBF SVM is based on 800 test samples,
while each result on linear SVM is based on 5000 test samples.
7.2 Attacking via Surrogate
Up until now, we have implicitly assumed that the attacker
possesses complete knowledge of the target classier, which
may be unrealistic in practice. Hence, we next examine at-
tacks carried out via a surrogate. For example, in order to
mislead a RBF SVM, the attacker rst trains their own RBF
SVM on a similar dataset, builds the attack path of how a test
sample should be modied, then applies it to the target SVM.
Previously, modication is terminated upon misclassica-
tion or once 30 steps are taken. However, since there is no
guarantee that the surrogate and target classiers misclassify
the test sample simultaneously, all test samples are modied
30 times by the surrogate in this experiment; an attack is con-
sidered as successful if the target classier also misclassi-
xt  t  (f1 (xt )/f2 (xt ))
es the adversarial sample within 30 steps. We modify the
method as
xt + t  (f1 (xt )/f2 (xt ))

prior to surrogate
misclassication
otherwise

xt+1 =

RBF
( = 0.5,
C = 10)

Linear
(C = 103 )

In other words, before the test sample is misclassied by
the surrogate, it travels downhill, but after crossing the de-
cision boundary it travels uphill. Otherwise the test case
continues oscillating back and forth around the boundary.
We reuse the RBF SVMs trained in the last section, each
of which serving as both surrogate (Si ) and target (Ti ) clas-
siers.');
INSERT INTO posts (postId,userId,title,body) VALUES (117,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470',' As can be seen from Table 4, the success rates are
all over 65%. Specically, those values inside the bracket
are the success rates when the target classier misclassies
before the surrogate, while the values outside are the overall
success rates. For comparison, the same experiments have
been performed for linear SVMs, producing similar success
rates (at around 60%), notably higher than previous ndings
(around 40%) reported by Papernot et al. [2016a].

Table 4: Success rate of evasion attacks via surrogate (RBF
SVM).a

T3
T2
T1
69.8 (9.5)
66.1 (9.2)
100
S1
67.2 (8.4)
100
68.0 (8.2)
S2
100
66.1 (9.4)
65.0 (8.4)
S3
67.0 (10.6)
67.2 (10.8)
65.3 (10.0)
S4
69.4 (9.0)
67.8 (8.9)
67.0 (9.2)
S5
a Each result is based on 800 test samples.

T4
68.3 (9.8)
72.4 (10.4)
67.3 (11.1)
100
69.4 (9.1)

T5
66.4 (7.1)
67.9 (7.4)
65.9 (8.5)
65.3 (8.8)
100

7.3 Mimicry Attacks
We tested the gradient-quotient method for mimicry attacks.
Our results show that in most cases this approach can suc-
cessfully make the original digit misclassied as any of the
other nine digits. Due to space limitations, those results are
omitted.

8 Conclusions and Future Work
Recent studies have shown that it is relatively easy to fool
machine learning models via adversarial samples. In this pa-
per, we demonstrate that the gradient-descent methodthe
leading approach to generate adversarial sampleshas limi-
tations against RBF SVMs, when the precision parameter 
controlling kernel smoothness is chosen properly. We nd
predictable phase transitions of attack success occur at thresh-
olds that are functions of geometric margin-like quantities
measuring inter-class support vector distances. Our charac-
terisation can be used to make RBF SVM more robust against
common evasion and mimicry attacks.
We propose a new method for manipulating target samples
into adversarial instances, with experimental results showing
that this new method not only increases attack success rate,
but decreases the required changes made to input points.
For
future work,
(1)
regarding the gradient-descent
method, we intend to replicate and expand ndings for  and
smoothness in general, in other settings and for other classi-
ers. (2) We will explore suitability of our new generation ap-
proach when the target is not an SVM, with direct attacks or
SVM surrogates. (3) Further investigation into light-weight
yet efcient countermeasures also serves as an important di-
rection for future work.

References
[Alfeld et al., 2016] Scott Alfeld, Xiaojin Zhu, and Paul Bar-
ford. Data poisoning attacks against autoregressive mod-
els. In AAAI, pages 14521458, 2016.
[Barreno et al., 2006] Marco Barreno, Blaine Nelson, Rus-
sell Sears, Anthony D. Joseph, and J. D. Tygar. Can ma-
chine learning be secure? In AsiaCCS, pages 1625, 2006.
[Barreno et al., 2010] Marco Barreno, Blaine Nelson, An-
thony D. Joseph, and J. D. Tygar. The security of machine
learning. Machine Learning, 81(2):121148, 2010.
[Biggio et al., 2012] Battista Biggio, Blaine Nelson, and
Pavel Laskov. Poisoning attacks against support vector
machines. In ICML, pages 18071814, 2012.
[Biggio et al., 2013] Battista Biggio, Igino Corona, Davide
Maiorca, Blaine Nelson, Nedim S rndi c, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attac');
INSERT INTO posts (postId,userId,title,body) VALUES (118,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470','ks against
In ECML PKDD, pages
machine learning at test time.
387402, 2013.
[Br uckner and Scheffer, 2011] Michael Br uckner and Tobias
Scheffer. Stackelberg games for adversarial prediction
problems. In KDD, pages 547555, 2011.
[Chang and Lin, 2011] Chih-Chung Chang and Chih-Jen
Lin. LIBSVM: A library for support vector machines.
ACM Transactions on Intelligent Systems and Technology,
2(3):127, 2011.
[Chang and Lin, 2016] Chih-Chung Chang and Chih-Jen
Lin.
LIBSVM data: Classication (binary class).
https://www.csie.ntu.edu.tw/cjlin/
libsvmtools/datasets/binary.html#
fourclass, 2016.
[Dalvi et al., 2004] Nilesh Dalvi,
Pedro Domingos,
Mausam, Sumit Sanghai, and Deepak Verma.
Ad-
versarial classication. In KDD, pages 99108, 2004.
[Goodfellow et al., 2014] Ian
J. Goodfellow,
Jonathon
Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. eprint arXiv:1412.6572, 2014.
[Huang et al., 2011] Ling Huang, Anthony D.
Joseph,
Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar.
Adversarial machine learning. In ACM AISec Workshop,
pages 4357, 2011.
[LeCun et al., 1998a] Yann LeCun, Leon Bottou, Yoshua
Bengio, and Patrick Haffner. Gradient-based learning ap-
plied to document recognition. Proc. IEEE, 86(11):2278
2324, 1998.
[LeCun et al., 1998b] Yann LeCun, Corinna Cortes, and
Christopher J.C. Burges. The MNIST database of hand-
written digits.
http://yann.lecun.com/exdb/
mnist/, 1998.
[Li et al., 2016] Bo Li, Yining Wang, Aarti Singh, and
Yevgeniy Vorobeychik.
Data poisoning attacks on
factorization-based collaborative ltering. In NIPS, pages
18851893, 2016.

[Lowd and Meek, 2005] Daniel Lowd
and Christopher
In KDD, pages 641647,
Meek. Adversarial learning.
2005.
[Moosavi-Dezfooli et al., 2016a] Seyed-Mohsen Moosavi-
Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
eprint
Frossard. Universal adversarial perturbations.
arXiv:1610.08401, 2016.
[Moosavi-Dezfooli et al., 2016b] Seyed-Mohsen Moosavi-
Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deep-
Fool: A simple and accurate method to fool deep neural
networks. In CVPR, pages 25742582, 2016.
[Nelson et al., 2012] Blaine Nelson, Benjamin I. P. Rubin-
stein, Ling Huang, Anthony D. Joseph, Steven J. Lee,
Satish Rao, and J. D. Tygar. Query strategies for evad-
ing convex-inducing classiers. J. Machine Learning Re-
search, 13(1):12931332, 2012.
[Nguyen et al., 2015] Anh Nguyen, Jason Yosinski, and Jeff
Clune. Deep neural networks are easily fooled: High con-
dence predictions for unrecognizable images. In CVPR,
pages 427436, 2015.
[Papernot et al., 2016a] Nicolas Papernot,
Patrick Mc-
Daniel, and Ian Goodfellow. Transferability in machine
learning:
from phenomena to black-box attacks using
adversarial samples. eprint arXiv:1605.07277, 2016.
[Papernot et al., 2016b] Nicolas Papernot,
Patrick Mc-
Daniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik,
and Ananthram Swami.
Practical black-box attacks
against deep learning systems using adversarial examples.
eprint arXiv:1602.02697, 2016.
[Papernot et al., 2016c] Nicolas Papernot,
Patrick Mc-
Daniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik,
and Ananthram Swami. The limitations of deep learning
In EuroS'||'&'||'P, pages 372387,
in adversarial settings.
2016.
[Rubinstein et al., 2009] Benjamin I. P. Rubinstein, Blaine
Nelson, Ling Huang, Anthony D. Joseph, Shing-hon Lau,
Satish Rao, Nina Taft, and J. D. Tygar. ANTIDOTE: un-
derstanding and defending against poisoning of anomaly
detectors. In IMC, pages 114, 2009.
[Russu et al., 2016] Paolo Russu, Ambra Demontis, Battista
Biggio, Giorgio Fumera, and Fabio Roli. Secure kernel
In ACM AISec Work-
machines against evasion attacks.
shop, pages 5969, 2016.
[Samaria and Harter, 1994] Ferdinando Samaria and Andy
Harter. Parameterisation of a stochastic model for human
face identicat');
INSERT INTO posts (postId,userId,title,body) VALUES (119,2920,'School of Computing and Information Systems, University of Melbourne {yi.han, benjamin.rubinstein}@unimelb.edu.au 12rA6 R.c 1470','ion. In IEEE Workshop on Applications of
Computer Vision, pages 138142, 1994.
[Szegedy et al., 2013] Christian
Wojciech
Szegedy,
Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus.
Intriguing properties of
neural networks. eprint arXiv:1312.6199, 2013.

');
INSERT INTO posts (postId,userId,title,body) VALUES (120,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il','Tsvi Kopelowitz
University of Waterloo
kopelot@gmail.com

Ely Porat
Bar Ilan University
porately@cs.biu.ac.il

April 7, 2017

Abstract

In the pattern matching with d wildcards problem one is given a text T of length n and a
pattern P of length m that contains d wildcard characters, each denoted by a special symbol
(cid:48) ?(cid:48) . A wildcard character matches any other character. The goal is to establish for each
m-length substring of T whether it matches P . In the streaming model variant of the pattern
matching with d wildcards problem the text T arrives one character at a time and the goal
is to report, before the next character arrives, if the last m characters match P while using
only o(m) words of space.
In this paper we introduce two new algorithms for the d wildcard pattern matching
problem in the streaming model. The rst is a randomized Monte Carlo algorithm that
is parameterized by a constant 0    1. This algorithm uses O(d1 ) amortized time
per character and O(d1+ ) words of space. The second algorithm, which is used as a black
box in the rst algorithm, is a randomized Monte Carlo algorithm which uses O(d + log m)
worst-case time per character and O(d log m) words of space.
12rA5 S.c 166047:ir
.

Part of this work took place while the second author was at University of Michigan. This work is supported
in part by the Canada Research Chair for Algorithm Design, NSF grants CCF-1217338, CNS-1318294, and
CCF-1514383, by ISF grant 1278/16, and by the BIU Center for Research in Applied Cryptography and Cyber
Security in conjunction with the Israel National Cyber Bureau in the Prime Ministers Oce .

1

Introduction

We investigate the pattern matching with d wildcards problem (PMDW) in the streaming model.
Let  be an alphabet and let (cid:48)?(cid:48) /  be a special character called the wildcard character which
matches any character in . The PMDW problem is dened as follows. Given a text string
T = t0 t1 . . . tn1 over  and a pattern string P = p0p1 . . . pm1 over alphabet   {?} such that
P contains exactly d wildcard characters, report all of the occurrences of P in T . This denition
of a match is one of the most well studied problems in pattern matching [22, 35, 26, 28, 19, 10].

The streaming model. The advances in technology over the last decade and the massive
amount of data passing through the internet has intrigued and challenged computer scientists,
as the old models of computation used before this era are now less relevant or too slow. To this
end, new computational models have been suggested to allow computer scientists to tackle these
technological advances. One prime example of such a model is the streaming model [1, 25, 34, 29].
Pattern matching problems in the streaming model are allowed to preprocess P into a data
structure that uses space that is sublinear in m (notice that space usage during the preprocessing
phase itself is not restricted). Then, the text T is given online, one character at a time, and the
goal is to report, for every integer   m  1, whether tm+1 . . . t matches P . This reporting
must take place before t+1 arrives. Throughout this paper we let  denote the index of the last
text character that has arrived.
Following the breakthrough result of Porat and Porat [36], recently there has been a rising
interest in solving pattern matching problems in the streaming model [7, 20, 33, 8, 27, 14, 15].
However, this is the rst paper to directly consider the important wildcard variant.

Related work. Notice that one way for solving PMDW (not necessarily in the streaming
model), is to treat (cid:48)?(cid:48) as a regular character, and then run an algorithm that nds all occurrences
of P (tha');
INSERT INTO posts (postId,userId,title,body) VALUES (121,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 2)','t does not contain any wildcards) in T with up to k = d mismatches. This is known
as the k-mismatch problem [32, 37, 2, 13, 12, 17, 15]. The most recent result by Cliord et

al. [15] for the k-mismatch problem in the streaming model implies a solution for PMDW in the
streaming model that uses O(d2 polylog m) words1 of space and O(
d log d + polylog m) time
per character. Notice that Cliord et al. [15] focused on solving the more general k-mismatch
problem.
We mention that while our work is in the streaming model, in the closely related online model
(see [18, 16]), which is the same as the streaming model without the constraint of using sublinear
space, Cliord et al. [11] presented an algorithm, known as the black box algorithm, which solves
several pattern matching problems. When applied to PMDW, the black box algorithm uses
O(m) words of space and O(log2 m) time per arriving text character. In the oine model the
most ecient algorithms for PMDW take O(n log m) time and were introduced by Cole and
Hariharan [19] and by Cliord and Cliord [10].

1.1 New results

We improve upon the work of Cliord et al. [15], for the special case that applies to PMDW, by
introducing the following algorithms (the O notation hides logarithmic factors). Notice that
Theorem 2 improves upon the results of Cliord et al. [15] whenever  > 1/2. We also emphasize
that our proof of Theorem 2 makes use of Theorem 1.

1We assume the RAM model where each word has size of O(log n) bits.

Theorem 1. There exists a randomized Monte Carlo algorithm for the PMDW problem in the
streaming model that succeeds with probability 1  1/poly(n), uses O(d log m) words of space and
spends O(d + log m) time per arriving text character.
Theorem 2. For any constant 0    1 there exists a randomized Monte Carlo algorithm for
the PMDW problem in the streaming model that succeeds with probability 1  1/poly(n), uses
O(d1+ ) words of space and spends O(d1 ) amortized time per arriving text character.

1.2 Algorithmic Overview

Our algorithms make use of the notion of a candidate, which is a location in the last m indices of
the current text that is currently considered as a possible occurrence of P . As more characters
arrive, it becomes clear if this candidate is an actual occurrence or not. In general, an index
continues to be a candidate until the algorithm encounters proof that the candidate is not a
valid occurrence (or until it is reported as a match). The algorithm of Theorem 1 works by
obtaining such proofs eciently.

Overview of algorithm for Theorem 1. For the streaming pattern matching problem
without wildcards, the algorithms of Porat and Porat [36] and Breslauer and Galil [7] have
three ma jor components2 . The rst component is a partitioning of the interval [0, m  1] into
pattern intervals of exponentially increasing lengths. Each pattern interval [i, j ] corresponds to
a text interval [  j + 1,   i + 1], where  is the index of the last text character that arrived3 .
Notice that when a new text character arrives, the text intervals are shifted by one location. The
second component maintains all of the candidates in a given text interval. This implementation
leverages periodicity p');
INSERT INTO posts (postId,userId,title,body) VALUES (122,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 3)','roperties of strings in order to guarantee that the candidates in a given
text interval form an arithmetic progression, and thus can be maintained with constant space.
The third component is a ngerprint mechanism for testing if a candidate is still valid. Whenever
the border of a text interval passes through a candidate, that candidate is tested.
The main challenge in applying the above framework for patterns with wildcards comes
from the lack of a good notion of periodicity which can guarantee that the candidates in a text
interval form an arithmetic progression. To tackle this challenge, we design a new method for
partitioning the pattern into intervals, which, combined with new fundamental combinatorial
properties, leads to an ecient way for maintaining the candidates in small space. In particular,
we prove that with our new partitioning there are at most O(d log m) candidates that are not
part of any arithmetic progression for any text interval. Remarkably, the proof bounding the
number of such candidates uses a more global perspective of the pattern, as opposed to the
techniques used in non-wildcard results.

Overview of algorithm for Theorem 2. The algorithm of Theorem 2 uses the algorithm
of Theorem 1 (with a minor adaptation) combined with a new combinatorial perspective on
periodicity that applies to strings with wildcards. The notion of periodicity in strings (without
wildcards) and its usefulness are well studied [21, 31, 36, 7, 24, 23]. However, extending the
usefulness of periodicity to strings with wildcards runs into diculties, since the notions are

2The algorithms of Porat and Porat [36] and Breslauer and Galil [7] are not presented in this way. However, we
nd that this new way of presenting our algorithm (and theirs) does a better job of explaining what is going on.
3The rst pattern interval starts at 0, and so the last text interval ends at location  + 1, which is a location
of a text character that has yet to arrive. To understand why this convention is appropriate, notice that initially
every text location should be considered as a candidate, but in order to save space we only address such candidates
a moment before their corresponding character arrives since this is the rst time the algorithm can obtain proof
that the candidate is not a match.

either too inclusive or too exclusive (see [5, 4, 6, 9, 38]). Thus, we introduce a new denition of
periodicity, called the wildcard-period length that captures, for a given pattern with wildcards,
the smallest possible average distance between occurrences of the pattern in any text. See
Denition 6. For a string S with wildcards, we denote the wildcard-period length of S by S .
Let P  be the longest prex of P such that P   d . The algorithm of Theorem 2 has two
main components, depending on whether P  = P or not. In the case where P  = P , the algorithm
takes advantage of the wildcard-period length of P being small, which, together with techniques
from number theory and new combinatorial properties of strings with wildcards, allows to spend
');
INSERT INTO posts (postId,userId,title,body) VALUES (123,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 4)','only O(1) time per character and uses O(d1+ ) words of space. This is summarized in Theorem 17.
Of particular interest is Lemma 16 which combines number theory with combinatorial string
properties in a new way. We expect these ideas to be useful in other applications.
If P  (cid:54)= P , then we use the algorithm of Theorem 17 to locate occurrences of P  , and by
maximality of P  , occurrences of prexes of P that are longer than P  must appear far apart
(on average). These occurrences are given as input to a minor adaptation of the algorithm of
Theorem 1 in the form of candidates. Utilizing the large average distance between candidates,
we obtain an O(d1 ) amortized time cost per character.

2 Preliminaries

2.1 Periods
We assume without loss of generality that the alphabet is  = {1, 2, . . . , n}. For a string
S = s0s1 . . . s(cid:96)1 over  and integer 1  k  (cid:96) , the substring s0s1 . . . sk1 is called a prex of S
and s(cid:96)k . . . s(cid:96)1 is called a sux of S .
A prex of S of length i  1 is a period of S if and only if sj = sj+i for every 0  j  (cid:96)  i  1.
The shortest period of S is called the principle period of S , and its length is denoted by S . If
S  |S |
2 we say that S is periodic.
The following lemma is due to Breslauer and Galil [7].

Lemma 3 ([7, Lemma 3.1]). Let u and v be strings such that u contains at least three occurrences
of v . Let t1 < t2 <    < th be the locations of al l occurrences of v in u. Assume that h  3 and
that for i = 1, . . . , h  2, we have ti+2  ti  |v |. Then, the sequence (t1 , t2 , . . . , th ) forms an
arithmetic progression with dierence v .

The following lemmas follow from Lemma 3.

Lemma 4. Let v be a string of length (cid:96) and let u be a string of length at most 2(cid:96). If u contains
at least three occurrences of v then the distance between any two occurrences of v in u is a
multiple of v and v is a periodic string.
Proof. Let 0  c1 < c2 < c3  |u|  1 be three occurrences of v in u. Thus, c3  (|u|  1)  (|v | 
1)  2(cid:96)  (cid:96) = (cid:96), and so c3  c1  (cid:96). Therefore, by Lemma 3, all the occurrences of v in u form an
arithmetic progression with common dierence v . In particular, the distance between any two
occurrences of v in u is a multiple of v . Hence, v + v  (c3  c2 ) + (c2  c1 ) = c3  c1  (cid:96) = |v |
and v  |v |
2 . Thus, by denition, v is a periodic string.

Lemma 5. Let u be a periodic string over  with principle period length u . If v is a substring
of u of length at least 2u then u = v .

Proof. Since v is a substring of u, we have by denition that u is a period length of v , and thus
v  u by the minimality of v .

It only remains to prove that u  v , which we do by showing that v is a period length of
u. We denote u = u0u1 . . . u|u|1 .
Let 0  i < |u|  v be an index in u, we have to prove that ui = ui+v .
(cid:109)
(cid:108) ai
Let a be an index such that v occurs in u in position a, thus uaua+1 . . . ua+2u1 is a substring
of both u and v . Since u is a period length of u, ui = ui+z u for any z  Z if 0  i + z  u < |u|.
(cid:109)
(cid:108) ai
. Let b = i + z  u . Notice that
In particular, for z =
we have that ui = uu
i+
a  b < a + u and a  b + v < a + 2u . Therefore, b and b + v are both indices of charactersin v , and thus ub = ub+v . Hence, we have that ui = ui+z u = ui+z u+v = ui+v , where the
last equality is based again on the fact that u is a period length of u.

Periods and wildcards. For a string u with no wildcards, there is an inverse relationship
between the maximum number of occurre');
INSERT INTO posts (postId,userId,title,body) VALUES (124,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 5)','nces of u in a text of a given length and the principle
period length of u. Next, we dene the wildcard-period length of a string over   {?} which
captures a similar type of relationship for strings with wildcards. The usefulness of this denition
for our needs is discussed in more detail in Section 6. Let occ(S (cid:48) , S ) be the number of occurrences
of a string S in a string S (cid:48) .
(cid:26)(cid:24)
(cid:25)(cid:27)
Denition 6. For a string S over   {?}, the wildcard-period length of S is
|S |
S = min
occ(S (cid:48) , S )
S (cid:48)2|S |1
For the following let u, v  (cid:83)n
2.2 Fingerprints
Breslauer and Galil [7] proved the existence of a sliding ngerprint function  : (cid:83)n
i=0 i be two strings of size at most n. Porat and Porat [36] and
i=0 i  [nc ],
for some constant c > 0, which is a function where:
1. If |u| = |v | and u (cid:54)= v then (u) (cid:54)= (v) with high probability (at least 1  1
nc1 ).
2. The sliding property: Let w=uv be the concatenation of u and v . If |w|  n then given
the length and the ngerprints of any two strings from u,v and w, one can compute the
ngerprint of the third string in constant time.

3 A Generic Algorithm

We start with a generic algorithm (pseudo-code is given in Figure 1) for solving pattern matching
problems in the streaming model. With proper implementations of the algorithms components,
the algorithm solves the PMDW problem. The generic algorithm makes use of the notion of a
candidate. Initially every text index c is considered as a candidate for a pattern occurrence from
the moment tc1 arrives. An index continues to be a candidate until the algorithm encounters
proof that the candidate is not a valid occurrence (or until it is reported as a match). A candidate
is alive until such proof is given.
The generic algorithm is composed of three conceptual parts that aect the complexities of
the algorithm. An example of an execution of the generic algorithm appears in Figures 2 and 3:
 Pattern and text intervals. The rst part is an ordered list I = (I0 , . . . , Ik ) of intervals.
The disjoint union of the intervals of I is exactly [0, m  1] and the intervals are ordered
such that I = [i, j ] precedes I (cid:48) = [i(cid:48) , j (cid:48) ] if and only if j < i(cid:48) . Each interval I  I is called

Init()
1 Q0 .Enqueue(0)

Process-Character(t )
for h = 0 to kc = Qh .Dequeue()3
if c exists and c is valid
if h = k5
report c as a matchelse Qh+1 .Enqueue(c)
7 Q0 .Enqueue( + 1)

Figure 1: Generic Algorithm. The purpose of the initialization is to consider location 0 as a
candidate before any candidate has arrived.

a pattern interval. For each pattern interval I = [i, j ]  I we dene a corresponding
text interval, text interval(I , ) = [  j + 1,   i + 1]. When character t arrives,
a text location c  text interval(I , ) is a candidate if and only if tc    tc+i1 matches
p0    pi1 . The candidate set C (I , ) is the set of text positions in text interval(I , )
which are candidates right after the arrival of t .
 Candidate queues. The second conceptual part of the generic algorithm is an imple-
mentation of a candidate-queue data structure. For any interval I  I , the algorithm
m');
INSERT INTO posts (postId,userId,title,body) VALUES (125,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 6)','aintains a candidate queue QI . At any time , which is the time right after t arrives,
but before t+1 arrives, QI stores a (possibly implicit) representation of C (I , ). Thus,
the operations of the data structure are time-dependent. Candidate-queues support the
following operations.
Denition 7. A candidate-queue for an interval [i, j ] = I  I supports the fol lowing
operations at time, where t is the last text character that arrived.
1. Enqueue(): add c =   i + 1 to the candidate-queue.
2. Dequeue(): remove and return a candidate c =   j , if such a candidate exists.

Since there is a bijection between pattern intervals and text intervals we say that a
candidate-queue that is associated with pattern interval I is also associated with the
corresponding text interval text interval(I , ).
 Assassinating candidates. The third conceptual part addresses the following. When
a new text character arrives, all the text intervals move one position ahead, and some
candidates leave some text intervals and their corresponding candidate sets. The third
conceptual part is a mechanism for testing if a candidate is valid after that candidate
leaves a candidate set. This mechanism is used in order to determine if the candidate
should enter the candidate-queue of the next text interval, or be reported as a match if
there are no more text intervals.

The implementation of each of the three components controls the complexities of the algorithm.
Minimizing the number of intervals reduces the number of candidates leaving text intervals

at a given time. Ecient implementations of the candidate-queue operations and testing if a
candidate is valid control both the space usage and the amount of time spent on each candidate
that leaves an interval. Notice that the implementations of these components may depend on
each other, which is also the case in our solution.

Figure 2: Example of a pattern and its arbitrarily chosen pattern intervals. The pattern length
is 10 and the pattern intervals are [0, 3], [4, 7] and [8, 9].

Figure 3: Example of an execution of the generic algorithm with the pattern of Figure 2. In
each row a new text character arrives. The bold borders illustrate the text intervals. Each blue
cell is a position of a candidate and the green cell corresponds to a match.
When t52 arrives, the candidate c1 = 45 is tested, since it exits a text interval. The candidate c1
remains alive because abababaa is a prex of the pattern. Notice that at this time the candidate
c2 = 47 in not a valid occurrence of the pattern, but, the algorithm does not remove c2 until c2
reaches the end of the text interval.
When t54 arrives, the candidates c1 = 45 and c2 = 47 are tested, as they have reached the end of
their text intervals. At this time, c2 is removed since the text ababaaab is not a prex of the
pattern. The candidate c1 remains alive and is reported as a match, since c1 reached the end of
the last text interval.

abababaaab"#$%'||'&'||'()*+[0										,			3][4										,			7][8			,			9]babacabaabab"#"$"%"'||'&'||'""""(")"*"+#$[43	,	44][45							,								48][49							,								52]ababacabaabab"#"$"%"'||'&'||'""""(")"*"+#$%[44	,	45][46							,								49][50							,								53]aababacabaabab"#"$"%"'||'&'||'""""(")"*"+#$%'||'&'||'[45	,	46][47							,								50][51							,								54]baababacabaabab"#"$"%"'||'&'||'""""(")"*"+#$%'||'&'||'"[46	,	47][48							,								51][52							,								55]A nave implementation. The following nave implementation of the generic algorithm is
helpful');
INSERT INTO posts (postId,userId,title,body) VALUES (126,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 7)',' for gaining intuition as to how the algorithm works. Let Inave = ([0, 0], [1, 1], . . . , [m 
1, m  1]). The implementation of candidate queue QI explicitly stores the set C (I , ) at time .
Notice that C (I , ) contains at most one candidate. The task of verifying that a candidate c
is valid in between text intervals is a straightforward comparison of pc with t . Each such
comparison costs O(1) time. The runtime of the algorithm is (m) time per character in the
worst-case, and the space usage is also (m).4 We refer to this algorithm as the nave algorithm.

Using ngerprints.
If there are no wildcards in P , then one can use the following nger-
print based algorithm that veries the validity of a candidate c only once all the characters
tc , tc+1 , . . . , tc+m1 have arrived. This algorithm is closely related to the Karp and Rabin [30]
algorithm. The algorithm uses a partitioning of [0, m  1] into only one interval containing all of
[0, m  1].
The algorithm maintains the text ngerprint which is the ngerprint of the text from its
beginning up to the last arriving character. For each text index c, just before tc arrives the
algorithm creates a candidate for the index c and stores the text ngerprint (t0 t1 . . . tc1 ) as
satellite information of the candidate c. Then, c (together with its satellite information) is added
to the candidate-queue via the Enqueue() operation. When the character tc+m1 arrives, the
text ngerprint is (t0 . . . tc+m1 ). At this time, the algorithm uses the Dequeue() operation to
extract c together with (t0 t1 . . . tc1 ) from the candidate-queue. Then, the algorithm tests if c
is valid by computing (tc . . . tc+m1 ) from the current text ngerprint (t0 t1 . . . tc+m1 ) and
the ngerprint (t0 t1 . . . tc1 ) (using the sliding property of the ngerprint function), and then
testing if (tc . . . tc+m1 ) equals (p0 . . . pm1 ). The ngerprint algorithm spends only constant
time per text character, but, like the nave algorithm, uses (m) words of space to store the
candidate-queue.

3.1 Fingerprints with Wildcards

Using ngerprints together with wildcards seems to be a dicult task, since for any string S with
x wildcards there are ||x dierent strings over  that match the string S . Each one of these
dierent strings may have a dierent ngerprint and therefore there are (||x ) ngerprints to
store, which is not feasible. In order to still use ngerprints for solving PMDW we use a special
partitioning of [0, m  1], which is described in Section 4. The partitioning in Section 4 is based
on the following preliminary partitioning.

The preliminary partitioning. We use a representation of P as P = P0?P1? . . .?Pd where
each subpattern Pi contains only characters from  (and may also be an empty string). Let
W = (w1 , w2 , . . . , wd ) be the indices of wildcards in P such that for all 1  i < d we have
wi < wi+1 . The interval [0, m  1] is partitioned into pattern intervals as follows:
J = ([0, w1  1], [w1 , w1 ], [w1 + 1, w2  1], . . . , [wd , wd ], [wd + 1, m  1]).

Since some of the pattern intervals in this partitioning could be empty, we discard such intervals.
The pattern intervals of the form [wi , wi ] are called wildcard interval s and the other pattern
intervals are called regular intervals. Notice that for a text index c, the substring tc . . . tc+m1
matches P if and only if for each regular interval [i, j ], tc+i . . . tc+j = pi . . . pj .

4For example, if the pattern is aa . . . a = am and the text is an , then each candidate c is alive as long as the
characters tc , . . . tc+m1 arrive. Therefore, after the arrival of the rst m  1 characters, any additional a');
INSERT INTO posts (postId,userId,title,body) VALUES (127,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 8)','rriving
character is compared with m pattern characters.

A preliminary algorithm. Given the preliminary partition J , one could use the following
algorithm for testing the validity of a candidate c whenever it leaves a text interval. During the
initialization of the algorithm we precompute and store the ngerprints for all of the subpatterns
corresponding to regular intervals. Each time a candidate c is added to a candidate-queue for
interval [i, j ]  J via the Enqueue() operation, the algorithm stores the current text ngerprint
(t0 . . . tc+i1 ) together with the candidate c. When the character tc+j arrives, the text ngerprint
is (t0 . . . tc+j ). At this time, the algorithm uses the Dequeue() operation to extract c together
with (t0 t1 . . . tc+i1 ) from the candidate-queue of interval [i, j ]. If [i, j ] is a regular interval,
then the algorithm tests if c is valid, and removes (assassinates) c if it is not. This validity test is
executed by applying the sliding property of the ngerprint function to compute (tc+i . . . tc+j )
from the current text ngerprint (t0 t1 . . . tc+j ) and the ngerprint (t0 t1 . . . tc+i1 ), and then
testing if (tc+i . . . tc+j ) is the same as (pi . . . pj ). If [i, j ] is a wildcard interval then c stays
alive without any testing.
A nave implementation of the candidate queues provides an algorithm that costs O(d) time
per character, but uses (m) words of space. To overcome this space usage we employ a more
complicated partitioning, which, together with a modication of the requirements from the
candidate-queues, allows us to design a data structure that uses much less space. However, this
space eciency comes at the expense of a slight increase in the time per character.

4 The Partitioning

The key idea of the new partitioning is to use the partitioning of Section 3.1 as a preliminary
partitioning, and then perform a secondary partitioning of the regular pattern intervals, thereby
creating even more regular intervals. As mentioned, the intervals are partitioned in a special
way which allows us to implement candidate-queues in a compact manner (see Section 5).
The following denition is useful in the next lemma.
Denition 8. For an ordered set of intervals I = (I0 , I1 , . . . Ik ) and for any integer 0  x  k ,
let I (x) = max0yx {|Iy |} be the length of the longest interval in the sequence I0 , . . . Ix . When
I is clear from context we simply write (x) = I (x)

The following lemma provides a partitioning which is used to improve the preliminary
partitioning algorithm. The properties of the partitioning that are described in the statement
of the lemma are essential for our new algorithm. The most essential property is property 3,
since it guarantees that for each pattern interval I = [i, j ], there exists a substring of P prior to
pi and with no wildcards whose length is |I |. If this substring is not periodic, then for any ,
C (I , ) does not contain more than two candidates. If this substring is periodic, then we show
how to utilize the periodicity of the string in order to eciently maintain all the candidates in
C (I , ) for any  (see Section 5). In the proof of the lemma we introduce a specic partitioning
which has all of the stated properties.

Lemma 9. Given a pattern P of length m with d wildcards, there exists a partitioning of the
interval [0, m  1] into subintervals I = (I0 , I1 . . . , Ik ) which has the fol lowing properties:

1. If I = [i, j ] is');
INSERT INTO posts (postId,userId,title,body) VALUES (128,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 9)',' a pattern interval then pi . . . pj either corresponds to exactly one wildcard
from P (and so j = i) or it is a substring that does not contain any wildcards.

2. k = O(d + log m).
3. For each regular pattern interval I = [i, j ] with |I | > 1, the length i prex of P contains a
consecutive sequence of |I | non-wildcard characters.

Figure 4: The general case: for each Jh  J we rst create two intervals of length h and then
we iteratively create pattern intervals where the length of each pattern interval is double the
length of the previous pattern interval.

4. |{I (0), I (1) . . . I (k)}| = O(log m).

Proof. We introduce a secondary partitioning of the preliminary partitioning described in
Section 3.1, and prove that the secondary partitioning has all the required properties; see
Figures 4, 5 and 6. Recall that we use a representation of P as P = P0?P1? . . .?Pd . Let Jh be
the preliminary pattern interval corresponding to Ph . The secondary partitioning is executed
on the pattern intervals J = (J0 , J1 , . . . , Jd ), where the partitioning of Jh is dependent on the
partitioning of J0 , . . . , Jh1 . Thus, for h > 0, the secondary partitioning of Jh takes place only
after the secondary partitioning of Jh1 .
When partitioning pattern interval Jh = [i, j ], let gh be the number of pattern intervals in
the secondary partitioning of [0, i  1], and let h be the length of the longest pattern interval in
the secondary partitioning of [0, i  1]. For the rst pattern interval let 0 = 1. If j  i + h  1
then the only pattern interval is all of Jh . If j  i + 2  h  1 then we create the pattern intervals
[i, i + h  1] and [i + h , j ]. Otherwise, we rst create the pattern intervals [i, i + h  1] and
[i + h , i + 2  h  1]5 , and for as long as there is enough room in the remaining preliminary
pattern interval Jh (between the position right after the end of the last secondary pattern interval
that was just created and j ) we iteratively create pattern intervals where the length of each
pattern interval is double the length of the previous pattern interval. Once there is no more
room left in Jh , let (cid:96) be the length of the last pattern interval we created. If the remaining part
of the preliminary pattern interval is of length at most (cid:96), then we create one pattern interval for
all the remaining preliminary pattern interval. Otherwise we create two pattern intervals, the
rst pattern interval of length (cid:96) and the second pattern interval using the remaining part of Jh .
The secondary partitioning implies all of the desired properties:

Property 1. Being that the secondary partitioning is a sub partitioning of the preliminary
partitioning and the preliminary partitioning already had this property, then the secondary
partitioning has this property as well.

Property 2. For a subpattern Ph , the length of every pattern interval created from Jh during
the secondary partitioning, except for the rst two pattern intervals and possibly also the last
two pattern intervals, is at least twice the length of the longest pattern interval preceding it. So
the total number of such pattern intervals is O(log m). The number of other regular pattern
intervals is at most 4(d + 1). Additionally, there are d wildcard pattern intervals. So the total
number of pattern intervals is at most 4(d + 1) + d + O(log m) = O(d + log m).
Property 3. If there is a regular pattern interval I (cid:48) = [i(cid:48) , j (cid:48) ] such that j (cid:48) < i and |I (cid:48) |  |I |,
then the subpattern associated with I (cid:48) meets the requirement.
5The choice of having the rst two intervals to be of the same length h is in or');
INSERT INTO posts (postId,userId,title,body) VALUES (129,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 10)','der to guarantee the third
property in the lemma, as shown below.

1124%2%?%=%1=max.:0%134567894%,%7878<478<6%Figure 5: Once there is no more room left in Jh , if the remaining interval is of length at most
(cid:96) (the top case), then we create one pattern interval for all the remaining interval. Otherwise
(the bottom case) we create two pattern intervals, the rst pattern interval of length (cid:96) and the
second pattern interval using the remaining part of Jh .

If there is no such pattern interval, it must be the case that the length of I is twice the
length of the pattern interval preceding I , and I is contained in a preliminary pattern interval
Jh for some h. Let the length of the rst pattern interval created in Jh be denoted by h . Let
Ih,1 , Ih,2 , . . . Ih,r be the rst r pattern intervals created in Jh such that Ih,r = I . The length of
any pattern interval Ih,r (cid:48) for 1 < r (cid:48)  r is 2r (cid:48)2 h (since |Ih,1 | = |Ih,2 | = h , and for 2 < r (cid:48)  r
we have |Ih,r (cid:48) | = 2|Ih,r1 |), and in particular the length of I is 2r2 h . Recall that I = [i, j ]. The
Ih,r (cid:48) for r (cid:48) < r . These lengths sum up to (1 + (cid:80)r1
length of the prex of Ph up to the index i is the sum of the lengths of all the pattern intervals
r (cid:48)=2 2r (cid:48)2 )h = 2r2 h = |I |. So the prex of Ph
fullls the requirement.
Property 4. We prove a stronger claim: for each 0  h  k , I (h) is a power of 2.
This is true by induction. The rst pattern interval is of length 1, and therefore I (0) = 1 = 20 .
If |Ih |  I (h  1) then I (h) = I (h  1) which is a power of 2 by the induction hypothesis.
Otherwise, if |Ih | > I (h  1) then by the secondary partitioning algorithm |Ih | = 2I (h  1),
and I (h) = 2I (h  1). Hence I (h) is also a power of 2.
The largest pattern interval is at most of length m, and therefore there are at most (cid:100)log m(cid:101)
dierent values in {I (0), I (1), . . . , I (k)}.

5 The Candidate-ngerprint-queue

The algorithm of Theorem 1 is obtained via an implementation of the candidate-queues that uses
O(d log m) words of space, at the expense of having O(d + log m) intervals in the partitioning.
Such space usage implies that we do not store all candidates explicitly. This is obtained by
utilizing properties of periodicity in strings. Since candidates are not stored explicitly, we cannot
store explicit information per candidate, and in particular we cannot explicitly store ngerprints.
On the other hand, we are still interested in using ngerprints in order to perform assassinations.
To tackle this, we strengthen our requirements from the candidate-queue data structure to
return not just the candidate but also the ngerprint information that is needed to perform the
test of whether the candidate is still valid. For our purposes, this data structure cannot explicitly
maintain all the ngerprints information. Thus, we extend the denition of a candidate-queue
to a candidate-ngerprint-queue as follows.

10

Figure 6: Example of patterns and their intervals in the secondary partitioning. Each bold
rectangle corresponds to an interval in the partition.

Denition 10. A candidate-ngerprint-queue for an interval [i, j ] = I  I supports t');
INSERT INTO posts (postId,userId,title,body) VALUES (130,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 11)','he fol lowing
operations, where t is the last text character that arrived.
1. Enqueue((t0 . . . ti )): add c =   i + 1 to the candidate-queue.
2. Dequeue(): remove and return a candidate c =   j , if such a candidate exists, together
with (t0 . . . tc1 ) and (t0 . . . tc+i1 ).

In order to reduce clutter of presentation, in the rest of this section we refer to the candidate-
ngerprint-queue simply as the queue.

5.1 Implementation

Our implementation of the queue assumes that we use a partitioning that has the properties
stated in Lemma 9. Let I = [i, j ] be a pattern interval in the partitioning and let c be a candidate
from C (I , ). The entrance prex of c is the substring tc . . . tc+i1 , and the entrance ngerprint
is (tc . . . tc+i1 ). By denition, since c  C (I , ), the entrance prex of c matches p0 . . . pi1
(which may contain wildcards). Recall that a candidate c is inserted into QI together with
(t0 . . . tc1 ), which we call the candidate ngerprint of c.

Satellite information. The implementation associates each candidate c with satel lite in-
formation (SI), which includes the candidate ngerprint and the entrance ngerprint of the
candidate. The SI of a candidate combined with the sliding property of ngerprints are crucial
for the implementation of the queue. When c is added to QI , for some I = [i, j ], we compute
the entrance ngerprint of c from the candidate ngerprint and from (t0 . . . tc+i1 ) which is
the text ngerprint at that time. When c is removed from QI , we compute (t0 . . . tc+i1 ) in
constant time from the SI of c. See Figure 7.

Arithmetic progressions and entrance prexes.
In order to implement the queue using
a small amount of space, we distinguish between two types of candidates for each interval
I = [i, j ]  I . The rst type are candidates that share a specic entrance prex, uI , which is
dened solely by p0 . . . pi1 and is chosen such that if there are more than two candidates in
C (I , ) with the same entrance prex then this entrance prex must be uI (see Lemma 11). In
Lemma 12 we prove that all the candidates in C (I , ) that have entrance prex uI , form an
arithmetic progression. This leads to Lemma 13 where we show that all of theses candidates

11

Figure 7: The satellite information of a candidate c in a text interval text interval(I , )
for I = [i, j ] includes the candidate ngerprint (t0 . . . tc  1) and the entrance ngerprint
(tc . . . tc+i1 ). The ngerprint of (t0 . . . tc+i1 ) can be computed in constant time, using the
sliding ngerprint property.

and their SI information can be stored implicitly using O(1) words of space. The second type of
candidates are the rest of the candidates, and these candidates are stored explicitly together
with their SI information. We prove in Lemma 14 that the total number of such candidates is
O(d log m), thereby obtaining our claimed space usage.
Lemma 11. Suppose I is a partitioning that satises the properties of Lemma 9. For a pattern
interval I = [i, j ]  I , there exists a s');
INSERT INTO posts (postId,userId,title,body) VALUES (131,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 12)','tring uI such that for any text T (cid:48) and time   0 the set
C (I , ) does not contain three candidates with the same entrance prex u (cid:54)= uI .
Proof. Let c1 < c2 < c3 be three dierent candidates in C (I , ) with the same entrance prex
u. By property 3 of Lemma 9 there is a string v of length |I | = j  i + 1 containing only
non-wildcard characters that is a substring of the length i prex of P .
Let r be an arbitrary location of v in p0 . . . pi1 (since v could appear several times in the
prex). The three candidates imply that after a shift of r characters from the candidates
locations, there are three occurrences of v in the text. These occurrences are within a substring
of the text of length at most 2|v |, since all three candidates are in C (I , ) and so the distance
between the rst and last occurrence is at most |I |  1 = |v |  1 (the 2 factor accommodates the
full occurrence of the third v). Thus, by Lemma 4, v must be periodic, and |v |  2v .
Since c1 , c2 , and c3 are all occurrences of u then c3  c2 and c2  c1 are period lengths of
2  (i+1)(j+1)
|I |
|v |
 ji
u. Thus, u  min{c2  c1 , c3  c2}  c3c1
2 . Therefore, by
2 =
2 <
Lemma 5, u = v . Similarly, let (cid:48) >  and suppose there are three candidates c4 , c5 , c6 inC (I , (cid:48) ). Notice that it is possible that c1 , c2 and c3 are not in C (I , (cid:48) ) since it is possible that
enough time has passed for them to leave. Suppose c4 , c5 and c6 share the same entrance prex
u(cid:48) . Then u(cid:48) = v = u .
Assume by contradiction that u(cid:48) (cid:54)= u. Notice that the only possible locations of mismatches
between u and u(cid:48) are the positions of wildcards in the i length prex of P , since both u and
u(cid:48) match this prex. In particular, v occurs in the rth location of both u and u(cid:48) . Let k be an
index of a mismatch between u and u(cid:48) . In particular, let the k th character of u be x, and the
k th character of u(cid:48) be x(cid:48) (cid:54)= x. Let  be an integer (possibly negative) such that the k +   v
location in u is within the occurrence of v in u (and so also within the occurrence of v in u(cid:48) ).
Notice that such a  must exist since |v |  2v . Since u(cid:48) = v = u , the character at location
k +   v in u must be x, while the character at location k +   v in u(cid:48) must be x(cid:48) . But u and
u(cid:48) match at all of the locations corresponding to v . Thus we have obtained a contradiction, and
so u = u(cid:48) is unique, as required.

12

"#T$	$'||'&'||'candidate fingerprint"$*#entrance fingerprint$$'||'&'||'*#"$'||'&'||'*#	$'||'&'||'*#$*#Lemma 12. Suppose I is a partitioning that satises the properties by Lemma 9. For a pattern
interval I = [i, j ]  I and time   0 if there are h  3 candidates c1 < c2 <    < ch in
C (I , ) that have uI as their entrance prex, then the sequence c1 , c2 , . . . , ch forms an arithmetic
progression whose dierence is uI .
Proof. The distance between any two candidates in C (I , ) is at most |I |, and |I |  i by
Property 3 of Lemma 9. Hence, by Lemma 3, all of the occurrences of uI in T that begin
in text interval(I , ) form an arithmetic progression with dierence uI . Each of these
occurrences matches the i len');
INSERT INTO posts (postId,userId,title,body) VALUES (132,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 13)','gth prex of P , and therefore is a candidate in C (I , ). Hence, all
the candidates of C (I , ) with uI as their entrance prex form an arithmetic progression with
dierence of uI .

Implementation details. For any pattern interval I = [i, j ] and time  we split the set of
candidates C (I , ) into two disjoint sets. The set Cap (I , ) = {c  C (I , ) | tc . . . tc+i1 = uI }
contains all the candidates whose entrance prex is uI , and the set Cap (I , ) = C (I , ) \ Cap (I , )
contains all the other candidates of C (I , ). We use a linked list LQI to store all of the candidates
of Cap (I , ) together with their SI. Adding and removing a candidate that belongs in LQI
together with its SI is straightforward. The candidates of Cap (I , ) are maintained using a
separate data structure that leverages Lemmas 11 and 12. Thus, during a Dequeue() operation,
the queue veries if the candidate to be returned is in LQI or in the separate data structure
for the Cap (I , ) candidates. Finally, for each pattern interval I the data structure stores the
ngerprint of the the principle period of uI .

Lemma 13. There exists an implementation of candidate-ngerprint-queues such that the queue
QI at time  > 0 maintains al l the candidates of Cap (I , ) and their SI using O(1) words of
space.
Proof. If |Cap (I , )|  2 then QI stores the candidates of Cap (I , ) explicitly in O(1) words of
space. Otherwise, by Lemma 12, all the candidates of Cap (I , ) form an arithmetic progression.
An arithmetic progression of arbitrary length can be represented using O(1) words of space.
However, QI also needs access to the SI for the candidates in this progression. To do this, QI
explicitly stores the rst candidate (min Cap (I , )) together with its SI, the common dierence of
the progression (uI ), the length of the current progression, and the ngerprint of the principle
period of uI . When a new candidate c with entrance ngerprint (uI ) enters QI , c becomes the
largest element in Cap (I , ), and so we rst increment the length of the arithmetic progression,
and if c is currently the only candidate in the arithmetic progression, then QI stores c and
its SI (since then c is the rst candidate in the progression). When a Dequeue() operation
needs to remove the rst candidate c in the progression, then QI removes c, which is stored
explicitly together with its SI, decrements the length of the progression, and if there are remaining
candidates in the progression then QI computes the information for the new rst remaining
candidate in order to store its information explicitly. To do this, QI rst computes the location
of the new rst candidate from uI and the location of c. The SI of the new rst candidate is
computed in constant time (via the sliding property) from the ngerprint of the principle period
of uI and the candidate ngerprint of c.

Space usage. The space usage of all of the queues has three components. The rst component
is the lists LQI , which maintains the candidates of Cap (I , ) for all the intervals I . The second
component is the data structures for storing the candidates with entrance prex uI (the candidates
of Cap (I , )) in each I  I . Since, by Lemma 13, for each I  I all the candidates with entrance
prex uI are maintained using O(1) words, all such candidates use O(|I |) = O(d + log m) words

13

of space. The third co');
INSERT INTO posts (postId,userId,title,body) VALUES (133,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 14)','mponent is storing for each pattern interval I the ngerprint of the the
principle period of uI , which takes a total of O(d + log m) words of space. In the following
(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12) = O(d log m).
Lemma 14. (cid:80)
lemma we prove that the total space usage of all of the lists LQI is O(d log m).
I I
Proof. By Lemma 9, we know that |{(0), . . . , (k)}| = O(log m). For each (cid:96)  {(0), . . . , (k)}
(cid:80)
let I(cid:96)  I be the sequence of all pattern intervals I  I such that (I ) = (cid:96). We show that
(cid:80)
|Cap (I , )| = O(|I(cid:96) | + d). Combining with property 4 of Lemma 9 which states that
I I(cid:96)
(cid:96){(0),...,(k)} |I(cid:96) | = |I | = O(d + log m) we have that:
(cid:88)
(cid:88)
(cid:88)
(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)
(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12) =
(cid:88)
I I(cid:96)
I I
(cid:96){(0),...,(k)}
O(|I(cid:96) | + d)
(cid:88)(cid:96){(0),...,(k)}
(cid:96){(0),...,(k)}

(cid:88)
(cid:96){(0),...,(k)}

O(d)

O(|I(cid:96) |) +

= O(d + log m) + O(d log m)
We focus on intervals for which (cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)  3, since if (cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)  2 the bound is
= O(d log m)
straightforward.
Let [i , j  ] be the leftmost interval in I(cid:96) . By denition of I(cid:96) , we have j   i + 1 = (cid:96), and
so by Property 3 of Lemma 9, there exists a string v of length (cid:96) containing only non-wildcard
characters that is a substring of the length i prex of P . Let r be an arbitrary location of v
in p0 . . . pi1 (since v could appear several times in the prex). For any [i(cid:48) , j (cid:48) ] = I (cid:48)  I(cid:96) the
entrance prex (which does not contain wildcards) of each candidate in C (I (cid:48) , ) matches the i(cid:48)
prex of P (which can contain wildcards), and in particular, the location which is r locations to
the left of any candidate in C (I (cid:48) , ) is a location of an occurrence of v in the text6 .
Since we focus on intervals I  I(cid:96) for which |Cap (I , )|  3, then there exist three occurrences
of v in the text in positions corresponding to a shift of r characters from locations of I s
candidates. These occurrences are within a substring of the text of length at most 2|v |, since
all three candidates are in C (I , ) and so the distance between the rst and the last candidates
is at most |I |  1  (cid:96)  1 = |v |  1. Thus, by Lemma 4, v must be periodic, and the distance
(cid:104)(cid:83)
(cid:105)
between any two candidates in C (I , ) must be a multiple of v .
C (I , )
I I(cid:96)
Let c = max
be the rightmost (largest index) candidate in the intervals
corresponding to pattern intervals in I(cid:96) . Since c is a candidate in some C (I (cid:48) , ) for I (cid:48)  I(cid:96) ,
then there is an occurrence of v at location c + r. Thus, tc+r . . . tc+r+(cid:96)1 = v . We extend this
occurrence of v to the left and to the right in T for as long as the length of the period does not
increase. Let the resulting substring be tL+1 . . . tR1 . See Figure 8. If L  0 then the index L is
called the left violation of v . Similarly, if R   then the index R is called the right violation of
v . Notice that the period of v extends all the way to the beginning of the text if and only if
L = 1, in which case there is no left violation. Similarly, the period of v extends all the way to
the current end of the text if and only if R =  + 1, in which case there is no right violation.
Finally, notice that L < c + r  c + r + (cid:96)  1 < R, since v is a substring of tL+1 . . . tR1 .
6Notice that this occurrence is well dened since i(cid:48)  i  r + |v |.

14

Figure 8: Positions L and R are the violations of the periodic substring that contains v . ');
INSERT INTO posts (postId,userId,title,body) VALUES (134,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 15)','Notice
that it is possible that L  c, and similarly it is possible that R is in the entrance interval of c.

For a candidate c  text interval([i, j ], ) we dene the entrance interval of c to be
[c, c + i  1]. In addition we denote ec = c + i  1, so the entrance interval of c is [c, ec ].
Claim 1. For any candidate c  C (I , ) where I  I(cid:96) we have c + r  [c, ec ].
Proof. Let c be a candidate in C (I , ) for I = [i, j ]  I(cid:96) . Recall that C (I , )  text interval(I , ) =
[  j + 1,   i + 1]. Since c is in this interval we have that   j + 1  c    i + 1. In
particular, ec = c + i  1    j + 1 + i  1 =   (j  i + 1) + 1 =   |I | + 1. By denition,
since I  I(cid:96) , we have that |I |  (cid:96) and so ec    (cid:96) + 1. Since tc+r . . . tc+r+(cid:96)1 = v , it must be
that c + r + (cid:96)  1  . Thus, c + r    (cid:96) + 1  ec . By the maximality of c, it is obvious that
c  c  c + r. Hence, we have that c  c + r  ec .
Claim 2. Suppose I = [i, j ]  I(cid:96) and |Cap (I , )|  3. Then for any candidate c  Cap (I , ) in
text interval(I , ) either L  [c, ec ] or R  [c, ec ].
Proof. For c  Cap (I , ) let u = u0 . . . ui1 be the entrance prex of c. Recall that L < c + r < R.
By Claim 1 it must be that c  c + r  ec and so we cannot have both L, R < c or both L, R > ec .
Assume by contradiction that L < c  ec < R. We claim that there exists a text input T (cid:48)
such that if we execute the algorithm with T (cid:48) as the text, then there exists some time  where
C (I ,  ) contains three candidates with u as their entrance prex. Then, by Lemma 11 we deduce
that u = uI , in contradiction to the denition of Cap (I , ).
Recall that the principle period length of tL+1 . . . tR1 is v . Since u = tc . . . tec is a substring
of tL+1 . . . tR1 , it must be that u  v . Recall that Cap (I , ) contains at least three candidates.
Let c1 , c2 , and c3 be three distinct candidates in Cap (I , ). Since c1 , c2 , and c3 are all occurrences
of u then c3  c2 and c2  c1 are period lengths of u. Thus, u  min{c2  c1 , c3  c2}  c3c1
2 
|v |
|I |
2 . Therefore, by Lemma 5, u = v . Thus, u  ji
ji
implying that i + 2u  j  0.
2 <
2 =
Consider a long enough (at least i + 2u  1) text T (cid:48) which is composed of repeatedconcatenation of u0 . . . uu1 . Notice that the substrings of T (cid:48) of length i starting at locations 0,
u and 2u are all exactly the string u, which matches p0 . . . pi1 . Consider an execution of the
algorithm with T (cid:48) as the input text, and at time  = i + 2u  1 consider the set C (I ,  ). We
have that text interval(I ,  ) = [i + 2u  1  j + 1, i + 2u  1  i + 1] = [i + 2u  j, 2u ].
Being that i + 2u  j  0 then the interval [0, 2u ] is a subinterval of text interval(I ,  ),
then 0, u and 2u are all within this interval. Thus, these locations are candidates in C (I ,  )

15

Ta+babcbabds entrance prefixViolations of ++1with u as their entrance prex. Thus, by Lemma 11, it must be that u = uI , which contradicts
c  Cap (I , ).
Let C left
ap (I , ) be the set of candidates in Cap (I , ) whose entrance interval contains L, and
(I , ) be the set of candidates in Cap (I , ) whose entrance interval contains R. C left
let C right
ap (I , )
ap
and C right
ap (I , )  C right
(I , ) are not necessarily disjoint. Notice that by Claim 2, C left
(cid:12)(cid:12)(cid:12)C right
(cid:12)(cid:12)(cid:12)C left
(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d).
(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d) and (cid:80)
');
INSERT INTO posts (postId,userId,title,body) VALUES (135,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 16)','(I , )
Claim 3. (cid:80)
ap
ap
contains all the candidates of Cap (I , ).
I I(cid:96)
I I(cid:96)
(I , )
ap (I , )
ap
(cid:12)(cid:12)(cid:12)C left
(cid:12)(cid:12)(cid:12) from all sets C left
Notice that the contribution to (cid:80)
Proof. Let I  I(cid:96) and let  denote the match relation between symbols in   {?}.
I I(cid:96)
ap (I , )
ap (I , ) that have less than
two candidates is at most O(|I(cid:96) |). Thus, we will prove that for any set C left
ap (I , ) with at least two
candidates, it must be that for any candidate c  C left
ap (I , ), except for possibly one candidate,
we have that pLc is a wildcard.
ap (I , ) contains at least two candidates and let cleft = max C left
Suppose C left
ap (I , ) be the most
recent candidate in C left
ap (I , ). Let c < cleft be a candidate in C left
ap (I , ). Since c  C left
ap (I , )
we have that pLc  tc+Lc = tL (recall that both L and c are indices in the text). Similarly,
since cleft  C left
ap (I , ) we have that pLc  tcleft+Lc = tL+(cleftc) . Recall that the distance
between any two candidates in C (I , ) is a multiple of v , since C (I , ) contains at least 3
candidates. In particular the distance (cleft  c) is a multiple of v and (cleft  c)  |I |  |v |.
Thus, tL (cid:54)= tL+(cleftc) since L violates the period of length v . Recall that tL  pLc  tL+(cleftc) ,
and so pLc must be a wildcard. Therefore, each c  C left
ap (I (cid:48) , ) for all I (cid:48)  I(cid:96) , then the contribution to (cid:80)
ap (I , ), except for possibly cleft , is in
a position c such that pLc is a wildcard. Since L is the same for all of the candidates in all
|C left
of the C left
ap (I , )| of the candidates
(cid:12)(cid:12)(cid:12)C left
(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d). The proof that
I I(cid:96)
recent candidates is at most O(|I(cid:96) |). Thus, (cid:80)
that are not the most recent in their set C left
ap (I , ) is at most d. The contribution of the most
(cid:80)
ap (I (cid:48) , )
I (cid:48)I(cid:96)
(cid:12)(cid:12)(cid:12)C right
(cid:12)(cid:12)(cid:12)C left
(cid:12)(cid:12)(cid:12) = O(|I(cid:96) | + d). Thus, we
(cid:12)(cid:12)(cid:12) + (cid:80)
(I , )| = O(|I(cid:96) | + d) is symmetric.
|C right
(cid:12)(cid:12)Cap (I , )(cid:12)(cid:12)  (cid:80)
Finally, (cid:80)
I (cid:48)I(cid:96)
ap
I I(cid:96)
I I(cid:96)
I I(cid:96)
(I , )
ap (I , )
ap
have completed the proof of Lemma 14.

6 The Algorithm of Theorem 2

The algorithm of Theorem 1 for PMDW uses O(d) time per character and O(d) words of space.
In this section we introduce the algorithm of Theorem 2 which extends this result for a parameter
0    1 to an algorithm that uses O(d1 ) time per character and O(d1+ ) words of space.
An overview of a slightly modied version (for the sake of intuition) of the tradeo algorithm
is described as follows. Let P  be the longest prex of P such that P   d . The tradeo
algorithm rst nds all the occurrences of P  in T using a specialized algorithm for patterns
with bounded wildcard-period length. If P  = P then this completes the tradeo algorithm.
Otherwise, let I = [i, j ] be the interval in the secondary partitioning of Theorem 1 such that
i  |P  |  1  j . We rst divide I into two new intervals [i, |P  |  1] and [|P  |, j ]. If [|P  |, j ] = 
then we discard [|P  |, j ]. It is straightforward to see that the properties of partitions that we

16

(a) The matrix M q

(b) The oset patterns and q

(c) The column pattern
Pq

Figure 9: Example of the matrix representation for pattern P = abcab?abcabcabcabcabc and
q = 5. Each color represents a unique oset pattern. The oset patterns P5,1 and P5,4 are equal
and therefore they have the same id (column color). Since P5,3 contains a wildcard, it is not
associated with any id.

dene in Lemma 9 are still satised. Let I  = [i = |P  |, j  ] be the interval immediately following
[i, |P  |  1]. Each occurrence of P  in the text is inserted into the algorithm of Theorem 1 as
a candidate directly into QI  . Thus, the entrance prexes of ca');
INSERT INTO posts (postId,userId,title,body) VALUES (136,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 17)','ndidates in the queues match
prexes of P that are longer than P  and, by maximality of P  , these prexes of P have
large wildcard-period length. This implies that the average distance between two consecutive
candidates that are occurrences of P  is at least d , and so, combined with a carefully designed
scheduling approach for verifying candidates, we are able to obtain an O(d1 ) amortized time
cost per character.

In Section 6.1 we describe the specialized algorithm for dealing with patterns
Overview.
whose wildcard-period length is at most  , for some parameter  > 1. In Section 6.2 we complete
the proof of Theorem 2 by describing the missing details for the tradeo algorithm. In particular,
the proof of Theorem 2 uses the algorithm of Section 6.1 with  = d .

6.1 Patterns with Small Wildcard-period Length

Let P be a pattern of length m with d wildcards such that P <  . Let q be an integer,
which for simplicity is assumed to divide m (see Appendix A.1 where we discuss how to get
rid of this assumption). Consider the conceptual matrix M q = {mq
x,y } of size m
q  q where
x,y = p(x1)q+y1 . An example is given in Figure 9. For any integer 0  r < q the r th column
mq
of M q corresponds to an oset pattern Pq ,r = pr pr+q pr+2q . . . pmq+r . Notice that some oset
patterns might be equal. Let q = {Pq ,r | 0  r < q and (cid:48)?(cid:48) / Pq ,r } be the set of all the oset
patterns that do not contain any wildcards. Each oset pattern in q is given a unique id. The
set of unique ids is denoted by IDq . We say that index i in P is covered by q if the column
containing pi does not contain a wildcard, and so Pq ,i mod q  q . The columns of M q dene a

17

abcabcbcab?acabcababcacabP5,0= acbacP5,1= P5,4= bacbaP5,2= cbacbP5,3= a?bac"={acbac, bacba, cbacb}?P5,1P5,0P5,1P5,2column pattern Pq of length q , where the j th character is the id of the Pq ,j column, or (cid:48)?(cid:48) if
Pq ,j / q (since Pq ,j contains wildcards).
We partition T into q oset texts, where for every 0  r < q we dene Tq ,r = tr tr+q tr+2q . . . .
Using the dictionary matching streaming (DMS) algorithm of Cliord et al. [14] we look for
occurrences of oset patterns from q in each of the oset texts. We emphasize that we do not
only nd occurrences of Pq ,r in Tq ,r , since we cannot guarantee that the oset of T synchronizes
with an occurrence of P . When the character t arrives, the algorithm passes t to the DMS
algorithm for Tq , mod q . We also create a streaming column text Tq whose characters correspond
to the ids of oset patterns as follows. If one of the oset patterns is found in Tq , mod q , then its
id is the th character in Tq . Otherwise, we use a dummy character for the th character in Tq .

Full cover. Notice that an occurrence of P in T necessarily creates an occurrence of Pq in
Tq . Such occurrences are found via the black box algorithm of Cliord et al. [11]. However, an
occurrence of Pq in Tq does not necessarily mean there was an occurrence of P in T , since some
characters in P are not covered by q . In order to avoid such false positives we run the process in
parallel with several choices of q , while guaranteeing that each non wildcard character in P is
covered by at least one of those choices. Thus, if there is an occurrence of Pq at location i in Tq
for all the choices of q , then it must be that P appears in T at location i. The choices of q are
given by the following lemma.

Lemma 15. There exists a set Q of O(log d) prime numbers such that any index of a non-
wildcard character in P is covered by at least one prime number q  Q, and each number in Q is
at most O(d).

Proof. The proof uses the probabilistic method: we show ');
INSERT INTO posts (postId,userId,title,body) VALUES (137,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 18)','that the probability that the set
Q exists is strictly larger than 0. Since our proof is constructive it provides a randomized
construction of Q.
It is well known that for a prime number q , every integer 0  z < q denes a congruence
class which contains all integers i such that i mod q = z . For any two distinct natural numbers
x, y  N, let Dx,y be the set of prime numbers q such that x and y are in the same congruence
class modulo q (i.e. x mod q = y mod q). Notice that in the interpretation of the pattern columns
in the conceptual matrix, if q  Dx,y then px and py are in the same column of the conceptual
matrix M q . Recall that W is the set of occurrences of wildcards in P . Thus, if 0  j < m is an
q > (cid:81)
the Chinese remainder theorem, |Dj,w | < log m (otherwise for  = (cid:81)
index such that j / W and if w  W such that q  Dj,w , then j is surely not covered by q . By
2  m,
For any 0  j < m such that j / W , let Dj = (cid:83)
wW Dj,w , so |Dj |  (cid:80)
qDj,w
qDj,w
and so j mod  = w mod  implying that j = w).
wW |Dj,w | <
|W | log m = d log m. If 2d  m
then the proof is trivialized by choosing Q to contain only
log2 m
the smallest prime number which is at least m. If 2d > m
, by Corollary 1 in [3], then there
log2 m
are at least 2d log m prime numbers whose value are upper bounded by 2d log2 m. Let Q be the
set of those prime numbers. For a random q  Q, the probability that a specic non-wildcard
|Dj |
| Q|  d log m
2d log m = 1
pattern index j is not covered by q is at most
2 . Let Q be a set of 2 log m
randomly chosen prime numbers from Q. The probability that a specic non-wildcard pattern
22 log m  1index j is not covered by any of the prime numbers in Q is less than
m2 . Thus, the
probability that there exists a non-wildcard pattern index j which is not covered by any of the
m2  1
prime numbers in Q is less than md
m . Therefore, there must exist a set Q that covers all of
the indices of non-wildcard characters from P .
From a space usage perspective, we need the size of |q | to be small, since this directly aects
the space usage of the DMS algorithm which uses O(k) space, where k is the number of patterns

18

Figure 10: For string S = s0 . . . s34 for pattern of length m = 18, S7 is marked by the blue
rectangle and the green indices are the characters of S7,3,1 . Notice that S7,3,1 = S8,3,0 .

in the dictionary. In our case k = |q |. In order to bound the size of q we use the following
lemma.
Lemma 16. If P   then for any q  N we have |q | = O( ).
Proof. Since P   , there exists a string S = s0 . . . s2m2 with no wildcards that contains ( m
 )
occurrences of P . Using the string S we show that |q | = O( ).
For each id in IDq we pick an index of a representative column in Mq that has this id, and
denote this set by Rq . Let r1 be the minimum index in Rq . For every index 0  i < m let
Si = si . . . si+m1 (see Figure 10). For every 0  r < q let Si,q ,r = si+r si+r+q . . . si+mq+r , and
so for any integer 0   < q  r we have Si,q ,r+ = Si+,q ,r . Notice that if Si matches P then
Pq ,r = Si,q ,r for each r  Rq .
Let i be an index of an occurrence of P in S . For any distinct r, r (cid:48)  Rq , it must be
that Si,q ,r = Pq ,r (cid:54)= Pq ,r (cid:48) = Si,q ,r (cid:48) . In particular, for any r  Rq such that r > r1 , we have
Pq');
INSERT INTO posts (postId,userId,title,body) VALUES (138,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 19)',' ,r1 = Si,q ,r1 (cid:54)= Si,q ,r = Si+rr1 ,q ,r1 . This implies that i + r  r1 cannot be an occurrence of P .
Hence, every occurrence of P in S eliminates |Rq |  1 locations in S from being an occurrence
of P . We now show that the sets of eliminated locations dened by distinct occurrences are
disjoint. Assume without loss of generality that S contains at least two occurrences. Let
i1 and i2 be two distinct occurrences of P in S , and assume by contradiction that an index
j is eliminated by both of these occurrences. Since si1 . . . si1+m1 matches P , we have that
Si1 ,q ,ji1 = Pji1 and j  i1  Rq . Similarly, we have that Si2 ,q ,ji2 = Pji2 and j  i2  Rq .
Being that Si1 ,q ,ji1 = Si2 ,q ,ji2 we have that Pji2 = Pji1 , contradicting the denition of Rq .
|S |
|Rq | = 2m1
Therefore, the maximum number of occurrences of P in S is at most
|Rq | . Since S
  2m1
|Rq | which implies that |q | = |Rq | 
contains at least m
 instances of P , it must be that m
2 = O( ).

Complexities. For a single q  Q, the algorithm creates q = O(d) oset patterns and texts.
For each such oset text the algorithm applies an instance of the DMS algorithm with a dictionary
of O( ) strings (by Lemma 16). Since each instance of the DMS algorithm uses O( ) words of
space [14], the total space usage for all instances of the DMS algorithm is O(d ) words. Moreover,
the time per character in each DMS algorithm is O(1) time, and each time a character appears
we inject it into only one of the DMS algorithms (for this specic q). In addition, the algorithm
uses an instance of the black box algorithm for Tq , with a pattern of length q . This uses another
O(q) = O(d) space and another O(1) time per character [11]. Thus the total space usage due to
one element in Q is O(d ) words. Since |Q| = O(log d) the total space usage for all elements in
Q is O(d ) words, and the total time per arriving character is O(1). Thus we have proven the
following.
Theorem 17. For any   1, there exists a randomized Monte Carlo algorithm for PMDW on
patterns P with P <  in the streaming model, which succeeds with probability 1  1/poly(n),
uses O(d ) words of space and spends O(1) time per arriving text character.

19

S70ss1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16s17s18s19s20s21s22s23s24s25s26s27s28s29s30s31S7s32s33s346.2 Proof of Theorem 2

In this section we combine the algorithm of Theorem 1 with the algorithm of Theorem 17
and introduce an algorithm for patterns with general wildcard-period length, thereby proving
Theorem 2.
Prior to Section 6.1 we presented an almost accurate description of the algorithm. The only
two parts of the description that require elaboration are regarding how to insert occurrences of
P  into the appropriate candidate-ngerprint-queue eciently, and how to schedule validations
of candidates so that the amortized cost is low. We rst focus on how to insert candidates and
later we discuss the scheduling.

Direct insertion of candidates. The challenge with inserting occurrences of P  into QI 
is that the candidate-ngerprint-queue data structure uses the SI of candidates, and so the
straightforward ways for providing this information together with the new candidates (which are
occurrences of P  ) cost either too much time or too much space. In order to meet our desired
complexities, we rst investigate the purposes of dierent parts of SI.
The SI for a candidate c in C (I = [i, j ], ) consists of the candidate ngerprint, (t0 . . . tc1 ),
and the entrance ngerprint, (tc . . . tc+i1 ). The SI has two purposes. The rst is to validate a
candidate after a Dequeue() operation, in which case the algorithm makes use of both parts of
the SI in order to compute (tc+i . . . tc+j ) by combining the SI with the text ngerprint. The
second purpose is to compute the next entrance ngerprints of candidates in order to distinguish
between ');
INSERT INTO posts (postId,userId,title,body) VALUES (139,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 20)','candidates that are stored as part of an arithmetic progression and candidates that are
not. The entrance ngerprint is obtained, via the sliding property, from the candidate ngerprint
in the SI and the current text ngerprint.
Notice that in order to validate c the algorithm only needs the ngerprint of (t0 . . . tci+1 ).
Also notice that entrance prexes are only used for candidates that are at some point part of
a stored arithmetic progression. Thus, for a specially chosen subset of strings   |P  | we
precompute all of the ngerprints of strings in . The set  is chosen so that for any occurrence
of P  that is injected as a candidate c where c is at some point part of a stored arithmetic
progression, the occurrence of P  at location c is in . We use the DMS algorithm [14] to
locate strings from  in the text, and whenever such a string appears, we compute the SI
for the corresponding candidate in constant time from the stored ngerprint and the current
text ngerprint. We emphasize that not all of the candidates that correspond to strings in 
need to necessarily at some point be a part of an arithmetic progression. However, in order to
reduce the space usage, we require that  is not too large, and in particular || = O(d + log m).
For a candidate c that does not correspond to a string in , instead of maintain the SI of c,
we explicitly maintain the ngerprint of (t0 . . . tci+1 ) where c  C (I = [i, j ], ). Notice that
whenever such a candidate enters a new text interval, the text ngerprint at that time is exactly
the information which we need to store.

Creating . Consider all pattern intervals I = [i, j ]  I with i  i . Notice that there are at
most O(d + log m) such pattern intervals. For each such interval I , let I be the prex of uI
of length |P  |. Since, by Lemma 11, a candidate c  Cap (I , ) implies an occurrence of uI at
location c, then I also appears at location c. Thus, we dene  to be the set containing I for
all such pattern intervals I . Since any candidate in an arithmetic progression at time  must be
in Cap (I , ) for some interval I , it is guaranteed that when c corresponded to an occurrence of
P  , that occurrence must have been I , and so  has the required properties.

20

Scheduling validations. Since the only bound we have proven on the number of pattern
intervals I = [i, j ]  I with i  i is O(d + log m), if each time a new text character arrives we
perform a Dequeue() operation for each one of the pattern intervals, then the time cost can be
as large as O(d + log m) which is too much. The solution for reducing this time cost is to only
perform a Dequeue() operation on QI when a candidate c actually leaves text interval(I , )
and needs to be validated. This is implemented by maintaining a priority queue on top of
the pattern intervals, where the keys that are used are the next time a candidate exits the
corresponding text interval. Each time a candidate leaves a text interval, the key for the queue
of that interval is updated to the time the next candidate leaves (if such a candidate exists).
When a candidate entering a text interval is the only candidate of that text interval, then the
key for the queue of this text interval is also updated.

Complexities. Recall that I  = ');
INSERT INTO posts (postId,userId,title,body) VALUES (140,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 21)','[i , j  ] is a pattern interval such that i = |P  |, and that each
time the algorithm nds an occurrence of P  , the corresponding candidate is inserted into QI  .
Let P (cid:48) be the prex of P of length j  + 1. By maximality of P  , it must be that P (cid:48) > d . We
partition the time usage of the algorithm into three parts. The rst is the amount of time spent
on nding occurrences of P  using the algorithm of Theorem 17, which is O(1). The second is
the amount of time spent performing Enqueue() and Dequeue() operations on QI  , which is also
O(1) since we perform O(1) operations on this queue per each arriving character. The third
is the amount of time spent on Enqueue() and Dequeue() operations on QI for I = [i, j ] with
i > j  . These operations only apply to candidates that are occurrences of P (cid:48) . For this part we
(cid:109)
(cid:108)
use amortized analysis.
By denition of wildcard-period length, for any string S of size 2|P (cid:48) |  1, we have d < P (cid:48) 
. Being that occ(S, P (cid:48) )  |P (cid:48) |, we have d < 2|P (cid:48) |
|P (cid:48) |
occ(S,P (cid:48) ) . Notice that for a text T of size
occ(S,P (cid:48) )
n  |P (cid:48) |, we must have occ(T , P (cid:48) ) < 2n
d . This is because otherwise, if n  2|P (cid:48) |  1 then there
exists a substring of n of length 2|P (cid:48) |  1 with at least 2|P (cid:48) |
d occurrences of P (cid:48) , and if n < 2|P (cid:48) |  1
then we can pad T to create such a string. In both cases we contradict d < 2|P (cid:48) |
occ(S,P (cid:48) ) for any
string S of length 2|P (cid:48) |  1.
The total amount of time spent on each occurrence of P (cid:48) is O(d), and so the total cost
for processing T on candidates that are also occurrences of P (cid:48) is at most O(occ(T , P (cid:48) )  d) =
d d) = O(n  d1 ). Thus, the amortized cost per character is O(d1 ).
O( 2n
For the space complexity, the most expensive part is the use of the algorithm of Theorem 17
which takes O(d  d ) = O(d1+ ) words of space. This completes the proof of Theorem 2.

References

[1] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the
frequency moments. J. Comput. Syst. Sci., 58(1):137147, 1999.

[2] Amihood Amir, Moshe Lewenstein, and Ely Porat. Faster algorithms for string matching
with k mismatches. J. Algorithms, 50(2):257275, 2004.

[3] J Barkley Rosser and Lowell Schoenfeld. Approximate formulas for some functions of prime
numbers. Il linois J. Math, 6:6494, 1962.

[4] Jean Berstel and Luc Boasson. Partial words and a theorem of ne and wilf. Theor. Comput.
Sci., 218(1):135141, 1999.

21

[5] Francine Blanchet-Sadri. Algorithmic Combinatorics on Words. Discrete mathematics and
its applications. CRC Press, 2008.

[6] Francine Blanchet-Sadri and Robert A. Hegstrom. Partial words and a theorem of ne and
wilf revisited. Theor. Comput. Sci., 270(1-2):401419, 2002.

[7] Dany Breslauer and Zvi Galil. Real-time streaming string-matching. ACM Transactions on
Algorithms, 10(4):22:122:12, 2014.

[8] Dany Breslauer, Roberto Grossi, and Filippo Mignosi. Simple real-time constant-space
string matching. Theor. Comput. Sci., 483:29, 2013.

[9] Sabin Cautis, Filippo Mignosi, Jerey Shallit, Ming-wei Wang, and Soroosh Yazdani.
Periodicity, morphisms, and matrices. Theor. Comput. Sci., 295:107121, 2003.

[10] Peter Cliord and Raphael Cliord. Simple deterministic wildcard matching. Inf. Process.
Lett., 101(2):5354, 2007.

[11] Raphael Cliord, Klim Efremenko, Benny Porat, and Ely Porat. A black box for online
approximate pattern m');
INSERT INTO posts (postId,userId,title,body) VALUES (141,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 22)','atching. Inf. Comput., 209(4):731736, 2011.

[12] Raphael Cliord, Klim Efremenko, Ely Porat, and Amir Rothschild. From coding theory to
ecient pattern matching. In Proceedings of the Twentieth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA, pages 778784, 2009.

[13] Raphael Cliord, Klim Efremenko, Ely Porat, and Amir Rothschild. Pattern matching with
dont cares and few errors. J. Comput. Syst. Sci., 76(2):115124, 2010.

[14] Raphael Cliord, Allyx Fontaine, Ely Porat, Benjamin Sach, and Tatiana A. Starikovskaya.
Dictionary matching in a stream. In Proceedings of Algorithms - ESA 2015 - 23rd Annual
European Symposium, Patras, volume 9294 of Lecture Notes in Computer Science, pages
361372. Springer, 2015.

[15] Raphael Cliord, Allyx Fontaine, Ely Porat, Benjamin Sach, and Tatiana A. Starikovskaya.
The k -mismatch problem revisited. In Proceedings of the Twenty-Seventh Annual ACM-
SIAM Symposium on Discrete Algorithms, SODA, pages 20392052, 2016.

[16] Raphael Cliord, Markus Jalsenius, Ely Porat, and Benjamin Sach. Space lower bounds for
online pattern matching. Theor. Comput. Sci., 483:6874, 2013.

[17] Raphael Cliord and Ely Porat. A ltering algorithm for k -mismatch with dont cares. In
Proceedings of String Processing and Information Retrieval, 14th International Symposium,
SPIRE, pages 130136, 2007.

[18] Raphael Cliord and Benjamin Sach. Pseudo-realtime pattern matching: Closing the gap.
In Proceedings of Combinatorial Pattern Matching, 21st Annual Symposium, CPM 2010,
pages 101111, 2010.

[19] Richard Cole and Ramesh Hariharan. Verifying candidate matches in sparse and wildcard
matching. In Proceedings on 34th Annual ACM Symposium on Theory of Computing, pages
592601, 2002.

[20] Funda Ergun, Hossein Jowhari, and Mert Saglam. Periodicity in streams. In Proceedings of
Approximation, Randomization, and Combinatorial Optimization. Algorithms and Tech-
niques, 13th International Workshop, APPROX 2010, and 14th International Workshop,
RANDOM 2010, pages 545559, 2010.

22

[21] Nathan J Fine and Herbert S Wilf. Uniqueness theorems for periodic functions. Proceedings
of the American Mathematical Society, 16(1):109114, 1965.

[22] Michael J Fischer and Michael S Paterson. String-matching and other products. Technical
report, DTIC Document, 1974.

[23] Zvi Galil and Joel I. Seiferas. Time-space-optimal string matching. J. Comput. Syst. Sci.,
26(3):280294, 1983.

[24] Pawel Gawrychowski. Optimal pattern matching in LZW compressed strings. ACM
Transactions on Algorithms, 9(3):25, 2013.

[25] Monika Rauch Henzinger, Prabhakar Raghavan, and Sridar Ra jagopalan. External Memory
Algorithms, chapter Computing on data streams, pages 107118. American Mathematical
Society, Boston, USA, 1999.

[26] Piotr Indyk. Faster algorithms for string matching problems: Matching the convolution
bound. In Proceedings of 39th Annual Symposium on Foundations of Computer Science,
FOCS, pages 166173, 1998.

[27] Markus Jalsenius, Benny Porat, and Benjamin Sach. Parameterized matching in the
streaming model. In Proceedings of 30th International Symposium on Theoretical Aspects
of Computer Science, STACS, pages 400411, 2013.

[28] Adam Kalai. Ecient pattern-matching with dont cares. In Proceedings of the Thirteenth
Annual ACM-SIAM Symposium on Discrete Algorithms SODA, pages 655656. ACM/SIAM,
2002.

[29] Daniel M. Kane, Jelani Nelson, Ely Porat, and David P. Woodru. Fast moment estimation
in data streams in optimal space. In Proceedings of the 43rd ACM Symposium on Theory
of Computing, STOC, pages 745754, 2011.

[30] Richard M. Karp and Michael O. Rabin. Ecient randomized pattern-matching algorithms.
IBM Journal of Research and Development, 31(2):249260, 1987.

[31] Donald E. Knuth, James H. Morris Jr., and Vaughan R. Pratt. Fast pattern matching in
strings. SIAM J. Comput., 6(2):323350, 1977.

[32] Gad M. Lan');
INSERT INTO posts (postId,userId,title,body) VALUES (142,7527,'Shay Golan Bar Ilan University golansh1@cs.biu.ac.il (part 23)','dau and Uzi Vishkin. Ecient string matching with k mismatches. Theor.
Comput. Sci., 43:239249, 1986.

[33] Lap-Kei Lee, Moshe Lewenstein, and Qin Zhang. Parikh matching in the streaming model. In
Proceedings of String Processing and Information Retrieval - 19th International Symposium,
SPIRE, pages 336341, 2012.

[34] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in
Theoretical Computer Science, 1(2), 2005.

[35] S. Muthukrishnan and H. Ramesh. String matching under a general matching relation. In
Proceedings of Foundations of Software Technology and Theoretical Computer Science, 12th
Conference, volume 652 of Lecture Notes in Computer Science, pages 356367. Springer,
1992.

23

[36] Benny Porat and Ely Porat. Exact and approximate pattern matching in the streaming
model. In Proceedings of 50th Annual IEEE Symposium on Foundations of Computer
Science, FOCS, pages 315323, 2009.

[37] Ely Porat and Ohad Lipsky. Improved sketching of hamming distance with error correcting.
In Proceedings of Combinatorial Pattern Matching, 18th Annual Symposium, CPM, pages
173182, 2007.

[38] William F. Smyth and Shu Wang. A new approach to the periodicity lemma on strings
with holes. Theor. Comput. Sci., 410(43):42954302, 2009.

A Missing Details
(cid:108) m
(cid:106) m
(cid:107)
(cid:109)
A.1 Dealing with q (cid:45) m
If q (cid:45) m, then the strings in q have two possible lengths; either
. This implies that
orq
one string in q could be a proper sux of another string in q . So if the longer one appears in
an oset text, then both ids need to be given to Tq - a situation in which it is not clear what to
do. So to avoid such scenarios, for each q  Q we run the algorithm twice, in parallel, where
one instance uses the DMS algorithm for one length while the other instance uses the DMS
algorithm on the other length. This creates two instances of Pq and Tq , one for each length of
columns under consideration. Notice that in order for the algorithm to work, when considering
one specic length, all of the columns that correspond to the other length are treated as a (cid:48)?(cid:48) in
the appropriate instance of Pq .

24

');
INSERT INTO posts (postId,userId,title,body) VALUES (143,8763,'Enabling Smart Data: Noise ltering in Big Data classication','Diego Garca-Gila,, Julian Luengoa , Salvador Garcaa , Francisco Herreraa

aDepartment of Computer Science and Articial Intel ligence, University of Granada,
CITIC-UGR, Granada, Spain, 18071

Abstract

In any knowledge discovery process the value of extracted knowledge is directly
related to the quality of the data used. Big Data problems, generated by mas-
sive growth in the scale of data observed in recent years, also follow the same
dictate. A common problem aecting data quality is the presence of noise,
particularly in classication problems, where label noise refers to the incorrect
labeling of training instances, and is known to be a very disruptive feature of
data. However, in this Big Data era, the massive growth in the scale of the
data poses a challenge to traditional proposals created to tackle noise, as they
have diculties coping with such a large amount of data. New algorithms need
to be proposed to treat the noise in Big Data problems, providing high qual-
ity and clean data, also known as Smart Data. In this paper, two Big Data
preprocessing approaches to remove noisy examples are proposed: an homoge-
neous ensemble and an heterogeneous ensemble lter, with special emphasis in
their scalability and performance traits. The obtained results show that these
proposals enable the practitioner to eciently obtain a Smart Dataset from any
Big Data classication problem.

Keywords: Big Data, Smart Data, Classication, Class Noise, Label Noise.

1. Introduction

Vast amounts of information surround us today. Technologies such as the
Internet generate data at an exponential rate thanks to the aordability and
great development of storage and network resources.
It is predicted that by
2020, the digital universe will be 10 times as big as it was in 2013, totaling an
astonishing 44 zettabytes [22]. The current volume of data has exceeded the
processing capabilities of classical data mining systems [49] and have created
a need for new frameworks for storing and processing this data. It is widely

Corresponding author
Email addresses: djgarcia@decsai.ugr.es (Diego Garca-Gil),
julianlm@decsai.ugr.es (Julian Luengo), salvagl@decsai.ugr.es (Salvador Garca),
herrera@decsai.ugr.es (Francisco Herrera)

Preprint submitted to Journal of LATEX Templates

April 7, 2017

accepted that we have entered the Big Data era [30]. Big Data is the set of
technologies that make processing such large amounts of data possible [20],
while most of the classic knowledge extraction methods cannot work in a Big
Data environment because they were not conceived for it.
Big Data as concept is dened around ve aspects: data volume, data veloc-
ity, data variety, data veracity and data value [24]. While the volume, variety
and velocity aspects refer to the data generation process and how to capture
and store the data, veracity and value aspects deal with the quality and the
usefulness of the data. These two last aspects become crucial in any Data Min-
ing process, where the extraction of useful and valuable knowledge is strongly
inuenced by the quality of the used data. In Big Data, the usage of traditional
preprocessing techniques [15, 33, 17] to enhance the data is even more time
consuming and resource demanding, being unfeasible in most cases.
The lack of ecient and aordable preprocessing techniques implies that the
problems in the data will aect the mode');
INSERT INTO posts (postId,userId,title,body) VALUES (144,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 2)','ls extracted. Among all the problems
that may appear in the data, the presence of noise in the dataset is one of the
most frequent. Noise can be dened as the partial or complete alteration of the
information gathered for a data item, caused by an exogenous factor not related
to the distribution that generates the data. Learning from noisy data is an im-
portant topic in machine learning, data mining and pattern recognition, as real
world data sets may suer from imperfections in data acquisition, transmission,
storage, integration and categorization. Noise will lead to excessively complex
models with deteriorated performance [48], resulting in even larger computing
times for less value.
The impact of noise in Big Data, among other pernicious traits, has not been
disregarded. Recently, Smart Data (focusing on veracity and value) has been in-
troduced, aiming to lter out the noise and to highlight the valuable data, which
can be eectively used by companies and governments for planning, operation,
monitoring, control, and intelligent decision making. Three key attributes are
needed for data to be smart, it must be accurate, actionable and agile:
 Accurate: data must be what it says it is with enough precision to drive
value. Data quality matters.
 Actionable: data must drive an immediate scalable action in a way that
maximizes a business ob jective like media reach across platforms. Scalable
action matters.
 Agile: data must be available in real-time and ready to adapt to the
changing business environment. Flexibility matters.

Advanced Big Data modeling and analytics are indispensable for discov-
ering the underlying structure from retrieved data in order to acquire Smart
Data. In this paper we provide several preprocessing techniques for Big Data,
transforming raw, corrupted datasets into Smart Data. We focus our interest
on classication tasks, where two types of noise are distinguished: class noise,
when it aects the class label of the instances, and attribute noise, when it aects

the rest of attributes. The former is known to be the most disruptive [38, 53].
Consequently, many recent works, including this contribution, have been de-
voted to resolving this problem or at least to minimize its eects (see [14] for
a comprehensive and updated survey). While some architectural designs are
already proposed in the literature[51], there is no particular algorithm which
deals with noise in Big Data classication, nor a comparison of its eect on
model generalization abilities or computing times.
Thereby we propose a framework for Big Data under Apache Spark for
removing noisy examples composed of two algorithms based on ensembles of
classiers. The rst one is an homogeneous ensemble, named Homogeneous
Ensembe for Big Data (HME-BD), which uses a single base classier (Random
Forest [4]) over a partitioning of the training set. The second ensemble is an
heterogeneous ensemble, namely Heterogeneous Ensembe for BigData (HTE-
BD), that uses dierent classiers to identify noisy instances: Random Forest,
Logistic Regression and K-Neare');
INSERT INTO posts (postId,userId,title,body) VALUES (145,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 3)','st Neighbors (KNN) as base classiers. For the
sake of a more complete comparison, we have also considered a simple ltering
approach based on similarities between instances, named Edited Nearest Neigh-
bor for Big Data (ENN-BD). ENN-BD examines the nearest neighbors of every
example in the training set and eliminates those whose ma jority of neighbors
belong to a dierent class. All these techniques have been implemented under
the Apache Spark framework [19, 40] and can be downloaded from the Sparks
community repository 1 .
To show the performance of the three proposed algorithms, we have carried
out an experimental evaluation with four large datasets, namely SUSY, HIGGS,
Epsilon and ECBDL14. We have induced several levels of class noise to eval-
uate the eects of applying such framework and the improvements obtained in
terms of classication accuracy for two classiers: a decision tree and the KNN
technique. Decision trees with pruning are known to be tolerant to noise, while
KNN is a noise sensitive algorithm when the number of selected neighbors is
low. These dierences allow us to better compare the eect of the framework in
classiers which behave dierently towards noise. We also show that, for the Big
Data problems considered, the classiers also benet from applying the noise
treatment even when no additional noise is induced, since Big Data problems
contain implicit noise due to incidental homogeneity, spurious correlations and
the accumulation of noisy examples [11]. The results obtained indicate that
the framework proposed can successfully deal with noise. In particular, the ho-
mogeneous ensemble is a suitable technique for dealing with noise in Big Data
problems, with low computing times and enabling the classier to achieve better
accuracy.
The remainder of this paper is organized as follows: Section 2 presents the
concepts of noise, MapReduce and Smart Data. Section 3 explains the pro-
posed framework. Section 4 describes the experiments carried out to check the
performance of the framework. Finally, Section 5 concludes the paper.

1 https://spark- packages.org/package/djgarcia/NoiseFramework

2. Related work

In this section we rst present the problem of noise in classication tasks
in Section 2.1. Then we introduce the MapReduce framework commonly used
in Big Data solutions in Section 2.2. Finally, we provide an insight into Smart
Data in 2.3.

2.1. Class noise vs. attribute noise

In a classication problem, several eects of this noise can be observed by
analyzing its spatial characteristics: noise may create small clusters of instances
of a particular class in the instance space corresponding to another class, displace
or remove instances located in key areas within a concrete class, or disrupt the
boundaries of the classes resulting in an increased boundaries overlap. All these
imperfections may harm data interpretation, the design, size, building time,
interpretability and accuracy of models, as well as decision making [52, 53].
As described by Wang et al. [47], from the large number of components that
comprise a dataset, class labels and attribute values are two essential elements
in classication datas');
INSERT INTO posts (postId,userId,title,body) VALUES (146,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 4)','ets. Thus, two types of noise are commonly dierentiated
in the literature [53, 47]:
 Class noise, also known as label noise, takes place when an example is
wrongly labeled. Class noise includes contradictory examples [42, 38] (ex-
amples with identical input attribute values having dierent class labels)
and misclassications [53] (examples which are incorrectly labeled).
 Attribute noise refers to corruptions in the values of the input attributes.
It includes erroneous attribute values, missing values and incomplete at-
tributes or do not care values. Missing values are usually considered
independently in the literature, so attribute noise is mainly used for erro-
neous values [53].

Class noise is generally considered more harmful to the learning process, and
methods for dealing with class noise are more frequent in the literature [53].
Class noise may have many reasons, such as errors or sub jectivity in the data
labeling process, as well as the use of inadequate information for labeling. Data
labeling by domain experts is generally costly, and automatic taggers are used
(e.g., sentiment analysis polarization [29]), increasing the probability of class
noise.
Due to the increasing attention from researchers and practitioners, numerous
techniques have been developed to tackle it [14, 53, 15]. These techniques include
learning algorithms robust to noise as well as data preprocessing techniques that
remove or repair noisy instances. In [14] the mechanisms that generate label
noise are examined, relating them to the appropriate treatment procedures that
can be safely applied:
 On the one hand, algorithm level approaches attempt to create robust
classication algorithms that are little inuenced by the presence of noise.

This includes approaches where existing algorithms are modied to cope
with label noise by either being modeled in the classier construction [25,
27], by applying pruning strategies to avoid overting as in [34] or by di-
minishing the importance of noisy instances with respect to clean ones [32].
Recent proposals exist which that combine these two approaches, which
model the noise and give less relevance to potentially noisy instances in
the classier building process [3].
 On the other hand, data level approaches (also called lters ) try to develop
strategies to cleanse the dataset as a previous step to the t of the classier,
by either creating ensembles of classiers [5], iteratively ltering noisy
instances [23], computing metrics on the data or even hybrid approaches
that combine several of these strategies.

In the Big Data environment there is a special need for noise lter methods.
It is well known that the high dimensionality and example size generate accu-
mulated noise in Big Data problems [11]. Noise lters reduce the size of the
datasets and improve the quality of the data by removing noisy instances, but
most of the classic algorithms for noisy data, noise lters in particular, are not
prepared for working with huge volumes of data.

2.2. Big Data. MapReduce and Apache Spark

The globalization of the Big Data paradigm is generating a large response in
terms of technologies that must deal with the rapidly growing rates of generated
data [39]. Among all of them, MapReduce is the seminal framework designed
by Google in 2003 [8]. It follows a divide and conquer approach to process and
generate large datasets with parallel and distributed algorithms on a cluster.
The MapReduce model is composed of two phases: Map and Reduce. The Map
phase performs a transformation of');
INSERT INTO posts (postId,userId,title,body) VALUES (147,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 5)',' the data, and the Reduce phase performs
a summary operation. Briey explained, rst the master node splits the input
data and distributes it across the cluster. Then the Map transformation is
applied to each key-value pair in the local data. Once that process is nished
the data is redistributed based on the key-value pairs generated in the Map
phase. Once all pairs belonging to one key are in the same node, it is processed
in parallel. Apache Hadoop [45] [1] is the most popular open-source framework
based on the MapReduce model.
Apache Spark [19, 40] is an open-source framework for Big Data processing
built around speed, ease of use and sophisticated analytics. Its main feature is
its ability to use in-memory primitives. Users can load their data into mem-
ory and iterate over it repeatedly, making it a suitable tool for ML algorithms.
The motivation for developing Spark came from the limitations in the MapRe-
duce/Hadoop model [28, 12]:
 Intensive disk usage
 Insuciency for in-memory computation

 Poor performance on online and iterative computing.
 Low inter-communication capacity.

Spark is built on top of a distributed data structure called Resilient Dis-
tributed Datasets (RDDs) [50]. Operations on RDDs are applied to each par-
tition of the node local data. RDDs support two types of operations: trans-
formations, which are not evaluated when dened and produce a new RDD,
and actions, which evaluate all the previous transformations and return a new
value. The RDD structure allows programmers to persist them into memory
or disk for re-usability purposes. RDDs are immutable and fault-tolerant by
nature. All operations are tracked using a lineage, so that each partition can
be recalculated in case of failure.
Although new promising frameworks for Big Data are emerging, like Apache
Flink [13], Apache Spark is becoming the reference in performance [18].

2.3. From Big Data to Smart Data

Big Data is an appealing discipline that presents an immense potential for
global economic growth and promises to enhance competitiveness of high tech-
nological countries. Such as occurs in any knowledge extraction process, vast
amounts of data are analyzed, processed, and interpreted in order to generate
prots in terms of either economic or advantages for society. Once the Big Data
has been analyzed, processed, interpreted and cleaned, it is possible to access
it in a structured way. This transformation is the dierence between Big and
Smart Data [26].
The rst step in this transformation is to perform an integration process,
where the semantics and domains from several large sources are unied under a
common structure. The usage of ontologies to support the integration is a recent
approach [9, 7], but graph databases are also an option where the data is stored
in a relational form, as in healthcare domains [35]. Even when the integration
phase ends, the data is still far from being smart: the accumulated noise in Big
Data problems creates problems in classical Data Mining techniques, specially
when the dimensionality is large [10]. Thus, in order to be smart, the data
still needs to be cleaned even after its integration, and data preprocessing is the
set of techniques utilized to encompass this task [15, 16].
Once the data is smart, it can hold the valuable data and allows interac-
tions in real time, like transactional activities and other Business Intelligence
applications. The goal is to evolve from a data-centered organization to a');
INSERT INTO posts (postId,userId,title,body) VALUES (148,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 6)',' learn-
ing organization, where the focus is set on the knowledge extracted instead of
struggling with the data management [21]. However, Big Data generates great
challenges to achieve this since its high dimensionality and large example size
imply noise accumulation, algorithmic instability and the massive sample pool
is often aggregated from heterogeneous sources [11]. While feature selection,
discretization or imbalanced algorithms to cope with the high dimensionality
have drawn the attention of current Big Data frameworks (such as Sparks ML-
lib [31]) and researchers [37, 41, 36, 43], algorithms to clean noise are still a

challenge. In summary, challenges are still present to fully operate a transition
between Big Data to Smart Data. In this paper we provide an automated pre-
processing framework to deal with class noise, enabling the practitioner to reach
Smart Data.

3. Noise ltering for Big Data

In this section, we present the framework for Big Data under Apache Spark
for removing noisy examples based on the MapReduce paradigm, proving its
performance over real-world large problems. It is a MapReduce design where
all the noise lter processes are performed in a distributed way.
For the implementation of the framework, we have used some basic Spark
primitives. Here, we outline those more relevant to the algorithm:
 map: Applies a transformation to each element of a RDD and returns the
resulting RDD.
 z ipW ithI ndex: Zips a RDD with its element indices.
 j oin: Return a RDD containing all pairs of elements with matching keys
between two RDDs.
 f ilter : Return a new RDD containing only the elements that satisfy a
predicate.

We have designed two algorithms based on ensembles. Both perform a k-fold
on the training data and identify noisy instances in the test partition. The rst
one is an homogeneous ensemble using Random Forest as a classier, named
HME-BD (Section 3.1). The second one, named HTE-BD (Section 3.2) is a
heterogeneous ensemble based on the use of three dierent classiers: Random
Forest, Logistic Regression and KNN. We have also designed a simple lter
based on the similarity between the instances using KNN as a classier, named
ENN-BD (Section 3.3).

Algorithm 1 HME-BD Algorithm

1: Input: data an RDD of type LabeledPoint (label, features)
2: Input: P the number of partitions
3: Input: nTrees the number of trees for Random Forest
4: Output: the ltered RDD without noise
5: kF old  kF old(data, P )
6: f ilteredData 
7: map train, test  kF old
rf M odel  randomF orest(train, nT rees)
8:
rf P red  predict(rf M odel, test)
j oinedData  j oin(z ipW ithI ndex(test), z ipW ithI ndex(rf P red))
f ilteredData 
map orig , pred  j oinedData
if label(orig) = label(pred) then
orig
14:
else
15:
LabeledP oint(noise, f eatures(orig))
16:
end if
17:
end map
18:
19: end map
20: return(f ilter(f ilteredData, label (cid:54)= noise))

9:
10:
11:
12:
13:

3.1. Homogeneous Ensemble: HME-BD

Algorithm 1 describes the noise ltering process in HME-BD. The homo-
geneous ensemble is based on a Cross-Validated Committees Filter [44]. The
algorithm lters the noise in a dataset by performing a k-fold on the input data.
Sparks kF old function returns an array of (train, test) for a given P . With a
Map function we iterate through each train and test. We learn a Random For-
est model using the train as input data and predict the test using the learned
model. Then we join the test data and the predicted data by index using the
z ipW ithI ndex operation in both RDDs in order to compare the classes. The
next step is to iterate using a Map function through each instance in order to
check if the original class and the predicted one are the same. If the predicted
class and the original are dierent, the instance is marked as noise. Once all in-
stances have been checked, marked ones are ltered and the dataset is returned.
The following are required as input parameters: the dataset, the number of
partitions and the number of trees fo');
INSERT INTO posts (postId,userId,title,body) VALUES (149,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 7)','r the Random Forest.

3.2. Heterogeneous Ensemble: HTE-BD
The noise ltering process in HTE-BD is shown in Algorithm 2. The hetero-
geneous ensemble is based on Ensemble Filter [5]. Like HME-BD, the algorithm
lters the noise in a dataset by performing a k-fold on the input data. For each
fold it learns three classication algorithms: Random Forest, Logistic Regres-
sion and 1NN using the train as input data. Then it predicts the test data with

Algorithm 2 HTE-BD Algorithm

1: Input: data an RDD of type LabeledPoint (label, features)
2: Input: P the number of partitions
3: Input: nTrees the number of trees for Random Forest
4: Input: vote the voting strategy (ma jority or consensus)
5: Output: the ltered RDD without noise
6: kF old  kF old(data, P )
7: f ilteredData 
8: map train, test  kF old
classif iersM odel  learnC lassif iers(train, nT rees)
9:
(rf , lr, knn)  predict(classif iersM odel, test)
10:
predictions 
11:
map orig  test
12:
count  0
13:
if rf (cid:54)= label(orig) then count  count + 1 end if
if lr (cid:54)= label(orig) then count  count + 1 end if
if knn (cid:54)= label(orig) then count  count + 1 end if
if vote = maj ority then
if count  2 then LabeledP oint(noise, f eatures(orig)) end if
if count < 2 then orig end if
else
20:
if count = 3 then LabeledP oint(noise, f eatures(orig)) end if
21:
if count (cid:54)= 2 then orig end if
22:
end if
23:
end map
24:
25: end map
26: return(f ilter(f ilteredData, label (cid:54)= noise))

14:
15:
16:
17:
18:
19:

the three learned models.
It iterates through each instance in the test data
comparing the three predictions and, depending upon the voting strategy, the
instance is marked as noise or as clean. Once all instances have been checked,
the data is ltered and returned. The following are required as input parame-
ters: the dataset, the number of partitions, the number of trees for the Random
Forest and the voting strategy.

3.3. Similarity: ENN-BD

The noise ltering process in ENN-BD is simpler than that the in two previ-
ous algorithms. ENN-BD is based on Edited Nearest Neighbor [46] and follows
a distance between instances approach. This lter performs a 1NN using the
euclidean distance and checks for each instance if its closest neighbor belongs to
the same class. If it has a dierent class, the instance is marked as noise. Once
all instances have been checked, marked instances are removed from the train-
ing data. This process is described in Algorithm 3. The only input parameter
required is the dataset.

Algorithm 3 ENN-BD Algorithm

1: Input: data an RDD of type LabeledPoint (label, features)
2: Output: the ltered RDD without noise
3: knnM odel  KN N (1, euclidean, data)
4: knnP red  z ipW ithI ndex(predict(knnM odel, data))
5: j oinedData  j oin(z ipW ithI ndex(data), knnP red)
6: f ilteredData 
7: map orig , pred  j oinedData
if label(orig) = label(pred) then
8:
orig
9:
else
10:
LabeledP oint(noise, f eatures(orig))
11:
end if
12:
13: end map
14: return(f ilter(f ilteredData, label (cid:54)= noise))

Table 1: Datasets used in the analysis

Dataset

Instances

Atts.

Total

CL

SUSY
HIGGS
Epsilon
ECBDL14

5,000,000
11,000,000
500,000
1,000,000

18
28
2,000
631

90,000,000
308,000,000
1,000,000,000
631,000,000
22

4. Experimental Results

This section describes the experimental details and the analysis carried out
to show the performance of the three noise lter methods over four huge prob-
lems. In Section 4.1, we present the details of the datasets and the parameters
used in the methods. We analyze the accuracy improvements generated by the');
INSERT INTO posts (postId,userId,title,body) VALUES (150,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 8)','
proposed framework and the study of instances removed in Section 4.2. Finally,
Section 4.3 is devoted to the computing times of the proposals.

4.1. Experimental Framework

Four classication datasets are used in our experiments:
 SUSY dataset, which consists of 5,000,000 instances and 18 attributes.
The rst eight features are kinematic properties measured by the parti-
cle detectors at the Large Hadron Collider. The last ten are functions of
the rst eight features. The task is to distinguish between a signal pro-
cess which produces supersymmetric (SUSY) particles and a background
process which does not [2].
 HIGGS dataset, which has 11,000,000 instances and 28 attributes. This
dataset is a classication problem to distinguish between a signal process
which produces Higgs bosons and a background process which does not.

10

Table 2: Parameter setting for the noise lters

Algorithm Parameters

Classier

HME-BD P = 4, 5

HTE-BD P = 4, 5
Voting = ma jor-
ity, consensus
ENN-BD K = 1

Random Forest:
featureSubsetStrategy = auto, im-
purity = gini, maxDepth = 10 and maxBins = 32
1NN, Random Forest: featureSubsetStrategy = auto,
impurity = gini, maxDepth = 10 and maxBins = 32

Table 3: Parameter setting for the classiers

Classier

Parameters

K = 1
KNN
impurity = gini, maxDepth = 20 and maxBins = 32
Decision Tree
 Epsilon dataset, which consists of 500,000 instances with 2,000 numerical
features. This dataset was articially created for the Pascal Large Scale
Learning Challenge in 2008. It was further pre-processed and included in
the LibSVM dataset repository [6].
 ECBDL14 dataset, which has 32 million instances and 631 attributes (in-
cluding both numerical and categorical). This dataset was used as a ref-
erence at the ML competition of the Evolutionary Computation for Big
Data and Big Learning held on July 14, 2014, under the international
conference GECCO-2014. It is a binary classication problem where the
class distribution is highly imbalanced: 98% of negative instances. For
this problem, we use a reduced version with 1,000,000 instances and 30%
of positive instances.

Table 1 provides a brief summary of these datasets, showing the number of
examples (Instances), the total number of attributes (Atts.), the total number
of training data (Total), and the number classes (CL).
We carried out experiments on ve levels of uniform class noise [42]: for each
level of noise, a percentage of the training instances are altered by replacing their
actual label by another label from the available classes. The selected noise levels
are 0%, 5%, 10%, 15% and 20%. In this case, a 0% noise level indicates that
the dataset was unaltered. We have conducted a hold-out validation due to the
time limitations of the KNN algorithm.
In Table 2 we can see the complete list of parameters used for the noise
treatment algorithms. In order to evaluate the eect of the number of partitions
on the behavior of the lters, we have selected 4 and 5 training partitions for
HME-BD and HTE-BD. For the heterogeneous lter, HTE-B');
INSERT INTO posts (postId,userId,title,body) VALUES (151,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 9)','D, we also use two
voting strategies: consensus (same result for all classiers) and ma jority (same
result for at least half the classiers).
Two classiers, one MLlib classier, a decission tree, and one algorithm

11

Table 4: KNN test accuracy. The highest accuracy value per dataset and noise level is stressed
in bold

DatasetVote

SUSY

HIGGS

Epsilon

ECBDL14

Noise (%) Original HME-BD
5

HTE-BD54
Ma jority Consensus Ma jority Consensus

ENN-BD
5
10
15
20
5
10
15
20
5
10
15
20
5
10
15
20

71.79
69.62
67.44
65.27
63.10

61.21
60.10
58.97
57.84
56.69

56.55
55.71
55.20
54.54
54.05

74.83
72.36
69.86
67.39
64.90

78.73
78.68
78.63
78.62
78.56

64.26
64.06
63.83
63.65
63.53

58.11
58.64
58.51
58.39
58.02

76.06
75.60
75.31
75.11
74.82

78.72
78.69
78.62
78.61
78.58

64.25
64.07
63.84
63.64
63.40

58.06
58.60
58.61
58.41
58.09

76.03
75.59
75.32
75.12
74.83

77.86
77.68
77.44
77.19
76.93

63.94
63.63
63.29
62.86
62.55

57.43
57.47
57.26
57.00
56.75

75.12
74.59
74.19
73.99
73.70

74.64
73.38
72.01
70.52
69.10

62.30
61.45
60.65
59.81
58.89

55.19
55.47
55.25
55.00
54.72

73.54
72.89
72.50
72.11
71.89

77.88
77.68
77.46
77.20
76.93

63.93
63.62
63.24
62.89
62.55

57.39
57.39
57.26
57.02
56.71

75.14
74.59
74.19
74.01
73.70

74.65
73.39
72.00
70.53
69.04

62.23
61.44
60.66
59.81
58.85

55.40
55.41
55.25
55.03
54.72

73.46
72.84
72.47
72.06
71.90

72.02
69.84
67.66
65.28
63.25

60.65
59.60
58.56
57.52
56.45

56.21
55.43
54.79
54.30
53.68

73.94
72.77
71.40
69.68
67.64

present in Sparks community repository, KNN2 , are used to evaluate the ef-
fectiveness of the ltering carried out by the two ensemble proposals and the
similarity lter. The decision tree can adapt its depth to avoid overtting to
noisy instances, while KNN is known to be sensitive to noise when the number
of selected neighbors is low. Prediction accuracy is used to evaluate the models
performance produced by the classiers (number of examples correctly labeled
as belonging to a given class divided by the total number of elements). The
parameters used for the classiers can be seen in Table 3.
For all experiments we have used a cluster composed of 20 computing nodes
and one master node. The computing nodes hold the following characteristics:
2 processors x Intel(R) Xeon(R) CPU E5-2620, 6 cores per processor, 2.00 GHz,
2 TB HDD, 64 GB RAM. Regarding software, we have used the following con-
guration: Hadoop 2.6.0-cdh5.4.3 from Clouderas open source Apache Hadoop
distribution, Apache Spark and MLlib 1.6.0, 460 cores (23 cores/node), 960
RAM GB (48 GB/node).

4.2. Analysis of accuracy performance and removed instances

In this section, we present the analysis on the performance results obtained
by the selected classiers after applying the proposed framework. We denote
with Original the application of the classier without using any noise treatment
techniques, in order to evaluate the impact of the increasing noise level in the
quality of the models extracted by the classication algorithms.
Table 4 shows the test accuracy values for the four datasets and the ve

2 https://spark- packages.org/package/JMailloH/kNN_IS

12

levels of noise using the KNN algorithm for classication. From these results
we can point out that:
 It is important to remark that the usage of any');
INSERT INTO posts (postId,userId,title,body) VALUES (152,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 10)',' noise treatment tech-
nique always improves the Original accuracy value at the same noise level.
Please note that the usage of the noise treatment technique allows KNN
to obtain better performance at any noise level, even at the highest ones,
than Original at 0% level for every dataset. Since Big Datasets tend to
accumulate noise, the proposed noise framework is able to improve the
behavior and performance of the KNN classier in every case.
 If we attend the best noise treatment strategy for KNN, we must point
out that the homogeneous lter, HME-BD, enables KNN to obtain the
highest accuracy values.
 The dierent number of partitions used for HME-BD has little impact in
the accuracy values, which, in this respect, makes it a robust method.
 The heterogeneous ensemble lter, HTE-BD, is also robust to the number
of partitions chosen, but its performance is lower than HME-BD. However,
the voting scheme is crucial for HTE-BD, as the consensus strategy will
result in worse accuracy for KNN, being close to 2% less accuracy for the
consensus voting strategy.
 The baseline noise ltering method, ENN-BD, is the worst option as KNN
obtains the lowest accuracy values among the three noise treatment strate-
gies. For ENN-BD, the accuracy drops around 2% for each 5% increment
in noise instances. However, as mentioned earlier, ENN-BD is still prefer-
able to not dealing with the noise at all. This is due to the noise sensitive
nature of KNN.

Table 5 gathers the test accuracy values for the three noise lter methods
using a decision tree. From these results we can point out that:
 Again, avoiding the treatment of noise is never the best option and using
the appropriate noise ltering technique will provide a signicant improve-
ment in accuracy. However, since the decision tree is more robust against
noise than KNN, not all the lters are better than avoiding ltering noise
(Original ). When the lters remove too many instances, both noisy and
clean, the decision tree is more aected since it is able to withstand small
amounts of noise while exploiting the clean instances. KNN was very af-
fected by the noisy instances left, in a higher degree than the decision
tree. Thus, a wrong ltering strategy will penalize the performance of the
decision tree. We will elaborate more on this later.
 In terms of the best ltering technique for the decision tree, for low levels
of noise, the heterogeneous ensemble HTE-BD can perform slightly better
than the homogeneous HME-BD for some datasets. Nevertheless, from
a 10% noise level onwards, HME-BD outperforms HTE-BD, making it a
better approach to deal with noise for the decision tree.

13

Table 5: Decision tree test accuracy. The highest accuracy value per dataset and noise level
is stressed in bold

DatasetVote

SUSY

HIGGS

Epsilon

Noise (%) Original HME-BD
5

HTE-BD54
Ma jority Consensus Ma jority Consensus

ENN-BD
5
10
15
20
5
10
15
20
5
10
15
20

80.24
79.94
79.15
78.21
77.09

70.17
69.61
69.22
68.65
67.82

62.39
61.10
60.09
59.02
57.73

79.78
79.99
79.85
79.81
79.71

71.16
71.14
71.06
71.03
71.05

66.86
66.64
66.87
66.62
66.46

79.79
79.97
79.84
79.80
79.73

71.17
71.11
71.04
70.99
71.02

66.19
66.83
67.00
66.85
66.79

79.69
80.07
79.81
79.32
79.35

69.61
69.34
68.95
68.52
68.18

65.13
65.32
65.46
65.33
65.08

80.27
80.36
80.04
79.47
78.95

70.41
69.98
69.56
69.04
68.38

66.07
66.09
66.11
65.99
65.69

79.17
80.10
79.81
79.61
79.31

69.68
69.36
68.97
68.65
68.35

65.11
65.33
65.47
65.29
64.98

80.29
80.34
80.22
79.48
79.41

70.33
69.92
69.58
69.06
68.39

66.02
66.09
66.10
66.00
65.65

78.56
77.49
77.00
75.81
74.21

68.85
68.29
67.52
66.93
66.05

61.54
');
INSERT INTO posts (postId,userId,title,body) VALUES (153,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 11)','60.41
59.20
58.09
56.71

ECBDL14

73.66
74.62
74.35
74.51
74.21
74.38
74.59
73.9873.48
74.75
74.25
74.54
74.16
74.40
74.64
72.8772.75
74.63
73.94
74.51
73.84
74.25
74.59
71.67
10
71.68
74.10
73.98
73.91
73.82
74.22
74.61
70.28
15
70.16
73.86
73.85
73.82
73.78
74.18
74.83
68.66
20
 Regarding the HTE-BD voting strategy, the consensus scheme achieves
better results than the ma jority voting strategy. Please note that the
opposite has been observed in KNN: since KNN is much more sensitive
and demands cleaner class borders achieved with the ma jority voting, the
decision tree benets from a more accurate noise removal provided by the
consensus voting.
 The baseline method, ENN-BD, is achieving around 1% less accuracy
than the rest for low levels of noise, but this dierence increases to 5%
less accuracy in higher noise levels.

The results presented have shown the importance of applying a noise treat-
ment strategy, no matter how much noise is present in the dataset, and the best
strategy overall: HME-BD. To better explain why HME-BD is the best ltering
strategy in the framework, we must study the amount of instances removed.
In Table 6 we present the average number of instances left after the appli-
cation of the three noise ltering methods for the four datasets. In Figure 1
we can see a graphic representation of the number of instances for the sake
of a better depiction. As we can expect, the higher the percentage of noise,
the lower the number of instances that remain in the dataset after applying
the ltering technique. However, there are dierent patterns depending on the
ltering technique used:
 For the homogeneous ensemble HME-BD, there is no eect in the number
of partitions P chosen with respect to the amount of removed instances.
On average, HME-BD removes around 20% of the instances at a 0% noise
level. At each noise level increment an average of 3% of the instances are
removed.

14

Table 6: Average number of instances for HME-BD, HTE-BD and ENN-BD

DatasetVote

SUSY

HIGGS

Epsilon

Noise Original

HME-BD
5

HTE-BD54
Ma jority Consensus Ma jority Consensus

0%
5%
10%
15%
20%

0%
5%
10%
15%
20%

0%
5%
10%
15%
20%

2,500,000
2,500,000
2,500,000
2,500,000
2,500,000

5,500,000
5,500,000
5,500,000
5,500,000
5,500,000

250,000
250,000
250,000
250,000
250,000

1,984,396
1,910,750
1,837,604
1,763,890
1,691,290

3,900,547
3,787,000
3,672,429
3,554,120
3,446,352

164,222
186,707
180,489
173,027
166,191

1,983,785
1,911,317
1,837,408
1,764,176
1,691,506

3,900,035
3,786,366
3,672,553
3,557,252
3,443,459

164,292
186,839
180,517
173,114
166,247

1,974,018
1,872,868
1,801,616
1,728,789
1,657,323

3,567,784
3,484,271
3,404,181
3,324,547
3,242,174

194,252
186,890
180,296
173,226
166,394

2,281,521
2,241,766
2,207,999
2,174,051
2,144,595

5,048,874
5,014,344
4,972,401
4,930,575
4,888,991

242,757
239,200
235,425
231,962
228,153

1,973,587
1,874,053
1,800,276
1,727,949
1,657,035

3,564,879
3,484,274
3,401,624
3,323,465
3,240,623

194,037
186,957
180,332
173,274
166,285

2,280,941
2,242,598
2,203,012
2,175,876
2,141,811

5,051,498
5,013,132
4,973,794
4,932,060
4,886,961

242,730
239,200
235,456
231,997
228,394

ENN-BD

1,262,317
1,260,781
1,258,44');
INSERT INTO posts (postId,userId,title,body) VALUES (154,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 12)','1
1,256,611
1,254,441

2,765,831
2,763,942
2,760,547
2,754,636
2,756,382

125,072
124,983
125,064
124,980
124,583

ECBDL14

0%
500,000
387,815
387,873
393,242
470,731
393,273
470,924
367,101
5%
500,000
370,991
371,094
377,451
458,758
377,239
459,212
344,717
10%
500,000
357,565
357,270
361,587
448,460
361,614
448,550
324,674
15%
500,000
344,363
344,427
346,454
439,633
346,633
439,028
306,832
20%
500,000
330,694
330,761
331,552
430,444
331,511
430,357
292,000
 For the Epsilon dataset, at 20% nosie, HME-BD does not remove as many
instances as expected, but it is still the best option out of the two classi-
ers. A high instance redundancy in this dataset may cause homogeneous
voting to not discard as many instances as the other lters.
 Like HME-BD, HTE-BD is not aected by the number of partitions, but
the voting scheme does have a great impact on its behavior. While the
ma jority voting strategy achieves almost the same number of removed
instances as HME-BD, the consensus voting strategy is more conservative.
Consensus voting removes 10% of the instances for 0% level of noise, and
it is increasing a 3% on average as the level of noise increases, the same
rate as HME-BD.
 ENN-BD is the lter that removes more instances. On average it removes
half the instances of the datasets for 0% level of noise and then increases
around 1% at each increment of noise level. This aggresive ltering hinders
the performance of noise tolerant classiers, such as the decision tree.
 In general, HME-BD is the most balanced technique in terms of instances
removed and kept. Although the amount of instances removed by HTE-
BD with ma jority voting is very similar to HME-BD, the instances se-
lected to be eliminated are dierent, severely aecting the classier used
afterwards.

In view of the results, we can conclude that HME-BD is the most suitable
ensemble option in the proposed framework to deal with noise in Big Data
problems. Even when we did not introduce any additional noise, the usage
of noise treatment methods has proven to be very benecial. As previously

15

(a) SUSY

(b) HIGGS

(c) Epsilon

(d) ECBDL14

Figure 1: Number of instances after the ltering process

Table 7: Average run times for HME-BD, HTE-BD and ENN-BD in seconds

DatasetVote

HME-BD
5

HTE-BD54
Ma jority Consensus Ma jority Consensus

ENN-BD

SUSY
HIGGS
Epsilon
ECBDL14

513.46
587.72
1,868.75
1,228.24

632.54
675.07
2,021.14
1,348.10

5,511.15
15,300.62
4,120.79
9,710.70

5,855.66
15,232.99
7,201.05
11,217.02

6,701.62
16,417.26
5,179.09
10,798.18

6,399.32
17,067.97
5,664.06
11,366.01

8,956.71
25,441.09
2,718.97
14,080.03

mentioned, Big Data problems tend to accumulate noise and the proposed noise
framework is a suitable tool to clean and proceed from Big to Smart Datasets.

4.3. Computing times

In the previous section we have shown the suitability of the proposed frame-
work in terms of accuracy. In order to constitute a valid proposal in Big Data,
this framework has to be scalable as well. This section is devoted to present
the computing times for the two prosposed ensemble techniques, HME-BD and
HTE-BD, and the simple similarity method, ENN-BD, used as a baseline.');
INSERT INTO posts (postId,userId,title,body) VALUES (155,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 13)','
In Table 7 we can see the average run times of the three methods for the
four datasets in seconds. As the level of noise is not a factor that aects the run

16

Figure 2: Run times chart

time, we show the average of the ve executions performed for each dataset. In
Figure 2 we can see a graphic representation of these times.
The measured times show that the homogeneous ensemble, HME-BD, is
not only the best performing option in terms of accuracy, but also the most
ecient one in terms of computing time. HME-BD is about ten times faster
than the heterogeneous lter HTE-BD and the similarity lter ENN-BD. This
is caused by the usage of the KNN classier by HTE-BD and ENN-BD, which
is very demanding in computing terms. As a result, HME-BD does not need
to compute any distance measures, saving computing time and being the most
recommended option to deal with noise in Big Data problems.

5. Conclusions

In this paper, we have tackled the problem of noise in Big Data classication,
which is a crucial step in transforming such raw data into Smart Data. We have
proposed several noise ltering algorithms, implemented in a Big Data frame-
work: Spark. These ltering techniques are based on the creation of ensembles
of classiers that are executed in the dierent maps, enabling the practitioner to
deal with huge datasets. Dierent strategies of data partitioning and ensemble
classier combination have led to three dierent approaches.
The suitability of these proposed techniques has been analyzed using several
data sets, in order to study the accuracy improvement, running times and data
reduction rates. The homogeneous ensemble has shown to be the most suitable
approach in most cases, both in accuracy improvement and better running times.
It also shows the best balance between removing and keeping enough instances,
being among the most balanced lter in terms of preprocessed training sets.
This work presents the rst suitable noise lter in Big Data domains, where
the high redundancy of the instances and high dimensional problems pose new
challenges to classic noise preprocessing algorithms. Thus, the presented frame-
work is a valuable tool for achieving the goal of Smart Data.
It also opens
promising research lines in this topic, where the presence of iterative algorithms

17

and the usage of noise measures are also known as viable alternatives for dealing
with noise.

Acknowledgment

This work is supported by the Spanish National Research Pro ject TIN2014-
57251-P and the Foundation BBVA pro ject 75/2016 BigDaPTOOLS.

References

References

[1] Apache Hadoop Pro ject, Apache Hadoop, http://hadoop.apache.org/
(2016).

[2] P. Baldi, P. Sadowski, D. Whiteson, Searching for Exotic Particles in High-
Energy Physics with Deep Learning, Nature Communications 5 (2014)
4308.

[3] C. Bouveyron, S. Girard, Robust supervised classication with mixture
models: Learning from data with uncertain labels, Pattern Recognition
42 (11) (2009) 26492658.

[4] L. Breiman, Random forests, Machine Learning 45 (1) (2001) 532.

[5] C. E. Brodley, M. A. Friedl, Identifying Mislabeled Training Data, Journal
of Articial Intelligence Research 11 (1999) 131167.

[6] C.-C. Chang, C.-J. Lin, Libsvm: A library for support vector machines,
ACM Transactions on Intelligent Systems and Technology (TIST) 2 (3)
(2011) 27:127:27.
URL http://doi.acm.org/10.1145/1961189.1961199

[7] J. Chen, D. Dosyn, V. Lytvyn, A. Sachenko, Smart data integration by
goal driven ontology learning, in: Advances in Intelligent Systems and
Computing, vol. 529, 2017, pp. 283292.

[8] J. Dean, S. Ghemawat, Mapreduce: Simplied data processing on large
clusters, Communications of the ACM 51 (1) (2008) 107113.
URL http');
INSERT INTO posts (postId,userId,title,body) VALUES (156,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 14)','://doi.acm.org/10.1145/1327452.1327492

[9] H. Fadili, C. Jouis, Towards an automatic analyze and standardization of
unstructured data in the context of big and linked data, in: 8th Interna-
tional Conference on Management of Digital EcoSystems, MEDES 2016,
2016, pp. 223230.

[10] J. Fan, Y. Fan, High dimensional classication using features annealed
independence rules, Annals of statistics 36 (6) (2008) 26052637.

18

[11] J. Fan, F. Han, H. Liu, Challenges of big data analysis, National Science
Review 1 (2) (2014) 293314.

[12] A. Fernandez, S. del Ro, V. Lopez, A. Bawakid, M. J. del Jesus, J. M.
Bentez, F. Herrera, Big data with cloud computing: an insight on the
computing environment, mapreduce, and programming frameworks, Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery 4 (5)
(2014) 380409.
URL http://dx.doi.org/10.1002/widm.1134

[13] A. Flink, Apache Flink, http://flink.apache.org/, http://flink.
apache.org.

[14] B. Frenay, M. Verleysen, Classication in the presence of label noise: A sur-
vey, IEEE Transactions on Neural Networks and Learning Systems 25 (5)
(2014) 845869.

[15] S. Garca, J. Luengo, F. Herrera, Data Preprocessing in Data Mining,
Springer Publishing Company, Incorporated, 2015.

[16] S. Garca, J. Luengo, F. Herrera, Tutorial on practical tips of the most
inuential data preprocessing algorithms in data mining, Knowledge-Based
Systems 98 (2016) 129.

[17] S. Garca, S. Ramrez-Gallego, J. Luengo, J. M. Bentez, F. Herrera, Big
data preprocessing: methods and prospects, Big Data Analytics 1 (1)
(2016) 9.
URL http://dx.doi.org/10.1186/s41044-016-0014-0

[18] D. Garca-Gil, S. Ramrez-Gallego, S. Garca, F. Herrera, A comparison on
scalability for batch big data processing on apache spark and apache ink,
Big Data Analytics 2 (1) (2017) 1.
URL http://dx.doi.org/10.1186/s41044-016-0020-2

[19] M. Hamstra, H. Karau, M. Zaharia, A. Konwinski, P. Wendell, Learning
Spark: Lightning-Fast Big Data Analytics, OReilly Media, 2015.

[20] C.-H. Hsu, Intelligent big data processing, Future Generation Computer
Systems 36 (2014) 16  18.

[21] F. Iafrate, A journey from big data to smart data, Advances in Intelligent
Systems and Computing 261 (2014) 2533.

[22] IDC, The Digital Universe of Opportunities, http://www.emc.com/
infographics/digital-universe-2014.htm (2014).

[23] T. M. Khoshgoftaar, P. Rebours, Improving software quality prediction by
noise ltering techniques, Journal of Computer Science and Technology 22
(2007) 387396.

19

[24] D. Laney, 3D data management: Controlling data volume, velocity, and
variety, Tech. rep., META Group (February 2001).
URL
http://blogs.gartner.com/doug-laney/files/2012/01/
ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.
pdf

[25] N. D. Lawrence, B. Scholkopf, Estimating a kernel sher discriminant in
the presence of label noise, in: Proceedings of the Eighteenth International
Conference on Machine Learning, ICML 01, 2001, pp. 306313.

[26] A. Lenk, L. Bonorden, A. Hellmanns, N. Roedder, S. Jaehnichen, Towards a
taxonomy of standards in smart data, in: Proceedings - 2015 IEEE Interna-
tional Conference on Big Data, IEEE Big Data 2015, 2015, pp. 17491754.

[27] Y. Li, L. F. Wessels, D. de Ridder, M. J. Reinders, Classication in the
presence of class noise using a probabilistic kernel sher method, Pattern
Recognition 40 (12) (2007) 33493357.

[28] J. Lin, Mapreduce is good enough? if all you have is a hammer, throw
away everything thats not a nail!, CoRR abs/1209.2191.
URL http://arxiv.org/abs/1209.2191

[29] B. Liu, Sentiment analysis: Mining opinions, sentiments, and emotions,
Cambridge University Press, 2015.

[30] V. ');
INSERT INTO posts (postId,userId,title,body) VALUES (157,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 15)','Mayer-Schonberger, K. Cukier, Big Data: A Revolution That Will
Transform How We Live, Work, and Think, Houghton Miin Harcourt,
2013.

[31] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Free-
man, D. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh,
M. Zaharia, A. Talwalkar, Mllib: Machine learning in apache spark, Journal
of Machine Learning Research 17 (34) (2016) 17.

[32] Q. Miao, Y. Cao, G. Xia, M. Gong, J. Liu, J. Song, Rboost: Label noise-
robust boosting algorithm based on a nonconvex loss function and the
numerically stable base learners, IEEE Transactions on Neural Networks
and Learning Systems 27 (11) (2016) 22162228.

[33] D. Pyle, Data preparation for data mining, Morgan Kaufmann, Los Altos,
1999.

[34] J. R. Quinlan, C4.5: programs for machine learning, Morgan Kaufmann
Publishers, San Francisco, CA, USA, 1993.

[35] P. Ra ja, E. Sivasankar, R. Pitchiah, Framework for smart health: Toward
connected data from big data, Advances in Intelligent Systems and Com-
puting 343 (2015) 423433.

20

[36] S. Ramrez-Gallego, S. Garca, H. Mourino-Taln, D. Martnez-Rego,
V. Bolon-Canedo, A. Alonso-Betanzos, J. M. Bentez, F. Herrera, Data
discretization: taxonomy and big data challenge, Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery 6 (1) (2016) 521.

[37] S. Ramrez-Gallego, I. Lastra, D. Martnez-Rego, V. Bolon-Canedo, J. M.
Bentez, F. Herrera, A. Alonso-Betanzos, Fast-mrmr: Fast minimum re-
dundancy maximum relevance algorithm for high-dimensional big data.,
International Journal of Intelligent Systems 32 (2) (2017) 134152.

[38] J. A. Saez, M. Galar, J. Luengo, F. Herrera, Analyzing the presence of
noise in multi-class problems: alleviating its inuence with the One-vs-One
decomposition, Knowledge and Information Systems 38 (1) (2014) 179206.

[39] H. Singh, S. Bawa, A mapreduce-based scalable discovery and indexing of
structured big data, Future Generation Computer Systems (2017) http:
//dx.doi.org/10.1016/j.future.2017.03.028.

[40] A. Spark, Apache Spark: Lightning-fast cluster computing, http://spark.
apache.org/ (2016).

[41] M. Tan, I. W. Tsang, L. Wang, Towards ultrahigh dimensional feature
selection for big data, Journal of Machine Learning Research 15 (2014)
13711429.

[42] C.-M. Teng, Correcting Noisy Data, in: Proceedings of the Sixteenth Inter-
national Conference on Machine Learning, Morgan Kaufmann Publishers,
San Francisco, CA, USA, 1999, pp. 239248.

[43] I. Triguero, S. del Ro, V. Lopez, J. Bacardit, J. M. Bentez, F. Herrera,
Rosefw-rf: the winner algorithm for the ecbdl14 big data competition: an
extremely imbalanced big data bioinformatics problem, Knowledge-Based
Systems 87 (2015) 6979.

[44] S. Verbaeten, A. Assche, Ensemble methods for noise elimination in clas-
sication problems, in: 4th International Workshop on Multiple Classi-
er Systems(MCS 2003), vol. 2709 of Lecture Notes on Computer Science,
Springer, 2003, pp. 317325.

[45] T. White, Hadoop: The Denitive Guide, OReilly Media, Inc., 2012.

[46] D. L. Wilson, Asymptotic properties of nearest neighbor rules using edited
data, IEEE Transactions on Systems, Man, and Cybernetics 2 (3) (1972)
408421.

[47] X. Wu, Knowledge acquisition from databases, Ablex Publishing Corp.,
Norwood, NJ, USA, 1996.

[48] X. Wu, X. Zhu, Mining with noise knowledge: Error-aware data mining,
IEEE Transactions on Systems, Man, and Cybernetics 38 (2008) 917932.

21

[49] X. Wu, X. Zhu, G.-Q. Wu, W. Ding, Data mining with big data, IEEE
Transactions on Knowledge and Data Engineering 26 (1) (2014) 97107.

[50] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J.
Franklin, S. Shenker, I. Stoica, Resilient distributed datasets: A fault-
tolerant abstraction for in-memory cluster computing, in: Proceedings of
the 9th USENIX Conference on Networked Systems Design and Implemen-
tation, NSDI12, USENIX Association, Berk');
INSERT INTO posts (postId,userId,title,body) VALUES (158,8763,'Enabling Smart Data: Noise ltering in Big Data classication (part 16)','eley, CA, USA, 2012, pp. 22.
URL http://dl.acm.org/citation.cfm?id=2228298.2228301

[51] B. Zerhari, Class noise elimination approach for large datasets based on a
combination of classiers, in: Cloud Computing Technologies and Applica-
tions (CloudTech), 2016 2nd International Conference on, IEEE, 2016, pp.
125130.

[52] S. Zhong, T. M. Khoshgoftaar, N. Seliya, Analyzing Software Measurement
Data with Clustering Techniques, IEEE Intelligent Systems 19 (2) (2004)
2027.

[53] X. Zhu, X. Wu, Class Noise vs. Attribute Noise: A Quantitative Study,
Articial Intelligence Review 22 (2004) 177210.

22

');
INSERT INTO posts (postId,userId,title,body) VALUES (159,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','Even as we advance the frontiers of physics knowledge, our understanding of how this knowledge
evolves remains at the descriptive levels of Popper and Kuhn. Using the APS publications data sets,
we ask in this letter how new knowledge is built upon old knowledge. We do so by constructing year-
to-year bibliographic coupling networks, and identify in them validated communities that represent
dierent research elds. We then visualize their evolutionary relationships in the form of alluvial
diagrams, and show how they remain intact through APS journal splits. Quantitatively, we see that
most elds undergo weak Popperian mixing, and it is rare for a eld to remain isolated/undergo
strong mixing. The sizes of elds obey a simple linear growth with recombination. We can also
reliably predict the merging between two elds, but not for the considerably more complex splitting.
Finally, we report a case study of two elds that underwent repeated merging and splitting around
1995, and how these Kuhnian events are correlated with breakthroughs on BEC, quantum telepor-
tation, and slow light. This impact showed up quantitatively in the citations of the BEC eld as a
larger proportion of references from during and shortly after these events.

According to Karl Popper, science progresses through
repeated hypothesis testing [1]. Hypotheses contrary to
empirical evidence must be rejected, while those consis-
tent with data survive to be tested another day. In this
picture of the scientic enterprise, our knowledge of the
world around us is always tentative, but becomes more
complete over time. On the other hand, Thomas Kuhn
believes that the accepted knowledge of a given time is
the result of consensus amongst scientists, based on evi-
dences consistent with their theories [2]. However, when
too many conicting evidences are found, a new con-
sensus can form around new theories in what he called
a paradigm shift. Kuhn gives special relativity and
quantum theory as examples of paradigm shifts. Look-
ing back, we realize these two theories have enormous im-
pacts on how we understand the world today. But could
there be paradigm shifts of various scales that have also
contributed to reshaping our knowledge of physics?
Many historians of science have noted the strongly re-
ductionistic avor of scientic research in the last cou-
ple of centuries[3]. Starting as natural philosophy, the
body of scientic knowledge became separated disciplines
of astronomy, biology, chemistry and physics. Within
physics itself, we also observe the emergence of high en-
ergy physics, condensed matter physics, biophysics, and
photonics. These are the results of the splitting of sci-
ence into more specialized elds. We also observe in par-
allel the merging of elds, such as the merging of as-
tronomy and physics to give astrophysics, biology and
chemistry to give biochemistry, and others that arose

 cheongsa@ntu.edu.sg

by division and recombination of specialties already ma-
tured [2]. These developments have been discussed ex-
tensively by philosophers and historians of science, but
unlike our quantitative understanding of physics, our ap-
preciation for the processes through which we acquired
our knowledge of physics remains at a highly descriptive
level. Some progress has been made in addressing this
problem [46]. In particular, the following three papers
provide the inspiration for our study. Chen and Red-
ner suggested that long-range connections can form be-
tween disparate elds because of the development of a
widely applicable theoretical technique, or cross fertil-
ization between theory and experiment [7]. Visualizing
the cross citations between neuroscience journals, Ros-
vall and Bergstrom traced the growth and maturation
of neuroscience as a discipline [8]. Using embryology as
a specic example, Chavalarias and Cointet created a
phylomemetic network visualization for the evolution of
science [9].

While these previous studies point to');
INSERT INTO posts (postId,userId,title,body) VALUES (160,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ',' the evolution of
scientic knowledge, they do not identify the entity that
is recognizably knowledge, or they do not study the in-
teractions between such ob jects. To clarify what consti-
tutes knowledge, we start with the bibliographic coupling
network (BCN) [10], proposed by Kessler and used ex-
tensively in computer science [11, 12]. In a BCN, nodes
represent papers, and if two papers share w common ref-
erences, we draw an edge with weight w between them
(see Fig. 1(a)). The BCN is suitable for our purpose for
two reasons: (i) the BCN for a given year consists only
of papers published that year and does not change after
more papers are published later, so features in the BCN
represent the state of knowledge in that year; and (ii)
12rA4 h-o.csh[ v70.01vXa

2
(cid:3); and
n ) = (cid:2)Rn
(cid:3) and Rt+1
(cid:2)Rm
m = R(C t
as Rt
cited by papers in C t
m and C t+1
m ) =
n = R(C t+11 , ..., Rn
1 , ..., Rmp
m , ...}. N (element, list) is the number of
Rt = {Rt
1 , ..., Rt
times element occurs in list, and L(list) is the length of
list. In this denition, we assume each citation instance
in t will be uniformly distributed over all instances of the
same citation in t + 1, while each citation in t + 1 receives
equal contributions from all instances of the same citation
mn (cid:54)= I b
in t. In general, this index is asymmetric, i.e. I f
mn ,
because the references are not cited the same number of
times in the two years, as illustrated in Fig. 1(b).

We visualize the sequence of TCs and their intimacy
indices, the evolution of physics research they represent
in the form of alluvial diagrams. For example, in Fig. 2
we can clearly see the birth of PRA, PRB, PRC and PRD
from PR in 1970. Each journal consist of several TCs,
which existed even in the PR era. The editorial decision
to split PR is consistent with the self-organized TCs even
though it was done without classication analysis. We
also see the consistent birth of PRE from PRA in 1993
In SI Sec. III.

More importantly, from the alluvial diagram we can
identity the key interactions between TCs that are cor-
related with important publications.
In Fig. S4 of SI
Sec. IV we showcase one such episode between 1991 and
2000. At the beginning of the decade, we see two PRA-
dominated TCs. Based on the papers they contain, we
can loosely associate one with quantum information (QI)
and trapped atomic ions (BEC), and the other with quan-
tum optics (QO). In 1993, the QI + BEC TC cited many
QO papers, and in 1994, the QO TC cited many QI +
BEC papers. Following this cross-fertilization, the two
TCs merged in 1995, the same year Cornell et al.
[15]
and Ketterle et al.
[16] published their seminal papers
demonstrating BEC in dilute atomic gases. In recogni-
tion of their works, Cornell, Wieman, and Ketterle were
awarded the 2001 Nobel Prize in Physics. The PRA-
dominated TC split after 1996 to give one that is exclu-
sively BEC, and another that is still a combination of QI
+ QO. It was after Zeilinger demonstrated in 1997 exper-
imental quantum teleportation [17] that the QI + QO TC
split into a QI TC and a QO TC. After receiving more
inuence from other PRB-dominated TC, the QO cluster
produced yet another breakthrough paper, in the form of
ultraslow light in hot atomic gases[18]. Without the data
visualization done here, few may suspect the existence of
such connections between BEC, quantum teleportation
and slow light.

From Fig. 2 we see a diversity of inows and outows
from one TC to another: some TCs are derived almost
exclusively from one source, others recei');
INSERT INTO posts (postId,userId,title,body) VALUES (161,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','ve strong contri-
butions from a small number of sources, or weak contri-
butions from a large number of sources. To quantify such
diversity, we construct a forward mixing degree of com-
munity C t
m and backward mixing degree of C t+1
analo-
FIG. 1. (a) Building a BCN (lower) from a citation network
(upper): circles with numbers are papers under consideration,
circles with letters are their references, and numbers on edges
are weights. (b) Topical clusters in year t (left) and in year
t + 1 (right), and their forward (left) and backward (right)
intimacy indices, shown as ows.

the appropriate collective unit of knowledge is a commu-
nity in the BCN instead of a few key papers or a journal.
For the American Physical Society (APS) data set, con-
sisting of about half a million publications between 1893
and 2013 [13], we show in Supplementary Information SI
Sec. I that the BCN edge weights are far more heteroge-
neous than expected from an appropriate null model.
This heterogeneity can be explained by the presence
of communities that we extracted using the modularity-
optimized Louvain method [14]. Using the null model
in SI Sec. I, we show that these communities are statis-
tically signicant. We also test in SI Sec. II how likely
the most common PACS number in a community of n
papers can appear with its observed frequency, within
random collections of n papers. For most communities,
this is highly unlikely, so we conclude that the groupings
of papers extracted are meaningful. We refer to these
validated units of knowledge as topical clusters (TCs).
To study how knowledge evolves, we investigate how
TCs {C t} in year t become {C t+1} in year t + 1. The
papers published in dierent years are distinct, but they
do overlap in their references. Therefore we use this fact
to dene a forward intimacy index I f
mn and a backward
(cid:1)
N (cid:0)Ri , Rt+1
(cid:88)
N (cid:0)Ri , Rt
(cid:1) / L (cid:0)Rt
(cid:1) ;
intimacy index I b
mn :(cid:88)
N (Ri , Rt+1 )
(cid:1) ,
(cid:1) / L (cid:0)Rt+1
N (cid:0)Ri , Rt+1mN (Ri , Rt
m )
I b
N (Ri , Rt )
mn =n(cid:8)C t+1
(cid:9), and we denote the references
to quantify how close C t
m is to C t+1
n . Here the TCs at
t and t + 1 are C t = {C t
u } and C t+1 =
1 , ..., C t
m , ..., C t
, ..., C t+1
n , ..., C t+11

I f
mn =

(1)

2111A1CBEDGFH2341234(a)0.70830.650.29160.3750.06250.150.56250.4512AABC345ACCCDD678AAACF91011CCCDE(b)tt+13

FIG. 2. The alluvial diagram of APS papers from 1965 to 1974. Each block in a column represents a TC and the height of
the block is proportional to the number of papers in the TC. Only communities comprising more than 100 papers are shown.
TCs in successive years are connected by streams whose widths at the left and right ends are proportional to the forward and
backward intimacy indices. The dierent colors in a TC represent the relative contributions from dierent journals.

nian pictures of the evolution of knowledge, where we ex-
pect incremental growth punctuated by abrupt paradigm
shifts. Certainly, at the aggregate level of PR series
of premier physics journals, the number of articles pub-
lished has grown over the years. When we partition these
articles into TCs, we naively expect that some clusters
will grow/shrink because of growing/declining interest in
their topics. From the alluvial diagrams, we realize that
the real picture is far more complex because of recom-
binations between TCs. Therefore, instead of measuring
the growth rates of pure TCs,');
INSERT INTO posts (postId,userId,title,body) VALUES (162,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ',' we need to measure the
growth of recombined TCs. To do this, we assume that
mn / (cid:80)
m to the size of C t+1
the contribution of C t
is proportionalto the size of C t
m and also the normalized forward inti-
mn / (cid:80)
n ) = (cid:80)
n I f
macy index I f
mn , i.e.
L(cid:48) (C t+1
n I f
m )(I f
m L(C t
mn ).
When we plot the predicted sizes L(cid:48) (C t+1
n ) against
the observed size L(C t+1
in Fig. 3, we nd
n )
(L(cid:48) (C t+1
n ), L(C t+1
n )) scattered about about a straight line
with slope with 1.06, which is the annual growth rate of
the number of papers in APS journals. This tells us that
the growth of recombined TCs is also Popperian.
Next, we consider the Kuhnian processes of splitting
and merging. Unlike the prediction above, where we
made use of information from years t and year t + 1, we
would like to predict the splitting and merging of TCs us-
ing information only from year t. Specically, for merging
m(cid:48) n / (cid:80)
mn / (cid:80)
m(cid:48) ) = (cid:80)
events, our ground truth will be the similarity
n(cid:48)(cid:48) I f
mn(cid:48) )(I f
n(cid:48) I f
S (C t
m , C t
n (I f
m(cid:48) n(cid:48)(cid:48) )
(4)
between two TCs in year t, taking on values between 0
and 1. If C t
m and C t
m(cid:48) merge perfectly into a single TC in
year t + 1, S = 1. On the other hand, if the osprings of

(3)

FIG. 3. (a) Plot of observed (y-axis) against predicted (x-
axis) sizes of recombined TCs, showing a linear growth with
slope 1.06 (dashed line). This linear growth is the same for
TCs with (b) high (red) or (c) low (blue) backward mixing
degree.

.

(cid:17)2
(cid:16)
mn / (cid:80)
m = 1  (cid:80)
gous to the Gini-Simpson index [19]:
(cid:1)2
(cid:0)I b
mn / (cid:80)
n = 1  (cid:80)
n(cid:48) I f
M f
I f
mn(cid:48)M b
m(cid:48) I b
m(cid:48) nA TC with low forward/backward mixing degree has
eectively one child/parent, whereas a TC with high
forward/backward mixing degree undergoes/results from
strong splitting/merging. As shown in SI Sec. V, neither
are frequent. It is more common to nd weak mixing be-
tween TCs, which we believe is due to most papers citing
small numbers of papers outside their elds.
At this point, let us recall the Popperian and Kuh-

(2)

1965196619671968196919701971197219731974PRPRLPRAPRBPRCPRDRMP0500100015002000250005001000150020002500(a)0500100015002000250005001000150020002500(b)0500100015002000250005001000150020002500(c)0.160.240.320.400.480.560.640.720.804

the decile of most strongly splitting TCs, increasing the
standardized B by one standard deviation will decrease
M f by about 0.05, whereas for the decile of the least
strongly splitting TCs, there is no obvious trend. In SI
Sec. VII we tested another index measuring a dierent
network aspect of the weight matrix and found the pre-
diction results are similar.
Finally, we want to know the impacts of such merging
and splitting events. We rst check for an increase in

FIG. 5. Proportions of a TCs references published in dierent
years, relative to the year (0) of the TC. The black solid line
is the proportions averaged over all TCs in the 1990s, while
the area shaded gray is up to one standard deviation away
from the mean. Other color lines represent the distribution
of four dierent BEC related TCs.

the number of publications after such events, but found
an insignicant dierence in paper numbers in strongly
and weakly mixing TCs (see ');
INSERT INTO posts (postId,userId,title,body) VALUES (163,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','Fig. 3(b) and (c)). We
suspected this is because our data set in conned to
APS publications, and a more careful check should in-
clude other physics journals to capture any inuence
spillover. We then check the highest, third quartile,
median, average number of citations range two and ve
years after the events, but still see no signicant eects
(SI Sec. VIII). Focusing on the highly productive chain
of knowledge processes that led to experimental real-
izations of BEC, quantum teleportation and slow light,
we checked the citation proles between 1995 and 1998.
While the 1995 BEC+QI+QO TC cited a slightly lower
proportion of 1995 papers than the APS 0-year average,
the 1996 BEC+QI+QO, the 1997 BEC TC, the 1998
BEC TCs all cited signicantly more 0-year papers. The
full eect of this BEC breakthrough can be seen in the
large proportions of 1996 papers cited by the 1997 and
1998 TCs and the proportion of 1997 papers cited by the
1998 TC (see Fig. 5). Indeed, we have provided early evi-
dence suggesting that strongly mixing Kuhnian processes
are associated with greater impact.
We thank Woo-Sung Jung for discussions.

FIG. 4. (left) S (C t
m , C t
m(cid:48) ) of 16 TCs in 1991, computed using
forward intimacy indices going from 1991 to 1992.
(right)
T (C t
m , C t
m(cid:48) ) of the same 16 TCs, using information from 1991
only. We use the same ordering of TCs in both matrices.

C t
m and C t
m(cid:48) are distinct, S = 0. To do prediction using
only information from year t, we dene

m )L(C t
m(cid:48) )/(L(C t
m , C t
m(cid:48) ) = W (C t
m , C t
T (C t
m(cid:48) )),

(5)

where W (C t
m , C t
m(cid:48) ) is the sum of weights of edges be-
tween papers in C t
m and C t
m(cid:48) , normalized against the sizes
of TCs involved. Fig. 4 shows that the two quantities are
highly correlated, with Spearmans rank correlation co-
ecient of about 0.8 (see SI Sec. VI.) A high T between
two TCs means they are likely to merge the next year.
We also tried to predict the splitting events. Here the
situation is more complex: when we examine the weight
matrix of a TC, we may nd a few large subcommunities
or many small subcommunites. Naively, we expect the
criterion for splitting is the opposite to merging, i.e. the
easier it is to tell one subcommunity from others, the
(cid:80)
(cid:80)
A(j1 , j2 )/ (cid:80)
higher the chances for a split. The boundary index
(cid:80)
(cid:80)
A(j1 , j2 )/ (cid:80)
j1Ci1
i1 (cid:54)=i2
ii (cid:54)=i2
j2Ci2
i L(Ci )L(Ci )
j1 ,j2Ci(6)
measures how indistinct the subcommunities are in a TC.
Here A(j1 , j2 ) is the weight of the edge between papers j1
and j2 , and Ci is a subcommunity in the given TC. How-
ever the picture we nd is not as simple as the merging
case. When we plot M f against B , we nd the expected
decreasing trend, but at the same time, the large scatter
makes it impossible to reliably predict a splitting event
using B . To better understand the relationship between
M f and B , we use quantile regression[20] to nd that
the B has no prediction power when M f is small, but
becomes predictive when M f is large. We summarize
these ndings in SI Sec. VII The slopes show that for

L(Ci1 )L(Ci2 )

B =

[1] K. R. Popper, Al l
Press, 1999).
[2] T. S. Kuhn, The Structure of Scientic Revolutions (1st
ed.) (University of Chicago Press, 1962).

life is problem solving (Psychology

[3] D. Wootton, The invention of science: a new history of
the scientic revolution (Penguin UK, 2015).
[4] J. Bollen, H. Van de Sompel, A. Hagberg, L. Bettencourt,
R. Chute, M. A. Rodriguez,
and L. Balakireva, PLoS

0246810121402468101214S(Ctm,Ctm0)0246810121402468101214T(Ctm,Ctm0)0.000.080.160.240.320.400.480.560.640.00000.00150.00300.00450.00600.00750.00900.01050.01200.01359876543210Relative year0%5%10%15%20%25%30%Proportion of refere');
INSERT INTO posts (postId,userId,title,body) VALUES (164,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','ncesAverage1995199619971998ONE 4, e4803 (2009).
[5] T. Kuhn, M. Perc, and D. Helbing, Physical Review X 4,
1 (2014), arXiv:1404.3757.
[6] T. Jia, D. Wang, and B. K. Szymanski, Nature Human
Behaviour 1, 0078 (2017).
[7] P. Chen and S. Redner, Journal of Informetrics 4, 278
(2010), arXiv:0911.0694.
[8] M. Rosvall and C. T. Bergstrom, PLoS ONE 5 (2010),
10.1371/journal.pone.0008694, arXiv:0812.1242.
[9] D. Chavalarias and J. P. Cointet, PLoS ONE 8 (2013),
10.1371/journal.pone.0054847.
[10] M. Kessler, IEEE Transactions on Information Theory 9,
49 (1963).
[11] E. Yan and Y. Ding, Journal of the American Society
for Information Science and Technology 63, 13131326
(2012).
[12] M. H. Huang, L. Y. Chiang, and D. Z. Chen, Sciento-
metrics 58, 489 (2003).
Sets
Data
[13] APS

Research,

for

see

http://journals.aps.org/datasets, .
[14] V. D. Blondel, J.-L. Guillaume, R. Lambiotte,
and
E. Lefebvre, Journal of Statistical Mechanics: Theory and
Experiment 2008, P10008 (2008), arXiv:0803.0476.
[15] M. H. Anderson, J. R. Ensher, M. R. Matthews, C. E.
Wieman, and E. A. Cornell, Science 269, 198 (1995).
[16] K. B. Davis, M. O. Mewes, M. R. Andrews, N. J. van
Druten, D. S. Durfee, D. M. Kurn,
and W. Ketterle,
Physical Review Letters 75, 3969 (1995).
[17] D. Bouwmeester, J.-W. Pan, K. Mattle, M. Eibl, H. We-
infurter, and A. Zeilinger, Nature 390, 575 (1997).
[18] M. M. Kash, V. A. Sautenkov, A. S. Zibrov, L. Hollberg,
G. R. Welch, M. D. Lukin, Y. Rostovtsev, E. S. Fry, and
M. O. Scully, Physical Review Letters 82, 5229 (1999).
[19] L. Jost, Oikos 113, 363 (2006).
[20] J. Sienkiewicz and E. G. Altmann, Royal Society Open
Science 3, 160140 (2016), arXiv:1605.07465.

Knowledge Evolution in Physics Research: An Analysis of Bibliography Coupling
Networks
Supplementary Material

I. NULL MODEL OF BIBLIOGRAPHY COUPLING NETWORK

To determine the statistical signicance of our empirical bibliographic coupling networks (BCNs) (Fig. S1(a)), we
build a null model for comparison. In our null model, we x the out degrees and in degrees of all papers (citing and
cited), but rewire the edges to get an ensemble of articial BCNs (Fig. S1(b)).

FIG. S1. (a) Original citation network and its BCN. (b) A rewired citation network keeping in degrees and out degrees xed
and its BCN. (c) Comparison of the degree and weight distributions of papers published in 1991, between the real BCN and
the null model. (d) Modularities of the best partitions extracted by the Louvain method for the real BCNs and the null model
between 1991 and 2000. Results from null model are averaged over 10 dierent rewirings, and the error bars are much smaller
than the marker size.

Compared to the null model, the real BCN has more high-weight edges. We suspect these are the most meaningful
edges, arising from the papers content. If two papers focus on close topics, they will likely have high chance to have
more than one common reference, and this eect also manifest itself in the degree distribution: the null model has
a atter degree distribution at small degrees because the edges are drawn by chance, whereas in the real BCN this
coupling is based on content, meaning that papers will have edges mostly with papers that are trying to solve the same
problems, so the real BCN will have more low-degree nodes, fewer high-degree nodes compared the null model. The
most prominent feature of this content-sensitive citation is community structure: in the real BCN, papers focussed
on the same topic share more common references with each other than papers focussed on dierent topics, so that
the densities of edges within topics are much higher than between topics. Therefore the modularities of communities
extracted by the Louvain method in the real network is much higher than in the null model, as s');
INSERT INTO posts (postId,userId,title,body) VALUES (165,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','hown in Fig. S1(d).

II. VALIDATION OF BCN COMMUNITIES

To verify that the communities extracted are really focussed on closely related questions, we check the Physics and
Astronomy Classication Scheme (PACS) numbers of members of the communities. Such numbers are provided by
authors to indicate the subelds of physics to which their papers belong. In our case, we only use the rst two digits
of the PACS numbers, as a balance between accuracy and coverage. To test whether the PACS numbers appearing

2111A1CBEDGFH2341234(a)311A1CBEDGFH2341234(b)100101102103Degree10-610-510-410-310-210-1Frequency(c)Real BCNNull model1991199219931994199519961997199819992000Year0.00.20.40.60.81.0Modularity(d)Real BCNNull model051015202530Weight10-510-410-310-210-1100FrequencyReal BCNNull modelin the communities could have occurred by chance, we choose one year t, build its BCN, extracting the community
structure with sizes {s1 , s2 , ..., sn}, and then randomly assign papers in year t into n pseudo-communities of the same
sizes, to remove any potential size eects. For a community of size s, we then identify the largest subset of papers
sharing the same PACS number. This PACS number can represent the subeld of the community to a certain extent,
and the fraction of papers in the largest subset reect the homogeneity of the community. On the other hand, the
largest subset of papers sharing the same PACS number in a random collection of s papers is typically small. Dividing
the sizes of the largest subsets in the empirical communities and in the random collections, we nd ratios are larger
than 1 for most cases. This means that the communities we extracted are meaningful (see Fig. S2).

FIG. S2. Comparison of PACS homogeneity between real BCN communities between 1991 and 2000 with more than 50 papers,
and their corresponding random collections. (a) The red squares correspond to the sizes of the largest subsets of papers sharing
at least one PACS number, nreal , in the empirical communities divided by the same quantity found in the corresponding
random collections, nrand , as a function of the community size s. (b) The fraction of the largest subset of papers sharing at
least one PACS number as a function of s for real communities in the BCN and random collections. For clarity, the small error
bars are not shown in the gures.

III. ALLUVIAL DIAGRAM FOR 1991-2000

In addition to Fig. 2 in the letter, we also plotted an alluvial diagram for 1991 to 2000, showing the splitting
of PRA into PRA and PRE. As we can see from Fig. S3, before 1993, there were several PRA-dominated TCs.
After the split in 1993, some PRA-dominated TCs remained PRA-dominated, whereas other PRA-dominated TCs
became PRE-dominated. This means that even before 1993, papers in PRA were already divided into groups based
on dierent topics, some of which are predecessors of the PRE TCs.

IV. CASE STUDY: QUANTUM OPTICS, QUANTUM INFORMATION AND BOSE-EINSTEIN
CONDENSATION

To illustrate the utility our knowledge evolution framework can oer, we use as a case study the interesting inter-
actions between quantum optics (QO), quantum information (QI), and Bose-Einstein Condensation (BEC). These
three elds experienced breakthroughs in the 1990s. In Fig. S4 we highlight the evolution of TCs which are related
to these three topics and Table S1 shows the three most cited papers in these TCs. Key merging and splitting events
are reported in the main paper, as are important publications these events are correlated with.

V. KNOWLEDGE METABOLISM

Some TCs have more references overlapping with those in');
INSERT INTO posts (postId,userId,title,body) VALUES (166,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ',' the previous year, while other TCs have less. To quantify
evolution of references, we count the sums of the forward and backward intimacy indices. These represent the
percentage of references going to the next year, and the percentage of references inherited from the previous year,
which we think of as the outow and inow respectively. As shown in Fig. S5(a) and (b), most outows and inows

0500100015002000s012345678nreal/<nrand>(a)0500100015002000s0.00.20.40.60.81.0nreal/s or nrand/s(b)Real communityRandom community3

FIG. S3. The alluvial diagram of APS papers from 1991 to 2000. Each block in a column represents a TC and the height of
the block is proportional to the number of papers in the TC. Only communities comprising more than 100 papers are shown.
TCs in successive years are connected by streams whose widths at the left and right ends are proportional to the forward and
backward intimacy indices. The dierent colors in a TC represent the relative contributions from dierent journals.

FIG. S4. The alluvial diagram of APS papers from 1991 to 2000, where we colored only TCs highly related to quantum optics,
quantum information and Bose-Einstein condensation.

are distributed within a narrow range, but there are exceptional cases as well: such as a single peak in Fig. S5(b),
whose references overlap signicantly less than normal with the previous year. In the context of birth, death, growth,
decay, split, and merge knowledge processes, we are inclined to call this event in 1993 the birth of a TC. Further
analysis shows that most common PACS codes are: 03 (Quantum mechanics, eld theories, and special relativity),
42 (Optics) and 63 (Lattice dynamics). Looking at the references of this TC, we nd that most of these comes from
1990, 3 year before. This interesting phenomenon is therefore more appropriately identied as a sleeping beautify[S1].
Every year, physicists absorb new references and drop old references as their elds progress. Although this
metabolism dier from TC to TC, the whole process is quite stable over all TCs, as shown in Fig. S5(c) and
(d). This universal curve can be used as a benchmark for the test of scientic impact, as we have done in Fig. 5 of
the letter.
As we see from Fig. S3, there is a great diversity of processes acting on the TCs: some TCs are derived almost
exclusively from one source, others receive strong contributions from a small number of sources, or weak contributions
from a large number of sources. To quantify such diversity, we introduce forward mixing degree and backward mixing
degree (Eq. 2 in the letter) As shown in Fig. S5(e), (f ), strong splitting/merging or almost isolated development are

1991199219931994199519961997199819992000PRLPRAPRBPRCPRDPRERMP19911992199319941995199619971998199920004

FIG. S5. The metabolic analysis of APS papers in the 1990s. (a) The distribution of outows of TCs. (b) The distribution
of inows of TCs. (c) Proportions of APS papers references published in dierent years. (d) Proportions of APS papers
references published in dierent years, relative of the year (0) of publication. (e) The distribu');
INSERT INTO posts (postId,userId,title,body) VALUES (167,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','tion of forward mixing degree of
TCs. (d) The distribution of backward mixing degree of TCs.

rare, and in most case, TCs undergo weak information exchange.

VI. PREDICTION OF MERGING

As mentioned in the letter, we found that the inter-TC connection is highly correlated with their mixing the next
year. High T (C t
m , C t
m(cid:48) ) leads with a large probability to a high S (C t
m , C t
m(cid:48) ). Analyzing APS papers in the 1990s, we
found a Spearmans rank coecient of 0.804 between T (C t
m , C t
m(cid:48) ) and S (C t
m , C t
m(cid:48) ) over all TCs (with at least 100
papers). However, because the average Pearson correlation coecient is only 0.504, such a relation is not linear (see
Fig. S6)

0.00.20.40.60.81.0Outflow 0% 5%10%15%20%25%30%Frequency(a)0.00.20.40.60.81.0Inflow 0% 5%10%15%20%25%30%35%Frequency(b)19801985199019952000Year 0% 5%10%15%20%25%Proportion of references(c)19911992199319941995199619971998199920009876543210Relative year 0% 5%10%15%20%25%Proportion of references(d)19911992199319941995199619971998199920000.00.20.40.60.81.0Forward mixing degree 0% 5%10%15%20%25%Frequency(e)0.00.20.40.60.81.0Backward mixing degree 0% 5%10%15%20%25%Frequency(f)5

m , C t
m(cid:48) ) and S (C t
m , C t
FIG. S6. The scatter plot between T (C t
m(cid:48) ) among all TCs (with at least 100 papers) in 1990s.

VII. HIERARCHY STRUCTURE OF COMMUNITY AND SPLITTING ANALYSIS

To predict a splitting event, we check the weight matrix of dierent TCs. Naively, we expect the components
that lead on to dierent TCs the next year to already form distinct subcommunities this year. However, when we
use the dendrogram extracted from the Louvain method to identify subcommunities, we found that dierent TCs
have dierent internal structures (see Fig. S7): some have a few large subcommunities, while others have many small
subcommunities.
Assuming that splitting is the time reversal of merging, i.e. two distinct subcommunities becoming two distinct
communities in the next year, we devise the boundary index B to measure how indistinct the subcommunities are
in a TC. Quantile regression (QR) shows that the relation between B and M f depends on the decile, as shown in
Fig. S8(a), (b). For dierent decile, the relation is dierent.
(cid:88)
We also try with fragmentation index
i:j [i]

wiS 2
j [i]

F =

(S1)

where wi is the size fraction of the top level subcommunity i, sj [i] is the relative size fraction of subsubcommunity j
inside subcommunity i. The more fragmentation a community is, the closer F is to 0. Quantile regression between F
and M f gives very similar results as B and M f , that is, for the decile of most strongly splitting TCs, increasing the
standardized F by one standard deviation will decrease M f by about 0.06, whereas for the decile of the least strongly
splitting TCs, there is no obvious trend as  close to 0, as shown in Fig. S8(c), (d).

VIII. CORRELATION ANALYSIS BETWEEN CITATION AND MIXING

When we think of high-impact research, we think of highly-cited papers. Therefore, to quantify the impact of
strongly-splitting events in the alluvial diagrams, we counted the citations of TCs resulting from splittings. As shown
in Fig. S9, we did this for number of citations 2 years after the events, and also 5 years after the events. There were
no obvious trends. The results of backward mixing degree, i.e. merging, are similar.

[S1] Q. Ke, E. Ferrara, F. Radicchi, and A. Flammini, Proceedings of the National Academy of Sciences of the United States
of America 2015, 40 (2015), arXiv:1505.06454.

0.0000.0020.0040.0060.0080.0100.0120.0140.016T(Ctm,Ctm0)0.00.10.20.30.40.50.60.7S(Ctm,Ctm0)0.0000.0010.0020.000.020.040.060.080.106

1991

1992

1993

10.1103/PhysRevA.44.5674

Lower

Upper

1994

1995

1996

1997

Upper

Lower

Upper

Lower

Upper

Lower

10.1103/PhysRevLett.70.1244

10.1103/PhysRevLett.70.1895

10.1103/PhysRevLett.69.3314

TABLE S1. The three most cited pap');
INSERT INTO posts (postId,userId,title,body) VALUES (168,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','ers in quantum optics, quantum information theory, quantum computation and Bose-
Einstein condensation related TCs.
Year TC
DOI
Title
10.1103/PhysRevLett.67.661 Quantum cryptography based on Bells theorem
10.1103/PhysRevLett.66.2593 Observation of electromagnetically induced transparency
10.1103/PhysRevLett.67.1855 Enhancement of the index of refraction via quantum coherence
Above-surface neutralization of highly charged ions: The classical over-the-barrier
model
10.1103/PhysRevB.43.13401 Strong magnetic x-ray dichroism in 2p absorption spectra of 3d transition-metal ions
10.1103/PhysRevLett.66.2601 Dynamic stabilization of hydrogen in an intense, high-frequency, pulsed laser eld
10.1103/PhysRevLett.69.2881 Communication via one- and two-particle operators on Einstein-Podolsky-Rosen states
Observation of the coupled exciton-photon mode splitting in a semiconductor quantum
microcavity
10.1103/PhysRevLett.68.580 Wave-function approach to dissipative processes in quantum optics
10.1103/PhysRevLett.68.1943 X-ray circular dichroism as a probe of orbital magnetization
10.1103/PhysRevLett.68.3535 High-order harmonic generation from atoms and ions in the high intensity regime
10.1103/PhysRevLett.69.1383 Absorption of ultra-intense laser pulses
Teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen
channels
10.1103/PhysRevA.47.4114 Threshold and resonance phenomena in ultracold ground-state collisions
Measurement of the Wigner distribution and the density matrix of a light mode using
optical homodyne tomography: Application to squeezed states and the vacuum
10.1103/PhysRevLett.71.1994 Plasma perspective on strong eld multiphoton ionization
10.1103/PhysRevLett.70.1599 Above threshold ionization beyond the high harmonic cuto
10.1103/PhysRevLett.70.774 High-order harmonic generation in rare gases with a 1-ps 1053-nm laser
10.1103/PhysRevA.50.67
Squeezed atomic states and pro jection noise in spectroscopy
10.1103/PhysRevLett.72.3439 Statistical distance and the geometry of quantum states
10.1103/PhysRevLett.73.58 Experimental realization of any discrete unitary operator
10.1103/PhysRevA.49.2117 Theory of high-harmonic generation by low-frequency laser elds
10.1103/PhysRevLett.73.1227 Precision Measurement of Strong Field Double Ionization of Helium
10.1103/PhysRevA.50.1540 Modeling harmonic generation by a zero-range potential
10.1103/PhysRevLett.75.3969 Bose-Einstein Condensation in a Gas of Sodium Atoms
10.1103/PhysRevLett.74.4091 Quantum Computations with Cold Trapped Ions
10.1103/PhysRevA.52.R2493 Scheme for reducing decoherence in quantum computer memory
10.1103/PhysRevA.54.3824 Mixed-state entanglement and quantum error correction
10.1103/PhysRevLett.77.1413 Separability Criterion for Density Matrices
10.1103/PhysRevLett.77.2360 Collective Excitations of a Trapped Bose-Condensed Gas
10.1103/PhysRevLett.78.985 Bose-Einstein Condensation of Lithium: Observation of Limited Condensate Number
10.1103/PhysRevLett.78.586 Production of Two Overlapping Bose-Einstein Condensates by Sympathetic Cooling
10.1103/PhysRevLett.78.5 Demonstration of the Casimir Force in the 0.6 to 6mum Range
10.1103/PhysRevLett.78.5022 Entanglement of a Pair of Quantum Bits
Quantum State Transfer and Entanglement Distribution among Distant Nodes in a
Quantum Network
10.1103/PhysRevLett.79.3306 Noiseless Quantum Codes
10.1103/PhysRevLett.81.3108 Cold Bosonic Atoms in Optical Lattices
Atomic Scattering in the Presence of an External Connement and a Gas of
Impenetrable Bosons
10.1103/PhysRevLett.81.742 Spinor Bose Condensates in Optical Traps
10.1103/PhysRevA.57.120 Quantum computation with quantum dots
10.1103/PhysRevLett.80.2245 Entanglement of Formation of an Arbitrary State of Two Qubits
Quantum Repeaters: The Role of Imperfect Local Operations in Quantum
Communication
10.1103/Phy');
INSERT INTO posts (postId,userId,title,body) VALUES (169,4821,'Wenyuan Liu (),1 Andrea Nanetti,2 and Siew Ann Cheong1 ,  1School of Physical and Mathematical Sciences, Nanyang Technological ','sRevLett.83.2498 Vortices in a Bose-Einstein Condensate
10.1103/PhysRevLett.83.5198 Dark Solitons in Bose-Einstein Condensates
10.1103/PhysRevLett.82.1975 Entanglement of Atoms via Cold Controlled Collisions
10.1103/PhysRevLett.83.4204 Quantum Information Processing Using Quantum Dot Spins and Cavity QED
10.1103/PhysRevB.59.2070 Coupled quantum dots as quantum gates
10.1103/PhysRevLett.82.2417 Dynamical Decoupling of Open Quantum Systems
Ultraslow Group Velocity and Enhanced Nonlinear Optical Eects in a Coherently
Driven Hot Atomic Gas
10.1103/PhysRevLett.83.2845 Transmission Resonances on Metallic Gratings with Very Narrow Slits
10.1103/PhysRevLett.83.967 Liquid-Crystal Photonic-Band-Gap Materials: The Tunable Electromagnetic Vacuum
10.1103/PhysRevLett.84.806 Vortex Formation in a Stirred Bose-Einstein Condensate
10.1103/PhysRevLett.85.1795 Stable 85Rb Bose-Einstein Condensates with Widely Tunable Interactions
10.1103/PhysRevLett.85.3745 Regimes of Quantum Degeneracy in Trapped 1D Gases
10.1103/PhysRevA.62.062314 Three qubits can be entangled in two inequivalent ways
10.1103/PhysRevLett.84.2722 Inseparability Criterion for Continuous Variable Systems
Electron-spin-resonance transistors for quantum computing in silicon-germanium
heterostructures
10.1103/PhysRevLett.85.5214 Double Resonant Raman Scattering in Graphite
10.1103/PhysRevLett.85.154 Electronic Structure of Deformed Carbon Nanotubes
10.1103/PhysRevB.62.13104 Carbon nanotubes, buckyballs, ropes, and a universal graphitic potential

10.1103/PhysRevLett.78.3221

10.1103/PhysRevLett.81.5932

10.1103/PhysRevLett.82.5229

Upper

Middle

10.1103/PhysRevA.62.012306

Lower

Upper

10.1103/PhysRevLett.81.938

Upper

Lower

Upper

1998

Lower

1999

2000

Middle

Lower

7

FIG. S7. Adjacency matrices of TCs in the 1990s. The blue lines indicate the boundaries of subsubcommunities, the red lines
indicate the boundaries of subcommunities. The red lines are absent from some plots because such TC have only one level
when the Louvain algorithm terminated.

0501001502002503003500501001502002503003500200400600800100002004006008001000010020030040001002003004000100200300400500600700800010020030040050060070080001002003004005000100200300400500020040060080010000200400600800100001002003004005006000100200300400500600010020030040050060070001002003004005006007000500100015000500100015008

FIG. S8. Relation between boundary index, fragmentation index and forward mixing degree of TCs in 1980s and 1990s. (a)
Each dot corresponds to one TC, dash lines show QR results for quantiles  = 0.1, 0.2, ..., 0.9. (b)  coecients (slopes of QR in
the (a) as a function of  . The red arrows show low   ( = 0.1), half   ( = 0.5) and top   ( = 0.9), as, respectively,
the nock, a circle on the shaft, and the head of the arrow, the blue solid line represents 0. (c) Each dot corresponds to one TC,
dash lines show QR results for quantiles  = 0.1, 0.2, ..., 0.9. (d)  coecients (slopes of QR in the (c) as a function of  . The
red arrows show low   ( = 0.1), half   ( = 0.5) and top   ( = 0.9), as, respectively, the nock, a circle on the shaft,
and the head of the arrow, the blue solid line represents 0.

3210123456Standardized boundary index0.00.20.40.60.81.0Forward mixing degree(a)0.00.20.40.60.81.0tophalflow-0.03-0.06(b)321012345Standardized fragmentation index0.00.20.40.60.81.0Forward mixing degree(c)0.00.20.40.60.81.0tophalflow-0.03-0.06(d)9

FIG. S9. The scatter plot between dierent citations received during 2 years and forward mixing degree among all TCs (with
at least 100 papers) in 1990s. (a) Highest citation, (b) Third quartile citation, (c) Median citation, (d) Mean citation.

050100150200250300350400450(a)Highest citation02468101214(b)3rd quartile citation0.00.20.40.60.81.001234567(c)Median citation0.00.20.40.60.81.00246810(d)Mean citationForward mixing degreeCitation');
INSERT INTO posts (postId,userId,title,body) VALUES (170,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES','CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

AB STRAC T. We introduce a generalization of the celebrated Lov asz theta number of a
graph to simplicial complexes of arbitrary dimension. Our generalization takes advantage
of real simplicial cohomology theory, in particular combinatorial Laplacians, and provides
a semidenite programming upper bound of the independence number of a simplicial com-
plex. We consider properties of the graph theta number such as the relationship to Hoff-
mans ratio bound and to the chromatic number and study how they extend to higher dimen-
sions. Like in the case of graphs, the higher dimensional theta number can be extended to a
hierarchy of semidenite programming upper bounds reaching the independence number.
We analyse the value of the theta number and of the hierarchy for dense random simplicial
complexes.

1. IN TRODUC T ION
The theta number (G) of a graph G was introduced by L. Lov asz in his seminal paper
[32], in order to provide spectral bounds of the independence number and of the chromatic
number of G. In modern terms, (G) is the optimal value of a semidenite program, and
as such is computationally easy; in contrast, the independence number (G) and the chro-
matic number (G) are difcult to compute. These graph invariants satisfy the following
inequalities, where G denotes the complement of G:
(G)  (G)  (G).
(1)
The inequality (G)  (G) was one of the main ingredients in Lov asz proof of the
Shannon conjecture on the capacity of the pentagon [32]. More generally, this inequality
plays a central role in extremal combinatorics, sometimes in a disguised form: to cite a few,
the Delsarte linear programming method in coding theory [8] and recent generalizations
of Erd os-Ko-Rado theorems [7, 12, 13] can be interpreted as instances of this inequality.
Analogs of the theta number in geometric settings have lead to many advances in packing
problems (see [36] and references therein), in particular the very recent solutions to the
sphere packing problems in dimensions 8 and 24 [5, 40].
Our aim in this paper is to generalize this graph parameter to higher dimensions, in
the framework of simplicial complexes. Let us recall that an (abstract) simplicial complex
X on a nite set V is a family of subsets of V called faces that is closed under taking
subsets. We refer to Section 1 for basic denitions and results about simplicial complexes.
Graphs t in this framework, being simplicial complexes of dimension 1. In recent years,
considerable work has been devoted to generalizing the classical theory of graphs to this
higher-dimensional setting. Much of the efforts have focused on the notion of expansion
(see, e.g., [9, 15, 20, 27, 33, 38]), but other natural concepts such as random walks [37],
trees [11, 26], planarity [35], girth [10, 34], independence and chromatic numbers [14, 19]
have been extended to higher dimensions. Some of these notions were introduced and
studied previously in the context of hypergraphs. Pure k-dimensional simplicial complexes

Date: April 7, 2017.

2

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

are essentially (k + 1)-uniform hypergraphs, but the topological point of view brings the
machinery of algebraic topology such as homology theory to the subject.
The familiar graph-theoretic notions of independence number and of chromatic number
extend in a natural way to this setting: For a k-dimensional simplicial complex X , an
independent set is a set of vertices that does not contain any maximal face of');
INSERT INTO posts (postId,userId,title,body) VALUES (171,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 2)',' X , and
the independence number (X ) is the maximal cardinality of an independent set. The
chromatic number1 (X ) is the least number of colors needed to color the vertices so that
no maximal face of X is monochromatic, in other words, it is the smallest number of parts
of a partition of the vertices into independent sets.
In order to dene the theta number k (X ) of a pure k-dimensional simplicial complex
X , we will follow an approach that leads in a natural way to the inequality (X )  k (X ).
The main idea is to associate to an independent set S a certain matrix, and then to design
a semidenite program that captures as many properties of this matrix as possible. The
matrix that we associate to an independent set is (up to a multiplicative factor) a submatrix
of the down-Laplacian of the complete complex. In the case of dimension 1, the down-
Laplacian is simply the all-one matrix, and we end up with one of the many formulations
of the Lov asz theta number.
Our rst task will be to compare k (X ) to the eigenvalue upper bound of (X ) proved
by Golubev in [19]. This upper bound involves for 0  i  k  1, the largest eigenvalues
(cid:19)
(cid:18)
i of the i-th up-Laplacians of X and the minimal degrees di of the i-faces of X :
1  (d0 + 1)(d1 + 2) . . . (dk2 + k  1)dk1
(X )  n
(2)0 . . . k1
When every possible (k  1)-face is contained in at least one k-face, i.e., when X has a
(cid:18)
(cid:19)
complete (k  1)-skeleton, this inequality simplies to
1  dk1
(X )  n
k1
(cid:19)
(cid:18)
and can thus be seen as a natural generalization of the celebrated ratio bound for graphs
attributed to Hoffman (see, e.g., [4, Theorem 3.5.2]). In that case, we will show that
1  dk1
k (X )  n
k1
therefore k (X ) provides an upper bound of (X ) that is at least as good as (3).
In
the case of a non-complete (k  1)-skeleton, Golubevs bound and k (X ) turn out to be
incomparable, as we will see in examples below.
The theta number of a graph has many very nice properties; some of them, although
unfortunately not all of them, can be generalized to higher dimensions. Most of this paper
is devoted to determining which of the properties of the graph theta number extend to our
notion of the theta number of simplicial complexes.
The relationship to the chromatic number generalizes only partially. Indeed, the in-
equality (X )  k (X ) immediately leads to the inequality n/k (X )  (X ). How-
ever, in the case of graphs, the stronger inequality (G)  (G) holds. We will see that
its natural analog in the setting of k-complexes would be that k (X )  k(X ) and that
this inequality does not hold in general. Instead, we will introduce an ad hoc notion of
chromatic number for simplicial complexes, denoted k (X ), and show that the inequality
k (X )  k (X ) holds. While (X ) is dened using vertex colorings, the denition of
1In the study of hypergraphs, the chromatic number (X ) is also known as the weak chromatic number while
(X1 ), the chromatic number of the 1-skeleton, is known as the strong chromatic number.

(3)

THE THETA NUMBER OF SIMPLICIAL COMPLEXESk (X ) is based on colorings of (k  1)-faces respecting orientations. Moreover, it is tightly
related to a notion of homomorphisms between pure k-dimensional simplicial complexes
that we introduce and that may be of interest by itself.
A very interesting benet of the theta number of a graph is that it is possible to expand
it into hierarchies of semidenite upper bounds of the independence number; Lasseres
hierarchy based on polynomial optimization principles is one of the m');
INSERT INTO posts (postId,userId,title,body) VALUES (172,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 3)','ost popular (see
[29, 30]). We will see that a similar situation holds in higher dimensions: to a pure k-
dimensional complex X we will associate a sequence (cid:96) (X ) for (cid:96) = k , . . . , (X ) such
that
(X ) = (X ) (X )      (cid:96) (X )      k (X )  k (X ).
In order to dene (cid:96) (X ), we will proceed in two steps: in a rst step, we dene a natural
sequence (cid:96) (X ) for (cid:96) = k , k + 1, . . . , (X ); in a second step, we modify the denition of
(cid:96) (X ) slightly in such a way that the sequence of its values decreases.
Our last results concern the theta number of random simplicial complexes X k (n, p)
from the model proposed by Linial and Meshulam in [31]. This model is a higher-di-
We show that k (X k (n, p)) is of the order of (cid:112)(n  k)(1  p)/p for probabilities p
mensional analog of the Erd os-R enyi model G(n, p) for random graphs and has gained
increasing attention in recent years (see [25] for a survey).
such that c0 log(n)/n  p  1  c0 log(n)/n for some constant c0 . This result extends
the known estimates for the value of the theta number of the random graph G(n, p).
The paper is organized as follows: Sections 2 and 3 recall basic denitions and prop-
erties of simplicial complexes and semidenite programming. Section 4 recalls proper-
ties of the theta number of a graph that serve as a guideline for the theta number of a
k-dimensional simplicial complex, which is introduced in Section 5. Section 6 computes
the theta number of certain basic families of 2-dimensional simplicial complexes. Section
7 discusses chromatic numbers and Section 8 the hierarchy of theta numbers. The nal
Section 9 contains the analysis of the theta number of random simplicial complexes.
Let V = {v1 , . . . , vn} be a nite set. We will use the notation (cid:0)V
(cid:1) for the set of k-
2. S IM P L IC IA L COM P LEX E Ssubsets of V . Let us recall that an (abstract) simplicial complex X on a vertex set V is a
family of subsets of V (called the faces of X ), such that if F  X , then all subsets of F
also belong to X . The dimension of a face F  X is |F |  1, and we denote by Xi the set
of i-dimensional faces of X , with the convention X1 = {}. Note that we do not require
every element in V to be a 0-face of X , so X0 can be a proper subset of V . The i-skeleton
of X is the simplicial complex X1  X0      Xi .
A simplicial complex X is said to be of dimension k  0, if k is the maximal dimension
of any of its faces. For example, a graph is a simplicial complex of dimension 1. Going
back to the general case, if X is of dimension k , and if moreover all maximal (with respect
to inclusion) faces of X are of dimension k , then X is said to be pure. Unless explicitly
mentioned, we will only consider pure complexes.
A basic example of a pure k-dimensional simplicial complex is the complete k-complex
n , whose faces are all the subsets of [n] = {1, . . . , n} that have at most (k + 1) elements.
K k
We note that in order to dene a pure simplicial complex of dimension k , it is enough
to specify its set of k-dimensional faces. In particular, the complementary complex X of a
pure simplicial complex of dimension k , is again a pure simplicial complex of dimension
k , whose k-dimensional faces are those (k + 1)-subsets of V that do not belong to Xk

4

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

(we adopt the convention that the empty complex, whose set of faces is empty, is pure of
dimension k for all k  0).
Let X be a simplicial complex; we assume that every face of X is endowed with an
orientation, i.e., a local ordering of its vertices. Then, if F  Xi and K  Xi1 , an
oriented incidence number [F : K ]  {0, 1} can be dened. Often, the orient');
INSERT INTO posts (postId,userId,title,body) VALUES (173,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 4)','ation
(cid:26) (1)j
of the faces is induced by a global ordering of the vertex set V ; in that case, if F =
{x0 , x1 , . . . , xi } where x0 < x1 <    < xi with respect to this ordering,
if K  F and F \ K = {xj },
[F : K ] =
otherwise.The vector space of functions from Xi to R is denoted by C i (X ; R) and its elements
are called i-dimensional cochains of X with coefcients in R. The coboundary map i :
(cid:88)
C i (X ; R)  C i+1 (X ; R) is dened for 1  i < dim(X ) by
[H : F ]f (F ).
(i f )(H ) =
F Xi
The image of i1 is the subspace B i (X ; R) of i-dimensional coboundaries, and the kernel
of i is the subspace Z i (X ; R) of i-dimensional cocycles. Because the coboundary maps
satisfy i  i1 = 0, we have B i (X ; R)  Z i (X ; R). The quotient group
H i (X ; R) := Z i (X ; R)/B i (X ; R).
spaces C i (X ; R) are endowed with the standard inner product (cid:104)f , g(cid:105) = (cid:80)
is then called the i-th cohomology group of X with coefcients in R.
Analogously, we can dene the homology groups of a simplicial complex. For this, the
f (F )g(F )
F Xi
: C i+1 (X ; R)  C i (X ; R) is dened as the adjoint of
and the boundary map i+1 = 
(cid:88)
the coboundary map i . We have, for F  Xi ,(i+1 f )(F ) =
HXi+1
The spaces of boundaries Bi (X ; R) := im i+1 and of cycles Zi (X ; R) := ker i are
subspaces of C i (X ; R) satisfying Bi (X ; R)  Zi (X ; R) and thus dene the i-th reduced
homology group of X

Hi (X ; R) := Zi (X ; R)/Bi (X ; R).
Moreover, by duality we have that Zi (X ; R) = B i (X ; R) and Z i (X ; R) = Bi (X ; R) .
The following diagram summarizes these linear maps for 0  i  dim(X )  1:
i1
/ C i1 (X ; R)
/ C i (X ; R)
C i+1 (X ; R)

[H : F ]f (H ).

i+1



i and i-th down-Laplacian L
The i-th up-Laplacian L
i of X are the following self-
adjoint and positive semidenite operators on C i (X ; R):


i := i1i , Li := i+1 i .





i = 0. Furthermore, it is not hard to see that ker L
By denition, L
i = L
i L
i L
i =



i = B i (X ; R), ker L
i = Z i (X ; R), and im L
Zi (X ; R), im L
i = Bi (X ; R). For
Hi (X ; R) := Zi (X ; R)  Z i (X ; R),
we have the Hodge decomposition of C i (X ; R) into pairwise orthogonal subspaces
C i (X ; R) = Hi (X ; R)  B i (X ; R)  Bi (X ; R).
o/o
THE THETA NUMBER OF SIMPLICIAL COMPLEXES
In particular, Hi (X ; R) (cid:39) H i (X ; R) (cid:39) Hi (X ; R).
The characteristic functions eF of faces F  Xi are called elementary cochains; they
form an orthonormal basis of C i (X ; R). In order to express the matrices of the Laplacian
operators in this basis we introduce the following notation: for F  Xi , let deg(F ) denote
the degree of F , i.e., the number of (i + 1)-faces of X that contain F . For (F , F (cid:48) )  X 2
i ,
such that |F  F (cid:48) | = i, let

and


i )F,F (cid:48) =
(L

F,F (cid:48) := [F : F  F (cid:48) ][F (cid:48) : F  F (cid:48) ].
We note that, if F  F (cid:48)  Xi+1 , we can express F,F (cid:48) also as
F,F (cid:48) = [F  F (cid:48) : F ][F  F (cid:48) : F (cid:48) ].
(cid:26) i + 1
i , such that |F  F (cid:48) | (ci');
INSERT INTO posts (postId,userId,title,body) VALUES (174,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 5)','d:54)= i, we set F,F (cid:48) = 0. Then, it is easy to see that
For (F , F (cid:48) )  X 2
if F = F (cid:48)

i )F,F (cid:48) =
(L
 deg(F )
otherwise
F,F (cid:48)
if F = F (cid:48)
if F  F (cid:48)  Xi+1
F,F (cid:48)
otherwisewhere we use the same notations for the operators and for their matrices in the basis of
elementary cochains.
Example 2.1. In the case of the simplicial complex associated to a graph G = (V , E ),

dened by X1 = {}, X0 = V and X1 = E , we nd that L
0 = J is the all-ones matrix

0 is equal to the combinatorial Laplacian L = D A where D is the diagonal matrix
and L
with the degrees of the vertices as diagonal elements and A is the adjacency matrix of the
graph.
n , and for 0  i  k  1, it is easy to verify
Example 2.2. For the complete k-complex K k
that


i + Li = nI .






of these operators gives the multiplicities of this eigenvalue, namely (cid:0)n1
(cid:1) for L
i .
i and that (L
Together with the property L
i = 0, we obtain that (L
i )2 = nL
i )2 = nL
i L
(cid:0)n1
(cid:1) for L
So n is the only non zero eigenvalue of the up and down Laplacians. Computing the traces

i and
(cid:18)n  1
(cid:19)
i . So we have
i+1


i  nI ) = im(L
(cid:18)n  1
(cid:19)i ) = Bi ,
ker(L
dim(Bi ) =
i + 1
and, as these dimensions add up to (cid:0) n
(cid:1) = dim(C i ), Hi = {0}.


i  nI ) = im(L
dim(B i ) =
i ) = B i ,
ker(Li+1
We conclude this section by recalling the denition of the adjacency matrix of a k-

k1 = D  A where D
(cid:26) F,F (cid:48)
dimensional simplicial complex X : it is the matrix A such that L
is the diagonal matrix encoding the degrees of the (k  1)-faces. In other words,
if F  F (cid:48)  Xk
otherwiseWe note that in dimension 1 this denition coincides with the usual notion of the adjacency
matrix of a graph.

AF,F (cid:48) =

6

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

3. S EM IDE FIN I TE PROGRAMM ING
In this section, we gather basic facts about semidenite programs. For further informa-
tion we refer to standard references such as [2], [3], [39].
Semidenite programs (SDP for short) are special cases of convex optimization pro-
grams that admit efcient algorithms, such as algorithms based on the so-called interior
point method. They generalize linear programs and have turned out to be very useful for
providing polynomial time approximations of hard problems in many areas, especially in
combinatorics (see, e.g., [18] and [1, Chapter 6]).
For a matrix A  Rnn we say that A is positive semidenite, denoted by A (cid:23) 0, if
A is real-valued, symmetric, and if all its eigenvalues are nonnegative. If moreover none
of its eigenvalues are equal to zero, A is positive denite (A (cid:31) 0). The set of all positive
semidenite matrices is a cone denoted by Rnn(cid:23)0 . The space of real symmetric matrices is
endowed with the standard inner product (cid:104)A, B (cid:105) = trace(AB ).
Given (c1 , . . . , cm )  Rm and symmetric matrices A0 , . . . , Am of size n, the following
optimization problem is a semidenite program in primal form:
p = sup{(cid:104)A0 , Z (cid:105) : Z  Rnn(cid:23)0 , (cid:104)Ai , Z (cid:105) = ci for all 1  i  m}.
In other words, this program asks for the supremum of a linear form, where this supre-
mum is taken over the intersection of the cone of positive semidenite matrices with an
afne space.
A feasible ');
INSERT INTO posts (postId,userId,title,body) VALUES (175,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 6)','solution of this program is a matrix Z that satises the required constraints:
Z  Rnn(cid:23)0
and (cid:104)Ai , Z (cid:105) = ci . It is an optimal solution if its objective value (cid:104)A0 , Z (cid:105) is
equal to p . If there is no feasible solution, we let p = .
The following dual program is attached to the primal program:
d = inf {c1x1 +    + cmxm : (x1 , . . . , xm )  Rm , A0 + x1A1 +    + xmAm (cid:23) 0}.
The terms primal and dual do not refer to a specic class of programs: Despite their
apparent difference, any of these programs can be put in the form of the other, and, as
expected, dualizing twice returns the initial program.
The inequality p  d , referred to as weak duality, always holds, and under some mild
conditions even strong duality, i.e., p = d , holds. Strong duality is guaranteed if the
SDP satises the so-called Slaters conditions, of which we will use the following version:
If an SDP has a strictly feasible primal solution, i.e., if there is a feasible solution Z of
the primal program such that Z (cid:31) 0, and a strictly feasible dual solution, i.e., there exists
(x1 , . . . , xm ) such that A0 + x1A1 +    + xmAm (cid:31) 0, then strong duality holds and,
moreover, there are optimal solutions for both the primal and the dual program.

4. TH E TH ETA NUMB ER O F A GRA PH
In this section, we introduce the theta number of a graph G = (V , E ). Our presentation
will serve as a guideline for the generalization to higher dimensional simplicial complexes.
Let S be an independent set of G, i.e., a subset of V not containing any edges. The set
S naturally denes a vector 1S  RV , namely its characteristic vector. We consider the
(cid:26) 0
S , whose entries are given by:
matrix Y S := 1S 1T
if {v , v (cid:48)} (cid:42) S
otherwise.The following properties of Y S motivate the denition of (G): Y S is a positive semidef-
v ,v (cid:48) = 0 if {v , v (cid:48)}  E . Furthermore, the cardinality of S can be
inite matrix such that Y S

Y S
v ,v (cid:48) =

THE THETA NUMBER OF SIMPLICIAL COMPLEXES

recovered in two different ways from Y S : If I and J stand as usual for the identity matrix
and the all-ones matrix, we have (cid:104)I , Y S (cid:105) = |S | and (cid:104)J, Y S (cid:105) = |S |2 . So, if we set
(4) (G) = sup{(cid:104)J, Y (cid:105) : Y  RV V , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1, Yv ,v (cid:48) = 0 if {v , v (cid:48)}  E }
the matrix |S |1Y S is feasible for (4) and we get that |S |  (G).
Because (4) is a semidenite program, its optimal value (G) can be approximated
numerically up to arbitrary precision in polynomial time in the size of G. If, instead of a
sharp numerical value, one aims for a rougher upper bound of (G), the dual formulation
of (4) is often more convenient:
(G) = inf {max (Z ) : Z  RV V , Z = J + T , Tv ,v (cid:48) = 0 if {v , v (cid:48) } / E }.
Here, max (Z ) denotes the largest eigenvalue of Z .
To illustrate this principle we consider a classical example. For any matrix T such
that Tv ,v (cid:48) = 0 for all {v , v (cid:48)} / E , the dual formulation of (G) provides the inequality
(G)  max (J + T ). A possible choice for T is a multiple of the adjacency matrix A of
G, say T = tA. The best bound is obtained for t minimizing max (J + tA). For d-regular
graphs, the matrices J and A commute, so the eigenvalues of J + tA are easy to analyze.
The optimal choice of t then leads to the so-called ratio bound attributed to Hoffman (see,
e.g., [4, Theorem 3.5.2]):

(5)

(6)

(G)  |V |min (A)
d  min (A)

5. TH E TH ETA NUMB ER O F A S IM PL IC IA L COM PL EX
We now move to higher dimensions and dene the theta number of a k-dimensional

k1 is the matrix indexed by (cid:0)V
(cid:1) that is dened by:
simplicial complex X . As suggested in the introduc');
INSERT INTO posts (postId,userId,title,body) VALUES (176,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 7)','tion, the down-Laplacian L
k1 of the
complete complex K k
n will play the role of the all-ones matrix J in (4) and (5). Recall that

(cid:26) kk
if F = F (cid:48)

k1 )F,F (cid:48) =
(L
otherwise
F,F (cid:48)
We note that this matrix may not be the down-Laplacian of the complex X . Obviously,
this is the case if and only if X has a complete (k  1)-skeleton, otherwise the down-

k1 . From now on, to avoid confusion, we
Laplacian of X is a principal submatrix of L



will denote the matrices associated to X by L
i (X ), L
i (X ) and reserve the notations L
i ,

graphs, we consider the following matrix Y S , indexed by (cid:0)V
(cid:1):
i for the complete complex.Let S  V be an independent set of X . Following the same strategy as in the case of
(cid:26) 0if F  F (cid:48) (cid:42) S
(7)
(Y S )F,F (cid:48) =

otherwise.
k1 )F,F (cid:48)
(L
We have Y S = (S
, where as a generalization of the characteristic vector of S , we
k ) T
(S
k )
(cid:26) 0
(cid:0)(S
(cid:1)
k ) dened as follows:
consider the matrix (S
k )
(S )K,F

if F (cid:42) S
otherwise,

K,F =

(10)

(9)

and

, the matrix Y S

Proposition 5.2. We have


(cid:104)Y S , L
k1 (cid:105) = k2

(cid:1), F  (cid:0)V
where K  (cid:0) V
(cid:1) and  is the matrix of the boundary operator k2 with respect
CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLOk1to the basis of elementary cochains. The properties of Y S lead to the following denition
of k (X ):

Denition 5.1. Let X be a pure k-dimensional complex on V , and let L
k1 be the down
k (X ) := sup (cid:8)(cid:104)L
Laplacian of the complete complex on V . Let:
(8)

k1 , Y (cid:105) : Y  R(V
k ) , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1,
k )(V
YF,F (cid:48) = 0 if F  F (cid:48)  Xk ,
F,F (cid:48) YF,F (cid:48) = F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F (cid:9)
YF,F (cid:48) = 0 if |F  F (cid:48) |  k + 2,
(X )  k (X ).
Proof. Let S be an independent set with |S | = (X ). As Y S = (S
k ) T
(cid:18)|S |
(cid:19)
(S
k )
is clearly positive semidenite. We have
(cid:104)Y S , I (cid:105) = k
(cid:18)|S |
(cid:19)(cid:88)(cid:18)|S |
(cid:18) |S |
(cid:18)|S |
(cid:19)
(cid:19)|F F (cid:48) |=k+1
F F (cid:48)Sk + 1Moreover, from the fact that S is an independent set, and from the denition of Y S (7), it
is clear that (Y S )F,F (cid:48) = 0 if F  F (cid:48)  Xk , or if |F  F (cid:48) |  k + 2.
The conditions F,F (cid:48) YF,F (cid:48) = F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F  are satised by the
To sum up, we have proved that the matrix k1 (cid:0)|S |
(cid:1)1

entries of L
k1 , so the matrix Y S inherits this property.
Y S is feasible for k (X ). Since
its objective value is equal to |S |, we can conclude that (X )  k (X ).
(cid:3)Now we consider the dual program of (8), in order to obtain another formulation of
k (X ), similar to (5).
k (X ) = inf (cid:8) max (Z ) : Z = L
Proposition 5.3. We have
TF,F = 0 for all F  (cid:0)V
(cid:1)

(cid:9)
F F (cid:48)=H F,F (cid:48) TF,F (cid:48) = 0 if H  (cid:0) V
(cid:1) \ Xk
(cid:80)
k1 + T ,
(11)same objective value because Slaters condition holds: Y = (cid:0)n
(cid:1)1
k+1
Proof. This is just a straightforward rewriting of the dual program. Both programs have the
I is a strictly feasible
(cid:3)solution of (8) and T = 0 gives rise to a strictly feasible solution of (11).
as expected, k  k (X )  n. Indeed, the lower bound follows by taking Y = (cid:0)n
(cid:1)1
Remark 5.4. Let us');
INSERT INTO posts (postId,userId,title,body) VALUES (177,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 8)',' make a few obvious observations about k (X ). The rst one, is that,
I in(8) while the upper bound follows by taking T = 0 in (11).

(cid:19)

= k2

+ (k + 1)k

= k

|S |.

9

( deg(F )).

THE THETA NUMBER OF SIMPLICIAL COMPLEXES
plete k-complexes. Indeed, if X is the empty k-complex, the matrix Y = k1 (cid:0)n
(cid:1)1
The second observation is that k (X ) is easy to determine for the empty and the com-

program (8) has only one feasible solution which is Y = (cid:0)n
(cid:1)1k1is feasible for (8) giving that k (X ) = n. If X is the complete k-complex, the semidenite
I so k (X ) = k .We note that, in these trivial cases, the equality (X ) = k (X ) holds.
The benet of the formulation (11) is that any feasible matrix T leads to an upper
bound of k (X ) and therefore to an upper bound of the independence number of X . Let us
illustrate this principle by showing that we can recover the upper bound proved by Golubev
[19] in the case of a k-dimensional simplicial complex X with complete (k  1)-skeleton.

k1 (X )  Dk1 (X )) for some   R that will be chosen later.
We take T =  (L
Clearly T satises the conditions required by (11). Then



k1 + T )  max (L
k1 + L
max (L
k1 (X )) + max
F Xk1


We assume that X has complete (k  1)-skeleton, so we have L
k1 (X ) and
k1 = L



k1 (X ) = 0. Let us denote by  the set of non zero eigenvalues of L
k1 (X ).k1L


Then, the eigenvalues of the matrix L
k1 (X ) are: n, associated to the eigenspace
k1 + L
B k1 , and , for   , corresponding to eigenvectors in Bk1 . For  =n

max (L
k1 (X ))
(cid:19)
(cid:18)


we have max (L
k1 (X )) = n and we get:
k1 + L
1  degmin (X )
(X )  k (X )  n
We note that, if X is regular, i.e., if deg(F ) is a constant number for F  (cid:0)V
(cid:1), then this
max (L
k1 (X ))upper bound is the exact analog of the ratio bound for graphs (6).
We have just seen that, in the case of a k-complex with complete (k  1)-skeleton,
k (X ) is an upper bound of the independence number of X which is as least as good
as the bound (2). The case of complexes with noncomplete (k  1)-skeleton turns out
to be more tricky; indeed, in some cases k (X ) provides a good bound of (X ), even
a sharp one, and beats the bound (2) given by Golubev, while in other cases, Golubevs
bound is better. We provide examples illustrating this situation in the next section, where
we explicitly work out the computation of 2 (X ) for certain families of 2-dimensional
complexes. This will also yield counterexamples for certain properties of the theta number
related to the chromatic number that we might expect (see Section 7).
It will also be
interesting to observe the prominent role plaed by the eigenvalues and eigenspaces of the
Laplacian operators in these examples .

6. TH E TH ETA NUMB ER O F CERTA IN FAM I L I E S O F 2-COM PL EX E S
6.1. The complete tripartite 2-complex. To dene this complex, we let n = 3m and
partition V = [n] into three subsets A, B , C of equal size m. As 2-dimensional faces
we select all triangles with exactly one vertex in each of these subsets; as 1-dimensional
faces all edges with at most one vertex in each of these subsets. A natural notation for
m,m,m ) = 2m because A  B is a maximal
this complex is K 2
m,m,m . It is clear that (K 2
independent set with 2m vertices. We will show that 2 (K 2
m,m,m ) = 2m.
With the notations of (2), d0 = 2m, d1 = m, 0 = 3m, 1 = 3m and the bound in (2)
equals (7m  1)/3, so this is an example where the theta number beats Golubevs bound.

10

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

m,m,m , we have 2 (K 2
We will also show that, for the complementary complex K 2
m,m,m ) =
m,m,m ). This complex has a complete 1-skeleton with d1 = 2m  2 and
3 = (K 2
1 = 3m, so Golubevs bound (2) equals (m + 2), which is not tight.
Proposition 6.1. We have 2 (K 2
m,m,m ) = 2m and 2 (K 2
m,m,m ) = 3.
Proof. To keep notations light we use the generic notation X for X = K 2
m,m,m throughout
');
INSERT INTO posts (postId,userId,title,body) VALUES (178,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 9)','the proof. We will verify that 2 (X ) = 2m, by constructing a suitable matrix T feasible for
(11). The matrix T will be constructed from the projection matrices associated to certain


eigenspaces of L
1 (X ) and L
1 (X ).
We denote by A  B the set of edges connecting one vertex in A and one vertex in B ,
and similarly for the other kinds of edges. So, X1 = (A  B )  (B  C )  (C  A).
We choose the orientations of the triangular faces and of the edges of X following the rule
A  B  C  A; this way, [G : F ] = +1 for all G  X2 and F  X1 .

1 (X ) has three non zero eigenvalues, 3m, 2m and
It turns out that the up-Laplacian L
m, respectively with multiplicity 1, 3(m  1), and 3(m  1)2 . We will need the projection


J3m2 /(3m2 ). The space VA = {(cid:80)
aA xa (1aB + 1aC ) : (cid:80)
matrices P
3m and P
2m associated to the eigenvalues 3m and 2m.


The all-one vector is clearly an eigenvector of L
1 (X ) for the eigenvalue 3m, so P
3m =
aA xa = 0} is easily

seen to be an eigenspace of L
1 (X ) for the eigenvalue 2m. Similarly, we have two other
(m  1)-dimensional eigenspaces VB and VC , and these spaces are pairwise orthogonal.

In order to express the projection matrix P
2m associated to the sum of these spaces, we
introduce the following notation: for (F , F (cid:48) )  X 2
1 , we denote F  F (cid:48) if F and F (cid:48) both

belong to A  B (respectively to B  C , C  A). Then,
2(m  1)
if F = F (cid:48)
if F  F (cid:48) and F  F (cid:48) = if F  F (cid:48) and F  F (cid:48) (cid:54)= , F (cid:54)= F (cid:48)
(m  2)if F (cid:54) F (cid:48) and F  F (cid:48) = 
(m  1)
if F (cid:54) F (cid:48) and F  F (cid:48) (cid:54)= 

1 (X ) has two non zero eigenvalues: 3m with multiplicity 2
The down Laplacian L
and 2m with multiplicity 3(m  1). The vector space {1AB + 1BC + 1AC :

++ = 0} is a two-dimensional space of eigenvectors for L
1 (X ) and for the eigenvalue
(cid:40)

3m is given by:
3m, and the corresponding projection matrix P
if F  F (cid:48)

3m2 1
3m )F,F (cid:48) =
(Potherwise.
C ). We now will consider matrices indexed by the whole set (cid:0)V
(cid:1), therefore we extend the
So far the matrices that we have dened are indexed by X1 = (A  B )  (B  C )  (A matrices introduced above by adding zero rows and columns for the indices not belonging
to X1 (we keep the same notation for the enlarged matrices). We are now ready to dene
the matrix T that will do the job for 2 (X ):
Lemma 6.2. With the previous notations, let


2m + P
3m + P
T = 2m(P
This matrix satises the following properties:



3m )  L
1 (X ).

2m2 

2m )F,F (cid:48) =

(P

11

We want to prove that

(1) TF,F = 0 for all F  (cid:0)V
(cid:1)
THE THETA NUMBER OF SIMPLICIAL COMPLEXES
(2) TF,F (cid:48) = 0 for all F , F (cid:48) such that F  F (cid:48) (cid:54)=  and F  F (cid:48) / X2
(3) 2mI  L
1  T (cid:23) 0.
Proof. Properties (1) and (2) follow by direct verication. In order to prove (3), we write






1 (X )  L
1 ,
3m and W = L
2m ), V = 2mP
1 + T = U + V + W where U = 2m(P
3m + Pand make the remark that the product of any two of these matrices is zero. Indeed, for U, V
and for U, W it follows immediately from the property that the product of up and down

3m is an eigenspace for
Laplacians is zero; for V , W , it is due to the fact that the image of P


1 . So, we need to prove that 2mI  U ,
the eigenvalue 3m not only for L
1 (X ) but also for L
2mI  V and 2mI  W are positive semidenite. For the rst two it is obvious because



2m ) and 2mI  V = 2m(I  P
2mI  U = 2m(I  P
3m  P
(cid:1) so that those in X1 = (A  B )  (B  C ) 
For this, we arrange the elements of (cid:0)V
3m ). So now the only missing


piece is a proof that 2mI  (L
1  L
1 (X )) (cid:23) 0.
(C  A) come before those in (A  A)  (B  B ');
INSERT INTO posts (postId,userId,title,body) VALUES (179,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 10)',')  (C  C ), and we accordingly write(cid:18)L
(cid:19)

1 by blocks:

1 (X ) M1 =
(cid:19)
(cid:18) 2mI
M TM
(cid:23) 0.
2mI  N
M T
By the Schur complement lemma, this is equivalent to 2mI  N  (2m)1M T M (cid:23) 0. A
direct computation shows that M T M = 2mN , so all boils down to mI  N (cid:23) 0, which is

m ). (cid:3)
indeed true because N is a block-diagonal matrix with three blocks equal to L
1 (K 2
Now, we turn our attention to K 2
m,m,m = X . In order to prove that 2 (X ) = 3, we will
use the primal formulation (8) and apply a symmetry argument. In the next section we will
(cid:18)Y1
(cid:19)
see a second, simpler, proof, using chromatic numbers, see Example 7.6.
With the previous notations, a feasible matrix Y must be of the form: Iwhere Y1 is supported on the diagonal and on the triangles that belong to X2 , i.e., the
triangles with one vertex in each of A, B , C . It is clear that the automorphism group of
X permutes transitively the elements of X2 and of X1 , and that, by convexity, (8) has a

1 (X )+  I.
symmetric solution. So, without loss of generality, we can assume that Y1 = L
Restricting the semidenite program on this set of matrices leads to a linear program in the
variables  ,  ,  that can be easily solved and leads to the optimal value 3. We skip the
details here.
We note that this approach would not work for 2 (X ) because X 2 has two orbits: the
triangles that are fully contained in one of the subsets A, B , C and the ones that have two
(cid:3)
vertices in one of these sets and one vertex in another one.

Y =

6.2. The complete bipartite 2-complex. Now n = 2m and V = [n] is partitioned in two
subsets A, B , of equal size m. As 2-dimensional faces we select the triangles that meet
both sets A and B , thus having two vertices in one of the parts and the third vertex in the
other. We denote this complex by K 2
m,m . It is clear that (K 2
m,m ) = m since A is an
independent set with m vertices. This complex has a complete 1-skeleton and d1 = m,

12

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

1 = 2m so the bound (3) equals m, showing that 2 (K 2
m,m ) = m and that the theta
number agrees with Golubevs bound.
m,m , which is nothing else than the disjoint union
For the complementary complex K 2
m,m ) = 4. Golubevs bound is twice the
of two complete complexes K 2
m , we have (K 2
value corresponding to K 2
m , thus 4, and it is sharp again. As we will see know, 2 (K 2
m,m )
is much larger:
m,m ) = 8m4
Proposition 6.3. We have 2 (K 2
m,m ) = m and 2 (K 2
m+1 .
m,m has two orbits in X1 = (cid:0)V
(cid:1): the set X in
Proof. We let X = K 2
m,m . To compute 2 (X ), we again apply the symmetry principle,
like in the case of the complement of the tripartite complex. The automorphism group of
1 of edges contained in A or in B , having
K 2
1 of crossing edges, with degree 2(m  1). It acts transitivelydegree m, and the set X out
on the 2-faces. So without loss of generality a feasible matrix Y of the primal formulation
of 2 (X ) can be assumed to be

1 (X ) +  Iout +  Iin =


Y = L
1 (X ) +  Iout +  Iin
where Iout and Iin denote the 0  1 diagonal matrices associated to respectively X out
and
1 . The expressions of (cid:104)I , Y (cid:105) and of (cid:104)L
1 , Y (cid:105) are linear in the variables  ,  ,  , but the
X in

will become clear if we write the matrices by blocks according to (cid:0)V
(cid:1) = X in
condition that Y is positive semidenite is slightly more complicated because L
1 (X ) does
not commute with Iout and Iin . In fact, this condition leads to quadratic constraints, as it
1  X out
(cid:19)
(cid:18) mI
1 . It iseasy to verify that
, M T M = mN  2J
M T
2mI  N1 (X ) =
and that N has two non zero eigenvalues: 2m, with multiplicity 1 and eigenvector the all-
one vector, and m, with multiplicity 2(m  1). Then, by the Schur complement lemma,
(cid:18)(m +  )I
');
INSERT INTO posts (postId,userId,title,body) VALUES (180,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 11)','(cid:19)
the condition(2m +  )I  N
M T
leads to quadratic inequalities. It is a bit technical but not difcult to see that an optimal
solution satises  =  , and nally that it is
21 (X ) +
Y =
m2 (m + 1)
m(m + 1)

1 , Y (cid:105) = (8m  4)/(m + 1).
leading to the optimal value (cid:104)L
7. CHROMAT IC NUMB ER S
Let us rst review the case of graphs. For a graph G, the clique number (G) = (G)
and the chromatic number (G) are related by the obvious inequality (G)  (G), and
the theta number (G) lies in between these numbers ([32, Lemma 3, Corollary 3]):
(G)  (G)  (G).
(12)
Moreover, the inequality (G)  (G) is always at least as strong as the inequality
n/(G)  (G); indeed, we know that n  (G)(G) from [32, Corollary 2].
Let us consider the situation for pure k-dimensional simplicial complexes. By analogy
with graphs, the chromatic number (X ) of a complex X , is usually dened to be the least
number of colors needed to color the vertices of X such that no k-face is monochromatic.

(cid:23) 0

I,

(cid:3)

THE THETA NUMBER OF SIMPLICIAL COMPLEXES

13

We remark that for the complete k-complex K k
n , the color classes of an admissible coloring
n ) = (cid:100)n/k(cid:101). So, for all
cannot have more than k elements, and consequently that (K k
k-dimensional complexes X , we have (X )  k(X ). Given that we have dened a
generalization of the theta number to k-complexes, that satises (X )  k (X ), it is
natural to wonder if the inequality
k (X )  k(X ).
(13)
is also satised. Unfortunately, this is not true in general.
Indeed, from the results of
Section 6, one can see that (13) is satised for the complete tripartite complex and for its
complement, but fails for the complete bipartite complex K 2
m,m , for which 2 (K 2
m,m ) =
(8m  4)/(m + 1) (Proposition 6.3) while (K 2
m,m ) = 2.
Let us now see if we can modify the denition of the chromatic number of a simplicial
complex, so that it ts better with our theta number. To achieve this, we will adapt the con-
cept of graph homomorphisms to simplicial complexes. Indeed, a nice way to understand
the notions of chromatic and clique numbers of graphs is through their connection to graph
homomorphisms, as we will recall now.
A homomorphism f from a graph G to a graph G(cid:48) is a mapping from the vertices of G
to the vertices of G(cid:48) that sends an edge of G to an edge of G(cid:48) . Then, the clique number
and the chromatic number have the following interpretations: the clique number (G) is
the largest number (cid:96) such that there is a homomorphism from the complete graph K(cid:96) to
G, and similarly (G) is the smallest number (cid:96) such that there is a homomorphism from
G to K(cid:96) . Moreover, one can prove that, if there is a homomorphism from G to G(cid:48) , then
(G)  (G(cid:48) ). The combination of these properties immediately leads to (12).
In order to follow a similar approach for simplicial complexes, we introduce an ad-hoc
notion of homomorphism.
Denition 7.1. Let X and X (cid:48) be two pure k-dimensional simplicial complexes. A homo-
morphism f from X to X (cid:48) is a mapping f : Xk1  X (cid:48)
k1 with the following property:
There exist orientations of X and X (cid:48) such that for every H  Xk , there is H (cid:48)  X (cid:48)
');
INSERT INTO posts (postId,userId,title,body) VALUES (181,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 12)','k such
that
(1) {f (F ) : F  Xk1 , F  H } = {F (cid:48)  X (cid:48)
k1 : F (cid:48)  H (cid:48)},
(2) [H (cid:48) : f (F )] = [H : F ] for all F  Xk1 with F  H .
We note that this denition coincides in dimension 1 with the usual notion of a graph
homomorphism as one can always nd suitable orientations.
Remark 7.2. In this denition, it is important to understand that a homomorphism f may
not necessarily be induced by a global mapping f0 between the vertices, i.e., it may be the
0 such that f (F ) = f0 (F ) for all F  Xk1 .
case that there is no mapping f0 : X0  X (cid:48)
As an example consider the 2-dimensional complex X depicted in Figure 1.
Furthermore, condition (2) is not automatically fullled. The 2-dimensional complex X
depicted in Figure 2 possesses a map f : X1  (K 2
3 )1 satisfying condition (1) but there is
3 .
no homomorphism from X to K 2
Proposition 7.3. Let X and X (cid:48) be two pure k-dimensional simplicial complexes, and let
f be a homomorphism from X to X (cid:48) . Then,
k (X )  k (X (cid:48) ).
(14)
Proof. Our strategy will be to start with an optimal solution Y of the primal formulation
(8) of k (X ), from which we construct a matrix Y (cid:48) , feasible for k (X (cid:48) ), and having the
same objective value as Y .

14

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

F IGURE 1 . The homomorphism of X to K 2
3 is not induced by a vertex map.

F IGUR E 2 . A complex X with no homomorphism to K 2
YF,F (cid:48)

So, let Y be primal optimal for the semidenite program dening k (X ). We remark
that, if F / Xk1 , then, for all F (cid:48)
(cid:54)= F , F  F (cid:48) / Xk , and so YF,F (cid:48) = 0. As a
consequence, by the optimality of Y , we have YF,F = 0.
For (K, K (cid:48) )  X 2
(cid:88)
k1 , we set
Y (cid:48)
K,K (cid:48) =
(F,F (cid:48) )X 2
k1
f (F )=K, f (F (cid:48) )=K (cid:48)
We have trace(Y (cid:48) ) = (cid:80)
K,K = (cid:80)
where the sum is zero if K or K (cid:48) does not belong to the image of f .
k ) Y (cid:48)
YF,F = trace(Y ).
F Xk1
K(V
By the property 1) of homomorphisms, if K (cid:54)= K (cid:48) and K  K (cid:48) is not an element of X (cid:48)
k ,
and if K = f (F ) and K (cid:48) = f (F (cid:48) ), then F  F (cid:48) cannot belong to Xk , and so YF,F (cid:48) = 0.
So, we have that Y (cid:48)
K,K (cid:48) = 0.
Thanks to property 2), if K  K (cid:48)  X (cid:48)
k and K  K (cid:48) = K (cid:48)(cid:48)  K  , the required condition
that K,K (cid:48) Y (cid:48)
K,K (cid:48) = K (cid:48)(cid:48) ,K Y (cid:48)
K (cid:48)(cid:48) ,K  holds. So, we have proved that Y (cid:48) is primal feasible
for k (X (cid:48) ).

k1 , Y (cid:48) (cid:105). We have
It remains to analyze the objective value (cid:104)L
(cid:88)

(cid:104)L
k1 , Y (cid:48) (cid:105) = k trace(Y (cid:48) ) +
K,K (cid:48) Y (cid:48)
K,K (cid:48) .
K,K (cid:48) : KK (cid:48)X (cid:48)
XK23X?XK23X15

K,K (cid:48)

YF,F (cid:48)

But

K,K (cid:48) Y (cid:48)
K,K (cid:48) =

(cid:88)
K,K (cid:48)
KK (cid:48)X (cid:48)
THE THETA NUMBER OF SIMPLICIAL COMPLEXES
(cid:88)
(cid:88)
(cid:88)
K,K (cid:48)
(F,F (cid:48) )X 2
k1
KK (cid:48)X (cid:48)
f (F )=K, f (F (cid:48) )=K (cid:48)(F,F (cid:48) )X 2
k1
F F (cid:48)Xk
where in the last equality we ignore the terms corresponding to F  F (cid:48) / Xk because they


are equal to zero, and we apply the property 2). It follows that (cid:104)L
k1 , Y (cid:48) (cid:105) = (cid:104)L
k1 , Y (cid:105).
(cid:3)

F,F (cid:48) YF,F (cid:48)

Denition 7.4. Let X be a pure k-dimensional simplicial complex. Let k (X ) denote
the smallest number (cid:96) such that there exists a homomorphism from X to the complete
(cid:96) .
k-complex K k
It is not hard to see that k (X )  (X1 ) holds for any pure simplicial complex X
as a vertex coloring with (cid:96) colors that is a proper graph coloring for X1 gives rise to a
homomorphism from X to K k
(cid:96) . The complex X depicted in Figure 1 serves as an example
that the three notions of chromatic numbers considered here differ. It has 2 (X )');
INSERT INTO posts (postId,userId,title,body) VALUES (182,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 13)',' = 3,
(X ) = 2 and (X1 ) = 4.
Proposition 7.5. We have

k (X )  k (X ).
Proof. If there is f : X  K k
(cid:96) then applying (14) leads to k (X )  k (K k
(cid:96) ) = (cid:96) (see
(cid:3)
Remark 5.4).
Example 7.6. Consider the complex X = K 2
m,m,m dened in Section 6. Clearly, 2 (X ) =
(X1 ) = 3, so we have 3 = (X )  2 (X )  2 (X ) = 3 and hence 2 (X ) = 3.
A k-dimensional subcomplex C of a pure k-dimensional simplicial complex X is a
connected component of X if for every (k1)-face F of C any k-face of X that contains F
is also in C . Note that this condition does not need to hold for lower dimensional simplices,
so two distinct connected components can, e.g., share a common vertex. Further observe
that the connected components of X correspond to the connected components of the graph
that has the k-faces of X as vertices with two vertices forming an edge if the correponding
k-faces intersect in a common (k  1)-face.
As different connected components do not share (k  1)-faces, the inequality k (X ) 
(X1 ) can actually be extended to the connected components of X .
Proposition 7.7. Let C be the collection of connected components of X . Then
k (X )  max
CC (C1 ).
It is well-known that a d-regular graph G has a bipartite connected component if and
only if the largest eigenvalue of the Laplacian is 2d.
In [23] Horak and Jost present a
combinatorial criterion that can be considered as a higher-dimensional analog of this: They

show that for a d-regular k-complex X the largest eigenvalue of the Laplacian L
k1 (X )
is (k + 1)d if and only if there is a connected component C of X and an orientation of the
k-faces of X such that [H : F ] = [H (cid:48) : F ] for all F  Ck1 , F  H, H (cid:48) . Note that for a
connected graph the existence of such an orientation is equivalent to bipartiteness.

16

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

If a k-dimensional simplicial complex X has chromatic number k (X ) = k + 1, this
guarantees the existence of such an orientation. Hence, we have the following observation.
Proposition 7.8. Let X be a d-regular k-dimensional simplicial complex. If k (X ) =

k + 1, then the maximal eigenvalue of the up-Laplacian L
k1 is (k + 1)d.
We remark that these results extend to arbitrary complexes for a normalized version of
the Laplacian that we do not study here.

8. A H I ERARCHY O F SEM ID E FIN I T E RE LAXAT ION S FOR TH E INDE PENDENCE
NUMBER O F A k - S IM PL IC IA L COM PL EX
In this section, X is again a pure k-dimensional simplicial complex. We consider a
straightforward generalization of k (X ) that leads to higher order theta numbers (cid:96) (X ) for
(cid:96) > k . We will see that all these numbers provide upper bounds of (X ), until (cid:96) = (X ),
where (X ) = (X ). Finally, we will modify this sequence of theta numbers in order to
get a decreasing sequence.
independence complex of X , and that it has complete (k  1)-skeleton, i.e., Indk1 = (cid:0)V
(cid:1).
It will be convenient to denote by Indi the set of independent sets of dimension i.
We make the remark that Ind := Ind1      Ind(X )1 is a simplicial complex, theFor (cid:96) > k , the matrices involved in the program dening (cid:96) (X ) are indexed by Ind(cid:96)1 .
We dene, for k  (cid:96)  (X ):
(ci');
INSERT INTO posts (postId,userId,title,body) VALUES (183,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 14)','d:96) (X ) = sup (cid:8)(cid:104)L
YF,F (cid:48) = 0 if F  F (cid:48)  (cid:0) V
(cid:1) \ Ind(cid:96) ,
(15)

(cid:96)1 (Ind), Y (cid:105) : Y  RInd(cid:96)1  Ind(cid:96)1 , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1,
F,F (cid:48) YF,F (cid:48) = F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F  (cid:9)
YF,F (cid:48) = 0 if |F  F (cid:48) |  (cid:96) + 2,
(cid:96)+1
(cid:96) (X ) = inf (cid:8) max (Z ) : Z = L
and its dual formulation:

(cid:9)
(cid:80)
(cid:96)1 (Ind) + T ,
TF,F = 0 for all F  Ind(cid:96)1 ,
F F (cid:48)=H F,F (cid:48) TF,F (cid:48) = 0 if H  Ind(cid:96)
The above denition matches for (cid:96) = k with that of k (X ). Both primal and dual programs
are strictly feasible: Y = I /(cid:104)I , I (cid:105) and respectively T = 0 give rise to strictly feasible
solutions. We note that, if (cid:96) = (X ), the feasible matrices of the primal program are
diagonal matrices and hence (cid:96) (X ) = (cid:96) = (X ). We have
Proposition 8.1.

(16)

(X )  (cid:96) (X ).
Proof. The same proof as the one of Proposition 5.2 works. For an independent set S such
(cid:26) 0
that |S |  (cid:96), we dene Y S  RInd(cid:96)1  Ind(cid:96)1 by

(cid:96)1 (Ind))F,F (cid:48)
(L
that (cid:96)1 (cid:0)|S |
(cid:1)1
It is then easy to verify, as every subset of an independent set S is also an independent set,
Y S is feasible for the primal program (15) and that its objective value is
equal to |S |.
(cid:3)
(cid:96)

if F  F (cid:48) (cid:42) S
otherwise.

(Y S )F,F (cid:48) =

THE THETA NUMBER OF SIMPLICIAL COMPLEXES

17

y(F ) + (cid:96)((cid:96) + 1)

y(H ).

and
(18)

However, it is not clear that the sequence ((cid:96) (X ))k(cid:96)(X ) is decreasing, because the
constraints on the (cid:96)-sets involved in (cid:96)1 (X ) do not occur explicitly in (cid:96) (X ). We now
dene a variant of (cid:96) (X ) that provides a decreasing sequence of upper bounds of (X ).
To start with, we note that, if a matrix Y is feasible for (15), then the value of F,F (cid:48) YF,F (cid:48)
for (F , F (cid:48) ) such that |F  F (cid:48) | = (cid:96) + 1 only depends on F  F (cid:48) . So, we can associate to Y a
function y  RInd(cid:96) such that F,F (cid:48) YF,F (cid:48) = y(H ) if H = F  F (cid:48) . If we extend y to Ind(cid:96)1
by y(F ) := YF,F , we see that y encodes every nonzero entry of Y . Said differently, we
YF,F (cid:48) = 0 if F  F (cid:48)  (cid:0) V
(cid:1) \ Ind(cid:96) ,
have a one to one correspondence between RInd(cid:96)1  Ind(cid:96) and the set
Y(cid:96)1 = (cid:8)Y  RInd(cid:96)1  Ind(cid:96)1 :
(cid:9)
YF,F (cid:48) = 0 if |F  F (cid:48) |  (cid:96) + 2,
(cid:96)+1
F,F (cid:48) YF,F (cid:48) = H,H (cid:48) YH,H (cid:48) if F  F (cid:48) = H  H (cid:48)
(cid:88)
We record for later use that, if y  RInd(cid:96)1  Ind(cid:96) corresponds to Y  Y as above, then
(cid:104)I , Y (cid:105) =
(17)
y(F )
F Ind(cid:96)1
(cid:88)
(cid:88)

(cid:104)L
(cid:96)1 (Ind), Y (cid:105) = (cid:96)
HInd(cid:96)
F Ind(cid:96)1
Now, we introduce, for (cid:96)  2, a map (cid:96)1 : Y(cid:96)1  Y(cid:96)2 . It will be more convenient
to dene (cid:96)1 on the corresponding functions y  RInd(cid:96)1  Ind(cid:96) , in the following way: let
(cid:96)1 : RInd(cid:96');
INSERT INTO posts (postId,userId,title,body) VALUES (184,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 15)',')1  Ind(cid:96)  RInd(cid:96)2  Ind(cid:96)1
(cid:55) (cid:96)1 (y) = zwhere (cid:40)
(cid:80)
(cid:80)
if K  Ind(cid:96)2
z (K ) = 1
F Ind(cid:96)1 : KF y(F )
(cid:96)
if F  Ind(cid:96)1
(cid:96)((cid:96)1) y(F ) + 1
z (F ) = 1
HInd(cid:96) : F H y(H )
(cid:96)1
We are now in the position to dene our strengthening of (cid:96) (X ): Let
(cid:96) (X ) = sup (cid:8)(cid:104)L
(19)

(cid:96)1 (Ind), Y (cid:105) : Y  RInd(cid:96)1  Ind(cid:96)1 , Y (cid:23) 0, (cid:104)I , Y (cid:105) = 1,
YF,F (cid:48) = 0 if F  F (cid:48)  (cid:0) V
(cid:1) \ Ind(cid:96) ,
i  i+1      (cid:96)1 (Y ) (cid:23) 0 for all i = 1, . . . , (cid:96)  1,
F,F (cid:48) YF,F (cid:48) = F (cid:48)(cid:48) ,F  YF (cid:48)(cid:48) ,F  if F  F (cid:48) = F (cid:48)(cid:48)  F  (cid:9).
YF,F (cid:48) = 0 if |F  F (cid:48) |  (cid:96) + 2,
(cid:96)+1
Theorem 8.2. The numbers (cid:96) (X ), k  (cid:96)  (X ), satisfy:
(1) (cid:96) (X )  (cid:96) (X )
(2) (X ) = (X ) (X )  (X )1 (X )      k (X ).
Proof. That (cid:96) (X )  (cid:96) (X ) is clear since we have only added constraints on Y in the
denition of (cid:96) (X ).
Let S be an independent set, with |S |  (cid:96). Let, like in the proof of Proposition 8.1,
(cid:40)
(cid:96)1  RInd(cid:96)1  Ind(cid:96)1 be dened by:
Y S
0 if F  F (cid:48) (cid:42) S

(cid:96)1 (Ind))F,F (cid:48) otherwise.
(L

(Y S
(cid:96)1 )F,F (cid:48) =

(20)

18

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO
(cid:96)1  RInd(cid:96)1  Ind(cid:96) corresponding to Y S
(cid:96)1 is given by: yS
The element yS
(cid:96)1 (F ) = (cid:96) if
(cid:96)1 (H ) = 1 if H  S , and otherwise yS
F  S , yS
(cid:96)1 takes the value 0. We will need the
following lemma:

Lemma 8.3. We have

(cid:96)1 (yS
(cid:96)1 ) =

|S |  (cid:96) + 1
(cid:96)  1

yS
(cid:96)2

z (K ) =

for yS
(cid:96)1 as dened in (20).
(cid:96)1 ). Let K  Ind(cid:96)2 . Every subset of S is independent so the
Proof. Let z := (cid:96)1 (yS
number of F  Ind(cid:96)1 such that K  F  S is |S |  (cid:96) + 1. So,
(cid:88)
(cid:96)1 (F ) = |S |  (cid:96) + 1.yS
(cid:96)
F Ind(cid:96)1 : KF
Now let F  Ind(cid:96)1 . It is clear that, if F is not contained in S , z (F ) = 0. If F  S ,
(cid:88)1
(cid:96)  1
(cid:96)((cid:96)  1)HInd(cid:96) : F HS
|S |  (cid:96) + 1
(|S |  (cid:96)) =
(cid:96)  1
(cid:96)  1
(cid:96)  1

z (F ) =

(cid:96) +

=

(cid:3)

(cid:96)1 ) is positive semidenite, and so, iteratively, that i 
Lemma 8.3 shows that (cid:96) (Y S
i+1      (cid:96)1 (Y S
(cid:96)1 ) is positive semidenite for every i  (cid:96)  1. We conclude that Y S
(cid:96)1
(after a suitable rescaling) is feasible for (cid:96) (X ), and consequently that (X )  (cid:96) (X ).
We have already remarked that (X ) = (X ) so also (X ) = (X ).
It remains to prove that the sequence of (cid:96) is decreasing. For this, we start from an
optimal solution Y of (cid:96) , and we show that Z := (cid:96)1 (Y ) is feasible for (cid:96)1 and that


(cid:104)L
(cid:96)1 (Ind), Y (cid:105) = (cid:104)L
(cid:96)2 (Ind), Z (cid:105).
It is clear that Z  Y(cid:96)2 and that Z is positive semidenite, as well as i  i+1     
(cid:96)2 (Z ) (cid:23) 0 for all i  (cid:96)  2. ');
INSERT INTO posts (postId,userId,title,body) VALUES (185,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 16)','That (cid:104)I , Z (cid:105) = 1 follows easily from (17) and from the
denition of (cid:96)1 . It remains to take care of the objective value. Applying (18),
(cid:88)
(cid:88)
F Ind(cid:96)1
KInd(cid:96)2
y(F ) + ((cid:96)  1)(cid:96)
(cid:96)((cid:96)  1)
where in the sums we restrict to elements in Ind. Taking account of the fact that every
(cid:88)
(cid:88)
subset of an independent set is also an independent set, we obtain

(cid:104)L
(cid:96)2 (Ind), Z (cid:105) = (cid:96)
y(H ) = (cid:104)L
F Ind(cid:96)1
HInd(cid:96)


(cid:96)2 (Ind), Z (cid:105) = ((cid:96)  1)
(cid:88)
(cid:88)
F : KF
z (K ) + (cid:96)((cid:96)  1)
(cid:16)
(cid:88)
(cid:88)
H : F H


(cid:96)1 (Ind), Y (cid:105).

= ((cid:96)  1)

y(F ) + (cid:96)((cid:96) + 1)
(cid:96)  1

(cid:17)

y(F ) +

y(H )

(cid:104)L
(cid:96)

z (F )

(cid:3)

THE THETA NUMBER OF SIMPLICIAL COMPLEXES

19

9. TH ETA NUMB ER S O F RANDOM COM P LEXE S
A random model X k (n, p) for simplicial complexes of arbitrary xed dimension k was
(k  1)-skeleton, and each element of (cid:0) [n]
(cid:1) is added as a k-dimensional face of X k (n, p)
introduced by Linial and Meshulam [31] as a higher dimensional analog of the Erd os-
R enyi model G(n, p) for random graphs. It has vertex set [n] = {1, . . . , n}, complete
independently with probability p. Here p = p(n) is a function of n, and we let q := 1  p.
k+1
In this section we analyze the theta number of X k (n, p) for dense complexes, i.e., for p
in the range [c0 log(n)/n, 1  c0 log(n)/n].
who proved that, in the case of constant probability p, (G(n, p)) = ((cid:112)nq/p) holds with
The study of the theta number of random graphs G(n, p) was initiated by Juh asz in [24]
probability tending to 1. In subsequent works, the range of probabilities for which Juh asz
result holds was extended, until in [6], Coja-Oghlan was able to cover c0 /n  p  1c0/n
for some sufciently large constant c0 .
We will restrict ourselves to the range c0 log(n)/n  p  1  c0 log(n)/n because we
will need the following estimates:
Theorem 9.1 ([16, 22]). Let A denote the adjacency matrix of G(n, p). For every c > 0
max (pJ  A)  c(cid:48)(cid:112)pq(n  1)
there exists c0 > 0, c(cid:48) > 0, c(cid:48)(cid:48) > 0 such that, if c0 log(n)/n  p  1  c0 log(n)/n,
(21)
|min (A)|  c(cid:48)(cid:48)(cid:112)pq(n  1).
and
(22)
with probability at least equal to 1  nc .
With the above, it is rather straightforward to obtain:
Theorem 9.2. For every c > 0 there exists c0 > 0, c1 > 0, c2 > 0 such that, if
(cid:112)(n  1)q/p  (G(n, p))  c2
(cid:112)(n  1)q/p.
c0 log(n)/n  p  1  c0 log(n)/n,
(23)
c1
with probability at least equal to 1  nc .
Indeed, following the method of Juh asz, the upper bound is obtained via the dual for-
mulation for the theta number (5) and the matrix Z = J  A/p, where A is the adjacency
matrix of G(n, p), while the lower bound follows from the choice Y = Y (cid:48)/(cid:104)I , Y (cid:48) (cid:105) in the
primal formulation (4), where Y = A  min (A)I , A being the adjacency matrix of the
complementary graph of G(n, p).
9.1. The theta number of X k (n, p). We will establish the following similar result for
random simplicial complexes X k (n, p):
Theorem 9.3. For every k  1 and c > 0, there exists c0 > 0, c1 > 0, c2 > 0 such that, if
(cid:112)(n  k)q/p.
(cid:112)(n  k)q/p  k (X k (n, p))  c2
c0 log(n)/n  p  1  c0 log(n)/n,
c1
with probability at least equal to 1  nc .
For comparison, the independence number of X k (n, p) is of the order (log(nk p)/p)1/k
In the range c0 log(n)/n  p  1  c0 log(n)/n, the eigenvalues of the
(see [28]).
adjacency matrix of X k (n, p) have been studied in [21]. We will closely follow the
methods developed in [21], in particular the role played by the so-called links of X , an

20

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

(YK )F,F (cid:48) =

K JK K .

K JK K ,

idea going back to the work of Garland [17]. By denition, for a k-di');
INSERT INTO posts (postId,userId,title,body) VALUES (186,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 17)','mensional simpli-
cial complex X and a (k  2)-face K of X , the link lkX (K ) is the graph with vertices
{v  V : K  {v}  Xk1}, and edges {{v , w} : K  {v , w}  Xk }. In view of the
proof of Theorem 9.3, we will rst establish a relationship between the theta number of a
simplicial complex and that of its links.
Proposition 9.4. Let X be a k-dimensional simplicial complex with complete (k  1)-
skeleton. Then
(24)

(lkX (K )).

if K  F  F (cid:48)
otherwise.

k (X )  k max
KXk2
Proof. Let K  Xk2 . For a matrix Y  R(V
k )(V
k ) , we introduce its localization at K
(cid:26) YF,F (cid:48)
denoted YK and dened by:Let K  R(V
k )(V
k ) denote the diagonal matrix with [F : K ] as diagonal entries. Then we
(cid:88)
observe that

(25)k1 =
KXk2
(cid:88)
and that, if YF,F (cid:48) = 0 for all (F , F (cid:48) ) such that |F  F (cid:48) |  k + 2,
YK  (k  1) diag(Y ).
(26)
Y =
KXk2
(cid:88)
k1 , Y (cid:105) = (cid:104)(cid:88)
Now let Y be an optimal solution of (8). Taking account of (25) and (26),


k (X ) = (cid:104)L
YK (cid:105)  (k  1)(cid:104)L
k1 , diag(Y )(cid:105)
(cid:88)K
(cid:104)K JK K , YK (cid:48) (cid:105)  k(k  1).K,K (cid:48)
(cid:26) YF,F
if K  K (cid:48) = F
(cid:104)K JK K , YK (cid:48) (cid:105) =
otherwise
(cid:88)
(cid:88)
(cid:104)K JK K , YK (cid:105) =K
Now, the crucial observation is that the matrix K YK K gives rise to a feasible matrix
of the semidenite program (4) dening the theta number of lkX (K ). Indeed, let ZK be
the matrix indexed by V \ K and dened by (ZK )v ,w = (K YK K )K{v},K{w} . This
matrix inherits some properties of Y : The matrix ZK is positive semidenite, the entries
of ZK associated to edges of lkX (K ) are equal to 0. With obvious notations, we have
k (X )  (cid:88)
(cid:104)JK , K YK K (cid:105) = (cid:104)J, ZK (cid:105) and (cid:104)I , ZK (cid:105) = (cid:104)I , YK (cid:105) so we obtain
(cid:104)I , YK (cid:105)(lkX (K )).
We have (cid:80)K (cid:104)I , YK (cid:105) = k(cid:104)I , Y (cid:105) = k so the announced inequality follows immediately.
(cid:3)

(cid:104)JK , K YK K (cid:105).

If K (cid:54)= K (cid:48) , we have

so, since trace(Y ) = 1,

k (X ) =

THE THETA NUMBER OF SIMPLICIAL COMPLEXES

21

Proof of Theorem 9.3. For the upper bound, we apply Proposition 9.4. The link lkX (K )
of a (k  2)-face K in a random complex X = X k (n, p) is an Erd os-Renyi random graph
on V \ K with the same probability p. We can thus apply Theorem (9.2) and a union bound
to obtain the result. We note that, since the number of such faces is of the order of nk1 ,
for the probability of the bad event to be, say, less than nc we need to apply Theorem
(9.2) for the larger value c + k  1 instead of c, explaining the need for an arbitrary large
power of n in the convergence speed of probabilities.
In order to nd a lower bound of k (X ), we consider the matrix Y = A  min (A)I
k1 , Y (cid:105) = k(k + 1)|X k |  k(cid:0)n
(cid:1)min (A). Moreover, (cid:104)L
(cid:1)min (A), so
have (cid:104)I , Y (cid:105) = (cid:0)n
where A denotes the adjacency matrix of the complementary k-complex X . The feasibility
conditions of (8) are fullled by Y except for the normalization condition (cid:104)I , Y (cid');
INSERT INTO posts (postId,userId,title,body) VALUES (187,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 18)',':105) = 1. We
(cid:18)
(cid:19)

(cid:0)n
(cid:1)min (A)k
(k + 1)|X k |
k (X )  k
1 +(cid:1)] with probability q . Hence, by a straightforward application of a Chernoff bound,
in [(cid:0) nThe number |X k | of k-faces of X = X k (n, q) is a random variable binomially distributed
for every c > 0, |X k | is at least of the order (cid:0) n
(cid:1)q with probability at least 1  nc . It
k+1
k+1
remains to upper bound |min (A)|. For this, we apply the localization procedure that we
(cid:88)
have already encountered in the proof of Proposition 9.4:
A =
AK .
KXk2
k ) , if xK denotes the vector obtained from x by setting to 0
Then, for every x = (xF )F (V
(cid:88)
(cid:88)
the coordinates of x associated to faces F not containing K ,
(cid:104)Ax, x(cid:105) =
(cid:104)AK x, x(cid:105) =
(cid:104)AK xK , xK (cid:105).K
The matrix AK has the same spectrum as K AK K . The latter is identical to the adjacency
matrix AlkX (K ) of the graph lkX (K ) on the entries indexed by {F = K {v}, v  V \K },
(cid:104)Ax, x(cid:105)  (cid:88)
and zero elsewhere. So, its non-zero spectrum is that of AlkX (K ) and hence:
min (AlkX (K ) )(cid:104)xK , xK (cid:105).The links lkX (K ) are random graphs G(n  k + 1, q) so, applying (22) and a union bound,
(cid:88)
(cid:104)xK , xK (cid:105) = c(cid:48)(cid:48)k(cid:112)pq(n  k)(cid:104)x, x(cid:105).
(cid:104)Ax, x(cid:105)  c(cid:48)(cid:48)(cid:112)pq(n  k)
we nd that, with probability at least equal to 1  nc , for a large enough constant c(cid:48)(cid:48) ,
We have obtained the desired upper bound |min (A)|  c(cid:48)(cid:48)(cid:48)(cid:112)pq(n  k). Putting every-(cid:3)
thing together, we obtain the announced lower bound for k (X ).
9.2. The hierarchy of theta numbers of G(n, p). In this last subsection, we restrict our-
selves to the case of random graphs G(n, p) and analyze the hierarchy of theta numbers
(cid:96) (G(n, p)) for constant values of (cid:96). The restriction to random graphs, i.e., random com-
plexes of dimension 1, is purely for simplicity. The assumption of constant (cid:96), however,
is essential. Analyzing the complete hierarchy (cid:96) (X ) of a random complex X for non-
constant (cid:96) appears to be a difcult task. It would be interesting to know for which values
of (cid:96) the theta number (cid:96) (G(n, p)) is close to the independence number. Unfortunately,
such questions seem to be out of the reach of the methods we apply here.

nq (cid:96)/p.

1 +

22
CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO
Theorem 9.5. For every (cid:96)  1 and c > 0, there exists c0 > 0, c1 > 0, c2 > 0 such that, if
(cid:113)
(cid:113)
q (cid:96)  c0 log(n)/n and pq (cid:96)1  c0 log(n)/n,
nq (cid:96)/p  (cid:96) (G(n, p))  c2
c1
with probability at least equal to 1  nc .
Proof. We will sometimes use the expression with high probability for an inequality that
holds with probability at least 1  nc for all c > 0, with appropriate constants depending
on c.
For an upper bound of (cid:96) (G(n, p)), we apply
(cid:96) (G)  (cid:96) max
(lkG (K )).
K( V
(cid:1) \ Ind(cid:96) . If K is independent, this condition
with edges {v , w} if K  {v , w}  (cid:0) V
(cid:96)1)
Here, lkG (K ) is the graph on VK := {v  V : {v , k} / E (G(n, p)) for all k  K }
simply means that {v , w} is an edge of G, so lkG (K ) is the graph G[VK ] induced by G
(cid:96)+1
on VK . If G = G(n, p), the number of vertices nK = |VK | is i');
INSERT INTO posts (postId,userId,title,body) VALUES (188,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 19)','tself a random variable.
Since |K | = (cid:96)  1, nK follows a binomial distribution with parameters (n  (cid:96) + 1) and
q (cid:96)1 . For nK to be concentrated around its expected value q (cid:96)1 (n  (cid:96) + 1) we need
q (cid:96)1  c0 log(n)/n for some c0 > 0.
Assuming nK  cq (cid:96)1n for some c > 0, we have
(G[VK ])  (G(cq (cid:96)1n, p))
because G[VK ] can be viewed as an induced subgraph of G(cq (cid:96)1n, p). We would like to
apply Theorem 9.2. It requires p and q to be greater that c(cid:48)
0 log(q (cid:96)1n)/(q (cid:96)1n) and holds
with probability at least 1  (q (cid:96)1n)c . All this will be ne if we assume:
pq (cid:96)1  c1 log(n)/n and q (cid:96)  c1 log(n)/n
(cid:113)
for a sufciently large c1 . With a union bound we obtain with high probability:
(cid:96) (G)  c
nq (cid:96)/p.
For the lower bound, we consider the matrix Y = A  min (A)I where A is the
(cid:17)
(cid:16)
adjacency matrix of the (cid:96)-skeleton of Ind and we apply (15). We obtain

(cid:96) (X )  (cid:104)L
(cid:96)1 (Ind), Y (cid:105)
((cid:96) + 1)| Ind(cid:96) |
In order to estimate |min (A)| we use A = (cid:80)
min (A)| Ind(cid:96)1 |
(cid:104)I , Y (cid:105)
= (cid:96)
AK and remark that AK has the
KInd(cid:96)2
same non-zero eigenvalues as the adjacency matrix of the graph lkInd (K ), itself being the
(cid:88)
(cid:88)
graph G[VK ] induced by G on VK . We have
(cid:104)Ax, x(cid:105) =
(cid:104)AK x, x(cid:105) =
 (cid:88)
KInd(cid:96)2
KInd(cid:96)2
min (AK )(cid:104)xK , xK (cid:105)
(cid:88)
KInd(cid:96)2
 min
min (AK )
KInd(cid:96)2
KInd(cid:96)2
min (AK )(cid:96)(cid:104)x, x(cid:105),
 min
KInd(cid:96)2

(cid:104)AK xK , xK (cid:105)

(cid:104)xK , xK (cid:105)

THE THETA NUMBER OF SIMPLICIAL COMPLEXES

23

so

min (A) = |min (A)|  (cid:96)  max
|min (G[VK ])|.|min (G[VK ])|  |min (G(cq (cid:96)1n, q))|  c(cid:48)(cid:112)
Like for the upper bound we have with high probability nK  cq (cid:96)1n for some c > 0
and thus
pq (cid:96)n
for some c(cid:48) > 0, under the same conditions on p and q .
It remains to deal with the ratio | Ind(cid:96) |/| Ind(cid:96)1 |. For this we will argue that Ind is
almost regular. To be more precise we apply double counting to the set
D = {(A, B )  Ind(cid:96)1  Ind(cid:96) : A  B }.
The number of (cid:96)-subsets of B is (cid:96) + 1 so |D | = ((cid:96) + 1)| Ind(cid:96) |. For a given A, the number
XA of B containing A follows a binomial distribution with parameters n  (cid:96) and q (cid:96) , with
expected value q (cid:96) (n  (cid:96)). With high probability (requires q (cid:96)  c log(n)/n) XA is larger
that c(cid:48) q (cid:96) (n  (cid:96)) and so
| Ind(cid:96)1 |  c(cid:48) q (cid:96) (n  (cid:96))
| Ind(cid:96) |
(cid:113)
(cid:96) + 1
Putting everything together and applying another union bound we obtain
(cid:96) (G)  c

nq (cid:96)/p.

(cid:3)

R E FER ENC E S

[1] M. Anjos and J.B. Lasserre, Handbook on Semidenite, Conic and Polynomial Optimization, Springer, 2012
[2] A. Ben-Tal and A. Nemirovski, Lectures on Modern Convex Optimization: Analysis, Algorithms, and Engi-
neering Applications, SIAM, Philadelphia, 2001.
[3] S. Boyd and L. Vandenberghe Convex Optimization, Cambridge University Press, 2004.
[4] A.E. Brouwer and W.H. Haemers, Spectra of Graphs, Springer, New York (2012).
[5] H. Cohn, A. Kumar, S.D. Miller, D. Radchenko, and M.S. Viazovska, The sphere packing problem in
dimension 24, preprint, arXiv:1603.06518 [math.NT], 2016, 12pp.
[6] A. Coja-Oghlan, The Lov asz number of random graphs, Combinatorics, Probability and Computing 14
(2005), 439-465.
[7] P.E.B. DeCorte, D. de Laat, and F. Vallentin, Fourier analysis on nite groups and the Lov asz theta-number
of Cayley graphs , Experimental Mathe');
INSERT INTO posts (postId,userId,title,body) VALUES (189,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 20)','matics 23 (2014), 146-152.
[8] P. Delsarte, An algebraic approach to the association schemes of coding theory, Philips Research Repts
Suppl. 10, (1973), 1-97.
[9] D. Dotterrer, T. Kaufman, and U. Wagner, On Expansion and Topological Overlap, preprint,
arXiv.math:1506.04558.
[10] D. Dotterrer, L. Guth, and M. Kahle. 2-complexes with large homological systoles, preprint,
arXiv.math:1509.03871.
[11] A.M. Duval, C.J. Klivans, and J.L. Martin, Simplicial and Cellular Trees, in Recent Trends in Combina-
torics, The IMA Volumes in Mathematics and its Applications, Springer (2016), 713752.
[12] D. Ellis and E. Friedgut, H. Pilpel, Intersecting families of permutations, Journal of the American Mathe-
matical Society 24 (2011), 649-682.
[13] D. Ellis, Y. Filmus, and E. Friedgut, Triangle-intersecting families of graphs, Journal of the European
Mathematical Society 14 (2012), 841-885
[14] S. Evra, K. Golubev and A. Lubotzky, Mixing properties and the chromatic number of Ramanujan complexes
International Mathematics Research Notices 22 (2015), 11520-11548.
[15] S. Evra, T. Kaufman, Bounded Degree Cosystolic Expanders of Every Dimension preprint,
arXiv.math:1510.00839.
[16] U. Feige and E. Ofek, Spectral techniques applied to sparse random graphs, Random Structures Algorithms
27 (2) (2005), 251275.
[17] H. Garland, p-adic curvature and the cohomology of discrete subgroups of p-adic groups, Annals of Math-
ematics (2), 97(3) (1973), 375423.

24

CHRISTINE BACHOC, ANNA GUNDERT, AND ALBERTO PASSUELLO

[18] B. G artner and J. Matousek, Approximation Algorithms and Semidenite Programming, Springer, 2012
[19] K. Golubev, On the chromatic number of a simplicial complex, to appear in Combinatorica.
[20] M. Gromov, Singularities, expanders and topology of maps. Part 2: From combinatorics to topology via
algebraic isoperimetry, Geometric and Functional Analysis, 20 (2) (2010), 416526.
[21] A. Gundert and U. Wagner, On eigenvalues of random complexes, Israel Journal of Mathematics,216 (2)
(2016), 545582.
[22] C. Hoffman, M. Kahle, and E. Paquette, Spectral gaps of random graphs and applications to random topol-
ogy, preprint, arXiv.math:1201.0425.
[23] D. Horak and J. Jost, Spectra of combinatorial Laplace operators on simplicial complexes, Advances in
Mathematics, 244:303  336, 2013.
[24] F. Juh asz, The aymptotic behaviour of Lov asz theta function for random graphs, Combinatorica 2 (2) (1982),
153155.
[25] M. Kahle, Random simplicial complexes, preprint, arXiv.math:1607.07069.
[26] G. Kalai, Enumeration of Q-acyclic simplicial complexes, Israel Journal of Mathematics, 45 (1983), 337
351.
[27] T. Kaufman, D. Kazhdan and A. Lubotzky, Isoperimetric Inequalities for Ramanujan Complexes and Topo-
logical Expanders, Geometric and Functional Analysis, 26 (1) (2016), 250287.
[28] M. Krivelevich, B. Sudakov, The chromatic numbers of random hypergraphs, Random Structures Algo-
rithms, 12 (4) (1998), 381403.
[29] J.B. Lasserre, An explicit equivalent positive semidenite program for nonlinear 0-1 programs, SIAM J.
Optim. 12 (2002), 756769.
[30] M. Laurent, A comparison of the Sherali-Adams, Lov asz-Schrijver and Lasserre relaxations for 0 1 pro-
gramming, Math. Oper. Res. 28 (2003), 470496.
[31] N. Linial and R. Meshulam, Homological connectivity of random 2-complexes, Combinatorica 26 (4)
(2006), 475487.
[32] L. Lov asz, On the Shannon capacity of a graph, IEEE Transactions');
INSERT INTO posts (postId,userId,title,body) VALUES (190,9778,'THE THETA NUMBER OF SIMPLICIAL COMPLEXES (part 21)',' on Information Theory 25 (1979), 17.
[33] A. Lubotzky, Ramanujan complexes and high dimensional expanders, Japanese Journal of Mathematics 9
(2) (2014), 137169.
[34] A. Lubotzky and R. Meshulam, A Moore bound for simplicial complexes, Bulletin of the London Mathe-
matical Society, 39 (3) (2007), 353358.
[35] J. Matousek, M. Tancer and U. Wagner, Hardness of embedding simplicial complexes in Rd , Journal of the
European Mathematical Society, 13 (2) (2011), 259-295.
[36] F.M. de Oliveira Filho and F. Vallentin, Computing upper bounds for packing densities of congruent copies
of a convex body, 30 pages, arXiv:1308.4893 [math.MG]
[37] O. Parzanchevski and R. Rosenthal, Simplicial complexes: Spectrum, homology and random walks, Random
Structures Algorithms, 50 (2) (2017), 225261.
[38] O. Parzanchevski, R. Rosenthal, and R. J. Tessler, Isoperimetric inequalities in simplicial complexes, Com-
binatorica, 36 (2) (2016), 195227.
[39] L. Vandenberghe and S. Boyd, Semidenite Programming, SIAM Review 38 (1996), pp. 4995.
[40] M.S. Viazovska, The sphere packing problem in dimension 8, arXiv:1603.04246 [math.NT], 2016, 22pp.

IN ST I TU T D E MATH EMAT IQU E S DE BORD EAUX , UMR 5251 , UN IVER S I T E D E BORD EAUX , 351 COUR S
D E LA L IB ERAT ION , 33400 TA LENC E , FRANC E .
E-mail address: christine.bachoc@u-bordeaux.fr

MATH EMAT I SCHE S IN ST I TU T, UN IVER S I T A T ZU K O LN , W EY ERTA L 86 -90 , 50931 K O LN , G ERMANY.
E-mail address: anna.gundert@uni-koeln.de

IN ST I TU T D E MATH EMAT IQU E S DE BORD EAUX , UMR 5251 , UN IVER S I T E D E BORDEAUX , 351 COUR S
D E LA L IB ERAT ION , 33400 TA LENC E , FRANC E .
E-mail address: alberto.passuello@u-bordeaux.fr

');
INSERT INTO posts (postId,userId,title,body) VALUES (191,1804,'Heterogeneous Systems ','Anthony Gregerson, Aman Chadha, Katherine Morrow 
Department of Electrical '||'&'||' Computer Engineering 
The University of Wisconsin  Madison 
Madison, WI  USA 
agregerson@wisc.edu, achadha2@wisc.edu,  kati@engr.wisc.edu 
 
AbstractDesign  flows  use  graph  partitioning  both  as  a 
precursor  to  place  and  route  for  single  devices,  and  to  divide 
netlists or  task  graphs among multiple devices.  Partitioners have 
accommodated  FPGA  heterogeneity 
via  multi-resource 
constraints,  but  have  not  yet  exploited  the  corresponding  ability 
to  implement  some computations in multiple ways (e.g., LUTs vs. 
DSP  blocks), which  could  enable  a  superior  solution.  This  paper 
introduces  multi-personality 
graph 
partitioning,  which 
incorporates  aspects  of  resource  mapping  into  partitioning.  We 
present a modified multi-level KLFM partitioning algorithm that 
also  performs  heterogeneous  resource  mapping  for  nodes  with 
multiple  potential  implementations  (multiple  personalities).  We 
evaluate  several  variants  of  our  multi-personality  FPGA  circuit 
partitioner  using  21  circuits  and  benchmark  graphs,  and  show 
that dynamic  resource mapping  improves  cut   size  on  average  by 
27% over static mapping for these circuits. We further show that 
it  improves  deviation  from  target  resource  utilizations  by  50% 
over post-partitioning resource mapping. 

Keywords partitioning; technology mapping 

INTRODUCTION 

I. 
Netlist  partitioning  is  a  critical  part  of  the  CAD  flow  for 
large circuit designs, both to achieve high-quality results and to 
reduce  the  CAD  flow  execution  time.  Existing  research  on 
multi-resource  partitioning  for  heterogeneous  FPGAs  has 
focused  on  partitioning  nodes with pre-assigned  resource  costs 
[1]  [2]  [3].  However, heterogeneity  presents  the  opportunity  to 
implement  some  computations  in  multiple  different  ways, 
using  different  combinations  of  resources.  For  example, 
multiply-accumulate  operations  can  be  implemented  in  DSP 
blocks  or  CLBs.  We  refer  to  logic  with  multiple  possible 
implementations  as multi-personality  logic,  and  the process of 
choosing  an  implementation  for  that  logic  as  personality 
selection.  CAD  tools  perform  personality  selection  during 
synthesis;  however,  allowing  a  post-synthesis  partitioner  to 
modify  personality  selections  can  improve  partitioning  both  in 
terms of cut size and heterogeneous resource utilization. 
 In  this  paper  we  define  the  multi-personality  partitioning 
problem  and  propose  specific  modifications  to  multi-level 
KLFM  (Kernighan-Lin/Fiduccia-Mattheyses)  to  incorporate 
personality  selection.  We  evaluate  our  modifications  using  a 
collection  of  graphs  based  on  real-world  multi-FPGA  designs 
and  publicly-available  benchmarks.  We  demonstrate  that  they 
result  in  better  cut  sizes  and  deviations  from  target  resource 
utilizations than partitioning of fixed-implementation netlists. 

II.  RELATED WORK 
Circuit  netlists  can  be  represented  as  hypergraphs,  where 
logic  elements  (or  sometimes  groups  of  elements)  are  graph 
nodes  and  the  nets  that  connect  the  nodes  are  hyperedges. 
Netlist/Hypergraph partitioning based on variants of the KLFM 
algorithm  [4]  often  optimize  for  metrics  such  as  channel 
width/congestion,  ease  of  routability,  and  operating  frequency 
[5].  Some  work  has  also  examined  multi-resource  constraints 
for  heterogeneous  devices  for  KLFM  [2]  and  other  algorithms 
[3],  but  unlike  our  work,  they  do  not  integrate  personality 
selection  for  multi-personality  nodes  into  partitioning.  In 
general,  our  proposed  changes  do  not  conflict  with  other 
KLFM  partitioning  extensions.  Furthermore,  s');
INSERT INTO posts (postId,userId,title,body) VALUES (192,1804,'Heterogeneous Systems  (part 2)','ome  of  our 
techniques,  such  as  phase-based  implementation  rebalancing 
and  multi-level  implementation  control,  could  also  be  adapted 
to non-KLFM-based partitioning algorithms.  

III.  PROBLEM DEFINITION 
We  define  graph  partitioning  for  heterogeneous  devices  as 
follows:  Partition  a  graph  with  nodes  that  have  weights  in  R 
resources  such  that  all  R  resources  are  balanced  within  their 
specified maximum  imbalance margins.  Each  nodes weight  is 
now  described  by  an  R-entry  weight  vector  w.  If  we  define 
wr(v) as node vs weight in resource r, and I
r  as the maximum 
weight imbalance between partitions for resource r, then  


v V
1
w vr




v V
2
w v
j



r

  

 r 1..
 =
       (1) 

The  resulting  cut  sizes  for  multi-resource  partitioning  are 
usually higher  than  those of  single-resource partitioning due  to 
the additional R1 constraints. 
For  devices  like  FPGAs  a  given  node  may  have  multiple 
implementations  using  different  resource  weights.  We  thus 
define  a  new  multi-personality  graph  partitioning  problem.  If 
we define  all possible  implementations  of node v  as Pv  and  the 
selected personality as p(v), we can reformulate the partitioning 
constraints as: 

) )


(
w p v
(w p v


v V
v V

j2
Whereas  multi-resource  partitioning  has  O(2N)  potential 
solutions,  where  N  is  the  number  of  nodes,  multi-personality 
partitioning has O(C(P)2N) potential solutions, where 

   

  r
 =

1 . .

I

) )



   (2) 

)

 

(3) 
C P

=    v
v V

The  number  of  personality  combinations  scales  non-
polynomially  with  the  number  of  multi-personality  nodes, 
resulting  in  a  much  larger  solution  space.  Although  this 
flexibility  may  make  it  possible  to  find  superior  solutions,  it 
also  increases  the  problem  complexity.  Furthermore,  within 
each  partition,  we  may  also  want  to  achieve  a  certain  relative 
utilization of resources, for example to find  the smallest usable 
FPGA  in  a  given  family  to  implement  the  netlists  partitions. 
We  refer  to  this  as  the  resource  utilization  ratio  (RUR),  and 
make it an additional goal for multi-personality partitioning. 

IV.  MULTI-PERSONALITY PARTITIONING ALGORITHM 
The base KLFM algorithm starts with an initial partitioning 
solution,  and  moves  nodes  between  partitions  in  multiple 
passes.  Each  pass,  nodes  swap  between  partitions  one-by-one 
until  all  that  can  move  have  moved,  and  at  the  end  of  each 
pass,  the  best  solution  achieved  during  that  pass  is  kept  as  the 
starting  point  of  the  next  pass.  In  single-resource  KLFM, 
structures  called  gain buckets  are  used  to  find  the  highest-gain 
node  (i.e.,  one  that  most  reduces  cutsize  or  other  cost 
measures)  that can move without  violating balance constraints. 
The  partitioning  quality  often  decreases  for  very  large  graphs, 
however, so multi-level KLFM [6] hierarchically clusters graph 
nodes,  and  partitions  at  each  level  (starting  with  the  coarsest) 
before un-clustering at that level and partitioning the next. 
This  section  describes  our  modifications  to  the  multi-level 
KLFM  partitioning  algorithm  to  support  multi-personality 
graph  partitioning.  The  required  changes  affect  key  data 
structures  and  introduce  unique  complexities.  As  part  of  our 
modifications,  we  added  dynamic  personality  selection, 
experimented  with  modifications  to  the  gain  buckets,  and 
introduced  new  pass-level  global  remapping.  We  also 
examined these techniques effects on multi-level partitioning. 
A.  Dynamic Gain Buckets 
Multi-personality  gain  buckets  also  perform  personality 
selection, with the goal of meeting balance and RUR goals. We 
experimented with two different approaches, described here. 
1)  Multi-Personality  Buckets:  Mul');
INSERT INTO posts (postId,userId,title,body) VALUES (193,1804,'Heterogeneous Systems  (part 3)','ti-Personality  Buckets 
are  similar  to  standard  gain buckets,  but  each  nodes  entry  has 
a  separate  set of  weights  (indicating  its  resource  requirements) 
for  each  of  its  personalities.  Each  step  within  a  pass, we  use  a 
tournament-style  selection  policy  to  determine  which  node  to 
move,  selecting  the  highest-gain  node  with  an  implementation 
that  would  allow  the  node  to  move  without  violating 
constraints.  For  all  of  that  nodes  implementations  that  do  not 
violate constraints, we compute an imbalance score, defined as 
the  root-mean-square  (RMS)  of 
the  resulting  fractional 
imbalances  in  all  resources.  The  RMS  tends  to  emphasize 
large  imbalances  in  one  resource  over  small  imbalances  in 
many  resources;  large  imbalances  are  more  likely  to  prevent 
future  high-gain  moves.  To  incorporate  an  RUR  goal,  we 
compute  a  similar  resource-ratio  score  for  each  possible 
implementation  as  the  RMS  of  the  percent  deviation  of  each 
resource from its target utilization, based on total utilization of 
all  resources  in  a  partition  and  the  target  utilization  ratio.  The 
node  with  the  best  imbalance  and/or  resource  ratio  score  is 
selected for movement (depending on the partitioner policy).  

2)  Resource-Affinity  Buckets:  Sometimes  it  is  beneficial  to 
move  nodes  with  implementations  in  specific  resources. 
Resource-Affinity  Buckets  separate  a  partitions  buckets  into 
several  resource-specific  queues,  making  it  easier  to  adjust  a 
specific  resource  to  improve  RUR  or  alleviate  partition 
imbalances. Resource-Affinity  buckets  can be used  in place of 
Multi-Personality buckets, or combined in a hybrid approach at 
the cost of additional computational overhead. 
B.  Pass-Level Implementation Remapping 
Global 
remapping  during  partitioning  can  modify 
personality  selections  to  reduce  resource  imbalance  and/or 
improve  RURs.  Because  it  is  a  relatively  expensive  operation 
we  apply  O(N)-complexity  global  remapping  algorithms  once 
per pass, and thus do not increase the asymptotic complexity of 
the  base  KLFM  algorithm.  We  experimented  with  two  global 
remapping  methods:  a  multi-phase  greedy  algorithm  that 
performs  a  random  walk  on  all  nodes  and  changes  their 
implementations  to  minimize  imbalance  and  RUR  scores,  and 
a fractured integer linear program (ILP) method. 
C.  Multi-Level Partitioning 
Implementation  selection  is  a major problem  in multi-level 
KLFM, as each additional component node creates a geometric 
increase  in  the  number  of  implementations  available  to  a 
supernode.  We  use  a  similar  approach  to  that  described  for 
populating  Resource  Affinity  Buckets  to  determine  the  subset 
of  implementations  for  each  supernode;  these  implementations 
provide  the  partitioner  with  the  greatest  ability  to  adjust 
resource  balance.  We  also  select  implementations  with  RURs 
closest  to  the  target  ratio,  since  achieving  a  similar  weight  for 
all nodes can at times lead to better partitioning results [7].  
During  experimentation,  we  observed  that  the  partitioner 
rarely  changed  supernode  implementations  at  the  coarsest 
levels  of  multi-level  partitioning,  and  t');
INSERT INTO posts (postId,userId,title,body) VALUES (194,1804,'Heterogeneous Systems  (part 4)','hat  enforcing  multi-
resource  constraints  at  these  levels  often  led  to  poor  results 
both at these and later levels. To address this, we relax resource 
constraints  based  on  the  coarseness  of  the  partitioning  level  to 
increase  freedom  and  improve  results at  the  coarsest  levels. As 
the  algorithm  progresses  to  less  coarse  graphs,  stricter  balance 
constraints are re-enforced.  
D.  Multi-Personality Partitioning Strategies 
We  developed  several  different  approaches  for  partitioning 
multi-personality,  heterogeneous-resource  graphs  using  the 
techniques  that  we  have  described  earlier  in  this  paper.  When 
choosing  these  partitioning  strategies,  our  goal  was  to  select 
those that were representative of the options currently available 
for  partitioning  and  implementation  mapping  algorithms.  We 
also aim to highlight tradeoffs in multi-personality partitioning. 
1)  Statically-Mapped Partitioning (SM):  
This  strategy 
is  based  on 
the  Native  Multi-Constraint 
Refinement 
for 
proposed 
[2] 
partitioning  method 
heterogeneous  FPGAs,  modified  to  use  Resource-Affinity 
Buckets.  We  first  apply  our  global  implementation  remapping 
algorithms  to  achieve  the  target  RUR.  Implementations  are 
then  fixed,  and  the  graph  is  partitioned  using  multi-constraint 
KLFM without dynamic personality selection or remapping. 
2)  Statically-Partitioned Remapping (SP):  
Statically-Partitioned  Remapping  converts  as  many  nodes  as 
possible  into  the  commonly-usable  resource  (CLBs  in  the  case 

of  FPGAs)  before  partitioning.  After  partitioning,  it  globally 
remaps  each  partition  based  on  the  target  RUR  and  balance 
constraints.  SP  is  conceptually  similar  to  the  Multi-Constraint 
Iterative  k-way  Balancing  method  [6],  however,  it  exploits 
nodes  multiple  personalities  and  our  global  remapping 
algorithms to enforce constraints. 
3)  Dynamic Multi-Personality Partitioning (DMP): 
DMP  uses  move-based 
implementation  selection  using 
Resource-Affinity  Buckets  and  pass-based  global  remapping 
using  the  multi-phase  greedy  algorithm.  Implementation 
selection  is  based  on  imbalance  score.  It  uses  fractured  ILP 
global remapping to improve the RUR after partitioning. 

TABLE I : SUMMARY OF BENCHMARK CIRCUITS/GRAPHS. 
% BRAM 
% DSP 
% CLB 
Total 

Benchmark 

144 
598a 
blob 
boundtop 
brack2 
cti 
diffeq1 
fe_ocean 
fe_rotor 
fe_tooth 
fft128 
isolation 
jet 
m14b 
mcm l 
memplus 
raygen 
rct 
sha 
wave 
wing 

Nodes 
144649 
110971 
11842 
29582 
62631 
16840 
4292 
143437 
99617 
78136 
91590 
187766 
189579 
214765 
346248 
17758 
11457 
241349 
3669 
156317 
62032 

Nodes 
94 
98 
100 
100 
99 
97 
100 
97 
99 
98 
100 
100 
100 
98 
100 
98 
100 
100 
100 
98 
99 

Nodes 
20 
18 
24 
0.5 
1 
14 
38 
3 
2.5 
4 
9.5 
1 
32 
3 
15 
2 
25 
31 
15 
6 
4 

Nodes 
1 
0.5 
0 
14 
8 
2 
0 
3 
1.5 
4 
0.5 
0 
0 
2 
0.1 
40 
0.1 
12 
0 
5 
2.5 

4)  Advanced Dynamic Multi-Personality (ADMP):  
ADMP  expands  on  DMP,  using  Hybrid  Buckets  for  move-
based selection and multi-level constraint relaxation. 
5)  DMP/ADMP with Fine-Grained Ratio Control  
          (DMP-FR / ADMP-FR):  
This  modification  considers  RUR  score  during  mo');
INSERT INTO posts (postId,userId,title,body) VALUES (195,1804,'Heterogeneous Systems  (part 5)','ve-based 
personality  selection  and  pass-based  global 
remapping, 
improving resource utilization at the cost of cut size. 

V.  EXPERIMENTAL METHODOLOGY 
We  implemented  a  complex-constraint,  multi-level  KLFM 
partitioner  and  modified  it  to  include  the  multi-personality-
aware  features  described  in  Section  IV.  Our  base  algorithm 
includes  common  KLFM  optimizations  such  as  LIFO  gain 
bucket  queues  and  randomized  initial  partitions  [4].  We 
validated performance of the base algorithm using  results  from 
the Walshaw partitioning archive [7]. 
We  then  compared  the  results  of  our  multi-personality 
algorithm 
against 
non-personality-aware 
partitioning 
approaches  in  both  cut  size  and  RUR  using  a  suite  of  21 
benchmark  graphs,  summarized  in  Table  I.  These  include 
several  heterogeneous  netlists  for  FPGAs  from  ERCBench  [8] 
and  VPR  [9];  graphs  from  the  Walshaw  graph  partitioning 
archive from domains such as communications, vibroacoustics, 
and  3D  meshes  [7];  several  FPGA  circuits  designed  for multi-
FPGA  high-energy  physics  experiments  such  as  particle 

isolation  (isolation),  jet  reconstruction  (jet),  and  calorimetry-
based  triggering  (rct)  [10];  a  ray  tracing  circuit  (raygen)  [11]; 
and  a  circuit  for  Monte  Carlo  simulation  for  photodynamic 
cancer  therapy  (mcml)  [12].  Where  HDL  was  available,  we 
assigned  node  weights  and  personalities  using  experimental 
synthesis  in  Xilinx  ISE  and  fixed  implementations  of  nodes 
along  critical  paths,  since  these may  not  have  the  flexibility  to 
use  alternate  personalities  due  to  timing  constraints.  For 
benchmarks  distributed  as  graphs,  we  computed  a  range  of 
potential  resource  costs  for  functional  units  by  synthesizing  a 
range  of  DSP  and  BRAM-based  units  and  assigned  weights 
within that range using a binomial distribution.  
For each experiment, we report the best result from 50 runs, 
since  KLFM  partitioners  often  use  the  best  result  of  multiple 
runs.  We  applied  an  imbalance  limit  of  1%  for  all  resources. 
We  set  a  target  ratio  of  1:1:1  for  the  percent  utilization  of  the 
three  primary  FPGA  resources  and  computed  the  root-mean-
square of the percent deviation from the target resource weight. 
If  a  circuit  did  not  use  a  resource,  it  was  ignored.  Resource 
capacity ratios are based on the Xilinx Virtex-7 2000T [13]. 

VI.  RESULTS 
Fig.  1  shows  a  comparison  of  cut  size  results  for  2-way 
multi-personality  partitioning,  normalized  to  the  result  of 
Statically-Mapped  Partitioning  (SM).  Cut  size  is  reported  as 
the  sum  of  the  weight  of  all  wires  that  span  more  than  one 
partition.  Due  to  space  limitations,  the  chart  includes  a 
representative  set  of  individual  results  for  nine  benchmarks, 
and the geometric mean of results for all 21 benchmarks. Fig. 2 
shows  a  comparison  of  target  resource  utilization  deviation. 
The  results  are  also  summarized  in  Table  II.  For  each  metric 
shown in these figures and this table, lower');
INSERT INTO posts (postId,userId,title,body) VALUES (196,1804,'Heterogeneous Systems  (part 6)',' values are better. 
A.  Cut Size and Target Resource Utilization 
SM achieves worse cut  sizes  than other strategies due to its 
difficult  task  of  balancing  multiple  resources.  Results  of  the 
dynamic  algorithms  demonstrate 
that 
changing  node 
personalities  during  partitioning  can 
improve  cut  sizes, 
resulting  in mean improvements of 27% over SM and 6% over 
SP  when  using  ADMP.  The  addition  of  dynamic  ratio  control 
harmed  cut  size  results  by  a  small  amount,  but  improved 
deviations  from  the  target  RUR.  SM,  however,  results  in  the 
best deviation from target RUR because its netlists are mapped 
prior to partitioning and each resource is limited to a maximum 
imbalance  of  1%  between  partitions.  However,  this  comes  at 
the  aforementioned  cut  size  penalty.  SP  had  the  worst 
deviationnot using personality  information during portioning 
makes  it difficult  to balance heterogeneous  resource utilization 
afterwardswhich in almost all cases came from over-utilizing 
CLBs  and  under-utilizing  specialized  resources.  Results 
demonstrate  that  the  dynamic  algorithms  offer  good  cut  sizes 
and a compromise in deviation between SP and SM. 
For both  cut  size  and RUR deviation,  the magnitude of  the 
difference  between  the  partitioning  strategies  varied  heavily 
from benchmark to benchmark, largely as a consequence of the 
topology  and  the  quantity  and  distribution  of  resources  in  the 
graph. The circuit for particle isolation, for example, has a very 
regular grid-based structure with uniform  resource distribution, 
so  almost  any  partitioning  would  lead  to  balanced  resource 
utilization.  Jet  and  boundtop,  on  the  other  hand,  had  most  of 

1.0

0.8

0.6

0.4

blob

boundtop brack2

fe_rotor fe_tooth

fft128

isolation

jet

mcml

geomean

SM
SP
DMP
DMP+
DMP-FR
DMP+FR

Fig . 1.   Cut size results, norma lized to statically-mapped partitioning. The geometric mean includes data from all 21 benchmarks. Lower values are better. These 
results are also summarized in Table I I. 

100%

107%

140%

90%

102%

80%
60%
40%
20%
0%

SM
SP
DMP
DMP+
DMP-FR
DMP+FR

blob
boundtop brack2
fe_rotor fe_tooth
fft128
isolation
jet
mcml
geomeanFig. 2.   RMS deviation from target resource utilization for a subset of  benchmarks. The geometric mean contains data from all 21 benchmarks. Lower values are 
better. These results are also summarized in Table II. 

their  non-CLB  resources  distributed  in  clusters,  allowing  for 
more 
significant 
tradeoffs  between  balanced 
resource 
utilization  and  cut  size.  The  multi-level  constraint  relaxation 
technique  included  in DMP+ proved particularly effective with 
the  highly  clustered  and  interconnected  topologies  of  large 
memories,  as  evidenced  by  its  improvement  in  cut  size  for 
boundtop, which includes a large shared memory. 
TABLE II:  GEOMETRIC MEANS OF THE EVALUATED COST METRICS AND 
NORMAL IZED WALL-CLOCK RUN TIME FOR EACH STRATEGY. 
SP  DMP  ADMP  DMP-FR  ADMP-FR 
SM 
Norm. 

Cut Size 

RUR 

Deviation 

Norm. 

Run Time 

1.00 

0.79 

0.78 

0.73 

0.83 

0.80 

1.6% 

61% 

24% 

19% 

4.6% 

4.0% 

1.0 

0.6 

1.3 

1.7 

1.3 

1.7 

B.  Run-Time Cost 
Although  our  modifications  do  not  alter  the  asymptotic 
complexity  of  the  base  multi-level  KLFM  algorithm,  they 
increase  algorithm  run-time  linearly.  To  roughly  estimate  the 
impact  of  various  techniques  on  run  time,  we  ran  the  multi-
personality  algorithm  with  different  combinations  of  ');
INSERT INTO posts (postId,userId,title,body) VALUES (197,1804,'Heterogeneous Systems  (part 7)','enabled 
features  over  a  subset  of  the  largest  benchmarks  and  recorded 
the wall clock time. Table II presents the results, normalized to 
the run time of Statically-Mapped Partitioning. 

VII.  CONCLUSION  
Although  existing  partitioning  algorithms  can  handle 
multiple  resources  on  heterogeneous  devices  in  a  primitive 
fashion,  they  do  not  exploit  the  overlapping  functionality 
provided  by  some  of  these  resources.    Integrating  dynamic 
selection of node  implementations  into  the partitioner  achieves 
up  to  a 27% mean  improvement  in partition  cut  size  compared 
to  partitioning  a  statically-mapped  circuit  for  our  suite  of  21 
benchmarks. Dynamically selecting node personalities for ratio 
control  can  achieve  up  to  a  15X  mean  improvement  in 
deviation  in  target  resource  utilization  compared  to  post-
partition  resource  mapping.  Combining  the  enhanced  dynamic 
algorithm  with  fine-grained  resource  ratio  control  makes  it 
possible  to  achieve  better  cut  size  benefits  than  statically 
partitioned  enforcement-based  strategies  while  also  achieving 

most  of  the  resource  utilization  advantages  of  statically-
mapped partitioning approaches. 

REFERENCES 

[1] T. Taghavi, et  al ., "Innovate  or perish: FPGA physical design," in 
ISPD, 2004, pp. 148-155. 
 [2] Selvakkumaran,  N.;   Ranjan,  A.;  Raje,  S.;  Karypis,  G.;,  "Multi-
resource  aware   partitioning  algorithms  
for  FPGAs  with 
heterogenous resources," in DAC, 2004, pp. 741-746. 
 [3] H.  Liu,  K.  Zhu,  and  D.F.  Wong,  "Circuit  partitioning  with 
complex  resource constraints  in FPGAs,"  in FPGA, 1998, pp. 77-
84. 
 [4] S.  Hauck  and  G.  Borriello,  "An  evaluation  of  bipartitioning 
techniques ,"  IEEE  Trans.  on  CAD,  vol .  16,  no.  8,  pp.  849-866, 
1997. 
 [5] M.  Inagi ,  et  al.,  "A  performance-driven  circuit  bipartitioning 
algorithm  for  multi-FPGA  implementation with  time-multiplexed 
I/Os ," in FPT, 2006, pp. 361-364. 
 [6] G.  Karypis  and  V.  Kumar,  "Multilevel   Algorithms  for  Multi-
Constraint  Graph  Partitioning,"  University  of  Minnesota, 
Minneapolis, MN, Tech Report 98-019, 1998. 
 [7] A.J.  Soper,  C.  Walshaw,  and  M.  Cross ,  "A  Combined 
Evolutionary  Search  and  Multilevel   Optimisation  Approach  to 
Graph  Partitioning,"  J.  Global  Optimization,  vol.  29,  no.  2,  pp. 
225-241, 2004. 
 [8] D.  Chang  et  al.,  "ERCBench:  An  Open-Source  Benchmark  Suite 
for   Embedded  and  Reconfigurable  Computing,"  in  FPL, Milano, 
2010, pp. 408-413. 
 [9] V.  Betz  and  J.  Rose,  "VPR:  A  new  packing,  placement   and 
routing  tool  for  FPGA  research,"  in  Field-Programmable  Logic 
and Applications, 1997, pp. 213-222. 
 [10] A.  Gregerson,  et   al.,  "FPGA  design  analysis  of  the  clustering 
algorithm for the CERN Large Hadron Collider," in FCCM, 2009, 
pp. 19-26. 
 [11] J. Fender  and J. Rose, "A high-speed ray  tracing engine built on a 
field-programmable system," in FPT, 2003, pp. 188-195. 
 [12] J.  Luu, ');
INSERT INTO posts (postId,userId,title,body) VALUES (198,1804,'Heterogeneous Systems  (part 8)',' et   al.,  "FPGA-based  Monte  Carlo  computation  of   light 
absorption  for  photodynamic  cancer  therapy,"  in  FCCM,  2009, 
pp. 157-164. 
 [13] P.  Dorsey,  "Xilinx  stacked  silicon  interconnect  technology 
delivers  breakthrough  FPGA  capacity,  bandwidth,  and  power  
efficiency," Xilinx White Paper: Virtex-7 FPGAs , 2010. 

');
INSERT INTO posts (postId,userId,title,body) VALUES (199,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','AbstractNear-term quantum computers will soon reach sizes
that are challenging to directly simulate, even when employing
the most powerful supercomputers. Yet, the ability to simulate
these early devices using classical computers is crucial for cal-
ibration, validation, and benchmarking. In order to make use
of the full potential of systems featuring multi- and many-core
processors, we use automatic code generation and optimization
of compute kernels, which also enables performance portabil-
ity. We apply a scheduling algorithm to quantum supremacy
circuits in order to reduce the required communication and
simulate a 45-qubit circuit on the Cori II supercomputer using
8, 192 nodes and 0.5 petabytes of memory. To our knowledge,
this constitutes the largest quantum circuit simulation to this
date. Our highly-tuned kernels in combination with the re-
duced communication requirements allow an improvement in
time-to-solution over state-of-the-art simulators by more than
an order of magnitude at every scale.

1. Introduction

Quantum computers promise to solve problems which
would be impossible to tackle with classical machines.
While such devices will not speed up every application,
there are certain areas which could be revolutionized by
quantum computers. This includes quantum chemistry [1],
[2], [3], material science [4], machine learning [5], [6], [7],
[8], and cryptography [9].
Experimental devices featuring close to 50 quantum bits
(qubits) will soon be available and may be able to perform
well-dened computational tasks which would classically
require the worlds most powerful supercomputers. Going
even beyond these capabilities means entering the domain
of Quantum Supremacy [10], [11]. While one of the compu-
tational tasks proposed to demonstrate this supremacy  the
execution of low-depth random quantum circuits, see Fig. 1
 is not scientically useful on its own, running such circuits
is still of great use to calibrate, validate, and benchmark
near-term quantum devices [10].
Therefore, in addition to verifying quantum algorithms
and carrying out studies of their behavior under noise, quan-
tum circuit simulators may provide the means to carry out
these calibrations and benchmarks and thereby enable more
efcient quantum hardware/software co-design. Quantum
circuit simulators are thus comparable to tools such as the
structural simulation toolkit (SST) [12], which allows to
simulate upcoming classical hardware.

Related work. There are many implementations of quantum
circuit simulators [13] available, most of them are meant to
simulate small systems on a single node. The most widely-
used single-node simulator is Microsofts LIQU i |(cid:105) [14],
which is implemented in F# and is thus not as fast as
simulators written in, e.g., C++ [15]. The massively parallel
simulator from [16], [17] was used to simulate 42 qubits
on the J ulich supercomputer in 2010, which set the new
world record in number of simulated quantum bits. Recently,
Intels qHiPSTER [18] was specialized for the simulation of
quantum supremacy circuits and then used to simulate these
circuits up to 42 qubits [10]. A topic similar to quantum
circuit simulation is emulation which, instead of simulating
individual gates, uses classical shortcuts in order to reduce
the time-to-solution for quantu');
INSERT INTO posts (postId,userId,title,body) VALUES (200,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','m operations whose action
is known in advance. An example for such a shortcut is
the quantum Fourier transform, which can be emulated by
applying a fast Fourier transform to the state vector [15].
However, such emulation techniques are not applicable to
quantum supremacy circuits.

Our contribution. We improve the strong scaling behavior
of the compute kernels underlying quantum circuit simu-
lation in order to reduce time-to-solution when employing
multi- and many-core processors. In the multi-node domain,
we employ a communication scheme similar to [17] and
introduce an additional layer of optimization to reduce the
amount of communication required: We apply a clustering
algorithm to the quantum circuit in order to improve the
scheduling of quantum gate operations. While this pre-
computation terminates in 1  3 seconds on a laptop, it
greatly reduces the number of communication steps. We
then simulate quantum supremacy circuits of various sizes
and report speedups of over one order of magnitude on all
scales. Finally, we simulate a 45-qubit quantum supremacy
circuit on the Cori II supercomputing system using 0.5
petabytes of memory and 8, 192 nodes. To our knowledge,
this constitutes a new record in the maximal number of
simulated qubits. The classical simulation of such circuits
is believed to be impossible already for 49 qubits which,
according to [19], is the threshold for quantum computers
outperforming the largest supercomputers available today at
the task of sampling from the output distribution of random
low-depth quantum circuits. While we do not carry out
a classical simulation of 49 qubits, we provide numerical
evidence that this may be possible. Our optimizations allow
reducing the number of communication steps required to
simulate the entire circuit to just two all-to-alls, making

Figure 1: Low-depth random quantum circuit proposed by Google to show quantum supremacy [10]. We generated identical
circuits using the following rules: At clock cycle 0, a Hadamard gate is applied to each qubit. Afterwards, eight different
patterns of controlled Z (CZ) gates are applied repeatedly until the desired circuit depth is achieved. See the 8 different
CZ patterns above in clock cycles 1-8 for a 6  6 qubit circuit, where the CZ gates are represented by a line between two
qubits. This pattern ensures that all possible two qubit interactions on this 2D nearest neighbor architecture are executed
every 8 cycles. In addition to the CZ gates, single qubit gates are applied to all qubits which in the previous cycle (but not
in the current cycle) performed a CZ gate. The single qubit gates are randomly chosen to be either a T (red), X 1/2 (blue),
or Y 1/2 (yellow) gate, except that the second single-qubit gate on each qubit (the rst is the Hadamard gate in cycle 0) is
always a T gate and when randomly choosing a single-qubit gate, it must be different from the previous single-qubit gate
on that qubit.

it possible to use, e.g., solid-state drives if the available
memory is less than the 8 petabytes required.

2. Basics of quantum computer simulation

A quantum computer consists of quantum bits (qubits).
A qubit is a two-level quantum systems whose state |(cid:105) can
be described by 2 complex numbers 0');
INSERT INTO posts (postId,userId,title,body) VALUES (201,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa',' and 1 as
|(cid:105) = 0 |0(cid:105) + 1 |1(cid:105) ,
where |0 |2 + |1 |2 = 1. The two computational basis states
|0(cid:105) and |1(cid:105) are orthonormal and the qubit, when measured,
collapses onto either |0(cid:105) or |1(cid:105) with probability p = |0 |2
and |1 |2 = 1  p, respectively. The measured qubit is then
just a classical bit. To store |(cid:105) on a classical computer,
it is more practical to choose two orthonormal vectors to
(cid:18)0
(cid:18)0
(cid:18)1
(cid:19)
(cid:19)
(cid:19)
represent |0(cid:105) and |1(cid:105),
|(cid:105) = 00Any operation on this qubit can then be represented as a
complex unitary 2  2 matrix. Applying, for example, a

+ 1

.

1 0 ), to the state |(cid:105),
bit-ip operation, denoted by X = ( 0 1
(cid:19)
(cid:18)1
(cid:19)
(cid:19) (cid:18)0
(cid:18)0
amounts to a matrix-vector multiplication:
= 1 |0(cid:105) + 0 |1(cid:105) .
X |(cid:105) ==00
(cid:0) 1 1
(cid:1), the T gate T =
(cid:0) 1+i 1i
(cid:1), and Y 1/2 = 1
(cid:0) 1+i 1i
(cid:1).
(cid:1), X 1/2 = 1
(cid:0) 1
Other single-qubit gates used in quantum supremacy circuits
are the Hadamard gate H = 1
1 10
1i 1+i
0 ei/4
1+i 1+i2
The state of a two-qubit system |(cid:105) can be represented
using 4 complex coefcients giving the contribution of all .
possible classical states featuring two bits, i.e.,2and operations acting on the entire state can be represented
by complex unitary matrices of dimension 44. An example
for a two-qubit gate which occurs in quantum supremacy

|(cid:105) = 00 |00(cid:105) + 01 |01(cid:105) + 10 |10(cid:105) + 11 |11(cid:105) =

(1)(2)(3)(4)(5)(6)(7)(8)C Z =

1 0
 ,
circuits is the controlled-Z or CZ gate,00
0 11
0 0
0 1
0 0
which adds a (1)-phase to the |11(cid:105) basis state. Also, note
that this gate is symmetric in terms of qubits  it does
not matter which qubit is the control / target qubit. This
symmetry of the CZ gate can also be seen in Fig. 1.
To build the 4  4 matrix acting on the entire system
when applying a single-qubit operation to just one qubit, one
performs a Kronecker product with a 2  2 identity matrix.
Applying an X -gate to the rst qubit (with bit-index 0) can
be achieved as follows:
X0 |(cid:105) = 12  X |(cid:105)
= 01 |00(cid:105) + 00 |01(cid:105) + 11 |10(cid:105) + 10 |11(cid:105)
= 00 |01(cid:105) + 01 |00(cid:105) + 10 |11(cid:105) + 11 |10(cid:105) .
More generally, the state of an n-qubit quantum computer
can be represented by a complex vector of size 2n
|(cid:105) = 0 |0    00(cid:105) + 1 |0    01(cid:105) +    + 2n1 |1    11(cid:105)
and operations on this state are 2n  2n unitary matrices.
Finally, applying a single-qubit gate U to the i-th qubit of
an n-qubit quantum computer amounts to multiplying the
state vector of coefcients i by the matrix
(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:124)
(cid:124)
 U  12      12
12      12
ni1which is just a complex sparse matrix-vector multiplication
of dimension 2n . Thus, for double-precision values, just
storing the state vector for 50 qubits would already require
16 petabytes of memory.

3. Optimizations

Our s');
INSERT INTO posts (postId,userId,title,body) VALUES (202,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','imulator was implemented and optimized using a
layered approach. The rst layer aims to improve the single-
core performance of our quantum gate kernels by employing
explicit vectorization using compiler intrinsics, instruction
reordering, and blocking to reduce register-spilling. The
second layer uses OpenMP to enable a good strong scal-
ing behavior on an entire node. The third and nal layer
implements the inter-node communication scheme using
MPI. This allows to simulate up to 45 qubits on current
supercomputers, in addition to reducing the time-to-solution
when executing quantum circuits featuring fewer qubits.

3.1. Standard optimizations

In order to be able to simulate large systems,
is
it
important not to actually store the 2n  2n matrix acting
on the state vector. Instead, one can exploit
its regular
structure and implement methods which, given the state
vector, mimic a multiplication by this matrix. A standard

implementation features two state vectors (one input, one
output). To determine one entry of the output vector, two
complex multiplications and one complex addition have to
be carried out on two entries of the input vector when
applying a general single-qubit gate. In total, there are thus
2  (4[mul] + 2[add]) + 2[add] = 14 FLOP
per complex entry of the output state vector. One complex
double-precision entry requires 16 bytes of memory and the
input vector has to be loaded from memory and the output
vector has to be written back to memory. The operational
intensity is therefore less than 1/2, which shows that this
application is memory-bandwidth bound on most systems.

3.2. Single-core

In order to reduce the memory requirements by a factor
of 2x, this complex sparse matrix-vector multiplication can
be performed in-place, at the cost of a cache-unfriendly
access pattern. Moreover, k-qubit gates require more op-
erations for larger k , allowing to better utilize hardware
with strong compute capabilities. In fact, the number of
operations grows exponentially with k , since applying a
k-qubit gate amounts to performing one scalar product of
dimension 2k per (output) entry.
To apply a k-qubit gate (of dimension 2k  2k ) to a
state vector of size 2n , where n denotes the number of
qubits, the entries corresponding to all 2k indices of the
gate matrix have to be loaded into a 2k -sized temporary
vector, which then gets multiplied by the matrix before it
is written back to the state vector. The indices these state
vector entries, when represented in binary, are bit-strings
of the form cnk1xik1 ...cj ...xi1 ...c0 , where i0 , i1 , ..., ik1
denote the k qubits indices to which the gate is being
applied. Extracting and combining the bits xij from the
index of an entry, i.e.,

x = xik1 ...xi1 xi0 ,
yields the index of this entry with respect to the tempo-
rary vector. All 2k entries which have an identical c =
cnk1 cnk2 ...c0 index substring are part of this matrix-
vector multiplication. Once all entries have been gathered,
multiplied by the matrix, and stored back into the state
vector, the next c(cid:48) = c + 1 index substring can be dealt
with. In total, this amounts to performing 2nk complex
matrix-vector multiplications of dimension 2k .
A rst observation is that the same matrix is used 2nk
times. ');
INSERT INTO posts (postId,userId,title,body) VALUES (203,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','One can thus permute the matrix entries before-hand
in order to always have sorted qubit indices, which results
in memory accesses to occur in a more local fashion.
When applying the matrix-vector product, doing so in
2k1(cid:88)
the usual manner, i.e.,
i=0
would require all entries of the temporary vector v to be
in register (and already loaded from memory). In order to

ml,i vi ,

vl =

(b) Rooine plot for one KNL node of Cori II.
(a) Rooine plot for one Edison socket.
Figure 2: Rooine plots illustrating the performance improvements from Sec. 3.2 and 3.3. Step 1 introduces lazy evaluation,
making the application more compute-bound. Step 2 adds explicit vectorization and instruction re-ordering, followed by
step 3 which applies blocking for registers in addition to a pre-computation on the gate matrix, re-ordering and permuting
the complex-valued matrix entries to improve the FLOP/instruction ratio. An additional optimization specic to KNL is the
blocking for MCDRAM, which is introduced in step 1.

vl +=

ml,i(b,j ) vi(b,j ) ,

address this issue, we employ blocking of the computation
and determine the block size using an automatic code-
generation / benchmarking feedback loop. For each block
B  1, all indices l of the temporary
index b = 0, 1, ..., 2k
(cid:88)
output vector v are updated according to
j<B
where i(b, j ) = b  B + j , before moving on to the next
block.
We employ explicit vectorization to parallelize updates
for consecutive values of l. Since we are dealing with
complex double-precision values, this theoretically allows
to speed up the execution by a factor of 2x or even 4x
when using AVX or AVX512, respectively. Denoting by aR
and aI the real and imaginary parts of a, respectively, we
now inspect the update above more closely. Multiplying one
complex entry vl = (vR , vI ) of the temporary vector v with
one complex entry of the gate matrix m = (mR , mI ) and
summing the result into the temporary output vector v can
be written as follows:
(vR , vI ) += (vR  mR  vI  mI , vI  mR + vR  mI ) (1)
Yet, implementing this update results in wasted compute
resources due to articial dependencies and additional per-
mutes. However, these instructions can be re-ordered as
follows

(vR , vI ) += (vR  mR , vI  mR )
(2)
(vR , vI ) += (vI  1  mI , vR  mI )
(3)
in order to increase the maximal achievable performance.
Namely, having both (mR , mR ) and (1mI , mI ) available,
this update requires only two fused multiply-accumulate
instructions instead of several
individual multiplications,
additions, and permutations. This is an improvement in both
FLOP/instruction and FLOP/FMA ratios.

Note that vl can be permuted once upon loading (and
then kept in register), as it is re-used for 2k such complex
multiplications. Also, since the matrix m is used in 2nk
matrix-vector multiplications, the pre-computation to build
up these two matrices consisting of (mR , mR ) and (1 
mI , mI ) is essentially free.

3.3. Single-node

The optimizations discussed above do not change the
fact that the operational intensity for applying a 1-qubit gate
is very low, making it harder to fully utilize the power of
multi- and manycore processors (see, e.g., Fig. 10)');
INSERT INTO posts (postId,userId,title,body) VALUES (204,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','. Yet,
as mentioned previously, applying a k-qubit gate requires
more operations for larger values of k and as long as the
application remains memory bound,
larger gates can be
applied in (almost) the same amount of time. The benet 
besides increased operational intensity  is that larger gates
can be used to execute an entire sequence of single- and
two-qubit gates at once. In particular, multiple gates acting
on k different qubits can be combined into one large k-qubit
gate.
Which value of k to choose depends on the peak per-
formance, the memory-bandwidth, the cache-size '||'&'||' asso-
ciativity of the system, and the circuit to simulate. The
cache specications are important especially when gates are
applied to qubits with larger indices, which cause memory
access strides of large powers of two. For low-associativity
caches, this causes conicts to arise already for small kernel
sizes. Since 2k values need to be loaded from the state vector
(which are at least 2m apart, where m is the lowest qubit
index) for each of the 2nk matrix-vector multiplications, a
2k -way cache should map the corresponding cache-lines to
different locations, no matter how large m is. This allows to
directly access these values from cache for the next matrix-
vector multiplication. See Fig. 6 for experimental results.

166.2 10 1000.54 1 10Performance [GFLOPS]Operational Intensity [FLOP/byte]Stream TRIAD BW (52 GB/s)Peak performance: 230.4 GFLOPS (12 cores, AVX)1-qubit kernel1.4-qubit kernel2.3.10100229.6442.7878.70.54 1 10Performance [GFLOPS]Operational Intensity [FLOP/byte]MCDRAM BW (460 GB/s)Peak performance: 3133.4 GFLOPS (68 cores, AVX512, FMA)1-qubit kernel1.4-qubit kernel2. (AVX)2. (AVX512)3.1/2 MCDRAM BW (230 GB/s)DRAM BW (115.2 GB/s)Finally, these k-qubit gate kernels are parallelized using
OpenMP with NUMA-aware initialization of the state vector
to ensure scaling beyond 1 NUMA node. Depending on
the qubits to which the gate is applied,
the outer-most
loop may perform very few iterations, prohibiting a good
strong scaling behavior. The OpenMP collapse directive
remedies this problem.
Please see Fig. 2a and Fig. 2b, which show the im-
provements in performance when applying all mentioned
optimizations and running the kernels on one socket of
Edison or Cori II, which feature one 12-core Intel R(cid:13) Xeon R(cid:13)
Processor E5-2695 v2 and one 68-core Intel R(cid:13) Xeon PhiTM
Processor 7250 (KNL), respectively.

3.4. Multi-node

The simulation of quantum computers featuring many
more than 30 qubits requires multiple nodes in order for the
state vector to t into memory. We use MPI to communicate
between 2g nodes, each node having its own state vector of
size 2l , where g and l denote the number of global and
local qubits, respectively. Gate operations on local qubits,
i.e., qubits with index i < l, require no communication.
Qubits with index i  l, on the other hand, do require
communication.
There are two basic schemes which can be used to
perform multi-node quantum circuit simulations. The rst
[16] keeps global qubits global and applies global gates by
employing 2 pair-wise exchanges of half the state vector.
The second scheme [17] swaps global q');
INSERT INTO posts (postId,userId,title,body) VALUES (205,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','ubits with local
ones, applies gates to local qubits in the usual fashion and,
if need be, swaps them again with global qubits. Note that
swapping in a global qubit and then immediately swapping
it back out requires the same amount of communication as
the rst scheme. We thus expect the global-to-local scheme
to perform better and focus on this scheme.
1-Qubit Example (see Fig. 3a). For the case of two ranks,
swapping the highest-order qubit (highest bit in the local
index) with the global qubit (rst bit of the rank number)
can be achieved as follows: The rst block of rank 0 remains
unchanged, since swapping 0 with 0 has no effect. Swapping
0 (global) and 1 (local) for the second block requires sending
the entire block to rank 1, where these coefcients are
associated with the local qubit being 0. Proceeding in this
manner results in an exchange of the colored blocks, which
is equivalent to an all-to-all.
2-Qubit Example (see Fig. 3b). To swap two global qubits
with the two highest-order local qubits for the case of four
ranks, each rank sends its i-th quarter of the state vector
to rank number i. Therefore, all identically-colored state
vector parts are exchanged, which results again in one
all-to-all.

Additionally, as done in [17], we generalize this scheme
to swap multiple or even all global qubits with local ones.
Yet, in contrast to [17], we do not iteratively copy out parts
of the state vector and carry out the pair-wise exchanges

(a) Single-qubit swap.

(b) Two-qubit swap.

Figure 3: Illustration of a single- and multi-qubit global-to-
local swap using one (group-) all-to-all. The blocks labeled,
e.g., 01... represent the coefcients corresponding to the
global basis state which starts with the bit-string r01, where
r is the bit-representation of the rank (see text).

manually. Instead, we employ higher-level abstractions to
achieve the same task, with the benet
that optimized
implementations for, e.g., specic network topologies are
likely to be already available. A q -qubit global-to-local
swap, which exchanges q global with q local qubits, can be
achieved using 1 group-local all-to-all for each of the 2gq
groups of processes. Therefore, turning all global qubits
into local ones amounts to executing one all-to-all on the
MPI_COMM_WORLD communicator. This allows swapping
the k qubits with highest local index with k global ones.
In order to allow for arbitrary local qubits to be exchanged,
we rst use our optimized kernels to achieve local swaps
between highest-index qubits and those to be swapped.
We then perform the group-local all-to-all and, if need be,
another local swap (with lower-index qubits) in order to
improve data locality in our k-qubit gate kernels.

3.5. Global gate specialization

While a general global gate always requires commu-
nication,
there are a few common ones which do not.
Examples include the controlled-NOT gate (or controlled-X)
which, when applied to global qubits, causes merely a re-
numbering of ranks. The (diagonal) controlled-Z gate either
turns into a conditional global phase or a local Z-gate which,
depending on the rank, is executed or not. Finally, the T-gate
is also diagonal and results in a global phase, which can be
absorbed into the next gate matrix to be applied. Making
use of such insights allows to further reduce the number
of global-to-local swaps without i');
INSERT INTO posts (postId,userId,title,body) VALUES (206,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','ncreasing the amount of
computation performed locally.

Rank0Basis states0...1...10...1...Rank00Basis states00...01...10...11...0100...01...10...11...1000...01...10...11...1100...01...10...11...For 36-qubit quantum supremacy circuits, this optimiza-
tion enables a reduction of the required communication
by another factor of 2x: Only one global-to-local swap is
required to run the entire depth-25 circuit. For 42- and 45-
qubit circuits, 2 global-to-local swaps are necessary, whereas
3 are required without gate specialization.

3.6. Circuit Optimizations: Gate scheduling and
qubit mapping

In addition to performing implementation optimizations,
also the circuit requires optimization in order to reduce the
number of communication steps and to use our highly-tuned
kernels in a more efcient manner. We will demonstrate
the different optimizations for gate scheduling and qubit
mapping using the quantum supremacy circuits from [10],
for which we also present performance results in the next
section. Our optimizations are general and can be applied to
any quantum circuit. In fact, these quantum supremacy cir-
cuits happen to be designed in a way that is least suitable for
these kinds of performance optimizations. We thus expect
even larger improvements when employing these techniques
for the simulation of other circuits.
The construction of these random, low-depth quantum
circuits is shown in Fig. 1. These circuits are designed to
be run on a quantum computer architecture featuring a 2D
nearest-neighbor connectivity graph. By design, all possible
two qubit gates are applied within 8 cycles, which makes
the system highly entangled. Note that a simulator can skip
the initial Hadamard gates in cycle 0 and initialize the wave
function directly to (2 n
2 , ...)T , instead of starting in state
|0...0(cid:105) = (1, 0, ..., 0)T . Furthermore, we do not simulate the
nal CZ gates as they only alter the phases of the probability
amplitudes i , but not the probabilities pi = |i |2 which we
are interested in.

3.6.1. Gate scheduling. The most important optimization
on the quantum circuit is gate scheduling, as it drastically
reduces the amount of communication in the multi-node
setting and also the number of k-qubit gate kernels on
the single-node level. The optimizations can be broken into
three steps:
1. Minimize number of communication steps. In a rst
optimization step, gate scheduling minimizes the number of
global-to-local swaps which is the most important parameter
in the multi-node setting. Executing every clock-cycle of the
circuit on its own requires at least one communication step
for every cycle which features a non-diagonal global gate.
However, as explained in the multi-node strategy, it is
benecial not to execute those global gates but rather swap
global qubits with local qubits and then execute these gates
locally. In order for this scheme to be most benecial, the
gate scheduling algorithm reorders (if possible) the gates
into stages, where each stage consists of a sequence of
quantum gates acting only on local qubits, see Fig. 4.
Gates acting on the same qubit never commute for quantum
supremacy circuits by design, making clas');
INSERT INTO posts (postId,userId,title,body) VALUES (207,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','sical simulation

harder. Nevertheless, we can reorder gates which act on
different qubits as they commute trivially. After completing
a stage, some local qubits are swapped with global qubits,
and a new stage is started. This scheme reduces the number
of communication steps signicantly. A depth-25 42-qubit
supremacy circuit requires only two global-to-local swaps,
see Fig. 5b. An important feature of our gate scheduling
algorithm is that the number of global-to-local swaps is
mostly independent of the number of local qubits (29, 30,
31, or 32). This allows for a good strong scaling behavior.
Fig. 5a shows how the number of global-to-local swaps
behaves as a function of circuit depth.
We decided to always swap global qubits with the
lowest-order local qubits to arrive at an upper bound for the
number of communication steps required. In addition, we
apply a cheap search algorithm to nd better local qubits
to swap with. In case of a 36-qubit supremacy circuit, this
results in a 2x reduction in the number of global-to-local
swaps, from two swaps to just one. Note that our stage-
nding algorithm assumes the worst-case scenario, in which
all randomly picked global single-qubit gates are dense,
meaning that we cannot apply our gate specialization for
T gates to reduce the amount of communication.
2. Minimize number of k-qubit gates. In a second step,
we schedule all the gates within a stage such that we can
merge sequences of consecutive 1- or 2-qubit gates into a
k-qubit gate and execute this k-qubit gate instead of many
single- and two-qubit gates. See Fig. 4, which shows how
such a cluster with k = 3 can be built. We greedily try
to increase the number of qubits k within a cluster while
still maintaining the condition that k < kmax , where kmax
is determined by the largest k-qubit gate kernel which still
shows good performance on the target system. To reduce
the over-all number of clusters, we perform a small local
search in order to build the largest cluster with gates not
yet assigned, before assigning the remaining gates to new
clusters. We summarize the required number of clusters to
execute a quantum supremacy circuit in Table 1. Clearly,
even for these circuits, more than k gates can be merged
into one k-qubit cluster on average.
3. Local adjustments of global-to-local swaps. The last
clusters within each stage tends to contain a lower number of
single- and two-qubit gates. In order to increase the average
number of gates in each cluster and thereby minimize the
total number of clusters in the circuit, we try to remove the
last clusters of each stage by performing the global-to-local
swap earlier if this is possible without increasing the total
number of global-to-local swaps.

3.6.2. Qubit mapping. Last, the bit-location of each qubit
is optimized in order to reduce the number of clusters ex-
periencing the performance decrease resulting from the set-
associativity of the last-level cache. Since this performance
decrease only occurs if the gate is applied to high-order bit-
locations, this can be achieved by remapping. The following
heuristic allowed for a 2x decrease in time-to-solution:
Assign the qubit to bit-location 0 such that the number
of clusters accessing bit-location 0 is maximal. From now

Number
of Qubits

Number
of Gates

Number of clusters

kmax = 3

kmax');
INSERT INTO posts (postId,userId,title,body) VALUES (208,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa',' = 4

kmax = 5

30
36
42
45

369
447
528
569

82
98
111
111

46
53
58
73

36
41
46
51

TABLE 1: Rescheduling of the gates of 25-depth quantum
supremacy circuits into clusters (using 30 local qubits).
Clusters are build to contain k < kmax qubits using a
heuristic which tries to maximize the number of gates
merged into one cluster. Clearly more than kmax individual
gates can be combined into one single cluster on average.
These optimizations take less than 3 seconds using Python
and can be reused for all instance of the same size.

4.1. Cori II

We performed simulations of quantum supremacy cir-
cuits featuring 30, 36, 42, and 45 qubits on the Cori II sys-
tem at the Lawrence Berkeley National Laboratory (LBNL).
Cori II consists of 9,304 single-socket compute nodes, each
containing one 68-core Intel R(cid:13) Xeon PhiTM Processor 7250
(KNL) at 1.40GHz. The nodes are interconnected by a Cray
Aries high speed dragony [20] topology interconnect
and offer a combined theoretical peak performance of 29.1
PFLOPS and 1 PB of aggregate memory.

4.1.1. Node-level performance. These experiments were
run on a single 68-core Intel R(cid:13) Xeon PhiTM Processor 7250
(KNL) node of the Cori II supercomputing system in the
quad/cache setting. For k  {1, 2, 3}-qubit gate kernels,
four threads per core were used, as this resulted in the best
performance. For k = 4 and k = 5, the best performance
was achieved when using two and one thread per core,
respectively. As mentioned in Sec. 3.3, the set-associativity
of caches plays a crucial role in the performance of these
k-qubit gate kernels. In particular, we nd the theoretical
predictions from Sec. 3.3 to agree perfectly with observa-
tions, see Fig. 6. The strong scaling behavior of executing
one k-qubit gate kernel on a state vector of 28 qubits can
be seen in Fig. 7.

4.1.2. Multi-node performance. The strong scaling of our
simulator for a 36- and 42-qubit quantum circuit running on
{16, 32, 64} and {1024, 2048, 4096} KNL nodes of Cori II,
respectively, is depicted in Fig. 8. Following these scaling
experiments, we ran a 45-qubit quantum supremacy circuit
using 8, 192 KNL nodes and a total of 0.5PB of memory.
To our knowledge, this is the largest quantum circuit simu-
lation ever carried out. Averaged over the entire simulation
time (i.e., including communication time), this simulation
achieved 0.428 PFLOPS. There are two reasons for this
drop in performance. First, the time spent in communication
is 75%, and overlaying computation and communication

Figure 4: Example of gate scheduling for a circuit with CZ
gates and dense single-qubit rotations gates (R). Note that
we use gate specialization for CZ gates, which means we can
apply them without communication on global qubits. First,
instead of applying the gates cycle by cycle, we identify
the largest rst stage of gates which can be applied without
communication. These are all the gates on the left of the
solid red line. Second, we schedule the gates within a stage
into clusters. For example, we can combine all the gates
on ');
INSERT INTO posts (postId,userId,title,body) VALUES (209,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','the left of the dashed green line into one 3-qubit gate
instead of applying 7 individual gates.

on, ignore all clusters which act on this qubit and assign bit-
locations 1, 2, and 3 in the same manner. Bit locations 4, 5,
6, and 7 are assigned the same way, except that after each
step, only clusters acting on two of these four bit-locations
are ignored when assigning the next higher bit-location. For
non-random circuits, it would pay off to perform a few
local swaps between some bit-locations over the course of
the algorithm, in order to maximize the number of clusters
acting on low-order qubits.

4. Implementation and Results

All optimizations mentioned in the previous sections
were implemented in C++, except for the code generator for
the k-qubit kernels and the circuit scheduler/qubit mapper,
which were both implemented in Python.

RRRRRRRRRRRlocalqubitsglobalqubitscycles(b) Scaling of the required communication for increasing
numbers of qubits for quantum supremacy circuits with a xed
circuit depth of 25.

(a) Scaling of the required communication for circuit depths 10
to 50 for 42-qubit quantum supremacy circuits. The number of
global-to-local swaps is mostly independent of the number of
local qubits per node, which allows for a good strong scaling
behavior.
Figure 5: Scaling of the required number of communication steps for quantum supremacy circuits as a function of circuit
depth (a) or number of qubits (b). The lower two panels show the number of global gates which require communication
if executed individually as in [10]. In contrast, the top two panels show the number of global-to-local swaps required to
execute the full circuit when using our strategy of reordering gates and swapping global with local qubits. Note that one
global-to-local swap (of all global qubits) requires the same amount of communication as one global gate. Averaged over
the different global qubits, executing a dense global gate takes approximately 1/2 of the time required to swap all global
qubits with local qubits, because applying a dense gate to low-order global qubits is faster due to the increased locality of
the communication, see [10]. Note that the dashed lines are for worst case instances (only dense random gates on global
qubits) and solid lines are for median hard instances, which we only consider in the two lower panels.

#Qubits #Gates #Nodes
6  5
6  6
7  6
9  5

8192

4096

569

369

447

528

64

Time [s] Comm.

Speedup

9.58

28.92

79.53

552.61

0%
37.6%
67.4%
75.0%

14.8x

12.8x

12.4x

N/A

TABLE 2: Summary of all simulation results carried out
on Cori II. Circuit simulation time and speedup are given
with respect to the depth-25 quantum supremacy circuit
simulations performed in [10]. The comm.-column gives the
percentage of circuit simulation time spent in communica-
tion (i.e., all-to-all).

would not improve this behavior due to the low k-qubit
gate times (less than 1 second). Second, the performance
of our kernels suffers in the regime where only few k-
qubit gates are applied before a global-to-local swap needs
to be performed. This is due to the fact that blocking for
MCDRAM requires a sequence of several gates acting on

qubits below bit-location 29. While our mapping procedure
aims to maximize this number, the total number of gates
being applied is not large enough. Yet, this is mainly due
to the articial construction of random circuits and does
not occur in actual quantum algorithms, where interactions
remain local over longer periods of time. As our 4-qubit
gate kernel achieves 1/2 of the M');
INSERT INTO posts (postId,userId,title,body) VALUES (210,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','CDRAM bandwidth which
corresponds to roughly 2x the bandwidth of DRAM (see
Fig. 2b), we expect a 2x drop in performance if memory
requirements exceed the MCDRAM size of 16GB. Aver-
aging the performance of our k-qubit kernels in Fig. 6
and including this 2x reduction yields approximately 250
GFLOPS per node. In total, we thus expect a performance of
25%  8, 192  250 GFLOPS  0.5 PFLOPS, which agrees
with the measurement results given that we also apply a few
3- and 2-qubit gate kernels for left-over gates.
For a summary of all runs carried out on Cori II, see
Table 2. Our implementation for, e.g., 42 qubits behaves as
expected from Fig. 5a: For a depth-25 circuit, the communi-
cation scheme used in [10] requires about 50 global gates,
while our simulator performs 2 global-to-local swaps (of

 1 2 3#Swaps29 local qubits30 local qubits31 local qubits32 local qubits 0 20 40 60 80 100 120 140 160 180 200101520253035404550#Global gatesCircuit depth 0 1 2#SwapsCircuit depth29 local qubits30 local qubits31 local qubits32 local qubits 0 20 40 60 80 100 120 1403036424549#Global gates#QubitsFigure 6: Decrease in performance when applying k-qubit
gate kernels to qubits with large indices (high-order qubits)
as opposed to low indices (low-order qubits). These experi-
ments were run on all 68 cores of a Cori II KNL node. As
mentioned in Sec. 3.3, this performance drop occurs when
2k is larger than the set-associativity of the last-level cache.
While the L2-cache is 16-way set-associative, it is shared
between 2 cores.

Figure 7: Strong scaling for applying k-qubit kernels to a
28-qubit system using 2p , p  {0, ..., 6} cores of the 68-
core Intel R(cid:13) Xeon PhiTM Processor 7250 and 4, 2, and 1
OpenMP thread(s) per core for k  3, k = 4, and k = 5,
respectively.

all global qubits). Including the fact that one such global-
to-local swap requires the same amount of communication
and that, averaged over all global qubits, a global gate is 2x
faster than if it is applied to the highest-order global qubit
due to the network bisection bandwidth (see [10]), yields a
reduction in communication of

50x
2  2

= 12.5x ,

and since we achieve a similar reduction in time-to-solution
for the circuit simulation on each node, this is also the
expected overall speedup.

Figure 8: Strong scaling of our simulator running a 36-
and 42-qubit quantum supremacy circuit on {16, 32, 64} and
{1024, 2048, 4096} nodes of Cori II, respectively.

4.2. Edison

In order to be able to compare our results directly to [10],
we also ran 30- and 36-qubit quantum supremacy circuits on
the Edison system, also at LBNL. We used up to 64 sockets,
each featuring a 12-core Intel R(cid:13) Xeon R(cid:13) Processor E5-
2695 v2 at 2.4GHz. The 5, 586 2-socket Edison nodes are
interconnected by a Cray Aries dragony [20] topology
interconnect and the theoretical peak performance of the
entire system is 2.57 PFLOPS.

4.2.1. Node-level performance. The performance reduction
from applying gates to high-order qubits due to the 8-way
set-associativity of the L1- and L2-caches in Intel R(cid:13) Ivy
BridgeTM processors can be seen in Fig. 9. These exper-
iments were run on an entire two-socket node on all 24
cores with one OpenMP thread per core and using AVX
vectorization.
The strong scaling of these k-qubit kernels with respect
to the number of cores is depicted in Fig. 10. While the
5-qubit gate kernel scales best to the full node, the perfor-
mance drop when applying it to high-order qubits is much
greater than ');
INSERT INTO posts (postId,userId,title,body) VALUES (211,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa','it is for 4-qubit gates. In addition, the 4-qubit
gate kernel scales nearly perfectly to all 12 cores of a single
socket, which suggests to use 2 MPI processes per node in
the multi-node setting.
Running a single-socket simulation of a 30-qubit quan-
tum supremacy circuit yields an improvement in time-to-
solution by 3x.

4.2.2. Multi-node performance.
to compare
In order
the present work directly to the state-of-the-art simulator
in [10], we performed a simulation of a 36-qubit quantum
supremacy circuit using identical hardware: 64 sockets of
the Edison supercomputer. We calculated the entropy of
a depth-25 quantum supremacy circuit in 99 seconds, of
which 90.9 seconds were spent in actual simulation and the
remaining 8.1 seconds were used to calculate the entropy,
which requires a nal reduction. This constitutes an im-
provement in time-to-solution of over 4x and indicates that

 0 100 200 300 400 500 600 700 800 900 1000 1100 1 2 3 4 5Performance [GFLOPS]Kernel size [#qubits]High-orderLow-order 5 10 15 20 25 30 35 40 45 50 55 60 6514816 32 64SpeedupNumber of coresOptimal scaling5-Qubit gate4-Qubit gate3-Qubit gate2-Qubit gate1-Qubit gate 1 2 3 41x2x4xSpeedupNumber of nodes1024 nodes2048 nodes4096 nodes16 nodes32 nodes64 nodes36 qubits42 qubits5. Summary and outlook

We demonstrated simulations of up to 45 qubits using
up to 8, 192 nodes. With the same amount of compute
resources, the simulation of 46 qubits is feasible when using
single-precision oating point numbers to represent the com-
plex amplitudes. The presented optimizations are general
and our code generator improves performance portability
across a wide range of processors. Extending the range of
the code generator to the domain of GPUs is an ongoing
project.
Additional optimizations on the quantum circuit descrip-
tion allows to reduce the required communication by an
order of magnitude. As a result, the simulation of a 49-qubit
quantum supremacy circuit would require only two global-
to-local swap operations. While the memory requirements
to simulate such a large circuit are beyond what is possible
today, the low amount of communication may allow the use
of, e.g., solid-state drives. The simulation results may then
be used for verication and calibration of near-term quantum
devices.

Acknowledgments

We thank Jarrod McClean for his outstanding help and
support throughout the course of this project. Special thanks
goes to our advisor, Matthias Troyer, for giving us the
opportunity to work on this project and for enlightening dis-
cussions. Moreover, we would like to thank Ryan Babbush,
Sergio Boixo, Brandon Cook, Jack Deslippe, Sergei Isakov,
and Hartmut Neven for helpful comments and discussions.
This research used resources of the National Energy
Research Scientic Computing Center, a DOE Ofce of
Science User Facility supported by the Ofce of Science
of the U.S. Department of Energy under Contract No. DE-
AC02-05CH11231. This work was supported by the Swiss
National Science Foundation through');
INSERT INTO posts (postId,userId,title,body) VALUES (212,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa',' the National Compe-
tence Center for Research NCCR QSIT.

References

[1] A. Aspuru-Guzik, A. D. Dutoi, P. J. Love, and M. Head-Gordon,
Simulated quantum computation of molecular energies, Science,
vol. 309, no. 5741, pp. 17041707, 2005.
[2] R. Babbush, J. McClean, D. Wecker, A. Aspuru-Guzik, and N. Wiebe,
Chemical basis of trotter-suzuki errors in quantum chemistry simu-
lation, Physical Review A, vol. 91, no. 2, p. 022311, 2015.
[3] M. Reiher, N. Wiebe, K. M. Svore, D. Wecker, and M. Troyer,
Elucidating reaction mechanisms on quantum computers, arXiv
preprint arXiv:1605.03590, 2016.
[4] B. Bauer, D. Wecker, A. J. Millis, M. B. Hastings, and M. Troyer,
Hybrid quantum-classical approach to correlated materials, arXiv
preprint arXiv:1510.03859, 2015.
[5] A. W. Harrow, A. Hassidim, and S. Lloyd, Quantum algorithm for
linear systems of equations, Physical review letters, vol. 103, no. 15,
p. 150502, 2009.
[6] N. Wiebe, D. Braun, and S. Lloyd, Quantum algorithm for data
tting, Physical review letters, vol. 109, no. 5, p. 050505, 2012.

Figure 9: Performance decrease when k-qubit gate kernels
are applied to high-order qubits instead of low-order ones
on a two-socket Edison node. The ndings again correspond
to the set-associativity of the caches, which is 23 = 8 in
this case. For k  3, there is only a negligible drop in
performance, since all 2k entries are mapped to different
locations in the cache and the next 2k -sizes matrix-vector
multiplication can access the next 2k values directly from
cache, see Sec. 3.3

Figure 10: Strong scaling of the k-qubit kernels using up
to 24 cores of a two-socket Edison node, which features
one 12-core Intel R(cid:13) Ivy BridgeTM processor per socket. Up
to and including k = 4, the kernels are memory bandwidth
limited. This in combination with Fig. 9 suggests that k = 4
is the best kernel size to use on this system (with 1 MPI
process per socket).

the obtained speedups were not merely a consequence of a
new generation of hardware.

The kernels perform at an average of 47% theoretical
peak, or 218 GFLOPS on every node during the execution
of a 36-qubit quantum supremacy circuit. When including
communication time, the entire simulation achieved 30%
of the theoretical peak performance of 64 Edison sockets,
which is 4.4 TFLOPS.

 0 50 100 150 200 250 300 1 2 3 4 5Performance [GFLOPS]Kernel size [#qubits]High-orderLow-order 0 5 10 15 20 25 5 10 15 20SpeedupNumber of coresOptimal scaling5-qubit kernel4-qubit kernel3-qubit kernel2-qubit kernel1-qubit kernel[7]

[8]

[9]

S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum algorithms
for supervised and unsupervised machine learning, arXiv preprint
arXiv:1307.0411, 2013.
P. Rebentrost, M. Mohseni, and S. Lloyd, Quantum support vector
machine for big data classication, Physical review letters, vol. 113,
no. 13, p. 130503, 2014.
P. W. Shor, Algorithms for quantum computation: Discrete loga-
rithms and factoring, in Foundations of Computer Science, 1994
Proceedings., 35th Annual Symposium on.
IEEE, 1994, pp. 124
134.
[10] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush, N. Ding,
Z. Jiang, J. M. Martinis, and H. Neven, Characterizing quantum
supremacy in near-term devices, arXiv preprint arXiv:1608.00263,
2016.
[11] S. Aaronson and L. Chen, Complexity-theoretic foundations of
quantum supremacy experiments, arXiv preprint arXiv:1612.05903,
2016.
[12] A. F. Rodrigues, K. S. Hemmert');
INSERT INTO posts (postId,userId,title,body) VALUES (213,5648,'Thomas H aner , Damian S. Steiger  Institute for Theoretical Physics, ETH Zurich, 8093 Zurich, Switzerland 12rA4 h-nu[ v21.01vXa',', B. W. Barrett, C. Kersey, R. Oldeld,
M. Weston, R. Risen, J. Cook, P. Rosenfeld, E. CooperBalls et al.,
The structural simulation toolkit, ACM SIGMETRICS Performance
Evaluation Review, vol. 38, no. 4, pp. 3742, 2011.
[13] L. of QC simulators. (2017) http://www.quantiki.org/wiki/List of
QC simulators (Last accessed 03/31/2017).
[14] D. Wecker and K. M. Svore, LIQU i |(cid:105): A software design ar-
chitecture and domain-specic language for quantum computing,
arXiv:1402.4467, 2014.
[15] T. H aner, D. S. Steiger, M. Smelyanskiy, and M. Troyer, High
performance emulation of quantum circuits, Supercomputing 2016,
2016.
[16] D. B. Trieu, Large-scale simulations of error prone quantum compu-
tation devices. Forschungszentrum J ulich, 2009, vol. 2.
[17] K. De Raedt, K. Michielsen, H. De Raedt, B. Trieu, G. Arnold,
M. Richter, T. Lippert, H. Watanabe, and N. Ito, Massively parallel
quantum computer simulator, Computer Physics Communications,
vol. 176, no. 2, pp. 121136, 2007.
[18] M. Smelyanskiy, N. P. Sawaya, and A. Aspuru-Guzik, qhipster:
The quantum high performance software testing environment, arXiv
preprint arXiv:1601.07195, 2016.
[19] M. Mohseni, P. Read, H. Neven, S. Boixo, V. Denchev, R. Babbush,
A. Fowler, V. Smelyanskiy, and J. Martinis, Commercialize quantum
technologies in ve years. Nature, vol. 543, no. 7644, pp. 171174,
2017.
[20] J. Kim, W. J. Dally, S. Scott, and D. Abts, Technology-driven,
highly-scalable dragony topology, SIGARCH Comput. Archit.
News, vol. 36, no. 3, pp. 7788, Jun. 2008. [Online]. Available:
http://doi.acm.org/10.1145/1394608.1382129

');
INSERT INTO posts (postId,userId,title,body) VALUES (214,1915,'Tobias Denkinger','Faculty of Computer Science

Technische Universitat Dresden

01062 Dresden, Germany

tobias.denkinger@tu-dresden.de

2017-03-30

Abstract

We use a non-deterministic variant of storage types to develop a framework for
the approximation of automata with storage. This framework is used to provide an
automata-theoretic view on the approximation of multiple context-free languages.

Contents

1 Introduction

2 Preliminaries

3 Automata with non-deterministic storage
3.1 Non-deterministic storage types . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Automata with non-deterministic storage
. . . . . . . . . . . . . . . . . .

2
3
7
4 Approximation of automata with storage
4.1 Superset approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . .4.2 Subset approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 Approximation of weighted automata with storage . . . . . . . . . . . . . 11

5 Approximation of multiple context-free languages

13
12rM9 ]Fs[ v19.01vXa

1 Introduction

In the application of formal languages, approximation is a well-established concept (see
Nederhof [Ned00a] for an overview). For a context-free grammar it is common (but not
exclusive [Ned00b, CPV+06]) to construct a pushdown automaton and then approximate
this automaton [KdT81, Pul86, LL87, BS90, PW91, Eva97, Joh98], e.g. by restricting
the height of the pushdown. Automata with storage [Sco67, Eng86, Eng14] are a gener-
alisation of pushdown automata and were studied in recent literature [HV15, VDH16].
Multiple context-free languages [SMFK91, VSWJ87] are among the language classes cap-
tured by automata with storage [Den16] and their approximation was studied recently
[BL05, vC12]. By attaching weights to the transitions of an automaton with storage,
we can model, for example, the multiplicity with which a word belongs to a language or
the cost of recognising a word. The resulting devices are called weighted automata with
storage [HV15, VDH16].
We develop a framework to study the approximation of weighted automata with ar-
bitrary storage. To deal with non-determinism that arises due to approximation, we
introduce weighted automata with non-deterministic storage (Sec. 3). Our storage dif-
fers from Engelfriets [Eng86, Eng14] in two aspects: As instructions we allow binary
relations instead of partial functions and each transition is associated with a weight
from a semiring. Inspired by the storage simulation of Hoare [Hoa72 ] (also studied by
Engelfriet and Vogler [EV86]), we formalise the strategy with which a storage type is
approximated (the approximation strategy ) as a partial function (Sec. 4). In contrast to
Engelfriet and Vogler [EV86], we do not utilise owcharts in our constructions. We use
our framework to provide an automata-based approach to the approximation of multiple
context-free languages (Sec. 5).

2 Preliminaries

The set of {0, 1, 2, . . . } is denoted by N and N \ {0} is denoted by N+ . The set {1, . . . , k}
is denoted by [k ] for every k  N. Let A be a set. The power set of A is denoted by
P (A). By ! we denote the quantier there is exactly one.');
INSERT INTO posts (postId,userId,title,body) VALUES (215,1915,'Tobias Denkinger (part 2)','
Let A, B , and C be sets and let r  A  B and s  B  C be binary relations. We
denote {(b, a)  B  A | (a, b)  r} by r1 , {b  B | (a, b)  r} by r(a) for every a  A,
and SaA r(a) by r(A ) for every A  A. The sequential composition of r and s is the
binary relation
r ; s = {(a, c)  A  C | b  B : ((a, b)  r)  ((b, c)  s)}.

We call r an endorelation (on A) if A = B . A semiring is an algebraic structure
(K, +, , 0, 1) where (K, +, 0) is a commutative monoid, (K, , 1) is a monoid, 0 is absorp-
tive with respect to , and  distributes over +. We say that K is complete if it has a
sum operation PI : K I  K that extends + for each countable set I [DKV09, Ch. 1].
Let  be a partial order on K . We say that K is positively -ordered if  is preserved
by +.

The set of partial functions from A to B is denoted by A 99K B . The set of (total)
functions from A to B is denoted by A  B . Note that every total function is a partial
function and that every partial function is a binary relation. Let f : A 99K B be a partial
function. The domain of f and the image of f are dened by dom(f ) = {a  A | b 
B : f (a) = b} and img(f ) = {b  B | a  A: f (a) = b}, respectively. Abusing the
notation, we may sometimes write f (a) = undened to denote that a / dom(f ).

3 Automata with non-deterministic storage

In addition to the nite state control, automata with storage allow the checking and
manipulation of a so-called storage conguration that comes from a possibly innite
set. We propose a syntactic extension of automata with storage where the set of unary
functions (the instructions ) is replaced by a set of binary relations on the congurations.

3.1 Non-deterministic storage types

Denition 3.1. A non-deterministic storage type (short: nd storage type) is a tuple S =
(C, P , R, ci ) where C is a set (of congurations ), P  P (C ) (predicates ), R  P (C  C )
(instructions ), and ci  C (initial storage conguration ).
(cid:3)

If every element of R is a partial
Consider an nd storage type S = (C, P , R, ci ).
function, we call S deterministic. The denitions of deterministic nd storage type
in this paper and storage type in previous literature [Eng86, Eng14, HV15, VDH16]
coincide.

Example 3.2. The deterministic nd storage type Count models simple counting ( Engel-
friet [Eng86, Eng14, Def. 3.4]): Count = (N, {true, iszero}, {inc, dec}, 0) where true = N,
iszero = {0}, inc = {(n, n + 1) | n  N} and dec = inc1 .
(cid:3)

Example 3.3. The following deterministic nd storage type models a pushdown stack
(similar to Engelfriet [Eng86, Eng14, Def. 3.2]): PD = (  , Ppd , Rpd , ) where  is a
nite set (pushdown symbols ); Ppd = {  , bottom}  {top |    } with

bottom = {}

and

top = {w | w   }

for every    ; and Rpd = {stay , pop}  {push , stay |    } with

stay = {(w, w) | w   },
push = {(w, w) | w   },

pop = {(w, w) | w    ,    },
stay = {( w, w) | w    ,     }

for every    .

(cid:3)

Example 3.4. The nd storage type Count models counting, but does not distinguish
dierent odd numbers: Count = ({2n | n  N}  {o}, {true, iszero}, {inc}, 0) where
true = {2n | n  N}  {o}, iszero = {0}, and inc = {(2n, o), (o, 2n) | n  N}. Note that
(inc )1 = inc and that Count is not deterministic.
(cid:3)

We call a nd storage type S = (C, P , R, ci ) nitely non-deterministic (short: nitely
nd) if the set r(c) is nite for every r  R and c  C . The nd storage type Count , for
example, is not nitely nd since inc (o) = {2n | n  N} is not nite.

3.2 Automata with non-deterministic storage

Denition 3.5. Let S = (C, P , R, ci ) be a nitely nd storage type and  be a nite set.
An (S,  )-automaton is a tuple M = (Q, T , Qi , Qf ) where Q is a nite set (of states ), T
is a nite subset of Q  (  {})  P  R  Q (transitions ), Qi  Q (initial states ), and
Qf  Q (nal states ).
(cid:3)

Let M = (Q, T , Qi , Qf ) be an ');
INSERT INTO posts (postId,userId,title,body) VALUES (216,1915,'Tobias Denkinger (part 3)','(S,  )-automaton and S = (C, P , R, ci ). A conguration
of M is an element of Q  C    . For every  = (q , v , p, r, q  )  T , the transition
relation of  is the endorelation  on the set of congurations of M that contains
(q , c, vw)  (q  , c , w) for every w    and (c, c )  r with c  p. The run relation of
M is M= S T  . The transition relations are extended to sequences of transitions
by setting 1 k = 1 ; . . . ; k for every k  N and 1 , . . . , k  T . The set of runs of
M is the set
RM = (cid:8)  T  | q , q   Q, c, c  C, w, w     : (q , c, w)  (q  , c , w  )(cid:9).
(1)
Let w    . The set of runs of M on w is
RM (w) = (cid:8)  T  | q  Qi , q   Qf , c  C : (q , ci , w)  (q  , c , )(cid:9).
The language accepted by M is the set L(M) = {w    | RM (w) 6= }.
Let S be a nitely nd storage type,  be a nite set, and L    . We call L
(S,  )-recognisable if there is an (S,  )-automaton M with L = L(M).

(2)

Proposition 3.6. For every nitely nd storage type S there is a deterministic nd storage
type Sdet such that for every nite set  the class of (S,  )-recognisable languages is
equal to the class of (Sdet ,  )-recognisable languages.

Proof. Let S = (C, P , R, ci ). Using a power set construction, we obtain the deterministic
nd storage type Sdet = (P (C ), Pdet , Rdet , {ci }) where Pdet = {pdet | p  P } with pdet =
{d  C | d  p 6= } for every p  P , and Rdet = {rdet | r  R} with rdet = {(d, r(d)) |
d  C } for every r  R.
Let M = (Q, T , Qi , Qf ) be an (S,  )-automaton and M = (Q , T  , Q
i , Q
f ) be an
(Sdet ,  )-automaton. We say that M and M are related if Q = Q , Qi = Q
i , Qf = Q
f ,
and T  = Tdet = {tdet | t  T } where tdet = (q , v , pdet , rdet , q  ) for each t = (q , v , p, r, q  ) 
T . Clearly, for every (S,  )-automaton there is an (Sdet ,  )-automaton such that both
are related and vice versa.
Now consider the (S,  )-automaton M = (Q, T , Qi , Qf ) and the (Sdet ,  )-automaton
M = (Q, Tdet , Qi , Qf ) that are related. We extend ()det : T  Tdet to a function
det by point-wise application. We show for every   T  by induction
()det : T   T 
on the length of  that
q , q   Q, c, c  C, w, w     :
(q , c, w)  (q  , c , w  )  d  c: !d  c : (q , d, w) det (q  , d , w  )

(3)

holds. The induction base is given by the sequence of length 0. For the induction step
we assume that (3) holds for all sequences of length n. Let q , q   Q, c, c  C, w, w     ,
  T n , and   T . We distinguish two cases.

(q , c, w)  (q  , c , w  ). Then there are c  C , u    , and  = ( q , v , p, r, q  ) 
Case 1:
T with w = uvw  and (q , c, uvw  )  ( q , c, vw  )  (q  , c , w  ). By (3) we know that
for every d  c there is exactly one d  c with (q , d, uvw  ) det ( q , d, vw  ). It remains
to be shown that ( q , d, vw  ) det (q  , d , w  ) for exactly one d  c . By construction,
we know that det = ( q , v , pdet , fdet , q  ). By denition of pdet , the fact that c  p, and
c  d, we have that d  pdet . Then ( d, d )  rdet by the denition of det and d is
unique by construction of rdet . By denition of rdet , the fact that (c, c )  r , and c  d,
we have that c  d . Finally, det clearly goes from state q to state q  and reads v .
Case 2: (q , c, w)  (q  , c , w  ). This can have two reasons.
Case 2.1: There is no ( q , c, w)  Q  C    such that (q , c, w)  ( q , c, w). Then it
follows by (3) that for every d  c there is no d  c such that (q , d, w) det ( q , d, w)
and hence there is no d  c such that (c, d, w) ( )det (c , d , w  ).
Case 2.2: There are ( q , c, u, v)  Q  C     (  {}) such that w = uvw  and
(q , c, w)  ( q , c, vw  ), but for none of them exists a transition  = ( q , v , p, r, q) such
that ( q , c, vw  )  (q  , c , w  ). Then by (3) we know that for every d  c there is
some d  c such that (q , d, uvw  ) det (');
INSERT INTO posts (postId,userId,title,body) VALUES (217,1915,'Tobias Denkinger (part 4)',' q , d, vw  ). It remains to be shown that there
is no d  c such that ( q , c, vw  ) det (q  , c , w  ). If such a d were to exist, then
there would exist a transition det = ( q , v , pdet , rdet , q  )  Tdet such that d  pdet
and ( d, d )  rdet . Then by the construction there would be some c  d such that
c  p and (c, c )  r which contradicts the assumption that there is no transition
 = ( q , v , p, r, q) such that ( q , c, vw  )  (q  , c , w  ).

We then obtain L(M) = L(M ) from (3) and from {ci } being the initial storage
conguration of M .
(cid:4)

For practical reasons it might sometimes be preferable to avoid the construction of
power sets. The proof presented here, however, only shows containment rather than
equality for the considered language classes.

Proposition 3.7. For every nitely nd storage type S there is a deterministic nd storage
type S  with the same set of congurations such that for every nite set  the class of
(S,  )-recognisable languages is contained in the class of (S  ,  )-recognisable languages.

Proof. Let S = (C, P , R, ci ) be a nd storage type. We construct the deterministic nd
storage type S  = (C, P , R , ci ) where R is constructed as follows: Let r  R and
r(c)1 , . . . , r(c)mr,c be a xed enumeration of the elements of r(c) for every c  C . Fur-
thermore, let kr = max{|r(c)| | c  C }. We dene for each i  [kr ] an instruction r 
i by
i (c) = r(c)i if i  mr,c and r 
r 
i (c) = undened otherwise. Let R contain the instruction
r 
i for every r  R and i  [kr ].
Now let M = (Q, T , Qi , Qf ) be an (S,  )-automaton. We construct the (S  ,  )-
automaton M = (Q, T  , Qi , Qf ) where T  contains for every transition t = (q , v , p, r, q  ) 

T and i  [kr ] the transition t
i = (q , v , p, r 
i , q  ). Then
M = [tT
t = [t=(q ,v,p,r,q  )T [i[kr ]
and thus L(M) = L(M ).
i

= [t T  t = M

(cid:4)

The author conjectures that there are an nd storage type S , a nite set  , and an
(S  ,  )-recognisable language L (where S  is dened as in the proof of Prop. 3.7) such
that L is not (S,  )-recognisable.

Proposition 3.8. Let  be a nite set, S = (C, P , R, ci ) be an nd storage type, and
L be an (S,  )-recognisable language. If C is nite, then L is recognisable (by a nite
state automaton).

Proof. We will use non-deterministic nite-state automata with extended transition func-
tion from Hopcroft and Ullman [HU79, Sec. 2.3] in a notation similar to that of automata
with storage (we leave out the storage-related parts of the transitions).
Let M = (Q, T , Qi , Qf ). We construct the fsa M = (Q  C,  , T  , Qi  {ci}, Qf  C )
where T  = {((q , c), v , (q  , c )) | (q , v , p, r, q  )  T , (c, c )  r, c  p}.
It remains to be shown that L(M) = L(M ). For this we rst show by induction on
the length of runs that
q , q   Q, c, c  C, w, w     :
M (q  , c , w  )  ((q , c), w) 
(q , c, w) 
M ((q  , c ), w  ).
The induction base is given by runs of length 0. For the induction step we assume that
(4) holds for all runs of length n and let (q , c, w) 
M ( q , c, w) be witnessed by a run of
length n + 1. We derive

(4)

(q , c, w) 
M ( q , c, w)
 q   Q, c  C, w     : (q , c, w) 
M (q  , c , w  ) M ( q , c, w)
 c  C, w     , (q  , v , p, r, q )  T :
M (q  , c , w  )  w  = v w  c  p  c  r(c )
(q , c, w) 
M (q  , c , w  )  w  = v w
 w     , ((q  , c ), v , ( q , c))  T  : (q , c, w) 
(by construction of M )
M ((q  , c ), w  )  w  = v w (by (4))
 w     ,');
INSERT INTO posts (postId,userId,title,body) VALUES (218,1915,'Tobias Denkinger (part 5)',' ((q  , c ), v , ( q , c))  T  : ((q , c), w) 
 q   Q, c  C, w     : ((q , c), w) 
M ((q  , c ), w  ) M (( q , c), w)
 ((q , c), w) 
M (( q , c), w )

For the languages we then derive

L(M) = {w    | RM (w) 6= }
= {w    | q  Qi , q   Qf , c  C : (q , ci , w) 
M (q  , c , )}
= {w    | q  Qi , q   Qf , c  C : ((q , ci ), w) 
M ((q  , c ), )}
= {w    | RM (w) 6= }
= L(M ).

(by (1))

(by (4))

(cid:4)

4 Approximation of automata with storage

An approximation strategy maps an nd storage type to another nd storage type. It is
specied in terms of storage congurations and naturally extended to predicates and
instructions.

Denition 4.1. An approximation strategy is a partial function A: C 99K C  where C
and C  are arbitrary sets.
(cid:3)

Denition 4.2. Let S = (C, P , R, ci ) be an nd storage type and A: C 99K C  be an
approximation strategy. The approximation of S with respect to A is the nd storage
type SA = (C  , PA , RA , A(ci )) where PA = {pA | p  P } with pA = A(p) for
every p  P , and RA = {rA | r  R} with rA = A1 ; r ; A for every r  R.
(cid:3)

Let S = (C, P , F , ci ) be a nitely nd storage type. We call an approximation strategy
A: C 99K C  S -proper if SA is nitely non-deterministic.

Example 4.3. Consider the approximation strategy Ao : N  {o}  {2n | n  N} that
assigns to every odd number the value o and to every even number the number itself.
Then Ao is not Count-proper since incAo (o) = decAo (o) = {2n | n  N} is not nite.
Note that CountAo = Count (cf. Ex. 3.4).
On the other hand, consider the approximation strategy Aeo : N  {e, o} that returns
o for every odd number and e otherwise. This approximation strategy is Count-proper
since incAeo (e) = decAeo (e) = {o} and incAeo (o) = decAeo (o) = {e} are nite.
(cid:3)

Example 4.4. Consider the (Count, {a, b})-automaton M = ([3], T , {1}, {3}) and its
approximation MAeo = ([3], T  , {1}, {3}) with

T : 1 = (1, a , true , inc , 1)
2 = (1, b, true , dec, 2)
3 = (2, b, true , dec, 2)
4 = (2,  , iszero, inc , 3)

T  :  
1 = (1, a , true
, toggle, 1)
2 = (1, b, true, toggle, 2)
3 = (2, b, true, toggle, 2)4 = (2,  , iseven, toggle, 3)

where true = trueAeo = {e, o} and iseven = iszeroAeo = {e} are the predicates
of CountAeo , and toggle = incAeo = decAeo = {(e, o), (o, e)} is the instruction of
CountAeo . The word aabb  {a, b} can be recognised by both automata:

(1, 0, aabb) 1 (1, 1, abb) 1 (1, 2, bb) 2 (2, 1, b) 3 (2, 0, ) 4 (3, 1, )
(2, e, )  
(2, o, b)  
(1, e, bb)  
(1, o, abb)  
(1, e, aabb)  
(3, o, ).13
On the other hand, the word bb can be recognised by MAeo but not by M:

(1, e, bb)  
(2, o, b)  
(2, e, )  
(3, o, ).

(cid:3)

Denition 4.5. Let M = (Q, T , Qi , Qf ) be an (S,  )-automaton and A be an S -
proper approximation strategy. The approximation of M with respect to A is the
(SA ,  )-automaton MA = (Q, TA , Qi , Qf ) where TA = {A |   T } with A =
(q , v , pA , rA , q  ) for every  = (q , v , p, r, q  )  T .
(cid:3)

Observation 4.6. Let M be an (S,  )-automaton with S = (C, P , R, ci ), and A1 : C 99K
C and A2 : C 99K C  be approximation strategies. If A1 is S -proper and A2 is SA1 -proper,
');
INSERT INTO posts (postId,userId,title,body) VALUES (219,1915,'Tobias Denkinger (part 6)','then (MA1 )A2 = MA1 ;A2 .
(cid:4)
Denition 4.7. Let A1 : C 99K C and A2 : C 99K C  be approximation strategies. We
call A1 ner than A2 , denoted by A1 (cid:22) A2 , if there is a total approximation strategy A
with A1 ; A = A2 . We call A1 less partial than A2 , denoted by A1  A2 , if there is an
injective approximation strategy A with A1 ; A = A2 .
(cid:3)

Note that if A1 (cid:22) A2 , we also know that A1 and A2 are equal ly partial (i.e. A1  A2
and A2 is less partial than A1 ). Similarly, if A1 is less partial than A2 , we know that A1
and A2 are equal ly ne (i.e. A1 is ner than A2 and A2 is ner than A1 ). Consequently,
(cid:22) and  are partial orders but not total orders.

4.1 Superset approximations

In this section we will show that total approximation strategies (i.e. total functions)
lead to superset approximations.

Theorem 4.8. Let M be an (S,  )-automaton and A be an S -proper total approxima-
tion strategy. Then L(MA )  L(M).

Proof. Let M = (Q, T , Qi , Qf ) and S = (C, P , R, ci ). We extend ()A from transitions
to sequences of transitions by point-wise application. We show for every   T  by
induction on the length of  that

q , q   Q, c, c  C, w, w     :
(q , c, w)  (q  , c , w  ) = (q , A(c), w) A (q  , A(c ), w  )

(5)

holds. The induction base is given by the sequence of length 0. For the induction
step we assume that (5) holds for all sequences of length n. Let q , q   Q, c, c  C ,
w, w     ,   T n , and   T such that (q , c, w)  (q  , c , w  ). Then there are u    ,
v    {}, and a conguration ( q , c, vw  )  Q  C    such that w = uvw  and
(q , c, uvw  )  ( q , c, vw  )  (q  , c , w  ). Hence  has the form ( q , v , p, r, q  ) for some p  P
and r  R with c  p and (c, c )  r . By (5) we have (q , A(c), uvw  ) A ( q , A(c), vw  )
and by Def. 4.5 we know that A = ( q , v , pA , rA , q  )  TA . Note that A(c )  C 
since A is a total function. Now from Def. 4.2 we immediately obtain that A(c)  pA
and (A(c), A(c ))  rA . Since the states and the read symbol (or ) are taken over in
A , we therefore know that ( q , A(c), vw  ) A (q  , A(c ), w  ).
The claim then follows from (5) and the denition of MA .

(cid:4)

Example 4.9. Recall M and MAeo from Ex. 4.4. Their recognised languages are
L(M) = {anbn | n  N+} and L(MAeo ) = {ambn | m  N, n  N+ , m  n mod 2}.
Thus L(MAeo ) is a superset of L(M).
(cid:3)

Corollary 4.10. Let M be an (S,  )-automaton, and A1 and A2 be S -proper approxi-
mation strategies. If A1 is ner than A2 , then L(MA1 )  L(MA2 ).

L(MA1 )

(Thm. 4.8)


(cid:4)

Proof. Since A1 is ner than A2 , we know that there is a total approximation strategy
A such that A1 ; A = A2 . We obtain
L(cid:0)(MA1 )A (cid:1) (Obs. 4.6)The following example shows four approximation strategies that occur in the litera-
ture. The rst three approximation strategies approximate a context-free language by a
recognisable language (taken from Nederhof [Ned00b, Sec. 7]). The forth approximation
strategy approximates a context-free language by another context-free language. It is
easy to see that the shown approximation strategies are total and thus lead to superset
approximations.

(Def. 4.7)
= L(MA2 ).

L(MA1 ; A )

Example 4.11. Let  be a nite set and k  N+ .

(i) Evans [Eva97] proposed to map each pushdown to its top-most element. The same
result is achieved by dropping condition 7 and 8 from Baker [Bak81]. This idea is
expressed by the approximation strategy Atop :      {@} with

Atop () = @

and

Atop (w) = 

for every w    and   

where @ is a new symbol that is not in  .

');
INSERT INTO posts (postId,userId,title,body) VALUES (220,1915,'Tobias Denkinger (part 7)','(ii) Bermudez and Schimpf [BS90] proposed to map each pushdown to its top-most k
elements. The following approximation strategy implements this idea:

Atop,k :  +  {w   + | |w|  k}
Atop,k (w) = (w if |w|  k
if w is of the form uv for some u   k and v   + .(iii) Pereira and Wright [PW91] proposed to map each pushdown to one where no
pushdown symbol occurs more than once. To achieve a unique approximation of
each pushdown, we provide an approximation strategy that retains exactly the
top-most (actually left-most) occurrence of each symbol: Consider

Auniq :  +  Seqnr ( )
Auniq (w) = (Auniq (uw  v)where Seqnr ( ) denotes the set of all sequences over  without repetition.

if w is of form uw   v with   
otherwise

(iv) In their coarse-to-ne parsing approach for context-free grammars (short: CFG),
Charniak et al. [CPV+06] propose, given an equivalence relation  on the set of
non-terminals N of some CFG G, to construct a new CFG G whose non-terminals
are the equivalence classes of .1 Let  be the terminal alphabet of G. Say that

1Charniak et al. [CPV+ 06] actually considered probabilistic CFGs, but for the sake of simplicity we
leave out the probabilities in this example.

g : N  N/ is the function that assigns for a nonterminal of G its corresponding
equivalence class; and let g  : (N   )  ((N/)   ) be an extension of g 
{(, ) |    }. Then g  is PDN  -proper and L(Mg  ) = L(G ) where M
is the (PDN  ,  )-automaton obtained from G by the usual construction [HU79,
Thm. 5.3].
(cid:3)

4.2 Subset approximations

In this section we will show that injective approximation strategies lead to a subset
approximation, this is proved by a variation of the proof of Thm. 4.8.

Theorem 4.12. Let M be an (S,  )-automaton and A be an S -proper injective approx-
imation strategy. Then L(MA )  L(M).

Proof. Let M = (Q, T , Qi , Qf ) and S = (C, P , R, ci ). We show for every   T  by
induction on the length of  that

q , q   Q, c, c  img(A), w, w     :
(q , c, w) A (q  , c , w  ) = (q , A1 (c), w)  (q  , A1 (c ), w  )

(6)

holds. The induction base is given by the sequence of length 0. For the induction
step we assume that (6) holds for all sequences of length n. Let q , q   Q, c, c 
img(A), w, w     ,   T n , and   T such that (q , c, w) A A (q  , c , w  ). Then
there are u    , v    {}, q  Q, and c  img(A) such that w = uvw  and
(q , c, uvw  ) A ( q , c, vw  ) A (q  , c , w  ). Hence A has the form ( q , v , pA , rA , q  )
for some p  P and r  R such that c  pA and (c, c )  rA . By (6) we have
(q , A1 (c), uvw  )  ( q , A1 (c), vw  ) and by Def. 4.5 we know that A = ( q , v , p, f , q  ) 
T . Note that A1 (c ) is uniquely dened since c  img(A) and A is injective. Now
from Def. 4.2 we immediately obtain that A1 (c)  p and (A1 (c), A1 (c ))  r . Since
the states and the read symbol (or ) are taken over in A , we therefore know that
( q , A1 (c), vw  ) A (q  , A1 (c ), w  ).
The claim then follows from (6) and the denition of MA .

(cid:4)

Corollary 4.13. Let M be an (S,  )-automaton and A1 and A2 be S -proper approxi-
mation strategies. If A1 is less partial than A2 , then L(MA1 )  L(MA2 ).

L(MA1 )

(Thm. 4.12)


(cid:');
INSERT INTO posts (postId,userId,title,body) VALUES (221,1915,'Tobias Denkinger (part 8)','4)

Proof. Since A1 is less partial than A2 , we know that there is a injective approximation
strategy A such that A1 ; A = A2 . We obtain
L(cid:0)(MA1 )A (cid:1) (Obs. 4.6)The following example approximates a context-free language with a recognisable lan-
guage (taken from Nederhof [Ned00b, Sec. 7]). It is easy to see that the shown approxi-
mation strategy is injective and thus leads to subset approximations.

(Def. 4.7)
= L(MA2 ).

L(MA1 ;A )

10

Example 4.14. Let  be a nite set and k  N+ . Krauwer and des Tombe [KdT81],
Pulman [Pul86], and Langendoen and Langsam [LL87] proposed to disallow stacks of
height greater than k . This can be achieved by a partial identity:
Abd,k (w) = (w
if |w|  k
undened otherwise.
4.3 Approximation of weighted automata with storage

Abd,k :  + 99K {w   | |w|  k}

(cid:3)

Denition 4.15. Let  be a nite set, S be a nitely nd storage type, and K be
a complete semiring. An (S,  , K )-automaton is a tuple M = (Q, T , Qi , Qf , ) where
(Q, T , Qi , Qf ) is an (S,  )-automaton and  : T  K (transition weights ). We sometimes
denote (Q, T , Qi , Qf ) by Muw .
(cid:3)

Consider the (S,  , K )-automaton M = (Q, T , Qi , Qf , ). The congurations of M,
the run relation of M, and the set of runs of M on w for every w    are the same
as for Muw . The weight of  in M is the value wtM () = (1 )  . . .  (k ) for every
 = 1    k with 1 , . . . , k  T . The weighted language induced by M is the function
JMK:    K where for every w    :
JMK(w) = XRM (w)
Let S be a nitely nd storage type,  be a nite set, K be a semiring, and r :    K .
We call r (S,  , K )-recognisable if there is an (S,  , K )-automaton M with r = JMK.
We extend Prop. 3.6 to the weighted case, using ()det as dened in Prop. 3.6.

wtM ().

(7)

Proposition 4.16. The classes of (S,  , K )-recognisable and of (Sdet ,  , K )-recognis-
able languages are the same for every nitely nd storage type S , nite set  , and semiring
K .
Proof. Let M = (Q, T , Qi , Qf , ) and M = (Q , T  , Q
i , Q
f ,   ) be an (S,  , K )-automaton
and an (Sdet ,  , K )-automaton, respectively. We call M and M related if Muw anduw are related, and   (det ) = ( ) for every   T . Note that ()det is a bijection
between T and Tdet . Clearly, for every (S,  , K )-automaton M there is an (Sdet ,  , K )-
automaton M such that M and M are related and vice versa. It remains to be shown
that JMK = JM K. For every w    , we derive
JMK(w) = XRM
= XRM
wtM (det )
= X RM
= JM K(w)
Denition 4.17. Let M = (Q, T , Qi , Qf , ) be an (S,  , K )-automaton, and A an
S -proper approximation strategy. The approximation of M with respect to A is the
(SA ,  , K )-automaton MA = (Q, TA , Qi , Qf , A ) where SA and TA are dened
as in Def. 4.5, and A (  ) = P T :A=  ( ) for every    TA .
(cid:3)
11

(by ()det being a bijection and (3))

(by Def. of det )

(by (7)) (cid:4)

wtM ()

wtM (  )

(by (7))

Lemma 4.18. Let M be an (S,  , K )-automaton, A be an S -proper approximation
strategy,  be a partial order on K , and K be positively -ordered.
(i) If A is total, then wtMA (  )  PRM :A=  wtM () for every    RMA .
(ii) If A is injective, then wtMA (  )  PRM :A=  wtM () f');
INSERT INTO posts (postId,userId,title,body) VALUES (222,1915,'Tobias Denkinger (part 9)','or every    RMA .
Proof. We prove (i) by induction on the length of   . For   = , we derive
wtMA () = 1  1 = wtM () = XRM :A=
wtM ()
For     RMA with    TA , we derive
wtMA (   ) = wtMA ( )  A (  )
 (cid:16) XRM :A=  wtM ()(cid:17)  A (  )
= (cid:16) XRM :A=  wtM ()(cid:17)  (cid:16) X T :A=  ( )(cid:17)
= XRM , T :(A=  )(A=  )
wtM ()  ( )
 X RM :( T )(( )A =    )
wtM ()  ( )
wtM ( )
= X RM :( A=    )
The proof for (ii) can be obtained from the proof for (i) by replacing every occurrence
of  by  and the use of (5) by (6).
(cid:4)

(by (5) and K being positively -ordered)

(by induction hypothesis)

(by distributivity of K )

(by Def. 4.17)

(by Def. 4.17)

Theorem 4.19. Let M be an (S,  , K )-automaton, A be an S -proper approximation
strategy, and  be a partial order on K , and K be positively -ordered.

(i) If A is total, then JMAK(w)  JMK(w) for every w    .

(ii) If A is injective, then JMAK(w)  JMK(w) for every w    .
Proof. ad (i): For every w    , we derive
JMA K(w) = X RMA (w)
wtMA (  )
 X RMA (w) XRM : A=  wtM ()
= X RMA (w) XRM (w) : A=  wtM ()
 XRM (w)
wtM ()
= JMK(w)
For (), we argue that, since A is total, we know that for every   RM (w), the
addend wtM () occurs at least once on the left side of the inequality and exactly
once on the right side. Hence  is justied.

(by Lem. 4.18 (i))

(by Def. 4.5)

(by (7))

(by (7))

(by ())

12

ad (ii): For every w    , we derive
JMA K(w) = X RMA (w)
wtMA (  )
 X RMA (w) XRM : A=  wtM ()
= X RMA (w) XRM (w) : A=  wtM ()
 XRM (w)
wtM ()
= JMK(w)
For (), we argue that since A is injective, we know that for every   RM (w), the
addend wtM () occurs at most once on the left side of the inequality and exactly
once on the right side. Hence  is justied.
(cid:4)

(by Lem. 4.18 (ii))

(by Def. 4.5)

(by (7))

(by ())

(by (7))

5 Approximation of multiple context-free languages

In Exs. 4.11 and 4.14 we recall some approximation strategies for pushdown automata
from the literature. Due to the equivalence of pushdown automata and context-free
grammars [HU79, Thms. 5.3 and 5.4], those approximation strategies can be used for
the approximation of context-free languages. The framework presented in this paper
together with the automata characterisation of multiple context-free languages [Den16,
Thm. 18] allows an automata-theoretic view on the approximation of multiple context-
free languages. The automata characterisation uses an excursion-restricted form of au-
tomata with tree-stack storage.2

Denition 5.1. Let  be a nite set. The tree-stack storage over  is the deterministic
nd storage type TreeStack = (TS , Pts , Rts , ci,ts ) where

 TS is the set of tuples h , i where  : N
99K   {@}, dom( ) is nite andprex-closed,3   dom( ), and  ( ) = @ i  = ;

 ci,ts = h{(, @)}, i;

 Pts = {TS , bottom}  {top |    } with bottom = {h , i  TS |  = } and
top = {h , i  TS |  () =  } for every    ; and

 Rts = {down}  {upn , pushn, | n  N,    } with
 down = {(h , ni, h , i) | h , i  TS , n  N+ , n  dom( )},
 upn = {(h , i, h , ni) | h , i  TS , n  dom( )}, and
 pushn, = {(h , i, h  {(n,  )}, ni) | h , i  TS , n / dom( )}
for every n  N+ and    .
2 See Denkinger [Den16] for details on the automata characterisation.
3A s');
INSERT INTO posts (postId,userId,title,body) VALUES (223,1915,'Tobias Denkinger (part 10)','et D  N
+ is prex closed if for each w  D, every prex of w is also in D.

(cid:3)

13

Example 5.2. Let  = {a, b, c}. Consider the (TreeStack{,#} ,  )-automaton M =
([4], T , {1}, {4}) where T contains the transitions

(1, a, TS , push1, , 1)
(1,  , TS , push1,# , 2)
(2,  , top# , down , 2)

, down, 2)
(2, b, top
, 3)
(2,  , bottom, up1

, 3)
(3, c, top , up1
(3, , top# , down, 4)

and note that L(M) = {anbn cn | n  N}.

(cid:3)

We present two approximation strategies for multiple context-free languages from the
literature, using the framework of this paper.

Example 5.3. Let  be a nite set.

(i) Van Cranenburgh [vC12, Sec. 4] observed that the idea of Ex. 4.11 (iv) also applies
to multiple context-free grammars (short: MCFG). The idea can be applied to
tree-stack automata similarly to the way it was applied to pushdown automata in
Ex. 4.11 (iv). The resulting nd storage type is still a tree-stack storage.

(ii) Burden and Ljunglof [BL05, Sec. 4] and van Cranenburgh [vC12, Sec. 4] proposed
to split each production of a given MCFG into multiple productions, each of fan-
out 1. Since the resulting grammar is of fan-out 1, it produces a context-free
language and can be recognised by a pushdown automaton. The corresponding
approximation strategy in our framework is

Acf, : TS    ,

Acf, (( , n1    nk )) =  (n1    nk )     (n1n2 ) (n1 )

for every ( , n1    nk )  TS with n1 , . . . , nk  N+ . The resulting nd storage type
is pushdown storage.
(cid:3)

Example 5.4. Let us consider the (TreeStack{,#} ,  )-automaton M from Ex. 5.2. The
transitions of the ((TreeStack{,#} )Acf, ,  )-automaton MAcf, (cf. Ex. 5.3) and the
((TreeStack{,#} )Acf, ;Atop ,  )-automaton MAcf, ;Atop (cf. also Ex. 4.11) are shown
below.

transitions of MAcf, :
(1, a,  
, 1)
, push
(1,  ,  
, 2)
, push#
, 2)
(2,  , top# , pop
(2, b , top
, 2)
, pop
(2,  , bottom, push  push# , 3)
, push  push# , 3)
(3, c , top
(3,  , top# , pop
, 4)

transitions of MAcf, ;Atop :
, 1)
(1, a, @ , {( , ) |   @}
, 2)
(1,  , @ , {( , #) |   @}
(2,  , {#}, {( ,   ) |  ,    @}
, 2)
(2, b , {} , {( ,   ) |  ,    @}
, 2)
(2,  , {@} , {( ,   ) |   @ ,     }, 3)
(3, c , {} , {( ,   ) |   @ ,     }, 3)
(3,  , {#}, {( ,   ) |  ,    @}
, 4)

Note that MAcf ;Atop has nitely many storage congurations, and thus its language is
recognisable by a nite state automaton (by Prop. 3.8).
(cid:3)

14

References

[Bak81]

[BL05]

Extending lookahead for LR parsers.
T. P. Baker.
and System Sciences,
22(2):243259,
1981.
of Computer
10.1016/0022-0000(81)90030-1.

Journal
DOI:

H. Burden and P. Ljunglof. Parsing linear context-free rewriting systems. In
H. Bunt, R. Malouf, and A. Lavie, editors, Proceedings of the 9th Interna-
tional Workshop on Parsing Technology (IWPT 2005), pages 1117, Strouds-
burg, PA, USA, 2005. Association for Computational Linguistics. URL:
http://dl.acm.org/citation.cfm?id=1654494.1654496.

[BS90]

M. E. Bermudez and K. M. Schimpf. Practical arbitrary lookahead LR pars-
ing. Journal of Computer and System Sciences, 41(2):230250, 1990. DOI:
10');
INSERT INTO posts (postId,userId,title,body) VALUES (224,1915,'Tobias Denkinger (part 11)','.1016/0022-0000(90)90037-l.

[CPV+06] E. Charniak, M. Pozar, T. Vu, M. Johnson, M. Elsner, J. Austerweil, D. Ellis,
I. Haxton, C. Hill, R. Shrivaths, and J. Moore. Multilevel coarse-to-ne
PCFG parsing. In Proceedings of the Main Conference on Human Language
Technology Conference of the North American Chapter of the Association of
Computational Linguistics (NAACL HLT). Association for Computational
Linguistics, 2006. DOI: 10.3115/1220835.1220857.

[Den16]

T. Denkinger. An automata characterisation for multiple context-free lan-
guages. In S. Brlek and C. Reutenauer, editors, Developments in Language
Theory (DLT 2016), pages 138150. Springer Berlin Heidelberg, 2016. DOI:
10.1007/978-3-662-53132-7_12.

[DKV09] M. Droste, W. Kuich, and H. Vogler. Handbook of weighted automata.
Springer, 2009. DOI: 10.1007/978-3-642-01492-5.

[Eng86]

[Eng14]

[EV86]

[Eva97]

J. Engelfriet. Context-free grammars with storage. Technical Report I86-11,
Leiden University, 1986.

J. Engelfriet. Context-free grammars with storage. Computing Research
Repository, 2014. arXiv:1408.0683 [cs.FL].

J. Engelfriet and H. Vogler.
Pushdown machines for the macro tree
transducer. Theoretical Computer Science, 42(0):251368, 1986. DOI:
10.1016/0304-3975(86)90052-6.

E. G. Evans. Approximating context-free grammars with a nite-state calcu-
lus. In Proceedings of the 8th Conference of the European chapter of the As-
sociation for Computational Linguistics (EACL 1997). Association for Com-
putational Linguistics, 1997. DOI: 10.3115/979617.979675.

[Hoa72]

C. A. R. Hoare. Proof of correctness of data representations. Acta Informatica,
1(4), 1972. DOI: 10.1007/bf00289507.

15

[HU79]

[HV15]

J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Lan-
guages and Computation. Addison-Wesley, 1st edition, 1979.

L. Herrmann and H. Vogler. A Chomsky-Schutzenberger theorem for
weighted automata with storage.
In A. Maletti, editor, Proceedings of
the 6th International Conference on Algebraic Informatics (CAI 2015), vol-
ume 9270, pages 90102. Springer International Publishing, 2015. DOI:
10.1007/978-3-319-23021-4_11.

[Joh98] M. Johnson. Finite-state approximation of constraint-based grammars us-
ing left-corner grammar transforms. In Proceedings of the 17th International
Conference on Computational Linguistics. Association for Computational Lin-
guistics, 1998. DOI: 10.3115/980451.980948.

[KdT81]

Transducers and grammars as the-
S. Krauwer and L. des Tombe.
Theoretical Linguistics,
ories of
language.
8(1-3),
1981.
DOI:
10.1515/thli.1981.8.1-3.173.

[LL87]

D. T. Langendoen and Y. Langsam. On the design of nite transducers for
parsing phrase-structure languages. Mathematics of Language, pages 191235,
1987.

[Ned00a] M.-J. Nederhof.
Practical experiments with regular approximation of
context-free languages. Computational Linguistics, 26(1):1744, 2000. DOI:
10.1162/089120100561610.

[Ned00b] M.-J. Nederhof.
a grammatical
Regular approximation of CFLs:
In H. Bunt and A. Nijholt, editors, Advances in Probabilistic
view.
and other Parsing Technologies, pages 221241. Springer, 2000. DOI:
10.1007/978-94-015-9470-7_12.

[Pul86]

S. G. Pulman. Grammars, parsers, and memory limitations. Language and
Cognitive Processes, 1(3):197225, 1986. DOI: 10.1080/01690968608407061.

[PW91]

F. C. N. Pereira and R. N. Wright. Finite-state approximation of phrase
structure grammars. In Proceedings of the 29th annual meeting on Associa-
tion for Computational Linguistics (ACL91). Association for Computational
Linguistics, 1991. DOI: 10.3115/981344.981376.

[Sco67]

Some denitional suggestions for automata theory.
D. Scott.
nal of Computer and Sy');
INSERT INTO posts (postId,userId,title,body) VALUES (225,1915,'Tobias Denkinger (part 12)','stem Sciences, 1(2):187212,
1967.
10.1016/s0022-0000(67)80014-x.

Jour-
DOI:

[SMFK91] H. Seki, T. Matsumura, M. Fujii, and T. Kasami. On multiple context-
free grammars. Theoretical Computer Science, 88(2):191229, 1991. DOI:
10.1016/0304-3975(91)90374-B.

16

[vC12]

context-free
Ecient parsing with linear
A. van Cranenburgh.
editor, Proceedings of
In W. Daelemans,
the
rewriting systems.
13th Conference of
the European Chapter of
the Association for
Computational Linguistics (EACL 2012), pages 460470, Stroudsburg,
PA, USA, 2012. Association for Computational Linguistics.
URL:
http://dl.acm.org/citation.cfm?id=2380816.2380873.

[VDH16] H. Vogler, M. Droste, and L. Herrmann. A weighted mso logic with stor-
age behaviour and its buchichi-elgot-trakhtenbrot theorem. In A.-H. Dediu,
J. Janousek, C. Martn-Vide, and B. Truthe, editors, Proceedings of the 10th
International Conference on Language and Automata Theory and Applica-
tions (LATA 2016), pages 127139. Springer International Publishing, 2016.
DOI: 10.1007/978-3-319-30000-9_10.

[VSWJ87] K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. Characterizing structural
descriptions produced by various grammatical formalisms.
In Proceedings
of the 25th Annual Meeting on Association for Computational Linguistics
(ACL 1997), pages 104111, Stroudsburg, PA, USA, 1987. Association for
Computational Linguistics. DOI: 10.3115/981175.981190.

17

');
INSERT INTO posts (postId,userId,title,body) VALUES (226,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com','Abstract:  This  paper  aims  to  introduce  an  application  to  Kalman  Filtering  Theory,  which  is  rather 
unconventional. Recent experiments have shown that many natural phenomena, especially from ecology or 
meteorology,  could  be  monitored  and  predicted  more  accurately  when  accounting  their  evolution  over 
some  geographical  area.  Thus,  the  signals  they  provide  are  gathered  together  into  a  collection  of 
distributed  time series. Despite  the common sense, such  time series are more or  less correlated each other. 
Instead  of  processing  each  time  series  independently,  their  collection  can  constitute  the  set  of measurable 
states provided by some open system. Modeling and predicting the system states can take benefit from the 
family  of  Kalman  filtering  algorithms.  The  article  describes  an  adaptation  of  basic  Kalman  filter  to  the 
context of distributed signals collections and completes with an application coming from Meteorology.  

1. INTRODUCTION AND PROBLEM STATEMENT 
Kalman Filtering (KF) Theory was originated at early 60s by 
the  works  of  R.E.  Kalman  and  R.S.  Bucy  (Kalman,  1960, 
Kalman-Bucy,  1961).  One  can  say  that  Kalman-Bucys 
approach acted like a switch within the scientific community, 
because,  nowadays,  the  literature  on  this  topic  is  one  of  the 
richest,  concerning  the  theory,  as  well  as  the  applications. 
Moreover,  new  and  sometimes  surprising  applications 
continue  to  keep  the  KF  field  alive.  For  example,  one  can 
mention  the  latest  results  from  avionics  (the  stellar  inertial 
navigation  problem) 
(Kayton,  1997), 
fault  diagnosis 
(Hajiyev,  2003)  or  robotics  (Negenborn,  2003).  This  paper 
focuses  on  the  problem  of  correlated  time  series  prediction. 
Evolution of some natural phenomena can be monitored with 
higher accuracy if the observation and measurement take into 
account  not  only  time  variation  of  some  parameter,  but  also 
its  distribution  over  a  geographical  area.  Take  for  example 
the monitoring of minimum and maximum  temperatures over 
a geographical area (see Fig. 1).  

 

 

suitable.  Sensors  could  thus  provide  several  time  series,  on 
different 
locations.  Such  data,  coming  from  different 
channels, are in general more or less correlated. For example, 
in  Fig. 1,  one  can  easily  notice  the  strong  correlations 
between  the  fourth  temperature  variations,  since  the  two 
cities  are  close  to  each-other.  It  is  even  possible  that  hidden 
correlations  (that  cannot  be  perceived)  be  crucial  for 
monitoring. Assume  that  the monitoring goal is to predict the 
temperature.  It  is  very  likely  that  better  prediction  results  be 
obtained  when  considering  the  collection  of  all  data  sets, 
rather  than when  building  independent  prediction models  for 
each  channel  in  isolation.  The  problem  is  then  to  build  and 
estimate  multi-variable  identification  models,  in  view  of 
prediction.  
The  solution  introduced  within  this  paper  relies  on  the  idea 
that  sensors  provide');
INSERT INTO posts (postId,userId,title,body) VALUES (227,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com (part 2)','  direct  noisy  data  from  the  states  of  an 
open  and  quasi-ubiquitous  system.  The  system  has  in  fact  a 
continuous collection of variable states. Placing a finite set of 
sensors  at  different  locations,  in  order  to  perform measuring, 
is  equivalent  to  sampling  the  system  both  in  time  and  space. 
The  prediction  problem  of  each  data  set  (a  time  series,  in 
fact)  is actually a problem of state prediction and can  thus be 
solved  in  context  of  KF  Theory.  Therefore,  an  adaptation  of 
basic KF algorithm to the context of distributed time series is 
presented  next.  The  article  is  structured  as  follows.  Next 
sections  introduce  the  Markov-Kalman-Bucy  method  and 
summarize the algorithm that allow the distributed prediction. 
The  simulation  case  study  is  based  on  the  example  in  Fig.1. 
A conclusion and the references list complete the paper.  

Fig. 1. Temperature monitoring in 2 cities from Romania. 

When  using  a  stand  alone  sensor,  there  is  a  problem with  its 
location.  Obviously,  the  temperature  varies  both  in  time  and 
space.  A  small  network  of  sensors  is  seemingly  more 

2. DISTRIBUTED PREDICTION FRAMEWORK 
The  distributed  prediction  relies  on  state  representation 
below:  



k1]
+ =k
[ ]
F w[k
D v[ ]
k  N

(1) 

A x[ ]C x[ ]
+
B u[ ]k
B u[ ]k
+

,   

, 

nx nx


ny nx


ny nu

 
where:  
xB R
C R
yB R
A R
, 
, 
, 
, 
nx nu
kkF R
D R
  are  matrices  including  all 
  and 
ny ny
nx nx

k
variable  (but  already  estimated)  parameters  of  some 
n=D
I y
stochastic process; (usually, 
); nxx R
 is the unknown state vector;  nuu R
 is the vector of (measurable) input signals;  nyy R
 is the vector of (measurable) output signals;  nxw R
 is the (unknown) endogenous system noise;  nyv R
  is  the  (unknown)  exogenous  noise,  which  is usually corrupting the measured data.  
Whenever  the  stochastic  process  cannot  be  stimulated 
artificially  (like  in  case  of  time  series),  the  input  vector  is assigned  to  null values. Therefore,  the noise 
 becomes  the virtual useful input, while the noise 
 is parasite.  

T

. 

=

E

k
[ ]
[ ]

(2) 

 and 
k
[ ]
k
[ ]
v[ ]
w w[ ]

The  following  noise  hypotheses  are  assumed  for  model  (1): 
(a) all noises are zero mean, Gaussian;  (b)  the  two noises are 
uncorrelated  each  other;  (c)  the  endogenous  noise  is  non 
auto-correlated,  but  its  compounds  could  be  correlated  at  the 
same  instant;  (d)  the  compounds  of  exogenous  noise  are 
white  and  uncorrelated  each-other. According  to  the  last  two 
hypotheses, the covariance matrices of noises are mostly null, 
 N
excepting  for  the  current  instant  k
,  when  they  are 
denoted by:  wv
Obviously,  the  matrix 
  is  diagonal.  This  is  perhaps  the 
most restrictive condition, in general case.   
In  case  of  distributed  time  series,  the  output  vector 
includes  all  the  data  sets  (on  channels),  whereas  the  state   encodes  the  invisible  correlations  between  them. 
vector 
Naturally,  there  is no reason  to consider  that  the white noises 
corrupting  the  data  are  correlated  each  other.  The  ');
INSERT INTO posts (postId,userId,title,body) VALUES (228,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com (part 3)','number  of 
states  is not necessarily equal  to  the number of  time series.  It 
depends  on  the  size  of  multi-dimensional  ARMA(X)  model 
assigned to the global process, as shown in next section.  
Two  main  problems  can  be  stated  within  this  context.  The 
first  one  is  to  identify  a  rough  ARMA  model  (by  using  the 
time series), to improve its accuracy and to transform it into a 
minimal  state  representation.  The  second  problem  is  to 
predict  the  states  via  an  adapted  version  of Kalman  filtering. 
Each problem is approached next.  

3. FROM ARMA(X) TO STATE REPRESENTATION 
The  rough  ARMA  model  can  be  constructed  by  simply 
considering  that  the  time  series  are  independent  each  other. 
 N , 
jy   (n y )  of  length  N
Thus,  for  each  time  series 
1,
the corresponding ARMA model:  (
A q
)je
C q 1
(3) 



, 



j

    

(with known notations), is identified via Minimum Prediction 
Error Method  (MPEM)  (Sderstrm and Stoica, 1989). After 
identification,  the  estimated  polynomials  A j   and  C j   (for n y ) yield the evaluation of prediction error:  
1,

j



)



A q
(


1 C q

 
 
which  actually  stands  for  the  (approximated)  input 
nu
ny=
overall stochastic process. So, 
, in this case.  

,    







j

(4) 

ju   of 















1

q

q

q

)

(

(

(

(

)

(

(

)

=

A

G

ny ny

1q


e , 

 '||'&'||' 

(5) 

,  (6) 

y B



v C
(
C 1q


To  refine  the  rough  model,  one  adopts  a  global  model,  of 
ARMAX type:  A)1A B C R
where  the  polynomial matrices 
  have  to q

be  identified  from  the  time  series  as  output  data  and  rough 
prediction  errors  (4)  as  input  data.  The  two  system  functions 
of model (5), i.e.:  Hencode  moreover  the  correlations  between  time  series. 
Identification makes use of the same MPEM, but applied to a 
multi-dimensional stochastic process. Implementation of such 
a  method  is  non  trivial  and  involves  many  numerical 
problems.  In  order  to  reach  for  a  suitable  tradeoff  between 
speed  and  accuracy,  some  simplifications  are  necessary.  For 
example,  in  MATLAB  environment,  the  following  general 
principle has been adopted: each output depends on  the  input 
signals  and  noises  only.  With  another  words,  output  signals )1q A
are  not  mixed  each  other,  which  implies  the  matrix is  diagonal.  Another  simplification  is  related  to  inputs:  since   should  not 
input  signals  are  actually  noises,  the  channel 
ju  (one for input and another one for 
account twice the input 
)1q Bnoise).  Consequently,  the  matrix 
  has  null  diagonal. 
Although  the  resulting model  is  seemingly  less  accurate  than 
the  one  obtained  by  applying  MPEM  at  once,  it  is  assumed 
that  the  accuracy  is  acceptable.  The  great  facility  of  such  an 
approach  is  that  the  system  functions  (7)  are  directly 
.  A
computed, without inverting the polynomial matrix 
Once the system functions being estimated, the MIMO model 
can  be  converted  into  a  state  representation  like  (1), 
following  at  least  three  rationales.  First  of  them  is  based  on 
Theorem  of  Division  with  Reminder  and  atomic  ratios 
decomposition 
idea 
the 
following 
(for  polynomials), 
introduced  in  (Proakis,  1996).)  The  second  one  starts  from 
the  linear  re');
INSERT INTO posts (postId,userId,title,body) VALUES (229,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com (part 4)','gression  form of ARMAX model and defines  the 
state  by  concatenating 
the  regressors  vector  and 
the 
parameters vector (Niedwiecki, 2000). The number of states 
can  increase  very  fast  in  case  of  MIMO  models.  This  is  the 

 in 
introduced 
technique 
third  conversion 
the 
reason 
(vanOverschee,  1996)  is  often  preferred,  since  it  leads  to  the 
minimum state representation.  

, 

E

}T

(7) 

VV
 0
4. ADAPTED KALMAN FILTER (PREDICTOR) 
The  filter  is  aiming  to  predict  the  states  of  model  (1),  by 
using  a  numerical  procedure  that  relies  on  the  recursive 
Markov  estimator  (Stefanoiu,  2005).  Hereafter,  the  Markov 
estimator and the filter equations are described.  
The main  result  of Gauss-Markov Theorem  (GMT)  (Placket, 
1950)  can  be  implemented  through  a  recursive  procedure. 
One  starts  from  the  canonic  form  of  linear  regression 
associated to an identification model (like (3) or (5)):  Y  V ,   
=NY Rwhere 
  is  the 
-length  output  (measurable)  data 
 R
  is  the  matrix  of  regressors  (either 
vector, 
N n 
  R   is  the  vector  of 
  unknown 
measurable  or  not), NV R
true  parameters  and is 
the  vector  of  (non 
N N R
measurable)  noise data with 
  as  covariance matrix. 
One  assumes  the  noise  is  Gaussian  with  zero  mean,  but  not 

necessarily white  (i.e.  the matrix 
  could  be  non  diagonal). 
The GMT states that the following Markov estimation:  ) 1

=      Y  T1
is  unbiased,  consistent  and  efficient.  Moreover, 
the 
estimation  accuracy 
inverse  of 
is  also  provided  (the 
estimation error covariance matrix):  
(8) 





) (




T

  1
. 

(9) 
The  problem  is  to  design  an  efficient  numerical  procedure  to 

 is usually 
compute (8) and (9) recursively. Since not only 
unknown,  but  its  size  could  be  quite  big,  computing  its 
inverse  is  non  trivial.  To  solve  the  problem,  one  extends  the 
canonical  model  (7)  by  the  following  virtual  model  (an 
identity, also known as random walk model):  (cid:4)=

(10) 

, 



=
(





} 1





(cid:4)((cid:8)(cid:11)(cid:9)(cid:11)(cid:10)

 
n(cid:4) R
  is  the  Markov  estimation  from  previous 
where 
nW R
  is  the  virtual  noise 
computation  stage,  whereas 

  is  non  measurable).  Interestingly,  the 
(unknown,  since 
(cid:4)
covariance  matrix  corresponding  to  virtual  noise,  P ,  is 
already  available  from  the  previous  computation  too.  After 
fusing equations (7) and (10), the extended model is:  

    

, 

(11) 




Y




(cid:4)



(cid:78)
(cid:105)






(cid:78)
(cid:105)






(cid:78)
(cid:105)
   is  not 
with  natural  notations.  Whenever  the  noise correlated  to  the  virtual  noise 
,  the  covariance  matrix  of 
(cid:105)V , includes two diagonal blocks only:  
extended noise (cid:105)

(cid:105) (cid:105)
V V


(12) 

}T

=

. 


= 


 0
0 P(cid:4)





. 
When  computing  again  the  Markov  estimation  with  the 
extended  stochastic  process  (11),  after  some  manipulations, 
the  direct  link  between  the  current  and  previous  values  of 
estimated parameters can be revealed:  ((cid:4)
(cid:4)
(cid:4)(
) . 


     P
  Y T1



= +(cid:8)(cid:11)(cid:9)(cid:11)(cid:10)
(cid:8)(cid:11)(cid:11)(cid:11)(cid:11)(cid:9)(cid:11)(cid:11)(cid:11)(cid:11)(cid:10)
(');
INSERT INTO posts (postId,userId,title,body) VALUES (230,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com (part 5)','cid:4)



(13) 

(14) 

(cid:4)
P   1

 
Two interesting terms are outlined in (13). The first one is the 
(cid:4)
N(cid:4) R
,  based  on  previous  estimation   . 
prediction  error 
 R
The  second  one  is  the  sensitivity  gain 
,  based  on 
n N
1P(cid:4)
previous  accuracy 
.  Supplementary  manipulations  can 
lead  to  an  equivalent  expression  of  the  gain,  which  is  more 
suitable  for numerical  evaluations  (only one matrix has  to be 
inverted instead of 4):  (cid:4)
(cid:4)
)T  P  P+ 
The covariance matrix of estimation error can also be updated 
without any matrix inversion (after computing the gain):  
PnIwhere 
.  Practically,  the 
  is  the  unit  matrix  of  size 
recursive  equations  (14),  (15)  and  (13)  constitute  de  core  of 
implementation  procedure 
for  Markov  estimator.  An 

  can  be  obtained  by  using  the  prediction 
estimation  of 
(cid:4)
(cid:4)  instead  of  noise 
  (since,  actually, 
  is  the  current 
error estimation  of 
.)  The  most  time  consuming  operation  is 
N N
computing  the  gain  (14),  because,  at  every  step,  a matrix has to be inverted. To reduce the computational effort, 
the  number  of  adaptation  data  should  be  used  instead  of   is  the  number  of  data  between 
whole  data  number.  So, 
successive parameters upgrading (usually, no bigger than 10).  
Recall  the  state  representation  (1)  and  assume  that  all 
parameters are already known at current instant  k  N . Then, 
the final goal is to estimate/predict the state values at the next 
1k + ,  i.e.   [
]+x
1k
,  depending  on  current  state  values 
instant 

) (cid:4) , 
 P

(15) 



(

n



I


k



)

. 





}
)

) (

def
,   
(
k
[ ]
k
[ ]

k
[ ]

k
[ ]

(16) 

k  N

(cid:4)=(cid:4)k
[ ]


D C P C[ ]Tkk
(

P C D kk


P  C P
kk
 yk[ ]
[ ]=
  [ ]kx
  and  newly  measured  data.  In  order  to  reach  for  this 
goal, Markov estimator has to be employed as main tool.  
The  main  variables  of  Markov  estimator  are  identified  as 
follows  from  the  last  equation  of  model  (1):  the  state  vector 
[ ]kx

kC

  is 
  and  the  output  matrix 
  is 
.  Consequently, 
(cid:4)

 [ ]kx
 [
1k +x
.  The  covariance 
  stands  for 
,  while 
  is matrix from (9) is defined as follows:  
P 
Before  starting  the  Markov  estimation  procedure,  it  is 
[ ]kv
  is  correlated  to 
necessary  to  verify  whether  the  noise (
 [ ]
[ ]kxxk
the  error 
  or  not.  Since  the  current  state [ ]
 [ ]kx 
 and the estimated state 
only depends on inside noise 
 [
1k x
  (i.e.  by  the  values 
is  determined  by  the  former  state 1k 
  at  most),  the  non  correlation 
of  noises  at  instant 
restriction  is  fully  verified  (under  the  noise  hypotheses  of 
section  2).  This  allows  the  current  state  to  roughly  be 
estimated as follows:  



kD
.  In 
since  the  exogenous noises are mixed  through matrix 
kP(cid:4)
[ ]kx(cid:4)
 [
1k +x
  is  different 
  is  not  yet  equal  to 
  and 
(17), 
k +P
from 
  as  well,  because  the  first  equation  of  model  (1) remained  untouched. They  are  only  rough  approximations  of 
the  targeted  terms.  To  refine  the  approximations,  the  next 
state  is  computed  from');
INSERT INTO posts (postId,userId,title,body) VALUES (231,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com (part 6)','  the  first  equation  of  model  (1),  with 
[ ]kx(cid:4)
 [ ]kx
0=w
 instead of 
 and 
. Thus:   [ 
The  next  covariance  matrix 
consequence of equation (18):  
P 
after  some  manipulations  where  the  non  correlation  between )
(cid:4)
[ ]kwxk
  and 
  played 
the  main  role.  After 
[ ]
[ ]
aggregating  equations  (17)-(19),  the  kernel  of  final  recursive 
procedure related to Kalman-Bucy filtering is obtained:  

k
k=


=
1k  N . 
)


P C D 
C P Ck
[ ]T+kkkk

C x
A x
B u
A  yk[ ]
[ ]
[ ]+
kkk(


A P
 C P A
F k
[ ]T
kkkk

k +P   results  then  as  direct 
k  N

k  N

k  N

(cid:4)
A P Ak

(cid:4)
A x[ ]

C x[ ]
B u[ ]k
k
[ ]k

def
+ =
B uk

1]
+ =

k ,   
)k
[ ]

(19) 

(18) 

(20) 

(17) 

F 
B uk
[ ]

,   

,  

1]





+

, 

, 

. 

, 

k



T


w

 

    

Obviously,  the procedure  (20) can be  implemented only after 
the  mixed  covariance  matrices  of  noises, 
estimating 
k  N .  This 
kwF 
kvD F
  and 
  at  any  instant 
[ ]
[ ] Tkkoperation involves 2 computation stages. At the first step, the 
mixed  exogenous  noise  is  estimated  with  the  help  of  the 
second equation of (1).  
D v[ ] 
At the second step, the covariance matrix is updated:  D 
B u
n n
[ ]
 y


D v[ ]

C x[ ]
n
D[ ]
n
[ ]

(21) 
n
[ ]

[ ]

0,


=



. 

, 

T
T


 

(22) 

1

1
+ 
D k


k
0k
=
+

(

T




D v[ ]

1]
k


T


)
D .T[ ]
 
For  the  endogenous  noise,  instead  of  repeating  the  steps 
above, one can compute  the estimation more elegantly. Thus, 
it is easy to see from (20), (21) and (1) that:  
A  D v[ ]k 
Since  the  noises  estimations  are  so  correlated,  the  estimation 
kwF of 
 follows straightforwardly:  
[ ] Tk


F w . [ ]

A x[ ]
B u[ ]k

1]
+ 

(23) 

[



=

 
F 
1
=

1
k
[ ]k

k
F k


(
1k
k
0F
1]
k



F w w[ ]
T
n
[ ]n

+


A  D v[ ]k

T
D  A .[ ]Tkk

 (25) 
Equations  above  can  be  gathered  together  into  a  numerical 
recipe  aiming  to  predict  the  discrete  stochastic  states  and 
outputs of model (1). The main steps are as follows.  



nx

(cid:190) Input  data:  a  small  collection  of  time  series  values  (the }
= y
training set 
) yielding initialization.  
n 
[ ] n01,1.  Initialization.  Produce  the  first  state  representation  (1). 
Then  complete  the  initialization  by  setting:  an  arbitrary 

0xI
state  vector 
,  the  covariance  matrices 
  (with 
= 
nx  R ), 
D 
F 0D
 and 
.  
[ 1]
[ 1] T

=1+




2.  For 
:  0
k 
2.1.  Estimate the exogenous mixed noise:  


C x
D vB uk
k . k[ ]
[ ]
[ ]
[ ]
 y
k2.2.  Update the covariance matrix of exogenous noise:  (

D v
D 
D D
DTkk[ ]
[ ]
1]
[ ]TTkkk
k+

k=Q C P
.  
2.3.  Compute the auxiliary matrix: k
D Q C
D 2.4.  Invert the matrix:  [ ]T+kkv
= Q R . 
2.5.  Evaluate the sensitivity gain: 1
kk=S
A  k
2.6.  Compute the auxiliary matrix 
.  
R

ny ny




. 

. 



)


ny



 
 

. 

.  
1

2.7.  Update the covariance matrix of endogenous noise:  )

D S
S D v
F 
F Fkkk
[ ]
1]
[ ]
[ ]TTT
=

kkkk1w

12.8.  Update  the  covariance  matrix  of  estimation  error: )


A P  Q A
F F
.  [ ] T+ =
kkkk1
2.9.  Predict the state: 



S D v
B u
A x. 
]kk[ ]
[ ][
1]++ =kk


C x
B u2.10. Predict the output: k1]1]y
+ +
+ =k}
+y
2.11. Acquire new data: 
.  [
1]
+ = Dk2.12. Update the state model.  
(cid:190) Output data:  
predicted time series values  { [ ] kkN ;  

estimated covariance matrices  {N .  
vD k
 [ ]
k
The  most  time  consuming  steps  of  algorithm  above  is  2.12 
(state  model  matrices  updating),  followed  by  2.4  (matrix 
inversion).  The  algorithm  above  can  easily  be  adapted  to 
multi-step  prediction,  thanks  to  Markov  estimator.  In  this 
case, the algorithm has two stages. The first one is concerned 
with  the  model  adaptation.  In  the  second  one,  multi-step 
prediction is performed. The');
INSERT INTO posts (postId,userId,title,body) VALUES (232,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com (part 7)',' main difference between the two 
algorithms (one step and multi-step prediction) consists of the 
exogenous noise estimation. As long as the measured data are 
available, equation (21) can successfully be employed. When 
the measured  data  are missing,  the  exogenous  noises  have  to 
be  estimated  by  a  different  technique.  For  example,  MIMO-
ARMA(X)  models  can  be  employed  in  this  aim;  the 
estimated  white  noises  can  directly  stand  for  mixed 
n=D
I y
exogenous noises (since, usually, 
).  
T
D NEstimations  of  covariance  matrices  [TN k
N kv
y +y
are  necessary  both  to  assess  the  prediction  performance  and 
to  estimate  the  SNRs.  Thus,  the  diagonal  of  each  matrix 
returns  the  set  {
}2
 j k
,  whose  values  play  the  role  of 
j
ny
1,
yN
prediction errors variances on every channel. Here, 
 is the K
data  length  and 
  is  the  current  prediction  step  on  the 
1,
prediction horizon. Then the following two types of SNR can 
be  evaluated  (one  for  measured  data  and  another  one  for 
predicted data):  SNRy  are standard deviations of data on measuring 
, 
where jyand  prediction  horizons,  respectively;  also, 
  is  the y
jstandard  deviation  of  prediction  error.  The  SNRs  (26)  allow 
one to define the prediction quality (
PQ) cost function below:  
j  y  (26) 
1,
def
= 

,1
 '||'&'||'j

def
= 

SNR

)2

/j
y
Kj

)


j
y


j

2

,



def

PQ 100 / 1+
j





j



j k
K
1SNR SNR





j

[%]

, 
 

1,

ny

. 

(27) 

    

PQ T

ny

(cid:34)

PQ

= 

1PQ

,  the better  the 

The  bigger  the norm of 
predictor performance.  
PQFinding the structural indices that maximize the norm of 
cannot  be  realized  through  an  exhaustive  search.  The structural  indices  are: 
    the  degree  of  polynomial  trend, 
   the orders of ARMA model and  nx    the number 
na nc
, 
of  states  for  the  linear  system  (1). An  evolutionary  searching 
technique has to be employed in this aim.  

5. SIMULATION RESULTS 
An  application  coming 
from  Meteorology  has  been 
considered.  Daily  minimum  and  maximum  temperatures  of 
two  cities  have  been  monitored  and  predicted  (as  Fig.1 
suggests). The cities are 60 km far each other on a plain. The 
data  block  consists  of  482  samples  on  4  channels.  Two 
predictors  are  compared  in  terms  of  PQ:  PARMA  and 
KARMA. The first one is based on ARMA prediction of each 
channel  in  isolation.  The  second  one  relies  on  the  adapted 
Kalman 
filter  predictor.  Both  algorithms  have  been 
implemented  within  MATLAB  environment.  In  order  to  find 
optimal  structural  indices,  the  technique  from  (Kennedy, 
1997)  has  been  adopted.  There  are  many  implementation 
details that cannot be described here. Just one word regarding 
KARMA:  numerical  stability  of  the  algorithm  required 
special attention at step 2.5.  
For  each  one  of  the  final  figures  (2-9),  three  variations  are 
depicted: the original data with the deterministic model (trend 
and  seasonal variation,  if any) on  top,  the  residual noise with 
estimated  SNR  in  the  middle  and  the  performance  on  the 
prediction  horizon  at  bottom.  Although  the  predictability 
varies  from  a  channel  to  another,  KARMA  succeeded  to 
perform  better  than  PARMA  (higher  PQ  and  SNR  values). 
(This  result  was  confirmed  by  other  data  blocks  as  well, 
where  correlation  between  channels  exits.)  However,  in 
general,  PARMA  has  superior  performance  on  data  blocks 
with  (almost)  uncorrelated  channels.  Below,  the  PQ  values 
and norms are shown:  PQ
[46.00 62.65 65.12 63.23]ARMA =
PQ
[49.59 70.18 79.47 71.70]KARMA = 
Only  4  states  were  necessary  to  represent  the  linear  system 
nx
ny=
  in  this  case  is  pure 
associated  to  data.  The  fact 
coincidence.  On  the  figures  corresponding  to  KARMA 
performance  (Figs. 6-9),  the  only  purpose  of  ARMA  models 
is  to  est');
INSERT INTO posts (postId,userId,title,body) VALUES (233,921,'E-mails: danny@indinf.pub.ro, jculita@yahoo.com (part 8)','imate  the  input  colored  noises  that  stimulate  the 
system.  The  data  on  the  first  channel  are  seemingly  the  less 
predictable. This  is proven by  the modest PQ values returned 
even  by KARMA  (only  slightly  superior  to PARMA  one).  It 
seems  that  data  from  this  channel  are  less  correlated  to  data 
from  the  other  channels,  which  cannot  be  noticed  by  simply 
inspecting the data.  

PQ119.50
ARMA 
PQ
. 
137.26
KARMA 
(28) 





 
 
The  prediction  accuracy  has  increased  at  the  expense  of 
computational complexity for KARMA. Therefore, if the data 
are  quite  uncorrelated  across  channels,  PARMA  should  be 
employed as the first option.  

6. CONCLUSION 
One can  say  that KF  is a new and old  topic at  the same  time. 
Concerning  the  theory,  KF  has  drawn  the  bottom  line  long 
time ago. The applications rejuvenate however this approach. 
The KF-based algorithm  introduced  in  this article  is genuine. 
Its major contribution consists of noises estimation during the 
prediction. The most KF algorithms try to avoid this problem. 
The simulation case study on natural data has proven that the 
prediction  quality  can  be 
improved  when  considering 
correlations between channels.  

REFERENCES 
Hajiyev  Ch.  and  Caliskan  F.  (2003).  Fault  Diagnosis  and 
Reconfiguration  in  Flight  Control  Systems.  Kluwer 
Academic, Boston, USA.  
Kalman  R.E.  (1960).  A  New  Approach  to  Linear  Filtering 
and Prediction Problems. Transactions of ASME, Journal 
of Basic Engineering, Vol. 82D, pp. 35-45. 
Kalman  R.E.  and  Bucy  R.S.  (1961).  New  Results  in  Linear 
Filtering  and  Prediction  Theory.  Transactions  of  ASME, 
Journal  of  Basic  Engineering,  Series  D,  Vol.  83, 
pp. 95-108.  
Kayton  M.  and  Fried  W.R.  (1997).  Avionics  Navigation 
Systems. John Wiley '||'&'||' Sons Inc., New York, USA.  
Kennedy 
J.,  Eberhart  R. 
(1995).  Particle  Swarm 
Optimization,  IEEE  International  Conference  on  Neural 
Networks, Piscataway, USA, pp. 1942-1948.  
Negenborn  R.  (2003).  Robot  Localization  and  Kalman 
Filters.  Ph.D.  Thesis,  Delft  University  of  Technology, 
Delft, NL.  
Niedwiecki  M.  (2000).  Identification  of  Time-Varying 
Processes. John Wiley '||'&'||' Sons, West Sussex, U.K.  
(1996),  Subspace 
vanOverschee  P.  and  deMoor  B. 
Identification 
of 
Linear 
Systems: 
Theory, 
Implementation,  Applications.  Kluwer  Academic 
Publishers, Holland.  
Plackett  R.L.  (1950).  Some  Theorems  in  Least  Squares, 
Biometrika, No. 37, pp. 149157. 
Proakis  J.G.  and  Manolakis  D.G.  (1996).  Digital  Signal 
Processing.  Principles,  Algorithms  and  Applications., 
third  edition,  Prentice  Hall,  Upper  Saddle  River,  New 
Jersey, USA.  
Sderstrm  T.  and  Stoica  P.  (1989).  System  Identification, 
Prentice Hall, London, UK.  
Stefanoiu D., Culita J. and Stoica P. (2005). A Foundation  to 
System  Identification  and  Modeling.  PRINTECH  Press, 
Bucharest, Romania.
   

    
                        

 
Fig. 2. PARMA performance on channel 1. 

Fig. 3. PARMA performance on channel 2. 

Fig. 4. PARMA performance on channel 3. 

Fig. 6. KARMA performance on channel 1. 

Fig. 7. KARMA performance on channel 2. 

Fig. 8. KARMA performance on channel 3. 

 

 

 

 

 

 

 

 

 

 

 

 

Fig. 5. PARMA performance on channel 4. 

Fig. 9. KARMA performance on channel 4. 

    

');
INSERT INTO posts (postId,userId,title,body) VALUES (234,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','AbstractThe Physical Unclonable Function (PUF) is a promising
hardware security primitive because of its inherent uniqueness and low
cost. To extract the device-specic variation from delay-based strong
PUFs, complex routing constraints are imposed to achieve symmetric
path delays; and systematic variations can severely compromise the
uniqueness of the PUF. In addition, the metastability of the arbiter circuit
of an Arbiter PUF can also degrade the quality of the PUF due to the
induced instability. In this paper we propose a novel strong UNBIAS
PUF that can be implemented purely by Register Transfer Language
(RTL), such as verilog, without imposing any physical design constraints
or delay characterization effort to solve the aforementioned issues.
Efcient inspection bit prediction models for unbiased response extraction
are proposed and validated. Our experimental results of the strong
UNBIAS PUF show 5.9% intra-Fractional Hamming Distance (FHD)
and 45.1% inter-FHD on 7 Field Programmable Gate Array (FPGA)
boards without applying any physical layout constraints or additional
XOR gates. The UNBIAS PUF is also scalable because no characterization
cost is required for each challenge to compensate the implementation bias.
The averaged intra-FHD measured at worst temperature and voltage
variation conditions is 12%, which is still below the margin of practical
Error Correction Code (ECC) with error reduction techniques for PUFs.

I . IN TRODUC T ION
Hardware security has become an important aspect in modern
Integrated Circuit (IC) design industry because of the global supply
chain business model. Identifying and authenticating each fabricated
components of a chip is a challenging task [1]. A Physical Unclonable
Function (PUF) has been a promising security primitive such that its
behavior, or Challenge Response Pair (CRP) [2], is uniquely dened
and is hard to predict or replicate. A PUF can enable low overhead
hardware identication, tracing, and authentication during the global
manufacturing chain.
Silicon delay based strong PUFs have been studied intensively
since its rst appearance in [3] because of its low implementation
cost and large CRP space compared with a weak PUF [4]. However,
there are still design challenges that restrain a strong PUF from
being put in a widespread practical use. One of the major design
challenges for a silicon delay based PUF is the strict symmetric delay
path layout requirement. The wire delays of the competing paths
should be designed and matched carefully to avoid biased responses,
otherwise low inter-chip uniqueness would make the PUF unusable
[5, 6]. In addition to asymmetric routing, another source of the biased
responses for silicon based PUF is the systematic process variation,
which can also degrade the quality of a PUF, such as uniqueness or
unpredictability. Finally, the metastability issue of the arbiter circuit
for an Arbiter PUF can cause unstable PUF responses, making a
portion of the CRP unusable due to their instabilities [7].

A. Asymmetric Path Delay Routing
For a delay based PUF, the randomness should be contributed
only by the subtle variations between devices, so having biased delay
differences due to asymmetric routing is detrimental to delay based
PUFs, and such impact should be eliminated. However, a precise
control of the routing can b');
INSERT INTO posts (postId,userId,title,body) VALUES (235,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','e a difcult and time consuming task.
An implementation of an Arbiter PUF on Field Programmable Gate
Array (FPGA) is considered much more difcult than a RO PUF
because the connections to the arbiter circuit must also be symmetric

[8], and performing completely symmetric routing is physically
infeasible in most cases [6], resulting small inter-Fractional Hamming
Distance (FHD) for an Arbiter PUF [9]. One of the most common
solutions to the asymmetric routing is to use hard-macros in FPGA
designs [10, 11], but it is not effective with Arbiter PUF, and some
less commonly-used features of the FPGA would be required [12].
Other approaches try to extract randomness by XORing the outputs
of multiple Arbiter PUFs at the cost of large hardware overhead and
less stability [13]. In [14], the authors proposed using middle bits
instead of the Most Signicant Bit (MSB) as the RO PUF response
measurement. The measurement can effectively eliminate the biased
responses, but an efcient way of predicting the inspection bit is not
described, and the presented RO PUF is not a strong PUF. A RTL-
based PUF bit generation unit was proposed in [15], but to the best
of our knowledge, a strong PUF that can be implemented efciently
without any layout constraints has not yet been proposed.

B. Systematic Process Variation
The existence of systematic process variation can degrade the
quality of silicon based PUFs because the local randomness should
be the only desired entropy source of the delay based PUF [16].
The effect of systematic process variation is similar to having biased
wire delay between two delay paths, which can also damage the
uniqueness of the PUF. Another possible vulnerability caused by
systematic variation is the induced process side channel attack as
described in [17]. Due to intra-wafer systematic variation [18], PUFs
fabricated at the same region on different wafers can have similar
systematic behavior, which can be exploited as a process side channel
attack.
To account for systematic variations, a compensation technique is
proposed in [10], which requires careful design decisions to compare
RO pairs that are physically placed close to each other. In [16],
the systematic variation is modeled and subtracted from the PUF
response to distill true randomness with the cost of model calculation.
Similarly, in [19], the averaged RO frequency is subtracted from the
original frequency, where the multiple measurements of each RO can
lead to large latency overhead. In [20], a method is proposed to extract
local random process variation from total variation, however, a second
order difference calculation is needed, and hard-macro technique
must be applied to construct symmetric delay paths.

C. Metastability of the Arbiter Circuit
The idea of an Arbiter PUF is to introduce a race condition on
two paths and an arbiter circuit is used to decide which one of the
two paths reached rst. The arbiter circuit is usually a D ip-op or
a SR latch. If two signals arrive at an arbiter within a short time,
the arbiter circuit may enter a metastable state due to setup time
violation [21]. Once the arbiter circuit is in metastable state, the
response becomes unstable. To eliminate the inconsistency caused
by metastability of the arbiter circuit, existing approaches use the
majority vote or to choose the paths that have a delay difference larger
than the metastable window  at the cost of CRP characterization and
discarding the unstable CRPs [7].

D. Our Contributions
In this paper, we propose the physical implementation bias agno');
INSERT INTO posts (postId,userId,title,body) VALUES (236,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','stic
(UNBIAS) PUF that is immune to physical implementation bias. The
contributions of this paper include:
 We proposed the rst strong UNBIAS PUF that can be imple-
mented purely by RTL without imposing any physical routing
constraints.
 Efcient inspection bit selection strategy based on intra-/inter-
FHD prediction models are proposed and veried on the strong
UNBIAS PUF.

I I . PRO PO SED S TRONG UNB IAS PUF
The proposed strong UNBIAS PUF compares two delay paths to
generate PUF responses. Similar to Arbiter PUF, each bit of the
challenge of the UNBIAS PUF species the path conguration of the
delay path. As shown in Figure 1, the challenge c1 and c2 specify
the path congurations, and an one-bit response is extracted from
the difference register, which can be of several bits long. Once a
challenge is given, a signal is applied at Trigger. Each of the Clock
counter begins to count the number of clock cycles of the system
clock (CLK) whenever the the signal from Trigger is propagated to
the START input of the counter, and stops counting whenever the the
signal from Trigger is propagated to the STOP input of the counter.
For each challenge, the difference value of the two Clock counters is
stored in the difference register for further response extraction, which
is described in details in Section III.

Fig. 1. The proposed strong UNBIAS PUF. The Clock counter starts counting
clock cycles of the system clock (CLK) when START arrives and stops when
STOP arrives. The difference of two Clock counters are stored in the difference
register for further response extraction.

The purpose of the ROs inserted between path congurations is to
increase the path delay so that it will take multiple clock cycles for
the signal to propagate to stop the clock counter. As shown in Figure
2, each RO is associated with a RO counter that counts the number
of oscillations of the RO. The RO counter starts counting when the
signal from its previous path conguration is arrived, and propagates
the signal to the next path conguration only when the count reaches
a certain threshold. All the ROs are composed of same number of
inverters and neither congurations nor any layout constraints are
needed.
Unlike the conventional Arbiter PUF, the strong UNBIAS PUF has
no metastability issues caused by a D ip-op or a latch. The delay
difference of the two paths is transformed into counter values of the
system clock. By judiciously extracting the response from the dif-
ference register, the physical implementation bias can be effectively
mitigated, therefore the UNBIAS PUF can be implemented purely by
RTL without any routing or layout constraints. Details of the response
extraction are described in Section III.

I I I . B IA S - IMMUN E R E S PON SE EX TRACT ION
A. Inspection Bit on Unbiased/Biased Paths
In this section we describe how different selections of the in-
spection bit can change the intra- and inter-FHD. Figure 3 shows
an example of a distribution of values from difference registers of

Fig. 2. ROs are inserted between path congurations to increase the path delay
for better stability. The signal from previous path conguration is propagated
only when the count of the RO counter reaches a certain threshold.

symmetrically routed UNBIAS PUFs. The length of the difference
register is 22-bit, so the range of the register value is between 221
and 221  1 as represented in 2s-complement. The large inter-chip
measurement curve gives the distribution of the values across all
PUFs. Since the PUF is unbiased, roughly half of the difference
values would be greater than zero due to random local process
variation, therefore the inter-FHD of the UNBIAS PUFs would be
close to 50%. In this case, the inspection bit is simply the MSB,
which divides the range of 22-bit difference value into two groups
bin 1 and bin 0. All measurements fall into bin 1 on the left output
a 1; others outpu');
INSERT INTO posts (postId,userId,title,body) VALUES (237,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','t a 0. The small intra-chip measurement curve gives
the distribution of multiple measurements of the PUF on a same
chip. Due to noise, the difference values could be different, so the
intra-FHD of the difference register may not be a perfect 0%.

Fig. 3. For a symmetrically routed PUF, the inter-FHD would be close to
50%. The intra-FHD may not be zero due to measurement noise.

Even though symmetric UNBIAS PUF layout is much preferred,
it is difcult and takes much effort and overhead to achieve such
requirement as described in Section I-A. In practice, if no layout con-
straints are imposed, the measurement distribution of the difference
register can be as shown in Figure 4, where most of the difference
values across chips are greater than zero. In this case, using the MSB
as the inspection bit would cause low inter-FHD of the PUFs because
most MSBs are 0s.

Fig. 4. For a biased PUF, most of the difference values across all chips could
be greater than zero, causing a low inter-FHD if the MSB is the inspection
bit.

For the same biased distribution shown in Figure 4, if the ith bit is
used as the inspection bit of the difference register as Figure 5 shows,
the range of the 22-bit difference value is divided into multiple bins
with width 2i , where the output of the measurement is decided by
the bin in which it resides. Note that in this case the response is
not an indicator of which delay is longer in the comparison. The
smaller the width of the bin is, the closer the inter-FHD is to 50%
because roughly half of the outputs would reside in bin 1 even with
biased delay. On the other hand, the width of the bin should be large
enough so that multiple measurements of a same PUF should always
fall into the same bin. In other words, the width of the bin should be
larger than the variation of the intra-chip measurement distribution.
Therefore, the choice of inspection bit is a tradeoff between inter-
FHD and intra-FHD for a PUF with asymmetric routing.

Fig. 5. For an asymmetrically routed PUF with proper inspection bit, roughly
half of the difference values across all chips would fall in bin 1, therefore
the inter-FHD would be close to 50%.

Fig. 6. Magnied view of Figure 5 with three bins. w is the bin width and the
measurement ranges for challenges C1 and C2 are specied. The expected
intra-FHD1 is 0% and the expected intra-FHD2 depends on the portion of
measured values that fall in bin 1.

shows. We rst prove that the worst-case inter-FHD happens when
 = 0.5w , followed by the prediction model of the inter-FHD for the
worst-case scenario.
1) Worst-Case Inter-FHD Identication: Given a xed w , dene
A1 () and A0 () to be the total underlying area in bin 1 and bin 0
as functions of , respectively. For any Normal distribution, A1 ()
(cid:88)
and A0 () are calculated as:
F ( + 2nw + w)  F ( + 2nw)
A1 () =
n=
A0 () = 1  A1 (, w)
(3)
where F () is the Cumulative Distribution Function (CDF) of the
Normal distribution, and n is the index for bin area summation.
The ratio Ratio() is dened as:

(2)

IV. IN S P EC T ION B I T ID EN T I FICAT ION
A. Intra');
INSERT INTO posts (postId,userId,title,body) VALUES (238,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','-FHD Prediction Model
The intra-FHD depends on the width of the bins w = 2i when
the inspection bit is biti . A straightforward way to determine the
associated intra-FHD for each inspection location is to gather multiple
measurements of the same challenge on a same PUF, and simply
calculate the intra-FHD for each biti . A more efcient approach is
to predict the intra-FHD without calculating it for each biti .
To predict intra-FHDk of a challenge Ck of an inspection bit, we
rst obtain t measured difference registers of the challenge Ck of
a same PUF. Since the bin width and the range of the difference
register is known, the t difference values can be divided into two
groups (responses) according to the bins they reside in. Let the
number of difference values fall in bin 1 be none , and number of
difference values fall in bin 0 be nzero . none and nzero represent
the number of responses of the challenge Ck to be one and zero
during the t measurements, respectively. Since the intra-FHD is
essentially calculated from the response difference between any two
measurements, the predicted intra-FHDk is calculated as:
(cid:0) t
(cid:1)
none  nzero
 100%,where the nal predicted intra-FHD would be the averaged intra-
FHDk of all challenges.
As shown in Figure 6, the expected intra-FHD1 is 0% because
all measurements fall in the same bin and none  nzero = 0. The
expected intra-FHD2 depends on the portion of measured values that
fall in bin 1. With larger bin width w , it is more likely that all
responses would fall into the same bin
B. Inter-FHD Lower Bound Prediction Model
The inter-FHD depends on the bin width w with a given inspection
bit biti . Assume the distribution of inter-chip difference value is a
Normal distribution N  (, 2 ). Dene  to be the distance between
the mean  and the closest bin boundary on the left as Figure 7

intra-FHDk =

(1)

(4)

(5)

(6)

0 <  < w

Ratio() =

Ratio() =

A1 ()
A0 ()
where the range of  is from 0 to w because of its periodic structure.
The closer the Ratio() is to one, the closer the inter-FHD would
be to 50% because the two areas are closer to each other. We want
to show that the largest (most unbalanced) ratio happens at  = 0.5w
as Figure 7 shows.
To nd the extreme value of Ratio() given a xed w , we take
derivative with respective to  of Equation 4 and replace A0 () by
1  A1 () from Equation 3:
A(cid:48)
1 ()(1  A1 ())2
d
From Equation 5 we see that to nd the extreme value of Ratio(),
it is equivalent to nd the solution of A(cid:48)
(cid:88)
1 (), which is given below:
f ( + 2nw + w)  f ( + 2nw)d
n=
where f () is the Probability Density Function (PDF) of the Normal
distribution. Equation 6 shows that A(cid:48)
1 () is the summation of
differences between two PDF terms where one is a shifted version
by w of another. Therefore, applying  = 0.5w to Equation 6, we get
a zero. Figure 7 shows that when  = 0.5w , each difference term in
Equation 6 has its counter part at the mirrored location to the center,
so that the summation becomes zero.
To conclude our derivation, given a w of an inspection bit, the
extreme value of Ratio() happens when  = 0.5w , and the inter-
chip stander deviation  is needed for the Ratio() calculation.
2) Inter-FHD Lower Bound Prediction: To predict inter-FHD,
we calculate the probability of which any pair of chips produce
different responses. The inter-FHD prediction given the width w of
the inspection bit is:

A1 () =

inter-FHD =

2Ratio()
(1 + Ratio())2

(7)

B. Prediction Model Validation
The inter-FHD is obtained from 7 FPGAs, and the intra-FHD
is calculated by measuring each PUF 10 times. To show inter-
chip variation and measurement noise of our experimental setup, we
measure the frequency of a single RO across the chips 10 times, and
the inter-');
INSERT INTO posts (postId,userId,title,body) VALUES (239,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','chip variation is 6.1% with 0.2% measurement noise.
To validate the intra-FHD prediction model, we follow the proce-
dure described in Section IV-A with t = 10 measurements. Figure 8
shows the results of the intra-FHD prediction of bit5 and bit10 . The
intra-FHD of bit5 is much higher than bit10 because its bin width is
much smaller.

Fig. 8. Strong UNBIAS PUF intra-FHD predictions of bit5 and bit10 of 7
FPGAs. bit5 has much larger intra-FHD because its bin width is smaller.

To validate the inter-FHD prediction model, for each challenge,
we obtain an inter-chip standard deviation  from 7 FPGAs, and
the nal  used in the prediction model is the median of the 
from 120 challenges, which gives  = 521. The results shown in
Figure 9 indicate that the inter-FHD lower bound prediction is well
matched with the measured data. To demonstrate that the inter-FHD
prediction model does not require an accurate inter-chip  estimation,
Figure 9 also shows the prediction range with   15% variation.
We can see that the differences of the predictions are limited, which
indicates that the  can either be obtained from pre-layout simulation
or measurements of a small number of chips. The prediction gap
is relatively large when w is much larger than  . However, as w
becomes comparable to  , where potential inspection bits begin to
occur, the prediction curve rises up quickly and matches the measured
data well. Figure 9 also shows that bit10 should be a proper inspection
bit because the intra-FHD is low and the inter-FHD is close to 45%

C. Uniqueness and Reliability Evaluation
The results of inter-FHD and intra-FHD with different inspection
bit selections are shown in Figure 9. As we can see from the gure,
using bits closer to the MSB gives low intra-FHD but also low
inter-FHD. This veries the fact that the delay paths are biased if
no physical implementation constraints are imposed. On the other
hand, using bits closer to the LSB gives 50% on both intra-FHD and
inter-FHD because of the measurement noise. As predicted, the best
inspection location appears at bit10 with 45.1% inter-FHD and 5.9%
intra-FHD. The results also indicate that the systematic variation is
mitigated because no constraints are imposed at all.
Table I shows comparison results with previous work. With con-
ventional Arbiter PUF (APUF) shown in the second column, the
results from [9] show that the circuit is essentially a constant number
generator with very little inter-FHD. The third column shows the 3-1
double Arbiter PUF with XORs [13], where symmetric layout is still
required, and the hardware overhead is 2X or 3X from the duplicated
circuits depending on the uniqueness requirement of the application.
The inter-FHD is close to 50% but the intra-FHD is high due to
the XORs. The fourth column shows the results from Path Delay
Line (PDL) PUF [6]. Symmetric PDL and delay characterization for
each CRP are required, which can cause scalability issues. Also the

Fig. 7. Worst Inter-FHD happens when the mean is at the middle of a bin.

the two areas are the same, resulting
With Ratio() = 1,
a predicted 50% inter-FHD. Given a selected bit i, plugging in
 = 0.5w to Equation 7 would give the predicted inter-FHD lower
bound.
the inter-chip standard
inter-FHD,
to predict
Please note that
deviation  is needed because the calculation involves the CDF.
However, the mean  does not affect the prediction because the
extreme value is obtained by nding the worst-case . Also, since
changing the inspection bit results at least a 2x change of w , the
inter-chip  does not have to be calcu');
INSERT INTO posts (postId,userId,title,body) VALUES (240,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','lated with high accuracy. It can
be obtained by pre-layout simulation or measuring a small number
of chips.

C. Inspection Bit Selection

Given the Error Correction Code (ECC) specication correspond-
ing to the PUF design, the intra-FHD threshold can be dened.
From the intra-FHD prediction model, choose a set of candidate bits
that would satisfy the intra-FHD threshold requirement. From the
candidate bits, a best inspection bit can be determined by applying
the inter-FHD prediction model given the standard deviation  of the
inter-chip delay distribution.
Please note that only one chip is needed for the inspection bit
selection since the measurement noise is similar for all chips from
our experiment and the  is obtained from pre-layout simulation. The
location of the nal inspection bit, which is a public information, is
passed to all PUFs for the secret response generation.

V. EX PER IM EN TA L R E SU LT S

A. Strong UNBIAS PUF Implementation

The strong UNBIAS PUF structure is implemented on 7 Altera
DE2-115 FPGA boards. In our implementation, no physical con-
straints, additional XORs, tunable delay units, or any systematic
variation compensation techniques are used. The design is purely
a RTL design.
The ROs inserted between path congurations are composed of
19 inverters, and the signal will be propagated to the next path
conguration when the RO counter associated to the RO reaches a
count of 50 thousand. The UNBIAS PUF has 10 path congurations,
therefore the length of the challenge is 10-bit long. The length of
the difference register is 19-bit, and the length of the nal response
for each challenge is one bit. For our experiment, 120 challenges are
applied, and 120 bits of responses are obtained for each PUF within
a second. Please note that the RO structure and the count of the RO
counter are selected given the 50 MHz system clock of the FPGA.
The results are similar as long as no overow occurs at the 19-bit
difference register.

determined efciently from the intra-FHD and inter-FHD prediction
models.
The strong UNBIAS PUF is implemented on 7 FPGAs without
imposing any physical layout constraints. Experimental results show
that the intra-FHD of the strong UNBIAS PUF is 5.9% and the inter-
FHD is 45.1%, and the prediction models are closely tted to the
measured data. The averaged intra-FHD of the strong UNBIAS PUF
at worst temperature and voltage variations is about 12%, which is
still within the margin of conventional ECC techniques. The fact
that the proposed scheme is immune to physical implementation bias
would allow the strong UNBIAS PUF to be designed and integrated
with minimum effort in a high-level description of the design, such
as during RTL design.

In Proc.

R E FER ENC E S
[1] S. U. Hussain, S. Yellapantula, M. Majzoobi, and F. Koushanfar. BIST-
PUF: Online, Hardware-based Evaluation of Physically Unclonable
Circuit Identiers. In Proc. ICCAD, 2014.
[2] Roel Maes and Ingrid Verbauwhede. Physically Unclonable Functions: A
Study on the State of the Art and Future Research Directions. In Towards
Hardware-Intrinsic Security, pages 337. Springer Berlin Heidelberg,
2010.
[3] Blaise Gassend et al. Silicon Physical Random Functi');
INSERT INTO posts (postId,userId,title,body) VALUES (241,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','ons.
CCSC, 2002.
[4] C. Herder et al. Physical Unclonable Functions and Applications: A
Tutorial. Proc. of the IEEE, pages 11261141, Aug 2014.
[5] Daihyun Lim et al. Extracting secret keys from integrated circuits. IEEE
Transactions on VLSI Systems, 2005.
[6] D. P. Sahoo, R. S. Chakraborty, and D. Mukhopadhyay. Towards Ideal
Arbiter PUF Design on Xilinx FPGA: A Practitioners Perspective. In
Euromicro Conference on DSD, 2015.
[7] Teng Xu and M. Potkonjak. Robust and exible FPGA-based digital
PUF. In International Conference on FPL, 2014.
[8] Durga Prasad Sahoo et al. Architectural Bias: a Novel Statistical Metric
to Evaluate Arbiter PUF Variants. Cryptology ePrint Archive, Report
2016/057, 2016.
[9] V. Gunreddy A. Maiti and P. Schaumont. A Systematic Method
to Evaluate and Compare the Performance of Physical Unclonable
Functions. In Embedded Systems Design with FPGAs, 2013.
[10] A. Maiti and P. Schaumont. Improving the Quality of a Physical Un-
clonable Function using Congurable Ring Oscillators. In International
Conference on FPL, 2009.
[11] Chongyan Gu and M. ONeill. Ultra-compact and Robust FPGA-based
PUF Identication Generator. In IEEE ISCAS, 2015.
[12] S. Morozov et al. An analysis of delay based PUF implementations
In Recongurable Computing: Architectures, Tools and
on FPGA.
Applications. Springer Berlin Heidelberg, 2010.
[13] Mitsugu Iwamoto Takanori Machida, Dai Yamamoto and Kazuo
Sakiyama. A New Arbiter PUF for Enhancing Unpredictability on
FPGA. In The Scientic World Journal, 2015.
[14] F. Kodtek and R. Lrencz. A Design of Ring Oscillator Based PUF on
FPGA. In IEEE International Symposium on DDECS, 2015.
[15] J. H. Anderson. A PUF design for secure FPGA-based embedded
systems. In Proc. ASP-DAC, 2010.
[16] Chi-En Yin and Gang Qu. Improving PUF Security With Regression-
based Distiller. In Proc. DAC, 2013.
[17] Wei-Che Wang, Yair Yona, Suhas Diggavi, and Puneet Gupta. LEDPUF:
Stability-Guaranteed Physical Unclonable. Functions through Locally
In IEEE International Symposium on HOST,
Enhanced Defectivity.
2013.
[18] Lerong Cheng et al. Physically Justiable Die-Level Modeling of Spatial
Variation in View of Systematic Across Wafer Variability. IEEE TCAD,
2011.
[19] Linus Feiten et al. Improving RO-PUF Quality on FPGAs by Incorpo-
rating Design-Dependent Frequency Biases. IEEE ETS, 2015.
[20] Qinglong Zhang et al. FROPUF: How to Extract More Entropy from
Two Ring Oscillators in FPGA-Based PUFs. IACR, 2015.
[21] L. Portmann and Teresa H.-Y. Meng. Metastability in CMOS library
elements in reduced supply and technology scaled applications. IEEE
JSSC, 1995.
[22] J. Guajardo, G.-J. Schrijen S. S. Kumar, and P. Tuyls. FPGA Intrinsic
PUFs and Their Use for IP Protections. In CHES, Sep 2007.
[23] M.-D. M. Yu and S. Devadas. Secure and Robust Error Correction for
Physical Unclonable Functions. In IEEE Des. Test, 2010.

Fig. 9.
Inter-, intra-FHD, and inter-FHD prediction using  = 521 with
different inspection bit selections of the strong UNBIAS PUF.

TABLE I
COM PAR I SON B ETW E EN PREV IOU S ARB I TER PUF S AND STRONG
UNB IAS PUF

inter-FHD
intra-FHD
Symm. Layout
Characterization

PDL
XOR
APUF
[13]
[6]
[9]
50.6% 45.25%
7.2%
4.1%
0.24% 11.8%
Yes
Yes
No
No
No
Yes

UNBIAS
PUF
45.1%
5.9%
No
No

ability of eliminating biased responses is limited because it depends
on the number of tuning stages inserted. The last column shows the
proposed strong UNBIAS PUF. Its behavior is unique and stable, and
most importantly no symmetric layout at all.

D. Temperature and Voltage Variations');
INSERT INTO posts (postId,userId,title,body) VALUES (242,1186,'Wei-Che Wang , Zhuoqi Li , Joseph Skudlarek , Mario Larouche , Michael Chen and Puneet Gupta Department of Electrical Engineerin','
For temperature and voltage variations, the reference responses
are measured at 20C with standard voltage 12V. The reference
responses are then compared with responses measured at 20C and
75C with 10% voltage variation. The results indicate the reliability
of the PUF when it is enrolled at normal condition but veried at a
high temperature environment with unstable voltage source.
Figure 10 shows the intra-FHD using bit10 as the inspection bit.
All intra-FHD at 20C with 10% voltage variation is below 8%, and
all intra-FHD at 75C with 10% voltage variation is below 14%,
which is still within conventional ECC margin with error reduction
techniques for PUFs [22, 23]. Compared with RO PUF presented in
[14], one possible explanation of smaller intra-FHD for our strong
UNBIAS PUF is that with multiple RO delay units, the overall delay
variation is canceled out, where for the RO PUF, the variation of
each RO is directly compared.

Fig. 10. Strong UNBIAS PUF intra-FHD under tempreature and voltage
variations.

V I . CONCLU S ION S
We proposed the rst strong UNBIAS PUF that can be imple-
mented purely by RTL without complex post-layout analysis or
hand-crafted physical design effort. The proposed measurement can
effectively mitigate the impact of biased delay paths and metastability
issues to extract local device randomness. The inspection bit can be

');
INSERT INTO posts (postId,userId,title,body) VALUES (243,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 ',' 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

 Regular Research Paper  NS 
Developing a FPGA  Supported Touchscreen Writing / 
Drawing System for Educational Environments Aslihan Tufekci 
Gazi University, 
Gazi Faculty of Education, 
Dept. of Computer Instruction Technologies, Turkey 
asli@gazi.edu.tr  Kamuran Samanci 
Turk Telekom, Turkey 
kamuransamanci@yahoo.com Utku Kose 
Usak University, 
Computer Sciences App. '||'&'||' Res. Center, Turkey 
utku.kose@usak.edu.tr   
Abstract Developments  in  information  and  communication  technologies  have  been  greatly  influential  on 
the practices  in all  fields, and education  is not an exception to this. To  illustrate with, computers 
were  first  used  in  computer    assisted  education  in  order  to  increase  the  efficiency  of  teaching 
process.  Recently,  computer  has  contributed  more  to  the  field  through  interactive  and  smart 
class  applications  that  are  specially  designed  for  classroom  use.  The  aim  of  this  study  is  to 
develop a  low  cost, portable and projection  supported touchscreen to be used  in educational 
environments  by  using  FPGA  technology  and  to  test  its  usability.  For  the  purposes  of  the  study, 
the above mentioned system was developed by using the necessary hardware and software, and 
later  it  was  tested  in  terms  of  usability.  This  usability  test  was  administered  to  teachers,  who 
were  the  target  end    users  of  this  touchscreen  writing  /  drawing  system.  The  aim  of  this  test 
was  to  determine  user    friendliness,  subservientness  and  usability  of  the  system.  Several 
tools  were  used  to  obtain  data  from  the  users  that  participated  in  the  study.  The  analysis  and 
evaluation of the data collected revealed that the system has achieved its objectives successfully. Keywords: FPGA, educational technologies, touchscreen writing / drawing system, usability  1. INTRODUCTION 

Education  has  always  been  a  field  of  study  making  use  of  technology  as  effectively  as  possible. 

Todays  popular  concept  educational  technologies  has  been  derived  from  the  process  of 

benefiting  from the advantages provided by technology as much as possible  in order to  increase 

the  effectiveness  and  efficiency  of  teaching  process  and  educational  studies.  In  other  words, 

60  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

educational  technologies  (also  called  teaching  technologies)  are  defined  as  the  studies  and 

ethical  applications  ensuring  facilitated  education  and  increased  performance  by  effectively 

using,  managing  and  developing  appropriate  technological  processes  and  resources  (Richey, 

2008).  

Technology  has  always  supplemented  educational  practices  in  several  ways.  It  is  clear  that 

extending and introducing these practices to larger audiences, presenting visual simulations and 

graphic  sup');
INSERT INTO posts (postId,userId,title,body) VALUES (244,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 2)','ports  for  students,  developing  various  audio    visual  educational  materials  and 

introducing  many  evaluation  systems  and  tools  result  in  more  efficient  and  easier  educational 

processes.  Similarly,  the  findings  of  the  studies  conducted  in  educational  technologies  are  to  a 

great  extent  reflected  in  real  classrooms  in  the  form of practical  applications. To  illustrate with, 

many  technological  tools  and  devices,  most  of  which  are  computer    supported,  are  used  for 

educational  processes  in  real  classroom  environments.  It  is  also  true  that  almost  all  the 

educational  materials  used  in  todays  classrooms  are  computer    controlled  or  computer   

operated.  Therefore;  thanks  to  computer  support,  classrooms  have  now  acquired  smart 

classroom  features.  Depending  on  the  developments  in  computer  technology,  the  users 

(students  or  teachers)  have  started  to  interact  with  the  devices  available  in  classrooms,  which 

have  inevitably  led  to  the  emergence  of  the  concept  interactive  classroom  as  the  next 

generation version of smart classrooms.  

In  interactive  classrooms,  there  is  an  ongoing  direct  interaction  between  the  users  and  the 

system,  and  this  process  requires  a  computer  enabling  this   interaction.  The  studies  conducted 

on  this  issue  show  that  almost  all  interactive  classroom  applications  require  the  use  of  at  least 

one computer. Thus, the dense use of computers has become inevitable in educational processes 

just  like  in  every  field  of  life.  Used  for  measurement  and  evaluation  purposes  at  the  beginning, 

computers  later  have  been  used  in  computer    assisted  teaching  intensively  as  well  (Dogan, 

1997).  The  most  recent  example  of  this  new  function  of  computers  is  computer    supported 

board, which  is  also  called  smart board. The  system  is  fully  controlled  by  a  computer,  and  the 

texts  and  visuals  are  projected  on  a  board  via  a  projector.  These  visual  elements  can  be  altered 

by using specially designed pens called smart magic pen. 

The  presence  of  hardware  does  not  suffice  for  computer    assisted  teaching  systems.  Some 

necessary  software  supporting  the hardware must  also be  installed  into  the  system. Developing 

such  software  requires  hardwork  and  time,  so  the  total  system  can  be  expensive  since  this 

hardwork  and  time  spent  are  taken  into  consideration  while  determining  the  price  of  the 

product. Moreover,  it  is  true  that  computers  are multi    purpose  devices  and  not  used  only  for 

educational purposes. Depending on the particular needs of the customers, various software and 

hardware    not  only  for  educational  purposes    can  be  installed  in  standard  computers  and 

61  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

marketed  by  computer  companies.    In  other  words,  the  consumers,  unfortunately,  have  to  pay 

for the software and hardware that will not be used in educational processes too.  Therefore; the 

use of computer in a system increases the overall cost considerably. In addition, since computers 

are  complex  devices,  it  is  necessary  to  employ  a  computer  technician  at  school  to  deal with  the 

maintenance and repair of the computers.  

Based on the explanations mentioned above, the cur');
INSERT INTO posts (postId,userId,title,body) VALUES (245,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 3)','rent study aims at the following: Developing 

simple,  low    cost  touchscreen  writing  /  drawing  system  for  educational  environments  which 

does not require the use of a computer and can be used as portable or stationary; and testing its 

usability  on  teachers,  who  are  the  real  target  users  of  this  touchscreen  writing  /  drawing 

system. In addition, the followings are targeted as secondary objectives: 

  To  assist  teachers, who  are  the  real  target  end    users,  in  teaching more  easily  and  give 

them  an  alternative  to  continue  their  in    class  teaching  in  case  of  illnesses  and  even 

disability  since  this  touchscreen  enables  them  to  teach  while  sitting  without  needing 

much physical activity or effort.  

  To  ensure more  visibility  of  the  board    considering  the  fact  that  the  users might  cast  a 

shadow  on  some  parts  of  the  board  in  the  systems  like  smart  board  depending  on  the 

light coming from the projector. 

Touchscreen writing  / drawing  system, which  is  likely  to meet  the  objectives mentioned  above, 

is  expected  to  be  an  alternative  tool  in  educational  processes  and  for  the  systems  requiring 

computers with expensive software and hardware. 

2. BACKGROUND 

Developments  and  improvements  in  the  context  of  especially  education  field  have  been  given 

rise  to  appearment  of  different  kinds  of  educational  technologies.  As  a  result  of  appearment  of 

the related  technologies, supportive educational materials  for ensuring more quality  in  teaching 

 learning processes has also been designed and developed in many scientific research works. At 

this  point,  improvements  especially  within  computer  and  electronic  technologies  has  an 

important role on rapid developments in the related scope. In this sense; computers, computer  

based  systems  and  other  technologies  from  general  perspective  (especially  information  and 

communication technologies) have been widely used  in educational studies  in performing many 

developments  and  improvements  (Deperlioglu  '||'&'||'  Kose,  2013;  Kose,  2010;  McCormack  '||'&'||'  Jones, 

1997). 

62  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

When  the  current  conditions  and  situations  have  been  examined,  it  can  be  seen  that  usage  of 

computer   based  and  electronic devices  in  teaching    learning processes  is widely preferred  in 

order  to  improve  quality  and  effectiveness  of  the  related  educational  approaches.  In  this  sense, 

when  this  issue  is  evaluated  from  students  perspective,  some  research  works  also  report  that 

there  is  a  large  range  of  capacities  for  especially  information  and  communication  technologies 

among  students  in  higher  education  (Lee,  2003;  Palaigeorgiou  et  al.,  2005;  Van  Braak,  2004). 

Related  to  the  discussed  issue,  such  reports  also  point  the  situation  of  remarkable  usage 

popularity of computer and electronic technologies among educational studies.  

As  being  parallel  with  the  research  subject  of  this  work,  usage  of  computer  and  electronic 

technologies  is  an  important  factor  that  must  be  taken  into  consideration  to  discuss  about  the 

background.  As  it  can  be  understood,  such  devices  based  on  computer  and   electronic 

technologies play active roles on improving teaching  learning processes and ensuring effective 

and  efficient  ways  for  achieving  the  educational  objectives.  Related  to  this  issue,  it  has  been 

argued  within  some  works  that ');
INSERT INTO posts (postId,userId,title,body) VALUES (246,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 4)',' the  use  of  such  devices  (for  example,  laptops,  mobile  devices) 

during  teaching    learning  processes  improves  students  grasp  and  comprehension  of  the 

subject  by  providing  flexible  interaction  and  using  experiences  (Alsaggaf  et  al.,  2012;  Dexter  et 

al.,  1999;  Nilson  '||'&'||'  Weaver,  2005;  Tufekci  et  al.,  2013).  On  the  other  hand,  it  has  also  been 

reported  that  students  think  very  positive  about  using  such  materials  although  some 

researchers also think that the related devices may affect the  learning process  in a negative way 

(Akbaba-Altun, 2006; Barak et al., 2006; Hembrooke '||'&'||' Gay, 2003; Ni '||'&'||' Branch, 2004). 

As  it  was  also  mentioned  before  in  the  Introduction  section  of  this  article,  more  advanced 

computer  assisted materials  like smart board or interactive tablet have also been designed 

and  developed  as  a  result  of  rapid  developments  in  the  mentioned  technologies  combined  in  a 

common  scope  for  improving  educational  processes.  Nowadays,  different  kinds  of  educational 

technology  based materials  like smart boards, touchscreen devices,  interactive tablets, camera 

  based  viewing  devicesetc.  are  widely  used  for  teaching    learning  approaches.  Within  the 

current  modern  life,  it  can  be  seen  that  the  touchscreen    based  system  are  used  usually  as 

kiosks  in  order  to  improve  standards  in  activities  in  airports,  train  stations,  grocery  stores, 

banks  and  any  other workplaces  such  as  food  service,  retail  and  health  care  fields  (Astell  et  al., 

2010;  Chourasia  et  al.,  2013;  Newman  et  al.,  2012;  Schultz  et  al.,  1998;  Shervin  et  al.,  2011; 

Wilson  et  al.,  1995).  As  being  parallel  with  the  expressed  points  and  because  of  the  main 

objective  of  this  work  (designing  and  developing  touchscreen  writing  /  drawing  system)  and 

other  following  objectives;  previously  performed  research  studies    works  within  the  subject 

must also be examined in order to enable readers to have more idea about the work.  

63  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

In  the sense of  the current  literature,  it can be seen  that  there  is not  too many studies regarding 

to design and develop of a touchscreen system for directly educational purposes. Because of this, 

some  recent  studies    works  employing  design,  development  and  usage  of  touchscreen 

technologies  are  also  discussed  as  follows  in  order  to  give  ideas  about  touchscreen  based 

applications: 

In their works, Raj et al. (2013) provides a research work based on design, development 

and  implementation  of  a  touchscreen  health  information  kiosk.  By  using  this  system, 

patients  at  St.  Johns  Medical  College  Hospital    Bangalore  have  ability  to  receive 

information  by  using  kiosk  systems  provided  in  the  context  of  specific  areas.  From  the 

view of receiving  information and  functions related  to especially feedback approaches, 

this  system  can  also  be  evaluated  as  some  kind  of  learning ');
INSERT INTO posts (postId,userId,title,body) VALUES (247,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 5)','   based  scientific  study 

providing many benefits in the context of using touchscreen technologies. 

  Caviglia-Harris  et  al.  (2012)  has  performed  a  study  on  using  computer    assisted  data 

collection approach for providing more accurate and effective way for performing survey 

 based works.  In  this study,  touchscreen  laptop systems have been used  for performing 

surveys  and  collecting  item  responses  in  this  way.  The  study  has  shown  that  using  the 

related devices rather than using the traditional method (paper and pencil interviewing) 

provide  a  effective  way  on  data  collecting  by  reducing  mistakes  and  ensuring  many 

advantages in the sense of time and place. 

In  his  Master  Thesis  work,  Taylor  (2012)  proposes  a  near  touch  user  interface  for 

touchscreen    based  systems.  This  system  has  been designed  and  developed  in   order  to 

overcome  interaction  limitations  of  traditional  touchscreen  applications.  In  this  sense, 

Taylor  uses  some  supportive  devices  to  form  the  proposed  touchscreen  system   

approach. 

In the context of medical applications, Aguirre et al. (2012) provides a system developed 

as  a  touchscreen    based,  low  cost  electrocardiograph.  The  results  obtained  with  the 

work  show  that  the  system  also  provides  a well  performance  within  applications  of  the 

electrocardiogram. 

  Dixon  and  Prior  (2012)  have  provided  a  research  work  on  a  typical  touchscreen 

electronic  queuing  system.  In  this  scope,  they  have  provided  a  work  on  design  and 

initial usability  testing of  the anonymous electronic waiting room system  for  improving 

patient  satisfaction.  The  obtained  results  show  that  the  system  has  positive  affects  on 

patients and provided successful rates on other related factors like interaction. 

64  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

  Related to usage of touchscreen  based systems, Chourasia et al. (2013) have provided a 

work  on  evaluating  effects  of  sitting  and  standing  on  performance  and  touch 

characteristics  during  a  digit  entry  touchscreen  task  in  individuals  with  and  without 

motor    control  disabilities.  The  work  provides  a  good  evaluation  approach  for 

evaluating different  factors  (for  example  button  sizes  of  a  touchscreen)  in  order  discuss 

about  the related effects and generally  the results points  that environmental conditions 

should  also  be  considered  to  improve  accessibility  and  usability  of  touchscreen.  As 

being  similar  to  this  work,  some  previous  works  on  effect  of  touch  screen  interface 

design  on  performance  have  also  been  provided  (Colle  '||'&'||'  Hiszem,  2004;  Jin   et  al.  2007; 

Martin, 1988; Sesto et al. 2012) 

In  their work,  Rahman  et  al.  (2012)  provides  a  touchscreen    based  automation  system 

to  control  electronic  devices  within  a  home.  In  the  context  of  the  proposed  system, 

electronic  components  that  can  be  bought  from  any  available  local  market  have  been 

used in order to form a low cost device. 

  Another  remarkable work  regarding  to  development  of  touchscreen  approach  has  been 

provided  by  Bi  et  al.  (2012).  In  this  work,  a multilingual  touchscreen    based  keyboard 

application has been designed and developed by the authors.  In this sense,  the keyboard 

button  layout  has  been  optimized  according  to  usage  of  the  keyboard  for  different 

languages like French, Spanish, German, and Chinese. 

  Regarding  to  optimization  of  touchscreen  systems  and  applications,  ano');
INSERT INTO posts (postId,userId,title,body) VALUES (248,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 6)','ther  work  has 

been performed by Bradley (2012), in order to determine factors affecting the adoption 

of  touchscreen  smartphones  among  individiuals  with  vision  loss.  According  to  the 

obtained  results  within  this  Master  Thesis  work,  more  user    friendly,  assistive 

technologies  can  be  performed  on mobile  devices,  in  order  to  fit  them  to  be  used  better 

by especially individiuals with vision loss. 

After  the  taking  a  brief  look  at  to  the  background  and  the  literature  status,  the  touchscreen 

writing  /  drawing  system,  which  was  provided  within  this  work  must  be  examined  in  the 

context  of  design  process.  In  this way,  the  low  cost  aspects  of  the  system  and  its  using  features 

and functions can also be understood better. 

3. DESIGN PROCESS OF TOUCHSCREEN WRITING / DRAWING SYSTEM   

As  it  was  also  mentioned  before,  a  low  cost  projection    supported  touchscreen  writing  / 

drawing  system, which  does  not  require  a  computer  and  can  be  used  as  portable  or  stationary, 

65  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

was  developed  in  the  current  study  by  using  the  advantages  provided  by  FPGA  technology.  In 

this sense, Figure 1 represents some  fotographs  taken  from  the designed and developed  system 

(as prototype). 

     

    Figure 1. Some fotographs taken from the designed and developed system (as prototype). 

Regarding  to  the  touchscreen writing / drawing  system,  it  is necessary  to define  the design and 

development  process  of  touchscreen  writing  /  drawing  system  in  terms  of  software  and 

hardware  in  order  to  learn  more  about  the  study  and  have  a  clearer  idea  of  its  importance.  By 

doing  so,  the  features  and  functions  of  the  system  are  also  explained  in  detail.  In  order  not  to 

confuse the readers, technical details were omitted from the explanations.   

3.1. Hardware Design of Touchscreen Writing / Drawing System  

The working mechanism of the system hardware is reprensented in the diagram under Figure 2. 

This figure also presents the preview of the system software. 

66  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Figure 2. System structure of the touchscreen writing / drawing system.  

 

As  seen  in  Figure  3.2,  X  and  Y  coordinates  of  the  writings  or  drawings  made  on  the  panel  with 

the help of a special pen and a  touch   operated panel  located on  touchscreen GLCD panel (LTM 

  LCD  Touch  Module)  are  converted  into  digital  values  through  an  ADC   (Analog  to  Digital 

Converter)  chip.  These  digitally  converted  X  and  Y  coordinate  values  are  transferred  to  FPGA 

content  through ADC serial port  interface (SPI) control module which  is  located on FPGA chip.  

These  transferred  values  are  stored  in  an  internal  register  RAM  (Random  Access  Memory)  via 

SDRAM  or  Register  control  module  located  on  FPGA  chip.  Internal  register  is  used  in  this 

current  system.  Coordinate  information  stored  in  the  register  again  via  the  same  module,  are 

synchronously  sent  to  GLCD  control  module,  7    segment  disp');
INSERT INTO posts (postId,userId,title,body) VALUES (249,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 7)','lay  module  and  VGA  control 

module.  GLCD  control  module  is  integrated  into  FPGA  chip  in  order  to  control  touchscreen 

GLCD  panel.  This  module  processes  the  coordinate  information  receieved  from  SDRAM  or 

Register  Control  Module  and  later  enables  this  processed  information  to  be  converted  into 

images.    7    segment  display  control module  is  used  to  control  six  7    segment  diplays. This 

module  seperates  X  and  Y  coordinate  values  as  3    segment  displays  that  are  recieved  from 

SDRAM  or  Register  Control  Module  and  realizes  the  visual  presentation.  On  the  other  hand, 

VGA Control Module is used to transfer the images to a computer screen (or a projector device) 

67  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

to be  connected  to main board with  the help  of  a VGA  (Video Graphics Array)  chip.  In  short,  the 

function of VGA control module is to process the coordinate information receieved from SDRAM 

or Register Control Module and to transfer the processes information to computer screen.   

 Touchscreen  Writing  /  Drawing  System  is  composed  of  two  hardware  units,  one  of  which  is 

LCD  touchscreen  panel  module  (also  called  LTM)  and  the  other  is  control  module  formed 

with FPGA chip.  

3.1.1. LCD touchscreen panel module 

As  seen  in  Figure  3,  LCD  Touch  Panel  Module  (LTM)  includes  a  4.3  inch  graphic  LCD  with 

800X400 pixels and 24  bit color resolution and also 4.3 inch resistive touchscreen.  

Figure 3 LTM module and the cable used for connection to main board. 

 

This module  is mainly  used  for  system  control  and  image  transfer  function.  However,  it  can  be 

used  for  various  applications  as  well  with  its  high  resolution  screen  and  touch  panel  support. 

The  aim  and  the  function  of  LTM  module  in  this  touchscreen  writing  /  drawing  system  are  as 

follows:  Firstly,  the  things written  or  drawn  by  a  special  pen  on  the  active  area  of  the module 

are  converted  in  to  12    bit  X  and  Y  coordinate  values  via  an  ADC  chip  to  which  touch  panel  is 

connected. Figure 4 shows the active area of the module and 12  bit X and Y coordinates. 

68  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Figure 4. The display of coordinates X and Y on LTM module as 12  byte groups. 

 

The connection between LTM panel and the main system is made by 40   pin socket connection. 

The  connection  of  digitally  converted  X  and  Y  coordinate  values  to  the  main  board,  which  also 

includes the FPGA chip, is also made via this socket (Figure 5).  

Secondly,  the  data  converted  into  meaningful  formats  in  FPGA  chip  is  tran');
INSERT INTO posts (postId,userId,title,body) VALUES (250,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 8)','sferred  back  to  the 

GLCD  screen  found  on  that  module.  By  doing  so,  the  visual  transfer  of  the  drawings  made  on 

touch panel is realized (Figure 5).  

Figure 5. Data flow diagram on LTM panel. 

 

3.1.2. Control module 

The  control  module  deals  with  the  following:  FPGA  chip,  which  receives  X  and  Y  coordinates 

from  GLCD  touch  panel  and  later  enables  the  conversion  of  these  processed  vaules  into  images 

69  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

on  the  LCD  display  of  GLCD  touch  panel  as  well  as  on  a  computer  screen  or  projection  device 

simultaneously;  and  hardware  connections  of  other  chips  which  are  involved  in  the  process  as 

supporting  components.  Figure  6  shows  FPGA  chip    which  is  the  main  component  of 

touchscreen system main board  and other supplementary chips, sockets and connections. 

Figure 6. Connections between the chip, sockets and other components on the system main 

 

board. 

The FPGA chip used in touchscreen writing / drawing system developed for the purposes of this 

current  study  is  called  EP2C70896C6N  chip,  which  is  the  most  advanced  chip  of  Cyclone  II 

family produced by Altera company.  

It is produced by using 90 nm technology. 

It includes 68,416 logical elements (LEs). 

  Mbit embedded RAM. 

  Performance upto 260 MHz. 

  Compatible with high  speed mobile external disks such as DDR, DDR2 and SDR SDRAM. 

  Multi  volt multiple input/output voltage support (1.5V, 1.8V, 2.5V, 3.3V). 

  Configuration in less than 100ms thanks to quick  configuration opition. 

  16 sensitive clock input pins. 

  A total of 150 18 x 18 multiplexer. 

It supports configurations in Active serial, Passive serial and JTAG mode.  

70  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

  896  pin BGA socket structured. 

 EP2C70896C6N  chip  is  found  to be  the  ideal one when  the  features mentioned above  and  the 

purposes  of  the  study  are  considered.  At  this  point,  learning  about  software  as  well  hardware 

design  of  the  touchscreen  writing  /  drawing  system  is  also  important  in  terms  of  being 

familiarized with its features and the operation process.  

3.2. Software Design of Touchscreen Writing / Drawing System   

Software  design  of  touchscreen writing  /  drawing  system mainly  depends  on  the  programming 

of  FPGA  chip.  It  is  clear  that  effective  and  efficient  operation  of  the  system  according  to  the 

predetermined  purposes  will  be  possible  only  if  the  chip  is  programmed   appropriately,  that  is 

software infrastructure should be designed and integrated into the chip. This process also refers 

to the configuration of FPGA chip.   

As  for  the  programming  of  FPGA  chip,  Verilog  software  programming  language  was  used.  In 

addition,  Quartus  II  software,  produced  by  Altera  Company,  was  used  for  the  same  purpose.  In 

terms of software, FPGA chip is compos');
INSERT INTO posts (postId,userId,title,body) VALUES (251,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 9)','ed of a total of 7 software modules (Figure 7).   

Figure 7. Software modules on FPGA chip. 

 

The basic features and functions of software modules can be explained briefly as follows:  

3.2.1. Main control module 

Main  control  module  refers  to  the  main  parts  of  software  system  on  FPGA  chip.  It  covers  all 

software modules  in  the  chip  and  enables  the  connections  among  the modules.  In  addition,  it  is 

71  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

responsible  for  the  connections  of  the modules with  external  world  and  the  necessary  controls 

accordingly.  

3.2.2. Delay control module  

Delay  control  module  enables  the modules  on  the  main  module  to  be  processed  after  a  certain 

delay  time.  The  aim  of  this  function  is  to  ensure  harmonius  operation  between  software  and 

hardware processes and to avoid a potential system error accordingly.  

3.2.3. ADC serial port interface control module 

ADC serial port  interface control module  is  responsible  for  the  software controls  related  to ADC 

serial  port  interface  communication  during  the  system  operation  process with  regards  to  FPGA 

chip.  

3.2.4. Graphic LCD control module 

GLCD  control  module  is  the  software  module  enabling  data  transfer  between  LCG  driver  chip 

and FPGA chip by establishing a protocol necessary for this transfer. This module consists of two 

sub   modules. One  of  them, which  is  GLCD  SPI  Control  Sub   module, was  formed  to  create  a 

common language between LCD driver chip and FPGA by forming the necessary protocol for the 

communication between  these  two chips. The other sub   module  GLCD Timing sub  module, 

is  a  software  module  that  determines  the  quality,  size,  resolution  and  speed  of  the  data  to  be 

sent to GLCD.  

3.2.5. VGA control module 

VGA  control module  is  a  software module  enabling  VGA  signal  synchronization  on  both  vertical 

and  horizontal  coordinates  in  order  to  obtain  the  desired  image  levels  in  the  devices  such  as 

projector or monitor that are connected to the system via VGA.  

3.2.6. Seven  segment control module 

This module  is  the  software module  that  realizes  the  processes  necessary  to  carry  out  accurate 

72  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

transfer of related data and to display it on a seven  segment display group. Here the aim is to 

convert the coordinate data of the points touched on touch panel on X and Y coordinates. 

3.2.7. SDRAM / Flash / EEPROM control module 

Being  the  last  software  module  in  FPGA,  SDRAM  /  Flash  /  EEPROM  control  module  is 

responsible  for  the  communication between memory units  and  the  system depending  on which 

touchscreen writing / drawing system is used.  

Touchscreen  writing  /  drawing  system,  whose  software  and  hardware  design  has  been 

explained  without  getting  into  technical  details,  was  planned  with  an  approach  to  enable  the 

users  to  complete  the  actions  effectively  based  on  the  purposes  of  the  study.  At  this  point,  it  is 

necessary  to make  an  evaluation  to  determine  the  effectiveness  of  the  system  and  its  adequacy 

to  meet  the  purposes  of  the  study.    To  achieve  this  aim,  the  following  evaluation  process  was 

planned. 

4. EVALUATION PROCESS 

The  evaluation  process  of  touchscreen ');
INSERT INTO posts (postId,userId,title,body) VALUES (252,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 10)',' writing  /  drawing  system  mentioned  in  this  study 

included  the  following  phases;  observing  usability  processes  and  the  analysis  of  the  obtained 

opinion  data.  Accordingly,  the  details  related  to  evaluation  process  will  be  presented  in  the 

following sub  titles and paragraphs.  

4.1. Evaluation Method 

In order to evaluate touchscreen writing / drawing system, an approach focusing on the usability 

of  the  system was  preferred.  In  order  to  achieve  this  purpose,  Heuristic  Evaluation Method was 

used.  This method  is  generally  applied  for  the  usability  evaluation  of  the  systems  that  are  based 

on learning through trial  error (Nielsen '||'&'||' Molich, 1990).  

In  this  study    specific  touchscreen  writing  /  drawing  system,  there  are  not  any  written 

documents  except  the  names  of  the  buttons  on  the  device  activating  the  related  functions.  In 

other  words,  the  information  about  the  system  usage  is  limited  to  only  this  text    based 

information. According to Nielsen, a total of 5 subjects are sufficient to test such systems since 75 

% of the problems can be determined by that number of participants (Nielsen '||'&'||' Landauer, 1993). 

Similarly, in the related studies conducted in the field, the number of the problems identified was 

73  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

not  found  to  increase  as  the  number  of  the  participants  increased.  The  graph  in  Figure  8 

represents the problem  identification data according to the number of participants  in  the studies 

conducted by Nielsen and Landauer between 1990 and 1993 (Nielsen '||'&'||'   Landauer,  1993).  

Figure 8. Problem identification rate according to the number of the subjects (Nielsen,  '||'&'||'  

Landauer,  1993). 

However; the study conducted by Faulkner (2003) shows that five participants might not suffice. 

In  the  study,  it  was  found  that  the  problems  identified  by  the  sub  groups  (five  participants) 

formed  out  of  a  larger  group  of  60  participants  corresponds  to  85%  of  the  problems  identified 

by  the whole  group. As  the  test  simulations  applied  increased,  the  usability  problems  identified 

by  groups  of  five  were  found  to  have  a  very  wide  range  of  match  or  mismatch  (between  55 % 

and  100 %)  with  the  problems  identified  by  the  whole  group.  This  wide  deviation  implies  that 

the potential problems are very  likely  to be missed when groups of  five participants are used  in 

similar usability  studies. However, using  groups of  ten participants  increases  this percentage  to 

the  average  95  %,  and  82  %  being  the  lowest  value.  Table  1  shows  the  changes  in  standard 

deviation in brief. 

 

 

Table 1. The changes in standard deviation with regards to the related variables. 

User Number 

Determined Minimum 
% 

Determined Mean % 

Std. 
');
INSERT INTO posts (postId,userId,title,body) VALUES (253,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 11)','Deviation 

5 

10 

15 

20 

30 

40 

50 

55 

82 

90 

95 

97 

98 

98 

85,55 

94,69 

97,05 

98,40 

99,00 

99,60 

100,00 

9,30 

3,22 

2,12 

1,61 

1,13 

0,81 

0,00 

74  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

When  above  mentioned  data  is  concerned,  a  group  of  10  users  as  the  participants  of  the  study 

can be considered to be better in identifying the usability problems.   

4.2. The Population and Sample for Evaluation 

The  population  of  the  current  study  involves  teachers  working  in  primary  schools  regardless  of 

their  fields  of  teaching.  On  the  other  hand,  the  sample  of  this  study  is  randomly  selected  20 

teachers from various fields of teaching who taught during 2011  2012 academic year.  

4.3. Evaluation Material 

Due  to  the  limited  number  of  similar  studies  in  the  related  literature,  the  authors  developed  a 

survey  in  order  to  test  the  usability  of  this  touchscreen  writing/drawing  system.  After  several 

prior  analyses,  a  35    item  list  was  prepared  by  the  authors.  Later,  these  items  were  reviewed 

and  edited  by  an  expert  and  a  15    item  evaluation  survey  form  was  finalized,  which  aims  at 

determining  subservientness,  user    friendliness  and  usability  factors  regarding  the 

system. The distribution of these 15 items according to the factors is as follows: first five 5 items 

for  subservientness,  the  next  5  for  user    friendliness  and  the  last  5  for  usability.  The 

participants of the study are asked to choose one of three options for each item; namely I agree 

(Yes), I partly agree (Partly) and I dont agree (No). 

4.4. Other Approaches used in the Evaluation 

Prior to the survey used as part of  the evaluation approach, the participants were asked  to  fill out 

an  information  form  in  order  to  learn  about  their  familiarity  with  the  study  topic.  The  data 

obtained  from  this  phase  revealed  that  the  participants  had  enough  knowledge  about  the  topic; 

and  therefore,  the  replies  to  the  survey  items  were  considered  to  be  quality  enough  for  the 

purposes  of  the  study.  The  details  of  this  form will  not  be  provided  here  so  as  not  to  confuse  the 

readers and to focus on the survey study more. 

Another  approach  used  for  the  evaluation  plan  is  the  observation  of  usability  processes, which  is 

based  on  completing  the  tasks  given  regarding  the  use  of  touchscreen  writing  /  drawing  system 

designed and developed for the purposes of this study. Although the survey study seemed to have 

the  primary  importance  with  regards  to  evaluation,  this  phase  was  also  crucial  since  it  provided 

valuable  data  about  the  use  of  the  system  before  the  participants  filled  out  the  survey.  The  next 

section presents the data obtained regarding the approach mentioned above. 

75  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

 

5. FINDINGS OF THE EVALUATION PROCESS 

As  mentioned  in  the  previous  section,  the  process  was  observed  and  the  participants  were 

administered  the  survey  developed  by  the  researchers,  and  later  the  necessary  data  was 

collected  for  the  evaluation  purposes  of  touchscreen  writing  /  drawing  system  designed  and 

developed  for  the purposes of  the current study. Therefore;  the data obtained will be presented 

in  the  next  sections  ');
INSERT INTO posts (postId,userId,title,body) VALUES (254,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 12)','in  the  order  followed  while  collecting  the  data;  namely  the  data  regarding 

the observations and the data obtained from the survey study respectively.  

5.1. Findings Obtained from Usability Process 

The  usability  process  deals  with  the  time  the  participants  of  the  study  spent  while  completing 

the  tasks  given  as  well  as  the  problems  they  encountered  during  this  completion  process.  The 

participants were  not  provided  any  information  about  the  use  of  touchscreen writing  / drawing 

system prior to the process. They were only given information about the objectives of the system 

and brief introduction to its working principles.    

As  part  of  the  process,  the  participants  were  given  a  list  including  a  total  of  8  tasks.  The  time 

they spent while working on the tasks was recorded by an observer as seconds. In addition, each 

participant was asked to decide on the difficulty level of each task by  marking a five point Likert 

scale available below each task in the list.   

The information and the findings obtained from this process are presented below:  

As  shown  in Table 2,  the  participants  spent  an  average  of  19  seconds  for  each  task  and  the  task 

list,  which  includes  8  tasks,  was  completed  in  an  average  of  3  minutes  by  per  participant.  The 

task that was completed  in the shortest time (15.1 second) was clear the screen and shut down 

the  sytem  (Task  8), while  the  longest  time  spent was  23,60  seconds  for  the  task which writes: 

connect  touch panel  to  the  screen  and  switch  it  on  in  adaptor mode  (Task  1). The  reason why 

Task 1 had the longest completion time might be explained by the fact that the participants used 

such  a  system  for  the  first  time.  In  other  words,  it  might  have  been  due  to  effect  of  first  time 

experience and momentary panic and nervousness. 

 

 

76  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Table 2. Usability process  task completion times (P1, P2.. P20= participants). 

Task Completion Times (sec.)  

Task 

P 
P 
P 
P 
P 
P 
P 
P 
P 
P 
P 
P1 P2 P3 P4 P5 P6 P7 P8 P9 
19 
20 
18 
17 
16 
15 
14 
13 
12 
11 
10 

Mean 

1. Connect touch panel to 
the screen and switch it 
on in Adaptor mode. 

2. After touch screen is 
displayed, shut it down 
and restart in Battery 
mode. 

3. Do some drawings on 
the screen when draw 
mode is on. 

4. Switch to Erase 
mode and erase what 
you have drawn. 

23 24 23 41 13 27 34 16 14 10 19 22 27 25 30 23 18 23 31 29 

23,6
0 

27 34 35 38 19 23 27 21 21 19 21 18 16 21 17 20 13 18 15 16 21,95 

10 19 15 25 17 12 15 18  7  12 17 26 21 14 23 16 19 17 20 18 17,05 

15 21 19 23 17 18 16  9  12 10 14 19 23 32 28 17 16 26 18 22 18,75 

5. Do drawings on the 
screen when Draw Bold 
18 18 26 32 23 16 19 16 15 13 16 21 18 17 22 19 15 22 15 17 18,90 
mode is on. 

6. switch to Erase Bold 
mode and erase what 
you have drawn. 

7. Change the font color 
from Red to Blue. 

8. Clear the screen and 
shut down the system. 

Total: 

Ov');
INSERT INTO posts (postId,userId,title,body) VALUES (255,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 13)','erall Mean: 

11 27 24 15 19 16 13 11 11 11 20 18 15 21 17 22 19 27 23 26 18,30 

17 15 20 21 21 17 14  8  14 10 23 21 19 25 16 18 22 14 17 15 17,35 

15 14 12 19 14 12  9  7  13 12 19 20 22 15 17 13 18 19 18 14 

15,1
0 

13
6 

17
2 

17
4 

21
4 

14
3 

14
1 

14
7 

10
6 

10
7 

14
97 
9 

16
5 

16
1 

17
0 

17
0 

14
8 

14
0 

16
6 

15
7 

15
7 

18,87 

 

The  Figure  9  represents  the  graph  of  the  average  times  spent  on  task  completion  as  part  of 

evaluation process. 

77  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Figure 9. Usability process  average task completion times. 

 

Table  3  shows  the  data  regarding  the  difficulty  level  of  the  tasks  as  stated  by  the  participants 

during the process. When the table is examined, it is seen that the  mean value for all tasks is 3,75 

(easy). The most difficult task was found to be Task 2 After touch panel is switched on, switch it 

off and restart in battery mode with an average of 3,15 while the easiest task was Task 4 with a 

balue of 4,45 which writes clear all drawings from touch panel by using the clear mode. 

 

Table 3. Usability process  difficulty levels of the tasks (P1, P2.. P20= participants).  

Task 

Difficulty levels of the tasks  
(1:very difficult; 2:difficult; 3:almost easy; 4:easy, 5:very 
easy) 

Mean 

P 
P 
P 
P 
P 
P 
P 
P 
P 
P 
P 
P1 P2 P3 P4 P5 P6 P7 P8 P9 
19 
20 
18 
17 
16 
15 
14 
13 
12 
11 
10 

1. Connect touch panel to 
the screen and switch it on 
in Adaptor mode. 

2. After touch screen is 
displayed, shut it down and 
restart in Battery mode. 

3. Do some drawings on 
the screen when Draw 
mode is on. 

4. Switch to Erase mode 
and erase what you have 
drawn  

3  4  3  4  3  4  2  3  2  4  4  3  4  3  4  2  3  2  4  4  3,25 

2  3  3  4  3  4  4  3  3  4  2  3  4  3  2  4  3  2  4  3  3,15 

5  4  5  4  4  4  4  3  3  5  4  5  4  4  4  4  3  3  5  4  4,05 

5  5  5  4  5  4  5  4  3  5  5  5  4  5  4  5  4  3  5  4  4,45 

78  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

5. Do drawings on the 
screen when Draw Bold 
mode is on. 

6. Switch to Erase Bold 
mode and erase what you 
have drawn. 

7. Change the font color 
from Red to Blue. 

8. Clear the screen and shut 
down the system. 

3  3  4  3  4  3  4  4  4  5  3  4  3  4  3  4  4  4  5  3  3,70 

3  4  5  4  4  4  5  4  5  5  4  5  4  4  4  5  4  5  5  3  4,30 

4  4  4  4  4  3  4  3  4  5  4  3  4  4  3  4  3  4  5  3  3,80 

3  4  3  4  3  3  4  4  3  3  4  3  4  3  3  4  4  3  3  4  3,45 

Total: 

28 31 32 31 30 29 32 28 27 36 30 31 31 30 27 32 28 26 36 28 

Overall Mean: 

3,76 

 

For  all  the  tasks  in  the  process,  the  difficulty  levels  stated  by  the  participants  are  showed  in 

Figure 10, which shows that Task 4 was the easiest one and Task 2 the most difficult. 

Figure 10. Usability process  difficulty levels of the tasks (1:very difficult; 2: difficult; 3:almost 

easy; 4: easy, 5: very easy). 

5.2 Usability Findings Obtained from the Survey Study  

As mentioned  earlier,  a  total  of  20  participants were  administered  a  15    item  survey  after  the 

usability process, which aims at evaluating the  touchscreen writing / drawing system in terms of 

subvertiness,  user    friendliness  and  usability.  In  this  regard,  basic  information  about  the 

approach and the findings obtained are presented as follows:  

79  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci,');
INSERT INTO posts (postId,userId,title,body) VALUES (256,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 14)',' A., Samanci, K. '||'&'||' Kose, U. 

As mentioned earlier, the reply options for the items in the survey were Yes, Partly and No. 

The  answers  given  to  the  questions  in  the  survey  were  analyzed  through  frequency  (f), 

percentage (%), and standard deviation calculations. The  items and  the replies    feedback given 

to these items are presented in the tables in the following pharagraphs.  

 

Table 4. The analysis of the replies provided for the statements determining the 

subservientness level of the touchscreen writing / drawing system. 

Statements 
Do you think that 

you can teach your lessons more 
easily and quickly by using the 
touchscreen writing/drawing 
system?  

your current teaching 
performance might increase by 
using the touchscreen 
writing/drawing system? 

your teaching efficiency might 
increase by using the touchscreen 
writing/drawing system?  

you will be more helpful to your 
students by using the touchscreen 
writing/drawing system? 

the touchscreen writing/drawing 
systemis a useful tool for your 
lessons? 

No 
(1) 

Partly 
(2) 

Yes 
(3) 

Total 

f  % 

f  % 

f  % 

f  % 

Mean 
2  10  7  35  11  55  20  100 

2,45 

2  10  5  25  13  65  20  100 

2,55 

3  15  5  25  12  60  20  100 

2,45 

4  20  6  30  10  50  20  100 

2,30 

1 

5 

8  40  11  55  20  100 

2,50 

Overall Mean: 

2,4  12  6,2  31  11,4  57  20  100 

2,45 

No (1) 1,00  1,66 %0    %33 
Partly (2) 1,67  2,33 %34    %66 
Yes (3) 2,34  3,00 %67    %100 

When  Table  4  is  examined  and  overall  mean  values  are  considered,  it  is  seen  that 

subservientness was given a high (subservient) value by the participants; that is 11,4 the highest 

out  of  20  (57 %).  Similarly,  the  overall mean  (2,45)  shows  that  the  system  is  subservient  since 

the value is between 2,34 and 3,00.   

80  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

When  Table  4  is  examined  in  terms  of  the  items,  the  following  implications  can  be  made: 

teachers  can  teach  their  lessons more  easily  and  quickly;  their  performances  and  their  teaching 

efficiency  can  be  affected;  touchscreen  writing  /  drawing  system  will  be  beneficial  for  their 

students; and this system will be a useful teaching tool.  

The  graphs  showing  the  data  regarding  subservientness  evaluation  results  are presented under 

the Figure 11. 

Figure 11. Usability survey study  average values with regards to subservientness evaluation 

 

results. 

The results of  the survey with regards  to  user    friendliness  factor  are represented  in Table 5. 

When  Table  5  is  examined  and  overall mean  values  are  considered,  the  user    friendliness  of 

the  system  is  found  to  be  evaluated  as  easy  with  the  highest  value  11,2  out  of  20  (56%). 

Similarly,');
INSERT INTO posts (postId,userId,title,body) VALUES (257,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 15)','  the  overall mean with  a  value  of  2,37  is  between 2,34  and 3 points, which  shows  that 

user  friendliness of the system is Yes, easy. 

81  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

When Table 5 is examined regarding to the items, the following can be concluded: touch panel is 

user    friendly  and  easy    to    learn;  it  is  easy  to  teach  something  by  using  the  touchscreen 

writing/drawing system; interaction with the screen is clear and simple; and finally it is easy to 

do drawings on the screen.  

 

Table 5. The analysis of the replies to the statements aiming at determining the user  

friendliness level of the touchscreen writing / drawing system. 

Statements 

Did you find it easy to use the 
touchscreen writing/drawing 
system? 

Was it easy to learn how 
touchscreen writing/drawing 
system worked? 

Do you think it will be easy to teach 
something by using touchscreen 
writing/drawing system? 

Do you think the interaction with 
touchscreen writing/drawing 
system is clear and simple (not 
complex)? 

Was it easy to make drawings you 
wanted to on touchscreen 
writing/drawing system?  

No 
(1) 

Partly 
(2) 

Yes 
(3) 

Total 

f  % 

f  % 

f  % 

f  % 

Mean 
3  15  4  20  13  65  20  100 

2,50 

4  20  5  25  11  55  20  100 

2,35 

5  25  5  25  10  50  20  100 

2,25 

2  10  5  25  13  65  20  100 

2,55 

5  25  6  30  9  45  20  100 

2,20 

Overall Mean: 

3,8  19  5  25  11,2  56  20  100 

2,37 

No (1) 1,00  1,66 %0    %33 
Partly (2) 1,67  2,33 %34    %66 
Yes (3) 2,34  3,00 %67    %100 

The graphs showing mean values regarding user  friendliness are presented under Figure 12.  

82  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Figure 12. Usability survey study  The average values regarding the findings of user  

friendliness evaluation. 

 

The results of  the survey with regards  to usability  factor are showed  in Table 6. When Table 6 

is examined and overall mean values are considered,  the  usability of  the system  is  found  to be 

evaluated  as  partly  usable  with  the  highest  value  10  out  of  20  (50%).  Similarly,  the  overall 

mean  with  a  value  of  2,36  is  between  2,34  and  3  points,  which  show  that  the  usability  level  of 

touchscreen writing/drawing system is Yes, it is usable.    

 

 

 

83  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Table 6. The analysis of the replies to the statements aiming at determining the usability levels 

of touchscreen writing / drawing system. 

 

Statements 

Do you thinktouchscreen 
writing/drawing system is useful 
for teaching your lesson? 

Do you think many features of the 
product are necessary and 
appropriate?  

Did you find portability featureof 
touchscreen writing/drawing 
systemuseful? 

Do you think that being able to 
teach your lesson while sitting is a 
useful feature? 

Do you think writing on a 
tuchscreeen is as easy as writing on 
a board?  

No 
(1) 

Partly 
(2) 

Yes 
(3) 

Total 

f  % 

f  % 

f  % 

f  % 

Mean 
2  10  6  30  12  60  20  100 

2,50 

1 

5 

8  40  11  55  20  100 

2,50 

2  10  7  35  11  55  20  100 

2,45 

1 

5 

6  30  13  65  20  100 

2,60 

3  15  6  30  11  55  20  100 

2,40 

Overall Me');
INSERT INTO posts (postId,userId,title,body) VALUES (258,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 16)','an: 

1,8  9  6,6  33  11,6  58  20  100 

2,49 

No (1) 1,00  1,66 %0    %33 
Partly (2) 1,67  2,33 %34    %66 
Yes (3) 2,34  3,00 %67    %100 

When  Table  6  is  examined  in  terms  of  the  items  the  followings  can  be  inferred:  touchscreen  is 

usable  to  teach  lesson;  the  features  of  the  system  are  necessary  and  used  appropriately;  being 

portable  increases  the  usability  of  the  system;  and  it  is  as  usable  as  blackboards  since  it  can  be 

used to teach while sitting. 

The graphs showing mean values regarding usability factor are presented under Figure 13. 

84  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Figure 13. Usability survey study  The mean values regarding the findings of usability 

evaluation. 

6. CONCLUSIONS AND FUTURE WORK  

The  main  objective  of  the  current  study  is  that  the  touchscreen  writing  /  drawing  system 

designed  and developed within  the  framework of  this  study will  supplement education process. 

In  addition,  this  system  is  expected  to  contribute  to  educational  technology  field  in  terms  of 

extending  the  field  and  giving  it  a multi    directional  feature.  In  this  respect,  the  authors  try  to 

introduce  the  system  as  an  efficient  and  effective  one  so  as  to  integrate  education  and 

technology.   

The  literature  review  carried  out prior  to  the  study  revelaed  that  there  is  not  adequate number 

of  materials  and  teaching  tools  that  are  likely  to  contribute  to  increase  the  quality  of  activities 

used in teaching and to reduce the problems faced by the teachers while teaching. Therefore; the 

touchscreen  writing/drawing  system  designed  and  developed  in  the  current  study  is  thought 

85  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

to be a solution to the needs mentioned.  

In  parallel  with  the  study,  software  and  hardware  design  process  of  this  system  was  explained 

without  mentioning  the  complex  technical  details.  In  addition,  the  study  was  evaluated  in 

general  first  by  asking  the  teachers,  who  are  the  real  target  users  of  the  system,  to  use  the 

system  and  then  analyzing  the  feedback  provided  by  them.  Thanks  to  this  evaluation  approach, 

which involves usability process and the survey study following this process, the authors tried to 

determine to what extent the system was effective and efficient and the objectives were met.    

When  the observations and the data obtained during evaluation process are considered,  it  is clear 

that the touchscreen writing / drawing system is used by teachers in a positive way and therefore 

is  likely  to  contribute  to  classroom  teaching  processe');
INSERT INTO posts (postId,userId,title,body) VALUES (259,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 17)','s  by  increasing  the  effectiveness  and 

efficiency of the lessons for both teachers and students. At this point, the usability process applied 

revealed  that  touchscreen  could  be  used  in  a  positive  way  and  is  an  effective  tool  to  provide 

solutions  to  the  limitations  mentioned  before.  The  survey  study  carried  out  after  the  usability 

process  aimed  at  evaluating  touchscreen  writing/drawing  system  with  regards  to  three 

variables,  namely  subservientess,  user    friendliness  and  usability,  and  the  feedback  received 

regarding  these  factors  revealed  positive  opinions  about  the  sytem.  As  a  result,  touchscreen 

writing/drawing  system  developed within  the  framework of  this study  is  thought  to be effective 

material  device in meeting the objectives of the study and to contribute the related literature to a 

great extent.  

The positive  findings and  feedback obtained  in this study are quite encouraging  for the authors to 

conduct further studies on the topic. Accordingly, improving software and hardware infrastructure 

of  touchscreen writing/drawing  system  is planned within  the  scope of  these  further  studies.  In 

this  regard,  certain  studies  for  improving  the  system  are  planned  to  have  a  more  effective, 

comprehensive and interactive software in the future versions of the system. Finally, the following 

actions are also planned in the future: to develop interfaces to enable the interaction with different 

systems; to use high storage memory units; and to develop touch panels in different sizes. 

ACKNOWLEDGMENT 

Patent  application  of  the  designed  and  developed  touchscreen  writing  /  drawing  system  was 

processed  by  the  Turkish  Patent  Institute  (Application No:  2013/01298)   and  supported  by The 

Scientific and Technological Research Council of Turkey (TUBITAK). 

 

 

86  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

 

REFERENCES 

Aguirre,  E.  J.  F.  R.,  Moreno,  J.  R.  '||'&'||'  Franco,  M.  F.  F.  (2012).  Design  and  construction  of  an 

electrocardiograph  prototype  with  touch  screen  interface  and  embedded  system  with  16-bit 

resolution, In Proocedings Simposio de Tratamiento de Seales, Imgenes Y Visin Artificial, Stsiva. 

Akbaba-Altun,  S.  (2006).  Complexity  of  integrating  computer  technologies  into  education  in 

Turkey, Educational Technology '||'&'||' Society, 9(1): 176-187. 

Alsaggaf,  W.,  Hamilton,  M.  '||'&'||'  Harland,  J.  (2012).  Mobile  learning  in  computer  sc ience  lectures, 

International Journal of e-Education, e-Business, e-Management and e-Learning, 2(6): 493-497. 

Astell,  A.  J.,  Ellis,  M.  P.,  Bernardi,  L.,  Alm,  N.,  Dye,  R.,  Gowans,  G.  '||'&'||'  Campbell,  J.  (2010).  Using  a 

touch  screen  computer  to  support  rela');
INSERT INTO posts (postId,userId,title,body) VALUES (260,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 18)','tionships  between  people with  dementia  and  caregivers, 

Interacting with Computers, 22: 267-275. 

Barak,  M.,  Lipson,  A.  '||'&'||'  Lerman,  S.  (2006).  Wireless  laptops  as  means  for  promoting  active 

learning in large lecture halls, Research on Technology in Education, 38(3): 245-63. 

Bi, X., Smith, B. A., '||'&'||' Zhai, S. (2012). Multilingual touchscreen keyboard design and optimization. 

HumanComputer Interaction, 27(4), 352-382. 

Bradley,  S.  L.  (2012).  Tactile media:  Factors  affecting  the  adoption  of  touchscreen  smartphones 

among  consumers  with  vision  loss.  Master  of  Sc.  Thesis,  Arts  (Radio,  Television  and  Film)   

University of North Texas, Denton, USA. 

Caviglia-Harris,  J.,  Hall,  S.,  Mullan,  K.,  Macintyre,  C.,  Bauch,  S.  C.,  Harris,  D.,  Sills,  E.,  Roberts,  D., 

Toomey,  M.  '||'&'||'  Cha,  H.  (2012).  Improving  household  surveys  through  computer-assisted  data 

collection: Use of touch-screen laptops in challenging environments, Field Methods, 24(1): 74-94. 

Chourasia, A. O., Wiegmann, D. A., Chen, K. B., Irwin, C. B. '||'&'||' Sesto, M. E. (2013). Effect of sitting or 

standing  on  touch  screen  performance  and  touch  characteristics,  Human  Factors: The  Journal  of 

the Human Factors and Ergonomics Society, 55(4), 789-802. 

 

87  
 

Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

Colle,  H.  A.  '||'&'||'  Hiszem,  K.  J.  (2004).  Standing  at  a  kiosk:  Effects  of  key  size  and  spacing  on  touch 

screen numeric keypad performance and user preference, Ergonomics, 47: 1406-1423. 

Deperlioglu, O '||'&'||' Kose, U. (2013). The effectiveness and experiences of blended learning  

approaches  to  computer  programming  education,    Computer  Applications  in  Engineering 

Education, 21(2), 328-342. 

Dexter, L., Anderson, E. '||'&'||' Becker, J. (1999). Teachers views of computers as catalysts for changes 

in their teaching practice, Journal of Research on Computing in Education, 31 (3): 221-239. 

Dixon, M. '||'&'||' Prior, M.  (2012). Design and usability  testing of anonymous  touch -screen  electronic 

queuing  system:  Towards 

improving  patient  satisfaction,  In  Proocedings  International 

Conference in Green and Ubiquitous Technology, Jakarta. 

Dogan,    H.  (1997).  Program  and  Instruction  Design  in  Education,  (In  Turkish),  Onder 

Publishing, Ankara: Turkey, 87 -122. 

Faulkner,  L.  (2003).  Beyond  the  five-user  assumption:  Benefits  of  increased  sample  sizes  in 

usability testing. Behavior Research Methods, Instruments and Computers, 35 (3): 379-383. 

Hembrooke,  H.  '||'&'||'  Gay,  G.  (2003).  The  laptop  and  the  lecture:  The  effects  of  multitasking  in 

learning environments, Journal of Computing in Higher Education, 15(3): 46-64. 

Jin, Z., Plocher, T. '||'&'||' Kiff, L. (2007). Touch screen user interfaces for older adults: Button size and 

spacing.  In  Universal  access  in  human  computer  interaction:  Coping  with  diversity  (pp.933941). 

Berlin, Germany: Springer-Verlag. 

Kose,  U.  (2010).  Web  2.0  Technologies  in  E -learning.  In  Free  and  Open  Source  Software  for  E -

learning: Issues, Successes and Challenges  (pp. 1-23). Hershey, USA: IGI Global. 

Lee,  A.  C.  K.  (2003).  Undergraduate  students  gender  d ifferences  in  IT  skills  and  attitudes, 

Journal of Computer Assisted Learning , 19(4): 488500. 

Martin, G. L. (1988). Configuring a numeric keypad for a touch screen, Ergonomics, 31: 945-953. 

McCormack, C. '||'&'||' Jones, D. (1997). Buildi');
INSERT INTO posts (postId,userId,title,body) VALUES (261,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 19)','ng a Web -based Education System, Wiley, New York.  

88  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

 

Newman, E. D., Lerch, V.,  Jones,  J. '||'&'||' Stewart, W.  (2012). Touchscreen questionnaire patient data 

collection  in  rheumatology  practice:  Development  of  a  highly  successful  system  using  process 

redesign, Arthritis Care '||'&'||' Research, 64: 589-596. 

Ni,  X.  '||'&'||'  Branch,  M.  (2004).  Experience  of  using  laptop  in  higher  education  institutions:  Effects 

with  and  of  ubiquitous  computing  under  natural  conditions,  Proceedings  International 

Conference of Association for Educational Communications and Technology, Chicago. 

Nielsen,  J.  '||'&'||'   Landauer,  T.  K.  (1993).  A  mathematical  model  of  the  finding  of  usability 

problems, Proceedings ACM/IFIP INTERCHI93  Conference , Amsterdam, 206-213. 

Nielsen,  J.  '||'&'||'  Molich,  R.  (1990).  Heuristic  evaluation  of  user  interfaces,  Proceedings  ACM 

CHI\'90 Conference, Seattle, 249-256. 

Nilson,  L.  '||'&'||'  Weaver,  E.  (2005).  Enhancing  learning  with  laptops  in  the  classroom.  In  New 

directions for teaching and learning, no. 101, Jossey-Bass, San Francisco. 

Palaigeorgiou,  G.  E.,  Siozos,  P.  D.,  Konstantakis,  N.  I.  '||'&'||'  Tsoukalas,  I.  A.  (2005).  A  computer 

attitude scale for computer science freshmen and its educational implications,  

Journal of Computer Assisted Learning,  21(5): 330342. 

Rahman,  M.  M.  '||'&'||'  Aktaruzzaman,  M.  (2012).  Design  and  development  of  a  simple  low -cost 

touchscreen  to  control  home  automation  system,  International  Journal  of  Information 

Technology and Computer Science, 11: 26-33. 

Raj,  T.  D.  S.,  Sarah,  T.,  Dhinagaran,  D.  '||'&'||'   Ugargol,  A.  P.  (2013).  Design,  development  and 

implementation  of  a  touch-screen  health  information  kiosk  for  patients  at  the  outpatient 

waiting area  in a  large  tertiary care hospital  in  India: An evaluation of user satisfaction,  Journal 

of Health Informatics in Developing Countries,  7(1): 8-19. 

Richey,    R.  C.  (2008). Reflections  on  the  2008  AECT  definitions  of  the  field,  TechTrends,  52:  24-

25.   

Schultz,  K.  L.,  Batten,  D.  M.  '||'&'||'  Sluchak,  T.  J.  (1998).  Optimal  viewing  angle  for  touch -screen 

displays: Is there such a thing?, International Journal of Industrial Ergonomics, 22: 343-350. 

89  
Journal of Multidisciplinary Developments. 1(1), 60-90, 2016 

 

     e-ISSN: 2564-6095 

Develop. a FPGA  Supported Touchscreen Writing / Drawing System for Educ. Env.  Tufekci, A., Samanci, K. '||'&'||' Kose, U. 

 

Sesto,  M.  E.,  Irwin,  C.  B.,  Chen,  K.  B.,  Chourasia,  A.  O.  '||'&'||' Wiegmann,  D.  A.  (2012).  Effect  of  touch 

screen  button  size  and  spacing  on  touch  characteristics  of  users  with  and  without  disabilities, 

Human Factors, 54(3): 425-436. 

Shervin,  N.,  Dorrwachter,  J.,  Bragdon,  C.,  Shervin,  D.,  Zurakowski,  ');
INSERT INTO posts (postId,userId,title,body) VALUES (262,3358,'Journal of Multidisciplinary Developments. 1(1), 60-90, 2016  (part 20)','D.  '||'&'||'  Malchau,  H.  (2011). 

Comparison  of paper  and  computer-based questionnaire modes  for measuring health outcomes 

in patients undergoing total hip arthroplasty, Journal of Bone and Joint Surgery, 93: 285. 

Taylor, N.  J.  (2012). A Near Touch User  Interface  for Touch  Screen Based  Systems. Master of  Sc. 

Thesis,  Electronics  and  Computer  Systems  Engineering   Massey  University,  Palmerston  North, 

New Zealand. 

Tufekci,  A.,  Ekinci,  H.,  '||'&'||'  Kose,  U.  (2013).  Development  of  an  internet-based  exam  system  for 

mobile environments  and  evaluation of  its usability. Mevlana  International  Journal of Education, 

3(4), 57-74. 

Van  Braak,  J.  P.  (2004).  Domains  and  determinants  of  university  students  self-perceived 

computer competence, Computers '||'&'||' Education, 43(3): 299312. 

Wilson,  K.  S.,  Inderrieden,  M.  '||'&'||'  Liu,  S.  (1995).  A  comparison  of  five  user  interface  devices 

designed for point-of-sale in the retail industry, Proceedings the Human Factors and 

Ergonomics Society 39th Annual Meeting, Santa Monica. 

 

JOMUDE 
http://www.jomude.com 
 

90  
');
INSERT INTO posts (postId,userId,title,body) VALUES (263,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk','Birger Larsen
Royal School of Library and
Information Science
Copenhagen Denmark
blar@iva.dk

Wei Lu
School of Information
Management
Wuhan University China
reedwhu@gmail.com
12rA5 R.c 195047:ir
ABSTRACT
Typically, every part in most coherent text has some plausi-
ble reason for its presence, some function that it performs to
the overall semantics of the text. Rhetorical relations, e.g.
contrast, cause, explanation, describe how the parts of
a text are linked to each other. Knowledge about this so-
called discourse structure has been applied successfully to
several natural language processing tasks. This work stud-
ies the use of rhetorical relations for Information Retrieval
(IR): Is there a correlation between certain rhetorical rela-
tions and retrieval performance? Can knowledge about a
documents rhetorical relations be useful to IR?
We present a language model modication that considers
rhetorical relations when estimating the relevance of a doc-
ument to a query. Empirical evaluation of dierent versions
of our model on TREC settings shows that certain rhetorical
relations can benet retrieval eectiveness notably (> 10%
in mean average precision over a state-of-the-art baseline).

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
Models; H.3.1 [Information Storage and Retrieval]: Con-
tent Analysis and Indexinglinguistic processing

Keywords
Rhetorical relations, discourse structure, retrieval model,
probabilistic retrieval

1.

INTRODUCTION
According to discourse analysis, every part in most coher-
ent text tends to have some plausible reason for its presence,
some function that it performs to the overall semantics of
the text. Rhetorical relations, e.g. contrast, explana-
tion, condition, are considered critical for text interpre-
tation, because they signal how the parts of a text are linked
to each other to form a coherent whole [23]. Unlike gram-
matical relations, which are generally explicitly manifest in

Preprint of: Christina Lioma, Birger Larsen, and Wei Lu. Rhetorical re-
lations for information retrieval. In: The 35th International ACM SIGIR
conference on research and development in Information Retrieval, SIGIR
12, Portland, OR, USA, August 12-16, 2012. Ed. by William R. Hersh,
Jamie Callan, Yoelle Maarek, and Mark Sanderson. ACM, 2012, pp. 931-
940. isbn: 978-1-4503-1472-5. doi: 10.1145/2348283.2348407.
Figure 1: Rhetorical relations example (from [11]).

language, rhetorical relations may be unstated. The goal of
discourse analysis is therefore to infer rhetorical relations,
and specically to identify their span, constraints and func-
tion.
There is a large body of research on both descriptive
and predictive models of rhetorical structure and discourse
analysis in natural language text. For instance, annotation
pro jects have taken signicant steps towards developing se-
mantic [12, 18] and discourse [5] annotated corpora. Some of
these annotation eorts have already had a computational
impact, making it possible to automatically induce semantic
roles [15] and to automatically identify rhetorical relations
[14], achieving near-human levels of performance on certain
tasks [27].
In addition, applications of discourse analysis
to automatic language processing tasks such as summarisa-
tion or classication (overviewed in section 2) indicate that
rhetorical relations can enhance the performance of well-
trained natural language processing systems.
Motivated by these advances, this work brings per');
INSERT INTO posts (postId,userId,title,body) VALUES (264,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 2)','spec-
tives from discourse analysis into Information Retrieval (IR)
with the aim of investigating if and how rhetorical relations
can benet retrieval eectiveness. Is there a correlation be-
tween certain rhetorical relations and retrieval performance?
Can knowledge about a documents rhetorical relations be
useful to IR? For example, consider the rhetorical relations
of the text shown in Figure 1 (borrowed from [11]). Should
some of the terms in this sentence be given extra weight by
an IR system, according to their rhetorical relations? Can
some rhetorical relations be considered more informative and
hence more useful for IR ranking than others? These ques-
tions have been posed before (see discussion in section 2),
however to our knowledge this is the rst time that a prin-
cipled integration of rhetorical relations into a probabilistic
IR model improves precision by > 10%.
Reasoning about query - document relevance using the
language modeling formalism [9], we present a model that
conditions the probability of relevance between a query and

a document on the rhetorical relations occurring in that doc-
ument. We present an application of this model to an IR
re-ranking task, where, given a list of documents initially
retrieved for a query, the goal is to improve the ranking
of the documents by rening their estimation of relevance
to the query. Experimental evaluation of dierent versions
of our model on TREC data and standard settings demon-
strates that certain rhetorical relations can be benecial to
retrieval, with notable improvements to retrieval eective-
ness (> 10% in mean average precision and other standard
TREC evaluation measures over a state-of-the-art baseline).

2. RELATED WORK
Discourse analysis and rhetorical structures have been stud-
ied in the context of several automatic text processing ap-
plications. This has been partly enabled by the availability
of discourse parsers - see [11, 14] for up-to-date overviews
of discourse parsing technology. Studies of discourse analy-
sis in relation to IR and its broader applications are briey
overviewed below. For a more general overview of discourse
analysis approaches, see Wang et al. [33], section 2.
Sun '||'&'||' Chai [28] investigate the role of discourse process-
ing and its implication on query expansion for a sequence
of questions in scenario-based context question answering
(QA). They consider a sequence of questions as a mini dis-
course. An empirical examination of three discourse theo-
retic models indicates that their discourse-based approach
can signicantly improve QA performance over a baseline of
plain reference resolution.
In a dierent task, Wang et al. [33] parse Web user forum
threads to determine the discourse dependencies between
posts in order to improve information access over Web fo-
rum archives. They present three dierent methods for clas-
sifying the discourse relationships between posts, which are
found to outperform an informed baseline.
Heerschop et al. [16] perform document sentiment analy-
sis (partly) based on a documents discourse structure. They
hypothesise that by splitting a text into important and less
important text spans, and by subsequently making use of
this information by weighting the sentiment conveyed by
distinct text spans in accordance with their importance,
they can improve the perf');
INSERT INTO posts (postId,userId,title,body) VALUES (265,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 3)','ormance of a sentiment classier.
A documents discourse structure is obtained by applying
rhetorical structure theory on a sentence level. They re-
port a 4.5% improvement in sentiment classication accu-
racy when considering discourse, in comparison to a non-
discourse based baseline. Similarly to this study, Somasun-
[26] report improvements to opinion polarity
daran et al.
classication when using discourse, and Morato et al.
[24]
report a positive dependence between classication perfor-
mance and certain discourse variables. An overview of dis-
course analysis for opinion detection can be found in Zhou
et al. [36].
In the area of text compression, Louis et al. [21] study the
usefulness of rhetorical relations between sentences for sum-
marisation. They nd that most of the signicant rhetorical
relations are associated to non-discriminative sentences, i.e.
sentences that are not important for summarisation. They
report that rhetorical relations that may be intuitively per-
ceived as highly salient do not provide strong indicators of
informativeness;
instead, the usefulness of rhetorical rela-
tions is in providing constraints for navigating through the
texts structure. These ndings are compatible with the

study of Clarke '||'&'||' Lapata [7] into constraining text com-
pression on the basis of rhetorical relations. For a more in-
depth look into the impact of individual rhetorical relations
to summarisation see Teufel '||'&'||' Moens [30].
In domain-specic IR, Yu et al. [34] focus on psychiatric
document retrieval, which aims to assist users to locate doc-
uments relevant to their depressive problems. They propose
the use of high-level discourse information extracted from
queries and documents, such as negative life events, depres-
sive symptoms and semantic relations between symptoms, to
improve the precision of retrieval results. Their discourse-
aware retrieval model achieves higher precision than the vec-
tor space and Okapi models.
Closer to our work, Wang et al. [31] extend an IR ranking
model by adding a re-ranking strategy based on document
discourse. Specically, their re-ranking formula consists of
the original retrieval status value computed with the BM11
model, which is then multiplied by a function that linearly
combines inverse document frequency and term distance for
each query term within a discourse unit. They focus on one
discourse type only (advantage-disadvantage) which they
identify manually in queries, and show that their approach
improves retrieval performance for these queries. Our work
diers on several points. We use an automatic (not man-
ual) discourse parser to identify rhetorical relations in the
documents to be retrieved (not queries). We consider 15
rhetorical relations (not 1) and we study their impact to re-
trieval performance using a modication of the IR language
model.
Finally, Suwandaratna '||'&'||' Perera [29] also present a re-
ranking approach for Web search that uses discourse struc-
ture. They report a heuristic algorithm for rening search
results based on their rhetorical relations. Their implemen-
tation and evaluation is partly based on a series of ad-hoc
choices, ');
INSERT INTO posts (postId,userId,title,body) VALUES (266,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 4)','making it hard to compare with other approaches.
They report a positive user-based evaluation of their system
for ten test cases.

3. RANKING WITH RHETORICAL
RELATIONS
There may be various ways of considering rhetorical rela-
tions in an IR setting. In this work, we view rhetorical rela-
tions as non-overlapping text spans, rather than a graph or
a tree with structure and overlapping nodes [27]. We select a
principled integration of rhetorical relation information into
the retrieval model that ranks documents with respect to
queries. The goal is to enable evidence about the rhetorical
relations in a document to have a quantiable impact upon
the estimation of relevance of this document to a query, and
to study that impact.

3.1 Model Derivation
Let q be a query, d a document, D a collection of docu-
ments, and g a rhetorical relation in the collection (so that
p(g |d) = 1). In probabilistic IR, each d in D can be
Pg
ranked by its probability p(d|q) of being relevant to q . Using
Bayes law:

p(d|q) =

p(q |d)p(d)
p(q)

rank= p(q |d)

(1)

where the right-hand side of Equation 1 is derived as follows:
p(q) is dropped because it is xed for all documents, and

p(d) can be dropped on the assumption that it is uniform
in the absence of any prior knowledge about any document.
Using the language modeling approach to IR [9], p(q |d) can
be interpreted as the probability of generating the terms in
q from a model induced by d, or more simply how likely it
is that the document is about the same topic as the query.
p(q |d) can be estimated in dierent ways, for instance using
Dirichlet, Jelinek-Mercer, or two-stage smoothing [35].
We introduce into Equation 1 the probability of generat-
ing the query terms from a model induced by d and by its
rhetorical relations   d as follows:

p(q |d, g )p(g |d)

p(q |d) = Xg
We now explain the two components in Equation 2. The
rst component, p(q |d, g ), can be interpreted as the prob-
ability of generating the query terms from a model induced
by d and g . We estimate p(q |d, g ) as a simple mixture of
the probabilities of generating q from d and g :

(2)

p(q |d, g ) = (1  )  p(q |d) +   p(q |g )

(3)

where p(q |d) is the (baseline) probability of relevance be-
tween q and d mentioned in the beginning of this section, 
is a free parameter, and p(q |g ) can be interpreted as the
probability of generating q from a model induced by the
rhetorical relation g , or more simply, the likelihood of rel-
evance between the terms in the query and the terms in the
rhetorical relation.
The second component of Equation 2, p(g |d), is the prob-
ability of the rhetorical relation given the document. Simi-
larly to above, this can be interpreted as the probability of
generating the terms in g from a model induced by d, or
more simply the likelihood of relevance between the terms
in the rhetorical relation and the terms in the document.

3.2 Model Induction
To make Equations 2-3 operational we need to compute
p(q |) and p( |d). One simple way of doing so is using the
respective maximum likelihood estimations:

');
INSERT INTO posts (postId,userId,title,body) VALUES (267,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 5)','log p(q |g ) =

|q|
Xi=1
where f (qi , g ) is the frequency of the query term qi in g ,
and |g | is the number of terms in g .

f (qi , g )
|g |

(4)

(5)

log p(g |d) =

f (gj , d)
|d|

|g |
Xj=1
where f (gj , d) is the frequency of the rhetorical relation
term gj in d, and |d| is the number of terms in d. In this
work, we use the above equations and, to compensate for
zero-frequency cases, we apply add-one smoothing.
Alternative principled estimations of Equations 4-5 are
possible (e.g. Dirichlet, Good-Turing) and could poten-
tially improve the performance reported in this work. For
instance, one could discount the frequencies in Equations
4-5 by a respective collection model using Dirichlet smooth-
f (qi ,g )+p(qi |)
log ps(q |g ) = P|q|
ing:
where  would
i=1
|g |+
be the smoothing parameter and  would be the collec-
tion of all rhetorical relations in D. A similarly Dirichlet

smoothed alternative estimation of Equation 5 would be:
log ps (g |d) = P|g |
f (gj ,d)+p(gj |D)
. We choose to use
j=1
|d|+
maximum likelihood instead of Dirichlet to avoid introduc-
ing the extra Dirichlet smoothing parameter  when inves-
tigating the eect of rhetorical relations upon retrieval.
Another alternative would be to use Good-Turing smooth-
ing, however doing so would scale down the maximum like-
lihood estimations in Equations 4-5 by a factor of 1  E (1)
|g |
respectively, where E (1)
|g | (resp. E (1)
and 1  E (1)
|d| ) is the
|d|
estimate of how many items in the numerator of Equation 4
(resp. Equation 5) have occurred once in the sample of the
denominator (see Gale '||'&'||' Sampson [13] for more on Good-
Turing smoothing).
In eect, for Equation 4 this scaling
down would reduce the probability of the query terms that
we have seen in g , making room for query terms that we
have not seen. For our setting this would not be necessary,
because in practice most queries and most rhetorical rela-
tions correspond to rather short text spans. Good-Turing
smoothing might be better suited for larger samples [13].
Overall, the model presented in this section can be seen as
a basic model for ranking documents (partly) according to
their rhetorical relations. Dierent variations on this basic
model are certainly possible, however we choose to use the
simple maximum likelihood version of this model for this
exploratory investigation into the potential benets of using
rhetorical relations for IR.

4. EVALUATION

4.1 Experimental Setup
We evaluate our model on the task of re-ranking an initial
list of documents, which has been retrieved in response to
a query. Re-ranking is a well-known IR practice that can
enhance retrieval performance notably [19]. The baseline
of our experiments consists of the top 1000 documents re-
trieved for each query using a state-of-the-art retrieval model
(language model with Dirichlet smoothing1 [9]). Our ap-
proach reranks these documents using Equation 2.

4.1.1 Dataset and Pre-processing
We experiment with the TREC datasets of the Web 2009
(queries 1-50) and Web 2010 (queries 51-100) tracks, that
contain collectively 100 queries and their relevanc');
INSERT INTO posts (postId,userId,title,body) VALUES (268,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 6)','e assess-
ments on the Clueweb09 cat. B dataset2 (50,220,423 web
pages in English crawled between January and February
2009). We choose these datasets because they are used
widely in the community, allowing comparisons with state-
of-the-art. We remove spam using the spam rankings of Cor-
mack et al. [8] with the recommended setting of percentile-
score < 70 indicating spam3 .
We consider a subset of this collection, consisting of the
top 1000 documents that have been retrieved in response to
each query by the baseline retrieval model on tuned settings
(described in section 4.1.2) using the Indri IR system4 for

1We also experimented with Jelinek-Mercer and two-stage
smoothing for the baseline retrieval model. Dirichlet and
two-stage gave higher scores. We chose Dirichlet over two-
stage because it includes one less parameter to tune.
2http://lemurpro ject.org/clueweb09.php/
3Note that removing spam from Clueweb09 cat B. is known
to give overall lower retrieval scores than keeping spam [3].
4http://www.lemurpro ject.org/

Table 1: Examples of the 15 rhetorical relations (in bold italics) of our dataset, identied by the SPADE
discourse parser [27]
Rhetorical relation Example sentences with rhetorical relations italicised and bold
... the islands now known as the Gilbert Islands were settled by Austronesian-speaking people ...
attribution
... many whites had left the country when Kenyatta divided their land among blacks ...
background
... I plugged wives into the search box and came up with the fol lowing results ...
cause-result
comparison
... so for humans, it is stronger than coloured to frustrate these unexpected numbers ...
... Conditional money based upon care for the pet ...
condition
... voltage drop with the cruise control switch could cause erratic cruise control operation ...
consequence
... Although it started out as a research project , the ARPANET quickly developed into ...
contrast
... order accutane no prescription required ...
elaboration
enablement
... The pro ject will also oer exercise programs and make eye care services accessible ...
... such advances will be reected in an ever-greater proportion of grade A recommendations ...
evaluation
... the concept cal led as evolutionary developmental biology or shortly evo-devo ...
explanation
... Fill current path using even-odd rule, then paint the path ...
manner-means
... Safety Last, Girl Shy, Hot Water, The Kid Brother, Speedy (al l with lively orchestral scores) ...
summary
temporal
... Take time out before you start writing ...
... Director Mark Smith expressed support for greyhound adoption ...
topic-comment

indexing and retrieval. For this subset, we strip HTML an-
notation using our in-house WHU-REAPER crawling and
web parsing toolkit5 . Rhetorical relations are identied us-
ing the freely available SPADE discourse parser [27]. Table
1 shows the 15 types of rhetorical relations identied by this
process, with examples taken from the re-ranking dataset.

4.1.2 Parameter Tuning
Two parameters are involved in these experiments: the
Dirichlet smoothing parameter  of the retrieval model (used
by both the baseline and our approach) and the mixture
parameter  of our model. Both parameters are tuned using
5-fold cross validation for each query set separately; results
reported are the average over the ve test sets.  is tuned
across {100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,
10000} (using the range of Zhai '||'&'||' Laerty [35]) and  is
tuned across {0.1, 0.3, 0.5, 0.7, 0.9}.
Performance is reported and tuned separately for Mean
Average Precision (MAP), Binary Preference (BPREF), and
Normalised Discounted Cumulated Gain (NDCG). These
m');
INSERT INTO posts (postId,userId,title,body) VALUES (269,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 7)','easures contribute dierent aspects to the overall evalua-
tion: BPREF measures the average precision of a ranked list;
it diers from MAP in that it does not treat non-assessed
documents as explicitly non-relevant (whereas MAP does)
[4]. This is a useful insight, especially for a collection as
large as Clueweb09 cat. B where the chances of retrieving
non-assessed documents are higher. NDCG measures the
gain of a document based on its position in the result list.
The gain is accumulated from the top of the ranked list to
the bottom, with the gain of each document discounted at
lower ranks. This gain is relative to the ideal based on a
known recall base of relevance assessments [17]. Finally, we
test the statistical signicance of our results using the t-test
at 95% and 99% condence levels [25].

4.2 Findings
Figure 2 shows the distribution of the rhetorical rela-
tions in our re-ranking dataset as a percentage of the total
number of rhetorical relations. Elaboration, attribution
and background are the most frequent rhetorical relations,
whereas topic-comment is the most infrequent. This hap-

5Freely available by emailing the third author.

pens because quite often in text a topic forms the nucleus
of the discourse, which is then linked by a number of dier-
ent rhetorical relations, for instance about its background,
elaborating on an aspect, or attributing parts of it to some
entity. As a result, several types of other rhetorical rela-
tions can correspond to a single topic-comment. Note that
the distribution of rhetorical relations reported here is in
agreement with the literature, e.g. Teufel '||'&'||' Moens [30] also
report a 5% occurrence of contrast, albeit in the domain
of scientic articles.

4.2.1 Retrieval-Enhancing Rhetorical Relations
Table 2 shows the performance of our model against the
baseline, for each rhetorical relation and evaluation measure.
The baseline performance is among the highest reported in
the literature for these setings; for instance Bendersky et al.
[3] report MAP=0.1605 for a tuned language model baseline
with the Web 2009 track queries on Clueweb cat. B without
spam.
We observe that dierent rhetorical relations perform dif-
ferently across evaluation measures and query sets. The four
rhetorical relations that improve performance over the base-
line consistently for all evaluation measures and query sets
(shaded rows in Table 2) are: background, cause-result,
condition and topic-comment. Topic-comment is one of the
overall best-performing rhetorical relations, which in simple
terms means that boosting the weight of the topical part of
a document improves its estimation of relevance.
A closer look at which rhetorical relations decrease per-
formance presents a more uneven picture as no relations
consistently underperform for all measures and query sets.
Some relations, such as explanation and enablement for
Web 2009, and summary and evaluation for Web 2010, are
among the lowest performing, but are not under the baseline
across all measures and both query sets. This implies that
separating rhetorical relations into those that generally can
enhance retrieval performance and those that cannot may
not be straight-forward. Even though exploring the fam-
ily likeness between useful relations and ones that give no
mileage is an interesting discussion, in the rest of the pa-
per we focus on those rhetorical relations that consistently
improve retrieval performance (for these datasets).

rhetorical relation

MAP

Table 2: Retrieval performance with rhetorical relations and without (baseline). * (**) marks stat. signif-
icance at 95% (99%) using the t-test. Bold means > baseline. % shows the dierence from the baseline.
Shaded rows indicate consistent improvements over the baseline at all times.
Web 2010 (queries 51-100)
Web 2009 (queries 1-50)
NDC');
INSERT INTO posts (postId,userId,title,body) VALUES (270,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 8)','G
BPREF
NDCG
BPREF
0.1625
0.2920
0.0986
0.3893
0.3230
0.2240
0.1654* +1.8% 0.3275** +1.4% 0.3927** +0.9% 0.0924
-6.2% 0.2549** +13.8% 0.3008** +3.0%
0.1646
+1.3% 0.3291** +1.9% 0.3910
+0.4% 0.1086* +10.2% 0.2623** +17.1% 0.3070** +5.1%
+11.2% 0.3079
+2.9% 0.2491*
+0.2% 0.1015
+0.1% 0.3255** +0.8% 0.3900
0.1626
+5.4%
+1.9% 0.3040** +4.1%
+3.1% 0.2282
-0.4% 0.1017
-0.9% 0.3251*
0.1610
+0.6% 0.3877
+0.5%
+1.3% 0.2470** +10.3% 0.2936
+0.3% 0.0999
+0.5% 0.3258** +0.9% 0.3903
0.1632
-0.5% 0.0945
-1.4% 0.3250
0.1602
+0.6% 0.3874
-4.1% 0.2377*
+6.1% 0.2840**
-2.7%
+0.1% 0.1103* +11.8% 0.2531** +13.0% 0.3069** +5.1%
-4.6% 0.3269** +1.2% 0.3897
0.1549*
0.1556*
-4.2% 0.3292** +1.9% 0.3866
-0.7% 0.0951
-3.5% 0.2598** +16.0% 0.3005** +2.9%
+2.5%
+3.4% 0.2992*
+2.4% 0.2316*
-0.6% 0.1010
+0.3% 0.3869*
-1.4% 0.3240
0.1601
-17.4% 0.2313*
-0.2% 0.0814**
+0.4% 0.3886
+0.5% 0.3242
0.1632
+3.3% 0.2902
-0.6%
+4.9% 0.2645** +18.1% 0.3069** +5.1%
-2.1% 0.1034
+0.9% 0.3813
-4.9% 0.3259*
0.1546
-0.8%
+3.7% 0.2897-0.2% 0.0986
+0.7% 0.3884
-0.1% 0.3253*
0.1623
0.2324*
+0.3%
-12.6% 0.2220*
-0.4% 0.0862
+0.1% 0.3241
0.1626
+0.3% 0.3879
-0.9% 0.2928
0.1615
-0.6% 0.3262** +1.0% 0.3887
-0.2% 0.0921
-6.6% 0.2546** +13.7% 0.3052
+4.5%
+3.1%
+10.5% 0.3009
+2.1% 0.1090* +10.5% 0.2476*
+4.5% 0.3976*
+3.0% 0.3375
0.1673

none (baseline)
attribution
background
cause-result
comparison
condition
consequence
contrast
elaboration
enablement
evaluation
explanation
manner-means
summary
temporal
topic-comment

MAP

Table 3: Eect of the rhetorical relation to the re-
trieval model as indicated by parameter  (see Equa-
tion 3), for the tuned runs of Table 2. Shaded
rows indicate rhetorical relations that consistently
improve performance over the baseline at all times.
rhetorical
Web 2009 (queries 1-50) Web 2010 (queries 51-100)
MAP BPREF NDCG MAP BPREF NDCG
relation
0.3
0.5
0.3
0.1
0.5
0.1
attribution
0.3
0.7
0.3
0.2
0.6
0.2
background
0.5
0.7
0.5
0.3
0.7
0.3
cause-result
0.3
0.5
0.3
0.4
0.7
0.4
comparison
condition
0.3
0.7
0.3
0.3
0.5
0.3
0.5
0.7
0.5
0.5
0.7
0.5
consequence
0.3
0.5
0.3
0.3
0.7
0.3
contrast
0.3
0.5
0.3
0.1
0.5
0.1
elaboration
0.3
0.5
0.3
0.1
0.9
0.1
enablement
0.5
0.7
0.5
0.5
0.7
0.5
evaluation
explanation
0.5
0.7
0.5
0.5
0.7
0.5
0.5
0.7
0.5
0.5
0.7
0.5
manner-means
0.3
0.7
0.3
0.5
0.7
0.5
summary
0.3
0.5
0.3
0.1
0.7
0.1
temporal
topic-comment
0.5
0.5
0.5
0.5
0.7
0.5

topic-comment
evaluation

consequence

summary
enablement

explanation
comparison

manner-means

cause-result

temporal
contrast

condition

background

attribution
elaboration

5

10

15

20

25

% of all rhetorical relations

Figure 2: % distribution of rhetorical relations in
our dataset.

Improvements over the baseline are generally higher for
Web 2010 than Web 2009, possibly because the former base-
line is weaker, with potentially more room for improvement.
An interesting trend is that more rhetorical relations im-
prove performance according to BPREF than according to
MAP and NDCG. As BPREF is the only of these evaluation
measures that does not consider non-assessed documents as
non-relevant, this indicates the presence of non-assessed doc-
uments in the ranking.
The scores shown in Table 2 are averaged over tens of
queries, meaning that they can be aected by outliers. Fig-
ure 3 presents a detailed per-query overview of the perfor-
mance of each query in relation to the baseline for each of
the 15 rhetorical relations6 . The plotted points represent
the dierence in MAP between our approach and the base-
line. Positive points indicate that our approach outperforms
the baseline. The points are sorted.
We observe that although the overall performance of the
Web 2010 query set is lower than that of the Web 2009 query
set, the improvements over the baseline of the 2010 set are
consistently larg');
INSERT INTO posts (postId,userId,title,body) VALUES (271,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 9)','er. Only in one case, topic-comment, do the
plotted points clearly cross. Overall both query sets show
similar plots with outliers at both ends of the scale. How-
ever, the 2009 query set tends to have a somewhat larger
proportion of negative outliers, which goes some way to-
wards explaining the lower improvements over the baseline
observed for Web 2009. The Web 2010 set shows improve-
ments over the baseline for most of the rhetorical relations
and for the ma jority of the queries.

4.2.2 Quantifying the Contribution of Rhetorical
Relations to the Ranking
Exactly how much impact each rhetorical relation has on
the ranking can be seen in Table 3. The table lists the  val-
ues for the best performing tuned runs from Table 2, where
high  values mean that the rhetorical relations are given
more weight in the ranking (see Equation 3). We see that
none of the values are above 0.5 for MAP and NDCG, in-
dicating that too much emphasis on the rhetorical relations
may not be benecial to performance. Consistent with Table
2, BPREF follows a dierent trend than MAP and NDCG,

6Similar trends are observed in the corresponding gures for
BPREF and NDCG, which are not included here for brevity.

0.08

0.02

0.1

0.1

0.2

0.1

0.05

0.1

0.1

0.1

attribution

background

cause-result

0.1

0.2

comparison

condition

consequence

0.1

0.1

0.05

0.1

contrast

elaboration

enablement

0.1

0.05

evaluation

explanation

0.1

0.2

0.15

0.05

0.05

0.1

manner-means

summary

temporal

topic-comment

0.05

0.05

0.05

0.05

Web 2009
Web 2010

Figure 3: Sorted per-query dierence in MAP between the baseline and our model (y-axis), for each rhetorical
relation. The horizontal line marks the baseline. + and o mark the 2009 and 2010 query sets.

which could be due to the fact that it is a dierent type
of evaluation measure as discussed above in section 4.1.2.
With BPREF, unassessed documents are not explicitly pe-
nalised in the evaluation (as in MAP and NDCG) - resulting
in overall higher  values for best performing runs, typically
of around 0.5-0.7.
Further we observe that the rhetorical relations that con-
sistently improve performance over the baseline, as indicated
in Table 2, dier in  values for their best performing runs.
For example,  = 0.2 - 0.3 for background and  = 0.5 for
topic-comment. This implies that, to use rhetorical rela-
tions successfully for IR, it is not sucient to know which
rhetorical relations should be considered in the ranking and
which not; also knowledge about how much emphasis to put
on each rhetorical relation is needed for optimal IR perfor-
mance.
Finally, note that the frequency of rhetorical relations
does not aect their impact to retrieval. For instance, the
three best performing rhetorical relations, topic-comment,
background and cause-result constitute respectively ap-
proximately >1%, 11% and 5% of all rhetorical relations, as
shown in Figure 2.

5. OPTIMISED RANKING WITH
RHETORICAL RELATIONS

5.1 Rhetorical Relation Selection
The ndings in section 4.2 show that some rhetorical re-
lations can be more benecial to retrieval performance than
others. An ideal solution would not consider the lexical
statistics of all rhetorical relations in a document, but rather
it would select to include in the ranking only those rhetorical
relations that have a higher likelihood of enhancing retrieval
performance. This can be formulated as nding the optimal
rhetorical relation  that maximises the expected retrieval
scores according to');
INSERT INTO posts (postId,userId,title,body) VALUES (272,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 10)',' an evaluation measure (e.g. MAP) for a
query-document pair:
 = arg max


E [y |q , d]

(6)

where E denotes the expectation and y the retrieval score
(rest of notation as dened in section 3).
Bayesian decision theory allows to reason about this type
of expectation, for instance see [32]. In this work, we treat
this as a problem of Bayesian posterior inference, where the
goal is to estimate the retrieval performance associated with
a rhetorical relation, given the observed retrieval scores it
fetches on a number of queries. Then, we can consider
the rhetorical relation associated with the highest retrieval
performance as optimal. For this estimation, we split our
dataset into dierent parts so that we use the observations
from one to make inferences about the other (see section 5.2
for details).
Let n = 15 be the rhetorical relations shown in Table 2,
and xj be the number of queries for which retrieval with
the j th rhetorical relation gets a retrieval score yj . For now
we assume that all rhetorical relations may be expected to
have similar retrieval performance, with the j th rhetorical
relation having an average performance ratio per query j
(estimated as yj
). Various densities can be used to t simi-
xj
lar data [22], one of which is the Poisson distribution. Let us
assume that, conditional on j , the retrieval scores yj have
independent Poisson distributions with means j xj . Let us

further assume that the j are independent realisations of a
gamma variable with parameters  and  , and that  itself
has a prior gamma distribution with parameters  and .
Thus

f (y |) =

 (| ) =
Yj=1Yj=1

(xj j )yj
yj !

exj j
j
()

ej

 ( ) =

  1
( )

so that the joint probability density of the retrieval scores
y , the average performance ratios , and  is

f (y |)f (| ) ( ) = c
Yj=1

yj +1
ej (xj + )}n+1 e

(7)

where c is a constant of proportionality.
The conditional density of  can be computed by vari-
ous numerical approximations, one of which is the Laplace
method [2], which we use here. To nd the conditional den-
sity of  we integrate over the j to obtain

f (y ,  ) = c
{(xj + )(yj +)(yj +)}n+1 e (8)
Yj=1
from which the marginal density of y is obtained by further
integration to give

(9)

f (y ) = c

eh( )d

(yj + )  Z 
n
Yj=1
where h( ) =   (n +   1)log + P(yj + )log (xj +  ).
Let I denote the integral in this expression. In this work,
we take an uninformative prior for  , with  = 0.1 and
 = 1 and use  = 1.87 . We then apply Laplaces method
to I , resulting in the approximate posterior density for  ,
 ( |y ) = I 1eh( ) .
To calculate approximate posterior densities for j we
integrate Equation 7 over i , i 6= j and then we apply
Laplaces method to the numerator and denominator inte-
grals of

 (j |y ) =



yj +1
ej xj R 
0 ehj ( )d(yj + ) R 
0 eh( )d

where

hj ( ) = ( + j )  (n +   1)log +Xi6=j
(yi + )log (xi +  )
The resulting denominator is again I1 , while the numerator
must be recalculated at each of a range of values for j .
The output is the (posterior) expected retrieval performance
associated with each rhetorical relation.

5.2 Experiments

7These values are not tuned; they are the default values
of this approach as illustrated in [10], chapter 11.3, pages
603-604.

Table 4: Retrieval performance with optimal rhetorical relations (inferred, observed) and without rhetorical
relations (baseline).
(1)-(5) refers to the ve randomised sampling');
INSERT INTO posts (postId,userId,title,body) VALUES (273,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 11)','s used to infer the optimal rhetorical
relations. Bold marks better than baseline.
Web 2010 (queries 51-100)
Web 2009 (queries 1-50)
rhetorical relation
NDCG
BPREF
MAP
NDCG
BPREF
MAP
0.0967
0.2890
0.2198
0.3894
0.1625
0.3230
+8.5% 0.1355 +40.1% 0.2859 +30.1% 0.3347 +15.8%
0.1879 +15.6% 0.3503
+8.5% 0.4224
+7.9% 0.1285 +32.9% 0.2841 +29.3% 0.3394 +17.4%
0.1948 +19.9% 0.3585 +11.0% 0.4202
+9.3% 0.4169
0.1984 +22.1% 0.3532
+7.1% 0.1358 +40.0% 0.2906 +32.2% 0.3388 +17.2%
+7.7% 0.4282 +10.0% 0.1360 +40.6% 0.2874 +30.8% 0.3336 +15.4%
0.1952 +20.1% 0.3479
0.1950 +20.0% 0.3528
+9.2% 0.4287 +10.1% 0.1340 +38.6% 0.2865 +30.3% 0.3322 +14.9%
0.2157 +32.7% 0.3660 +13.3% 0.4412 +13.3% 0.1474 +52.4% 0.2978 +35.5% 0.3569 +23.5%

none (baseline)
optimalinf erred (1)
optimalinf erred (2)
optimalinf erred (3)
optimalinf erred (4)
optimalinf erred (5)
optimalobserved

5.2.1 Setup
The observations required to make the above inference
are triples of rhetorical relation - query number - retrieval
score. To avoid overtting, we pool randomly 50% of the
observations from the 2009 Web query scores and 50% of
the observations from the 2010 Web query scores. We use
this pool to infer the expected retrieval performance of each
rhetorical relation. We repeat this randomised pooling ve
times, each time randomly pertrubing the data, producing
ve dierent sets of observations. We then use each set to
infer the expected best performing rhetorical relation per
query, in accordance to Equation 6. Following this, we use
the model introduced in section 3, Equation 2, to rank docu-
ments with respect to queries only for optimal (as inferred)
rhetorical relations. We evaluate the above method using
the same experimental settings described in section 4.1.

5.2.2 Findings
Table 4 shows the runs corresponding to the ve dier-
ent inferences of the best rhetorical relation that use our
model (optimalinf erred (1)-(5) respectively). We also report
the optimal retrieval performance actually observed in the
dataset when using the best rhetorical relation per query
(optimalobserved ). Optimal here means with respect to the
choice of rhetorical relation, not with respect to the Dirichlet
 parameter of the baseline retrieval model.
Table 4 shows that our optimised ranking model for rhetor-
ical relations is better than the baseline for any of the ve
random inferences on all three evaluation measures. The
probability of getting such a positive result by chance is25 < 0.05, and thus the improvements are statistically sig-
nicant. The improvements over the baseline are consider-
able, a very promising nding given the relatively low num-
ber of observations used for optimising the choice of rhetor-
ical relations. Experiments involving larger query sets can
be reasonably expected to perform on a par with state-of-
the-art performance.
More generally, the improvements in Table 4 signal that
rhetorical relations (derived automatically as shown in this
work) could potentially be useful features for linguistically-
uninformed learning-to-rank approaches.

6. DISCUSSION

6.1 Rhetorical Relation Distribution
The distribution of the 15 rhetorical relations we identi-
ed in our dataset is not the same for all rhetorical relations
(see Figure 2). Some types, e.g. topic-comment, tend to be
very sparse, whereas relations such as elaboration prevail.

This has no impact on the model presented ');
INSERT INTO posts (postId,userId,title,body) VALUES (274,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 12)','in section 3, but
it can bias the optimised inference of the model presented
in section 5. The lower the occurrence of a rhetorical rela-
tion in the dataset, the fewer the observations of retrieval
performance associated with it, and hence the weaker the
predictions we can infer about whether it is optimal or not.
A fairer setting would be to have the same number of query
- retrieval performance observations for all rhetorical rela-
tions - however that would imply ddling with the document
distribution of our dataset signicantly, potentially harming
its quality as a test collection.

6.2 Limitations
A general limitation of discourse analysis is that not all
types of text are susceptible to it. For instance, legal text,
contracts, or item lists often lack rhetorical structure. In this
work, we made no eort to identify and exempt such types
of text from the discourse parsing. We reasoned that, as the
SPADE parser includes a rst-step grammatical parsing, the
initial grammatical parsing of these types of text would ag
out ill-formed parts (e.g. missing a verb, or consisting of ex-
tremely long sentences), which would then be skipped by the
discourse analysis. This was indeed the case, however at a
certain eciency cost. Overall processing speed for SPADE
was approximately 19 seconds per document (including the
initial grammatical parsing), on a machine of 9 GB RAM,
8 core processor at 2.27GHz. One way of improving this
performance would be to update the rst-step grammatical
parsing. Currently this depends on the well-known Charniak
parser [6], which is one of the best performing grammatical
parsers, however no longer supported. Other state-of-the-
the Stanford parser8 ,
art faster grammatical parsers, e.g.
could be adapted and plugged into SPADE instead.
The choice of applying out model for re-ranking as op-
posed to ranking all documents was closely related to the
eciency concerns discussed above. Our model is not spe-
cic to re-ranking only, however, using SPADE on more than
50 million documents was too expensive at this point. Im-
proving the discourse parsers eciency is something we are
currently working on, with the aim to apply our model for
full ranking and see if the conclusions drawn from this work
hold.
Finally, the accuracy of the discourse parser was not con-
sidered in this work, apart from indications in the litera-
ture that SPADE is a generally well-performing parser [27].
Given that the default version of the parser we used is trained
on news articles, one may reason that its accuracy could
improve if we train it on the retrieval collection, or on doc-

8http://nlp.stanford.edu/software/lex-parser.shtml

uments of the same domain. Note that, parsing accuracy
aside, rhetorical relations assignment is not an entirely un-
ambiguous process, even to humans [23]. For the purposes of
this work, this type of ne-grained ambiguity may however
not be important to retrieval performance.

6.3 Future Extensions
Future extensions include primarily making SPADE scal-
able on large collections of documents as discussed above,
as well as using more than one rhetorical relation per docu-
ment. For instance, the posterior probabilities estimated in
section 5.1 could be used to weight the text in each rhetor-
ical relation.
If those posteriors are too at, an exponent
could make them peakier. As the exponent goes to inn-
ity, the maximum relation model presented in ');
INSERT INTO posts (postId,userId,title,body) VALUES (275,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 13)','section 5.2
would be recovered.
In addition, we intend to rene the
discourse analysis by considering the nucleus (i.e. central)
versus satellite (i.e. peripheral) rhetorical relations for IR,
as well as to improve the eectiveness of the discourse parser
by training it on data of the same domain. As discussed in
section 3.2, we will also investigate alternative estimations
of Equations 2-3.
An interesting future research direction is the potential
relation between rhetorical relations and user context: for
instance, in a search session including several query refor-
mulations, is there a correlation between the progression of
the information need of the user and the rhetorical rela-
tions that the retrieval system should boost in a document
(e.g. elaboration), as indicated by Sun '||'&'||' Chai [28]? An-
other interesting future extension of this work is in relation
to evaluation measures of graded relevance measures on an
inter-document level, as investigated in XML retrieval [20]
for instance. If parts of a document can be regarded as more
or less relevant, this may be reected to their discourse struc-
ture. This might be especially useful for multi-threaded doc-
uments, such as multiple-user reviews and opinions, where
the discourse relations tend to shift markedly. Finally, the
current operationalisation of our model is simplistic in the
sense that the term rhetorical relation is coerced into mean-
ing non-overlapping text fragment and the actual relation
between bits of text is discarded in the process. In future
work we could apply elded XML retrieval models in order
to investigate nested structuring among rhetorical relations.

7. CONCLUSIONS
Rhetorical relations, e.g. contrast, explanation, con-
dition, indicate the dierent ways in which the parts of a
text are linked to each other to form a coherent whole. This
work studied two questions: Is there a correlation between
certain rhetorical relations and retrieval performance? Can
knowledge about a documents rhetorical relations be use-
ful to IR? To address these, we presented a retrieval model
that conditions the probability of relevance between a query
and a document on the rhetorical relations occurring in that
document. We applied that model to an IR re-ranking sce-
nario for Web search. Experimental evaluation of dierent
versions of our model on TREC data and standard settings
demonstrated that certain rhetorical relations can be bene-
cial to retrieval, with >10% improvements to retrieval pre-
cision. Furthermore, we showed that these improvements
over the baseline can improve signicantly, when the opti-
mal rhetorical relation per document is selected for retrieval.
Overall, three rhetorical relations were found to benet

retrieval performance notably and consistently for dierent
evaluation measures and query sets: background, cause-
result and topic-comment. In retrospect, this is perhaps
not surprising, since these are among the most salient dis-
course relations on an intuitive basis:
the main topic or
theme of a text, its background, causes and results [21]. Fu-
ture extensions and research directions of this work include
applying our model for ranking all documents (as opposed
to re-ranking only) and experimenting with alternative esti-
mations of its components.

8. ACKNOWLEDGMENTS
We thank Kasper Hornbk, Jakob Grue Simonsen, Raf
Guns, Qikai Cheng and the anonymous reviewers for help-
ing improve this paper. Work partially funded by the Dan-
ish International Development Agency DANIDA (grant no.
10-087721) and the National Natural Science Foundation of
China (grant no. 71173164).

9. REFERENCES

[1] Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2011, 27-31 July 2011, John McIntyre Conference
Centre, Edinburgh, UK, A meeting of SIGDAT, a
Speci');
INSERT INTO posts (postId,userId,title,body) VALUES (276,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 14)','al Interest Group of the ACL. ACL, 2011.
[2] A. Azevedo-Filho and R. D. Shachter. Laplaces
method approximations for probabilistic inference in
belief networks with continuous variables. In R. L.
de Mantaras and D. Poole, editors, UAI, pages 2836.
Morgan Kaufmann, 1994.
[3] M. Bendersky, W. B. Croft, and Y. Diao.
Quality-biased ranking of web documents. In I. King,
W. Nejdl, and H. Li, editors, WSDM, pages 95104.
ACM, 2011.
[4] C. Buckley and E. M. Voorhees. Retrieval evaluation
with incomplete information. In M. Sanderson,
K. Jarvelin, J. Allan, and P. Bruza, editors, SIGIR,
pages 2532. ACM, 2004.
[5] L. Carlson, D. Marcu, and M. E. Okurowski. Building
a discourse-tagged corpus in the framework of
rhetorical structure theory. In Current Directions in
Discourse and Dialogue, pages 85112. Kluwer
Academic Publishers, 2003.
[6] E. Charniak. A maximum-entropy-inspired parser. In
Proceedings of the rst conference on North American
chapter of the Association for Computational
Linguistics, pages 132139, San Francisco, CA, USA,
2000. Morgan Kaufmann Publishers Inc.
[7] J. Clarke and M. Lapata. Discourse constraints for
document compression. Computational Linguistics,
36(3):411441, 2010.
[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke.
Ecient and eective spam ltering and re-ranking for
large web datasets. CoRR, abs/1004.5168, 2010.
[9] W. B. Croft and J. Laerty. Language Modeling for
Information Retrieval. Kluwer Academic Publishers,
Norwell, MA, USA, 2003.
[10] A. C. Davison. Statistical Models. Cambridge
University Press, New York, 2009.
[11] D. A. duVerle and H. Prendinger. A novel discourse
parser based on support vector machine classication.

Agreement among statistical signicance tests for
information retrieval evaluation at varying sample
sizes. In J. Allan, J. A. Aslam, M. Sanderson, C. Zhai,
and J. Zobel, editors, SIGIR, pages 630631. ACM,
2009.
[26] S. Somasundaran, G. Namata, J. Wiebe, and
L. Getoor. Supervised and unsupervised methods in
employing discourse relations for improving opinion
polarity classication. In EMNLP, pages 170179.
ACL, 2009.
[27] R. Soricut and D. Marcu. Sentence level discourse
parsing using syntactic and lexical information. In
HLT-NAACL, 2003.
[28] M. Sun and J. Y. Chai. Discourse processing for
context question answering based on linguistic
knowledge. Know.-Based Syst., 20:511526, August
2007.
[29] N. Suwandaratna and U. Perera. Discourse marker
based topic identication and search results rening.
In Information and Automation for Sustainability
(ICIAFs), 2010 5th International Conference on,
pages 119125, 2010.
[30] S. Teufel and M. Moens. Summarizing scientic
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409445,
2002.
[31] D. Y. Wang, R. W. P. Luk, K.-F. Wong, and K. L.
Kwok. An information retrieval approach based on
discourse type. In C. Kop, G. Fliedl, H. C. Mayr, and
E. Metais, editors, NLDB, volume 3999 of Lecture
Notes in Computer Science, pages 197202. Springer,
2006.
[32] J. Wang and J. Zhu. On statistical analysis and
optimization of information retrieval eectiveness
metrics. In F. Crestani, S. Marchand-Maillet, H.-H.
Chen, E. N. Efthimiadis, and J. Savoy, editors, SIGIR,
pages 226233. ACM, 2010.
[33] L. Wang, M. Lui, S. N. Kim, J. Nivre, and
T. Baldwin. Predicting thread discourse structure over
technical web forums. In EMNLP [1], pages 1325.
[34] L.-C. Yu, C.-H. Wu, and F.-L. Jang. Psychiatric
document retriev');
INSERT INTO posts (postId,userId,title,body) VALUES (277,9004,'Christina Lioma Computer Science University of Copenhagen Denmark c.lioma@diku.dk (part 15)','al using a discourse-aware model.
Artif. Intel l., 173:817829, May 2009.
[35] C. Zhai and J. D. Laerty. Two-stage language models
for information retrieval. In SIGIR, pages 4956.
ACM, 2002.
[36] L. Zhou, B. Li, W. Gao, Z. Wei, and K.-F. Wong.
Unsupervised discovery of discourse relations for
eliminating intra-sentence polarity ambiguities. In
EMNLP [1], pages 162171.

In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL 09, pages
665673, Stroudsburg, PA, USA, 2009. Association for
Computational Linguistics.
[12] C. J. Fillmore, C. F. Baker, and S. Hiroaki. The
framenet database and software tools. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation (LREC), pages 11571160,
2002.
[13] W. A. Gale and G. Sampson. Good-turing frequency
estimation without tears. Journal of Quantitative
Linguistics, 2(3):217237, 1995.
[14] S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli.
Shallow discourse parsing with conditional random
elds. In Proceedings of the 5th International Joint
Conference on Natural Language Processing
(IJCNLP), pages 10711079, Chiang Mai, Thailand,
2011.
[15] D. Gildea and D. Jurafsky. Automatic labeling of
semantic roles. In ACL. ACL, 2000.
[16] B. Heerschop, F. Goossen, A. Hogenboom,
F. Frasincar, U. Kaymak, and F. de Jong. Polarity
analysis of texts using discourse structure. In
Proceedings of the 20th ACM international conference
on Information and know ledge management, CIKM
11, pages 10611070, New York, NY, USA, 2011.
ACM.
[17] K. Jarvelin and J. Kekalainen. Cumulated gain-based
evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422446, 2002.
[18] P. Kingsbury and M. Palmer. From treebank to
propbank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC), pages xx, 2002.
[19] E. Krikon and O. Kurland. A study of the integration
of passage-, document-, and cluster-based information
for re-ranking search results. Inf. Retr., 14(6):593616,
2011.
[20] M. Lalmas. XML Retrieval. Synthesis Lectures on
Information Concepts, Retrieval, and Services.
Morgan '||'&'||' Claypool Publishers, 2009.
[21] A. Louis, A. K. Joshi, and A. Nenkova. Discourse
indicators for content selection in summarization. In
R. Fernandez, Y. Katagiri, K. Komatani, O. Lemon,
and M. Nakano, editors, SIGDIAL Conference, pages
147156. The Association for Computer Linguistics,
2010.
[22] R. Manmatha, T. M. Rath, and F. Feng. Modeling
score distributions for combining the outputs of search
engines. In W. B. Croft, D. J. Harper, D. H. Kraft,
and J. Zobel, editors, SIGIR, pages 267275. ACM,
2001.
[23] W. C. Mann and S. A. Thompson. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8:243281, 1988.
[24] J. Morato, J. Llorens, G. Genova, and J. A. Moreiro.
Experiments in discourse analysis impact on
information classication and retrieval algorithms. Inf.
Process. Manage., 39:825851, November 2003.
[25] M. D. Smucker, J. Allan, and B. Carterette.

');
INSERT INTO posts (postId,userId,title,body) VALUES (278,3252,'Pengda Huang','70 p  ]Is[ v11.01vXa

AbstractThe industry of wearable remote health monitoring
system keeps growing. In the diagnosis of cardiovascular disease,
Electrocardiography (ECG) waveform is one of the major tools
which is thus widely taken as the monitoring objective. For the
purpose of reducing bit expenditure in the monitoring systems,
we study the compression of ECG signal and propose a new
compressor in low complexity. Different from the traditional
ECG compressors, most of which are built on a single sensor,
our compression scheme is based on multiple ECG sensors.
The multi-sensor based compression scheme is able to provide
more accurate sensing results. Besides the investigation into the
structure of the compressor, we also jointly optimize the period
and the bit number per sample in the transmission of ECG signal.
Experiments are performed on records in MIT-BIH Arrhythmis
database and European ST-T database. Experimental results
show that our method outperforms conventional ones with
respect to ECG reconstruction accuracy at the same bit rate
consumption.

I . IN TRODUC T ION
Thanks to the development of mobile communication and
positioning technologies [1][3] in the past several decades,
remote health monitoring technology is near to practical
application in our everyday life. ECG signal is one of the
main tools of diagnosing cardiovascular diseases which are
the major mortality causes in current societies, especially
in developed countries. Remotely monitoring ECG signal
provides an effective approach to avoiding the mortality caused
by abrupt seizure of cardiovascular diseases.
Basically, in a remote monitoring system a wearable device
collects biomedical information, and transmit the collected
information to a remote data unit for prompt or delayed diag-
nosis. The remote monitoring replies on the transmission of the
bits which carry ECG signal. The bit transmission induces cost
due to consumption of resources provided by infrastructures in
mobile communication systems. For the purpose of reducing
the cost, we investigate how to reduce the cost with respect to
the two aspects, lowering the complexity of ECG compressor
and reducing the rate of bits conveying ECG signal. The cost
reduction efforts are under the prerequisite that the accuracy
of the received ECG signal at the remote data unit should be
under control and not degrade the diagnosis of cardiovascular
diseases.
In literature, a single sensor is widely used to monitor
ECG signal. In this paper, we consider multiple-sensor based
ECG compression scheme. Generally, a multiple sensors based
monitoring system provide more accurate and prompt sensing
results since the sensors equipped at different places of our
body are able to monitor the conditions of different parts of
a heart. Fig. 1 presents an example of our proposed ECG

Fig. 1. Demonstration of two ECG signal compression scheme
compressor built on two sensors, a primary ECG sensor and
a secondary sensor. As an arbitrary example shown in Fig. 1,
the secondary sensor is put on the wist which compresses
ECG signal and the transmit the compression results to the
primary sensor shown in the breast part. Battery capacity
and computation capability of the secondary sensor ar');
INSERT INTO posts (postId,userId,title,body) VALUES (279,3252,'Pengda Huang (part 2)','e at a
lower than the primary one since the targeted transmission
distance of the secondary sensor is shorter. The primary sensor
compresses and sends out the ECG signal from the secondary
one and itself to a remote data center.
No matter a single- or multiple-sensor based monitoring
system, energy consumption is widely recognized as a major
concern [4]. The energy consumption is affected by diverse
factors, such as hardware chip, circuit board design, encoder,
modulation, or even selection of radio frequency (RF) antenna.
Thus, we can hardly evaluate the energy consumption in all
terms of the mentioned and unmentioned factors. Independent
of the diverse factors, rate of the bits carrying the ECG signal
provides us an effective approach to evaluating the energy
consumption at a high level.
Single-sensor based ECG compression scheme have been
studied in [5][14]. Basically, compression methods can be
divided into to two categories, direct and differential ECG
compression methods.
Uniform quantization is basic one of the direct ECG signal
compression methods. In [8], discrete cosine transform (DCT)
was used to compress ECG signal. Similarly, DCT is also used
in ECG signal compression [9] while Huffman coding was
used to compress DCT results further. Still as a direct compres-
sion method, wavelet transform followed by run length coding
was taken to compress ECG signal in [10], [11]. Compressive
sensing was utilized to compress ECG signal in [12][14].

The compression of ECG signal is implemented in differen-
tial structures. Differential schemes built on a linear prediction
model are used to compress ECG signal in [15], [16]. Multiple
ECG samples in the past are taken to predict ECG value in one
step ahead. Then, the difference between the prediction and
its real value is quantized. In [7], [17], [18], adaptive signal
processing methods are taken to update the coefcients of the
linear prediction model.
We observe that adjacent ECG samples are not independent
to each other. The dependence means there exits redundant
information between the samples. Differential compression
can effectively reduce the redundant information. After the
redundant information reduction, less bits are needed for the
quantization. Therefore, differential ECG compression is taken
as one of the research objectives in this paper.
There is an important but not solved problem in existing
differential ECG compression methods. As we know, coef-
cients of an adaptive lter for predicting a stationary signal
do not change versus time. However, ECG signal
is not
stationary. Furthermore, ECG signals from a same person can
be signicantly different. Let us consider such a scenario that
one person, sitting on a bench for a long time, stands up to
leave. The period of R-R waves in ECG waveform will be
different before and after his or her status transition, from
sitting to walking. In this case, coefcients of the adaptive lter
for predicting his or her ECG waveform will also be different.
To keep ECG reconstruction at a high delity, the coefcients
need to be recalculated and retransmitted; otherwise, there will
be huge reconstruction error. To transmit the coefcients of
an adaptive lter, a large number of bits will be consumed
which is thus harmful to ECG transmission efciency. In [7],
[15][18], adaptive lters based differential ECG compression
sche');
INSERT INTO posts (postId,userId,title,body) VALUES (280,3252,'Pengda Huang (part 3)','mes are investigated. In their compression schemes, either
updating or transmitting coefcients of adaptive lters may
cause signicant increase of computation resources.
Different
from the existing ECG signal compression
schemes, we proposed a new structure which is built on
multiple ECG sensors. The proposed ECG compressor is at
a low complexity. More specically, the contributions in this
paper are presented as follow,
First, we investigate ECG signal compression system with
multiple sensors. Simple superposition of multiple sensors is
not considered. From a same person, the ECG signals acquired
by different ECG sensors at the same time instant usually
have similarity in waveform shapes. The similarity means the
redundant information. After realizing the signal redundancy
between ECG sensors, we design a new ECG compression
scheme which effectively saves the bits by reducing the
redundancy.
Second, we propose a novel differential ECG compression
scheme which is implemented via comparison and addition
operations, and free of multiplications. The traditional differ-
ential ECG compressors are built on adaptive lters which
rely on the updating lter coefcients and thus increase
resource consumptions. This problem does not exists in our
compression scheme. Furthermore, we optimize the codebook
used for compressing the differential ECG signal.
Third, we optimize compression ECG compression bit rates

in two dimensions, the sampling period and the number of bits
per sample. To my best knowledge, bit number per sample was
considered in literature while the joint optimization is absent.
The remainder of this paper is organized as follows. In
Section II, the potential problems of the existing compression
methods will also analyzed. In Section III, a novel ECG com-
pression scheme built on multiple sensors will be presented.
The joint optimization of bit rate over quantization level and
sampling period will be performed in Section IV. Experiments
and simulations will be presented in Section V which are
followed by conclusions in Section VI.

I I . R E LATED WORK AND POTENT IA L PROBL EM S
In this section, we investigate the potential problems in the
existing differential ECG compression schemes. Due to the
large number of existing reports on ECG compression, our
study will not cover all methods but only target at several
typical ones.

A. Open-loop Predictive ECG Compression
1) Open-loop based differential ECG compression method:
Finite impulse response (FIR) predicator was widely used in
the open-loop based ECG compression. One example of the
ECG compressors is shown in Fig. 2.

Fig. 2. Block diagram of open loop differential ECG compressor

Let x(t) denote the time continuous ECG signal to be
compressed and xi denote periodical samples of x(t), i  Z.
Assume the FIR predicator is in the order of M . Let am ,
m  {1, 2,    , M } denote coefcients of the predicator. At
the i-th time instance, estimation of ECG signal is denoted byM(cid:88)
i which is calculated as follows
m=1
Estimation error between x
i and xi is determined by
i  xi .
ei = x
In a differential ECG signal compression scheme, the esti-
mation error ei should be encoded and transmitted to a remote
receiver. The receiver decodes the codewords and obtains the
reconstruction of ei , which is denoted by ei . With ei , ECG

amxim .
i =

(1)

(2)

(3)

xi = ei +

am xim .

signal is reconstructed by

M(cid:88)
m=1
The major concern for the open-loop based differential
ECG compressor is the stability at the decoder. If errors in
quantizing ei will be accumulated, the compressor system is
unstable. Unfortunately, there was no attention paid to the
stability problem for open-loop compressors.
2) Stability of open-loop based differential ECG compres-
sion: An unstable ope');
INSERT INTO posts (postId,userId,title,body) VALUES (281,3252,'Pengda Huang (part 4)','n-loop compressor will accumulate
quantization errors which will eventually cause the failure of
ECG signal reconstruction at the decoder. Therefore, we need
to analyze the quantization error accumulation problem at the
decoder side. As dened in Section II-A1, ei is the difference
between xi and its estimation x
i . At the decoder side, the
difference between xi and its reconstruction xi is denoted byi ,
i = xi  xi .Furthermore, we dene eqi as the difference between ei and
ei ,

(4)
i

= xi 
(b)

= xi 
(a)

ximam + ei

ximam + ei  eqi

ei = ei + eqi ,
where eqi is essentially the quantization error in compressing
ei .
We can realize that e
i measures the bias of the reconstructed
ECG sample with respect to its real value. Only if e
i stays
within a small bounded range, the decoder is able to obtain
accurate ECG samples. The quantization error eqi is the factor
which may cause e
to be outside of the bounded range.Therefore, we construct e
i as a function of eqi . Via analyzing
the stability of the function, we can understand whether the
(cid:32) M(cid:88)
(cid:33)
ECG compressor is stable. The function is derived as follows,
(cid:33)
(cid:32) M(cid:88)
m=1
(cid:32) M(cid:88)
m=1
m=1
(xim  xim ) am + eqi

= xi 
(c)
M(cid:88)
M(cid:88)
m=1
m=1
where (a) follows (3); (b) follows (5); (c) follows (2); (d)
follows (1); and (e) follows (4).
We calculate z transform of (6) as follows,
1  (cid:80)M
Z{e }Z{eq } =HOLP =
m=1 am zm
where Z{} denotes the operator of Z transformation.
The stability of (7) depends on coefcients am , m  M.
Indeed, shapes of ECG waveforms will differ with different

ximam + xi  x
i  eqi
imam + eqi ,

(cid:33)

(d)
(e)
(5)

(6)

(7)

people or different health conditions. The change of ECG
waveform generates the different am . Furthermore, the incon-
sistence of am means no guarantee of the stability in (7).
Aligning with the work in literature, we consider 4-
th order FIR predictor. Under MMSE rule,
the two sets
of {am } corresponding to No. 106 and No.118 ECG
to {0.1436, 0.2120, 0.1582, 1.1548}
records are equal
and {0.2276, 0.2041, 0.2512, 1.1761} respectively. With
the calculated coefcients, HOLP |a and HOLP |b , are cor-
respondingly determined. Then, poles of
the two im-
pulse response functions are calculated which are equal to
pa = {0.9823, 0.0761  j 1.0866, 0.9908} and pa =
{0.9868, 0.1200  j 1.0856, 0.9991} respectively. From the
poles, we can easily realize that HOLP |a and HOLP |b are not
necessary to be stable which means there exists the risk of
inducing the failure of ECG reconstruction at the decoder.

B. Closed-Loop Predictive ECG Compression
From Section II-A2, open-loop differential ECG compres-
sors have the risk of being instable at the decoder. This prob-
lem can be solved by adding a feedback to the quantization
of ei .
1) Closed-loop differential ECG compression method: The
differential compressor with a feedback is called as closed-
loop differential ECG compressor. Still M denotes the order
of the linear model used to estimate the value of an ECG
sample. When M = 1, the differential compressor degenerates
into Differential pulse code modulation (DPCM).

Fig. 3. Block diagram of closed loop differential ECG compressor

Fig. 3 plots the block diagram of closed-loop differential
compressors. Compared with open-loop compressor, the major
difference in the closed-loop one is that reconstruction is
performed at the encoder side, and the reconstructed sample is
taken as a reference of modifying the threshold for quantizing
the next ECG sample.
Let xi denote the estimation of ECG sample at i-th time
M(cid:88)
instance at the encoder side which is calculated by
i=m

am xim .

xi =

(8)

The estimation bias ei is determined as ');
INSERT INTO posts (postId,userId,title,body) VALUES (282,3252,'Pengda Huang (part 5)','follows,
ei = xi  xi .
(9)
Afterwards, ei is rst quantized and the quantization result
is denoted by ei , and the quantization error is stilled rep-
resented by eqi . At the encoder side, the reconstruction of
an ECG sample, denoted by xi , is obtained by adding the
quantized ei to xi ,

(10)
xi = xi + ei .
As shown in (10), xi is feed back to the input of the linear
lter. Since xi contains the error occurring in the quantization
of the previous ECG sample, the feedback is benecial for
avoiding the accumulation of the quantization error.
2) Stability of closed-loop based differential ECG com-
pression: Let e
C i denote the difference between the ECG
sample xi and its reconstruction at the decoder. For closed-
loop compressor, the reconstructions of an ECG sample at
both the encoder and decoder are the same. Therefore, the
reconstruction at the decoder is also denoted by xi . Due to
the same reason mentioned in Section II-A2, we calculateC i as a function of eqi . Via analyze the stability of the
calculated function, we can understand whether there exists
the risk of accumulating quantization errors. The calculation
of the function e
(cid:32) M(cid:88)
(cid:33)
C i of eqi is presented as follows,
= xi 
= xi  xi
(cid:33)
(cid:32) M(cid:88)
(a)
am xim + ei
i=m
am xim + ei  eqi
(cid:32) M(cid:88)
am xim + xi  M(cid:88)
i=m
i=m
i=m

am xim  eqi

= xi 
C i

(cid:33)

(11)

= xi 
(b)

= eqi

where (a) follows (10); (b) follows (8) and (9).
From (11), ECG reconstruction error in the closed-loop
compressor is fully determined by the error in quantizing ei .
In practice, quantization error is nite in a given quantizer.
Therefore, the closed loop ECG compressor is always stable.

I I I . PRO PO SED ECG COM PR E S S ION SCH EM E BA SED ON
MU LT I P LE S EN SOR S
Besides the absence of the stability analysis of ECG com-
pressor, there is another unsolved problem in the existing
studies, that is, only signal sensor is considered to compress
ECG signal. Indeed, more sensors are able to provide more
observations on the heart conditions since ECG signals ob-
tained by sensors placed on different places of a body reect
the health conditions of different parts of a heart. Therefore,
we investigate the ECG signal compression based on multiple
sensors.
For multiple sensors, independent quantization is an inef-
cient practice since the redundancy between ECG signals from
the multiple sensors is not removed. The retaining redundant
information induces more bits for quantization. We propose a
compression method used for multiple sensors.

The multiple sensors are divided into two tiers, that is,
one primary sensor is taken as the rst tier and the all the
other sensors are at the secondary tier. The primary sensor has
more powerful computation and transmission abilities which
is responsible for remotely transmitting the ECG signal. The
secondary sensors transmit their collected ECG signal to a pri-
mary one and the transmission range is smaller than that for the
primary sensor. At the secondary sensor, conditional quantizer
is used to compress ECG signal which can effectively reduce
the redundant information. For analysis simplicity, we consider
the case with one primary sensor and one secondary sensor.

A. Structure of Multiple Sensors Based ECG Compression
Scheme
1) System Overview: Fig. 4 presents the block diagram
of the compression scheme built on the primary sensor and
secondary sensor. The secondary sensor transmits quantized
ECG signal xS to the primary one. The primary sensor
quantizes xP to obtain xP and transmits the two quantized
ECG signals ( xS and xP ) to a remote data unit. In the scheme,
waveform features of xP are priorly known by the secondary
sensor.

Fig. 4. Block diagram of double sensor quantization scheme

With the ');
INSERT INTO posts (postId,userId,title,body) VALUES (283,3252,'Pengda Huang (part 6)','waveform feature of xP , we perform conditional
quantization at the secondary sensor. The output from the con-
ditional quantizer at the secondary sensor is sent to the primary
one. The primary sensor takes differential compression scheme
to quantize ECG signal.
After introducing the functions of the modules in the
compression scheme, we present the details of how to im-
plement the differential compression method at the primary
sensor which is followed by the stability analysis. Then, the
conditional quantization at the secondary sensor is introduced.
2) Differential compression scheme at primary ECG sen-
sor: Block diagram of our proposed differential ECG com-
pressor is presented in Fig. 5. Fig. 5 (a) and (b) describe the
encoder and decoder respectively. Compared with conventional
closed-loop compressors, only addition and comparison opera-
tions are needed, and multiplication is absent in the proposed
one. Furthermore, we will illustrate our compressor outper-
forms the conventional ones in terms of ECG reconstruction
accuracy.
In the new differential compressor, the difference between
two adjacent ECG samples is rst calculated as follows,
xi = xi  xi1 .
Next, a modication factor, denoted by A, is added to
xi . The factor A is designed to counteract the accumulation

(12)

the following equation,

e(cid:48)
i = ei + eqi .
For analysis convenience, we simplify (16) into a form as
follows,

(17)

(18)

A =  (xi1  xi1 ) ,
where  is variable which absolute value is bounded into a
small range and the sign of  is opposite to the sign of (xi1 
xi1 ).
Next, we determine the expression of e
i as follows,
i = xi  xi = xi  ( xi + xi1 )= xi  (xi + A  eqi + xi1 )
(a)
= xi  (xi  xi1 + A  eqi + xi1 )
(b)
(cid:26)(1  | |) e
= (xi1  xi1 )  ( (xi1  xi1 )  eqi )
(c)
for xi1  xi1 > 0
i1 + eqi
(1 + | |) e
for xi1  xi1 < 0i1 + eqi
= (1  | |) e
i1 + eqi .
where (a) follows (17) and (13); (b) follows (12); (c) follows
(18).
The Z -transformation of (19) is written as
Z{e }(20)
Z{eq } =
1  (1  | |)z1 .
H (z ) =
From (20), the pole is equal to p = 1  | | which locates in
inner of a unit circle. Therefore, our proposed ECG processor
can avoid the accumulation of quantization error.

(19)

B. Quantizer Design in the Proposed ECG Compressor
In the optimum sense of minimizing average quantization
error at a given number of quantization levels, the statistics
of the quantization objective affects the design of an optimum
quantizer. Thus, we rst analyze the statistic features of the
differential ECG signal. Afterwards, we present the details
of how to design the differential ECG compressor. Then, the
conditional quantization by the secondary sensor is introduced.
1) Statistical Features of One-step Differential ECG Data:
There are two important issues determining statistical features
of a signal, dynamic range of source and distribution of it. We
will numerically analyze the differential ECG signal at the two
aspects using two factors
Fig. 6 shows us the dynamical ranges of differential ECG
and original ECG waveform which are calculated from 38
records in MIT-BIH database.
For each record, we calculate the maximum and minimum
values of both original and differential ECG signals. All the
extreme values are plotted in Fig. 6.
To determine the dynamical range of differential ECG sig-
nal, we rst calculate the upper bound of the maximum points
and lower bound of minimum points via linear interpolation.
Afterw');
INSERT INTO posts (postId,userId,title,body) VALUES (284,3252,'Pengda Huang (part 7)','ards, we perform curve tting on the two bounds
using two horizontal lines. The two horizontal lines label the
boundaries of the differential ECG dynamical range. The same

Fig. 5. Block digram of proposed ECG data compression scheme

of quantization errors. After the addition of A, the adjacent
difference xi is derived into x(cid:48)
i as follows,
x(cid:48)
i = xi + A.

(13)

Afterwards, x(cid:48)
is quantized and quantization index isdenoted by li . The quantization levels constitute a set ,
 = {l , l  L}, L = {1, 2, 3,    , L}, where N is equal
to the total number of quantization levels. The elements in 
are ordered incrementally. As we know, the set  has impact
on the quantization performance. The details in designing 
will be introduced in Section III-B2. After the quantizing x(cid:48)
i ,
the compressor will perform modulation according to li .
From the previous paragraph, the modication factor A is
an important parameter. Next, we discuss the details of how
to determine A.
Let  xi denote the quantized x(cid:48)
i . To reconstruct the ECG
sample at i-th time instance ( xi ) at the encoder, we add the
 xi to xi1 ,

xi = xi1 +  xi .

(14)

Since A is used to counteract the accumulation of quan-
tization error, A is designed to be a function for reducing
the quantization error at
the previous time instance. The
quantization error at the previous time instance is calculated
by
si = xi1  xi1 .

(15)

A =

To avoid the accumulation of quantization error, A is set to
be a small positive value, when si < 0; and A is a negative
value, when si  0. This correlation between A and si < 0 is

(cid:16)
(cid:17)
mathematically described by
(cid:17)
 (cid:16)
 lx(cid:48)
iflx(cid:48)i
+1  lx(cid:48)
if
si < 0
lx(cid:48)i
denotes the index of the quantized x(cid:48)
where lx(cid:48)
i withinthe ordered set , and lx(cid:48)
is the corresponding quantizationresult.
Stability Analysis: To analyze the stability of the proposed
compression scheme, we derive the compression bias e
i as a
function of the quantization error eqi due to the same reason
presented in Section II-A2. The quantization error eqi satises

si > 0

(16)

Fig. 6. Dynamical range of differential ECG and original ECG signal
method is also used to determine the dynamical range of the
original ECG signal. From the calculation, we can observe that
dynamical range of original ECG signal is approximately equal
to 6. The differential ECG data has the dynamic range from -
0.4854 to 0.6044. Since the dynamical range of the differential
ECG signal is smaller than that of the original signal, less bits
are needed for quantizing the differential ECG signal at a given
quantization accuracy.
After analyzing the dynamical range, we study the dis-
tribution of the differential ECG signal. First, we calculate
histogram of differential ECG signal which is plotted by
the blue stars in Fig. 7. With the calculated histogram, we
use the curve tting technology to abstract an approximated
probability model of the differential ECG signal.

esqQ =

x0
l =

xl =

(22)

(23)

(21)

is

f (x)dx

f (x)dx,

');
INSERT INTO posts (postId,userId,title,body) VALUES (285,3252,'Pengda Huang (part 8)','quantizer is optimized in the sense of minimum mean square
(cid:90) xl+1
L1(cid:88)
(cid:1)2
(cid:0)x  x0
of quantization error. The mean square error is calculated byxl
l=0
where L is the number of total quantization levels and x0quantization output at l-th quantization level.
We select Lloyd-Max algorithm [19] to determine each
quantization zone (xl , xl+1 ) and the value of quantization
output x0
l . According to Lloyd-Max algorithm, the parame-
(cid:82) xl+1
ters are iteratively calculated as follows
(cid:82) xl+1
xf (x)dx
xl
xl
l + x0
x0
l+1In a partial summarization of the quantization on the pri-
mary sensor, the histogram of the rst order differential ECG
signal is calculated rst; second, via curve tting, a PDF in
an explicit form is calculate to approximate the histogram;
third, the number of the bits for the quantization is determined;
fourth, the codebook and quantization zones are determined
according to (22) and (23) respectively.
3) Quantization on the Secondary ECG Sensor: The pro-
posed differential ECG compression method at the primary
sensor achieves the bit rate saving by reducing the redundancy
between ECG samples from a same sensor. Besides,
the
redundancy within the ECG samples from a single sensor,
there also exits inter-sensor redundancy which can be observed
from the waveform similarities between the ECG signals from
different sensors. Without loss of generality, No. 100 ECG
recording in MIT-BIH arrhythmia database is plotted in Fig. 8
which is taken as an example of showing the existence of
inter-sensor redundancy. We will reduce to the inter-sensor
redundancy to save the bit rate for the quantization on the
secondary sensor.

Fig. 7.
functions

Approximation of histogram via curve tting using exponential

Let f (x) denote the probability. An exponential function
with peak clipping is used to represent the differential ECG
histogram. The red bold curve in Fig. 7 plots the probability
function f (x) which takes a form of exponential function
with the exponent of -50. With the calculated probability
model, we analytically study the quantizer design in the
following part.
2) Quantization of differential ECG at primary sensor:
Assume the quantizer in our analysis is labeled by Q. The

Fig. 8. Number 100 ECG recording in MIT-BIH Arrhythmia database

The ECG data collected by the primary sensor is denoted
by xP , and xS is for the data from the secondary sensor. Let
fP () and fS () denote the approximated PDFs from xP and
xS respectively. At the secondary sensor, the approximated
PDF of xP is priorly known. The quantization based on the
prior information is denoted by Q(xS |fP ).
Since there exists the connection between the waveforms
of xP and xS , we build an afne relation between fP () and

fS (). The afne is denoted by (cid:122) which is determined as
follows,

(24)

(cid:122) = {(a, b)| min(fP  a  fS  b)2},
where a and b are constants for a group of ECG data from a
same person, such as the group consisting of xP and xS .
With the established afne relation, the conditional quan-
tization and codebook can be calculated according to the
following four steps:
 First, a small number of bits, which number is denoted
by N1 , are used to quantize the support area of fS ().
Since bits number is smaller, the quantization bins are
1i denote the i-th quantization
sparse in the step. Let bS
1i 
bin. Boundaries of bS
1i are xS
1i and xS
1(i+1) , where xS
1(i+1) and i  {0, 1, 2,    , 2N1 }.
1i < xS
bS
 We calculate the boundaries xP
i from xS
i according to the
(cid:122) afne dened in (24). More explicitly, the calculation
is presented below

i = a  xS
(25)
xP
1i + b.
1i , i  {0, 1, 2,    , 2N1 }, we utilize
 Within each bin of bS
Lloyd-Max algorithm to calculate a sub-codebook which
is de');
INSERT INTO posts (postId,userId,title,body) VALUES (286,3252,'Pengda Huang (part 9)','noted by cS
i . Let N2 denote the number of bits used
in the sub-level quantization.
 Using the calculated sub-codebooks, we quantize ECG
1i , i  {0, 1, 2,    , 2N1 }. The
signal within all bins of bS
corresponding quantization indexes, denoted by I s , are
the nal outputs of the compressor on the secondary
sensor.
To assist our explanation, Fig. 9 presents a toy example of
the conditional quantization method. In Fig. 9, the rst three
segments labeled with 1, 2, and 3 constitute a set. Near to the
rst set, the 5 numbers ({1, 2, 3, 4, 5}) labeled ve segments
form the second set. Beside the second one, the third set is
constituted in the same way. Each of three sets ({1, 2, 3},
{1, 2, 3, 4, 5} and {1, 2, 3, 4, 5}) covers the range of a bS
1i ,
i  {1, 2, 3}, and all the three sets cover the full dynamic
range of xS without overlapping. In each bin of bS
1i , sub-
codebook is calculated following the third step above. Then,
the quantization is performed in each bin according to the
calculated codebook and the numbers noted in Fig. 9 are the
nal results of the compression on the secondary sensor.

Fig. 9. Demon of compression on secondary ECG sensor

After the conditional quantization,
the secondary sensor
sends the quantization results to the primary sensor where the

reconstruction is performed. To reconstruct ECG signal at the
decoder, two steps are needed.
i+1 ), 0  i  2N1 , we
 Among the set of sections [xP
i , xP
determine which section xP belongs to. For example, xP
k  xP < xP
belongs to the k-th section, xP
k+1 .
 We take the k-th sub-codebook cS
k to determine the
reconstruction corresponding to the quantization index I s .
Based on the description above, we can realize that there are
N2 bits used in the compression at the secondary sensor. In the
experiments of this paper, we will show that N2 is smaller than
the number of bits for direction quantization, N2 < W , where
W denote the number of the bits used for direct quantization.
In this section, the structure of implementing the proposed
ECG compression scheme is presented which is at low com-
plexity and thus easy to be implemented at less hardware
resource cost. Furthermore, the new compression scheme saves
the consumed bits per sample. Indeed, besides the bits per
sample, the sampling rate will also affect the accuracy of ECG
compression and the hardware resource consumption. In the
next section, we perform joint optimization with respect to the
two aspects.

IV. TWO D IM EN S IONAL B I T RAT E O PT IM I ZAT ION
In a remote health monitoring system, there are two factors
signicantly affecting the complexity and power consumption,
the quantization bits per sample and and transmission period.
Let W denote the average number of the bits used for
quantizing each sample, and Tt is the period of transmitting
the compressed ECG data. The bit rate r is calculated by
. In the optimization, our objective is to minimize
r = W
Tt
average square error of the reconstructed ECG signal. The
minimization is under the constraint of a given bit rate. With
respect to the single dimension of bits number, the related
minimization work refers to [20] The theoretic knowledge
about the two-dimension optimization refers to [21][23].
For an ECG sensor, let T0 denote the minimum sampling
period. After the sampling, the ECG signal written as x(mT0 ),
m  Z. In practice, the sampling frequency 1
is over high for
T0
ECG signal. Thus, the ECG signal tor be transmitted should
be down sampled. Let K denote the down sampling rate. After
the down sampling, the ECG signal is transmitted. Therefore,
we can realize that Tt = K T0 . Essentially, the optimization in
this section is performed with respect to K and W .

A. Calculation of Bounds on Tt and W
1) Upper bound on Tt : In general cases, upper bound on
sampling period is determined according to Nyquist sampli');
INSERT INTO posts (postId,userId,title,body) VALUES (287,3252,'Pengda Huang (part 10)','ng
theorem. For sampling ECG signal, there are some differences.
ECG data is usually taken to assist diagnosis of cardiovas-
cular diseases. In a heart beat period, a ECG signal consists
of different waves, such as P wave, QRS wave and T wave.
These waves provide assisting information for diagnosing
different diseases. For example, ST segment depression or
elevation accompanying with T wave inversion is used to
diagnose myocardial infarction and cardiogenic shock. QRS
voltage, ST-T wave, and R-wave changes are used to diagnose
Cardiomyopathy.

We can easily realize that ner sampling is able to keep
more information of ECG waveform. In general cases, dura-
tions of different waves are not the same in a ECG waveform.
The wave having the smallest duration is most sensitive to
sampling period. According to our observations, either Q-
R segment or R-S segment has the smallest duration. Time
interval between Q and R is denoted by tQR . And interval
between R and S is tRS . To avoid information loss of QRS,
we need to guarantee the smaller one between tQR and tRS
is larger than the sampling period, min{tQR , tRS }  Tt .
As mentioned before, ECG signals signicantly changes
for different people and different health conditions. Thus, we
still use numerical method to investigate the smallest average
duration of tQR and tRS . In the numerical analysis, we use
ECG data published by Michael Oeff [24] for higher accuracy.
The ECG data in [24] are sampled by 16 bits at the frequency
of 10kHz. We estimate the durations of tQR and tRS of ECG
data from 549 persons. According to our calculation, average
value of tRS is smaller than that of tQR . Furthermore, from the
549 recordings, the smallest tRS is equal to 56.9ms. Therefore,
we need to guarantee sampling period Tt to provide the time
resolution smaller than 56.9ms. Since sampling period of Tt
2 , the upper bound on Tt is 113.8ms,
generates resolution of Tt
t = 0.1138s.
T U
2) Lower Bound on T0 : The ner sampling generates the
more accurate ECG signal while more hardware resources
are consumed. In the joint optimization on bit rate, smallest
sampling period at ADC is considered as the lower bound on
T0 . In our analysis, 1/360s is taken as the low bound on Tt ,
360 s.
t = 1
T L
3) Upper Bound on W : Larger bit width means ner
quantization which provides more accurate description on
ECG amplitude. For a given wearable device, bit width W
is upper bounded by the implementable largest number of
quantization levels. The number of largest bits varies for
different wearable devices. We consider 12 as the upper bound
on W , W U = 12.
4) Lower Bound on W : As introduced in Section IV-A1,
waves in a beat rate period of ECG signal are used in diag-
nosing different types of diseases. These waves have different
sensitivities to bit number. To determine lower bound on bit
width W , we need to nd out the wave which has the smallest
peak average power ratio.
We still use data from [24] in the analysis of bit width lower
bound. There are four steps in the calculation.
First, we select the ECG signals in which all waveform fea-
tures can be observed by a doctor in medicine. The waveform
features includes P, Q, R, S and T-waves.
Next, we measure the waves summit-to-average distance,
which are denoted by g ( includes an arbitrary member of
the alphabet group {P , Q, R, S }). Essentially, g is equal to
the distance between the locally maximum point of each wave
to the base of ECG signals. Companying with each element
of g , an envelope amplitude (distance between upper and
lower ');
INSERT INTO posts (postId,userId,title,body) VALUES (288,3252,'Pengda Huang (part 11)','envelope of a ECG recording) is measured. We use 
to denote the envelope amplitude companying with g .
Third, at each ECG recording, we calculate the ratio of g
over its corresponding  . The ratio is denoted by k , k  K

where |K| is equal to the number of all calculated ratios.
Finally, the k which has the smallest absolute value is
selected to help us determine the lower bound on W . Let W L
2W U +1 
denote the lower bound. We select W L such thatmin(|k |). According to our calculation, the lower bound on
W is equal to 4, W L = 4.

x(nK T0 )

x(mT0 ) =

B. Joint Optimization on Bit Rate
At the wearable device, the quantized ECG data are trans-
mitted to a data server for storage and analysis. We assume
the time interval Tt
to K T0 , Tt = K T0 . After
is equal
receiving the quantized data, the data server reconstructs ECG
signal. The reconstructed ECG data is denoted by x which is
(cid:20)
(cid:21)
+(cid:88)
calculated as follows,
u((m  nK )T0 )
u((m  (n + 1)K + 1)T0 )
n=
(26)
With the reconstructed ECG data x, we evaluate the recon-
struction accuracy in terms of average square error which is
2(cid:88)
denoted by .  is calculated bym= M
2 +1
In the ECG data compression and transmission system, the
bit rate budget is R which is essentially a upper bound on the
actual bit rate r , that is,

(x(mT0  x(mT0 )))2 .

 = lim
1
(27)

r =

(28)

 R.

limM 1
W
Tt
Under the constraint shown in (28), we minimize average
square error in reconstructing the ECG signal. The optimiza-
(cid:80) M
tion problem is formulated as
(c(mT0  c(mT0 )))2m= M
2 +1
 RK T0

minimize:
W,K
subject to:

(29)
In the optimization shown in (29), the variables include
the average quantization number per sample (W ) and the
transmission period Tt = K T0 . Numerical methods are used
to solve the optimization problem. Fig. 10 presents an example
of solving optimization problem.
In Fig. 10 the colorful curves are contour of . The contours
are plotted within a red dash rectangular. The boundaries of
the rectangular is formed by the calculated bounds on W and
Tt . The darker color means the smaller . The slope of a dot
dash black lines are equal to an ideal bit rate budget R. A line
with markers indicates the actual bit rate. Since W and K are
both in discrete values, the actual bit rate lines with markers
can hardly exactly match the ideal lines.
There two steps to determine the minimum MSE under the
constraint of R. We rst draw a bit rate budget line with the
slope of R. Next, we nd the contour curve which is tangent
to the bit rate budget line. Then, the contour curve tangent to
the budget line informs us the minimum MSE achievable at
the bit rate of R.

TABLE I
COM PAR I SON IN COM PUTAT ION COM P LEX I TY
Mul.
Addl.
(/sample) Memory U.
(/sample)5LMS5LMS (no. coef.)
391601
DCT
1216
143
68
Wavelet+SPIHT2Delta modulator
2022
183
212
Compressive Sensing3New method
of basis vectors. The volume of required multiplications is
related to the length of a ECG segment. Besides the extensive
demand on multipliers, a large number of memory units are
also needed. LMS based compressor has lower computation
complexity than the previous two compressors. From Table I,
we can easily nd that both DPCM based compressor and our
method can be implemented in low complexity. Different from
Delta modulator based compressor, our method does not need
multiplication operations.
To present an intuitive impression on the performance of
compression algorithms, we present the reconstructed ECG
waveforms by all the mentioned algorithms. Due the page
limits, the graphic performance comparison is performed on
two ECG records, No. 112 record in MIT-BIH database and
No. 103 record in European ST-T database. The reconstruction
accura');
INSERT INTO posts (postId,userId,title,body) VALUES (289,3252,'Pengda Huang (part 12)','cy comparison for the two records are presented in
Fig. 11 and Fig. 12 respectively. The computation is the
compressions are performed in 8-bits numbers.

Fig. 11. Comparison of the reconstructions of No. 112 record in MIT-BIH
arrhythmia database

From Fig. 11, the reconstruction ECG via the DCT based
method retains the key features, such as P, Q, R, S, and
T waves. Wavelet based compression incurs some noise-like
distortion. In Fig. 11 (d), coefcients of adaptive lter are
not updated and we can observe ECG waveform distortion.
In Fig. 11 (e), the coefcients are adaptively updated which
generate satisfying reconstruction accuracy. However, the bits
used for updating coefcients are in a large number. The
accuracy of the reconstructed ECG signal from the compres-

Fig. 10. ECG reconstruction ASE versus word length in bits and transmission
period in second
V. EX PER IM EN TA L V ER I FICAT ION S
In this section, experiments are performed to evaluate the
effectiveness of the proposed method. We compare our method
with existing ones in three aspects, computation complexity,
ECG waveform distortion after reconstruction, and the ef-
ciency of compression method in saving bits. Second, we
investigate the performance of the conditional quantization at
the secondary ECG sensor. Finally, experiments on joint bit
rate optimization are performed.
In the experiments, our objective is to evaluate the effective-
ness of proposed ECG compression method. In our knowledge,
the compression is not as sensitive to the change of wave-
form shapes as the algorithms for R-wave detection or other
cardiovascular disease diagnosis. Therefore, the experiments
are performed on the data from only two databases, MIT-BIH
Arrhythmia database [24] and European ST-T database [25].

A. Complexity and Reconstruction Accuracy Comparison
In this subsection, we investigate the performance of the
proposed differential ECG compression method. To evaluate
the performance, we compare our method with the ones in
literature. The ECG compression schemes based on DCT
and wavelet are considered since they are widely adopted
in ECG compression. Different from pure wavelet algorithm,
wavelet compression by the set partitioning in hierarchical
trees algorithms is implemented. Since compressive sensing is
widely discussed and applied, ECG signal is compressed via a
compressive sensing algorithm. We also evaluate performance
of two differential ECG compression methods, least mean
square (LMS) based compression algorithm and DPCM based
one.
Table I presents computation complexities of different ECG
compression methods. Average numbers of multiplications,
additions per data sample and required memory units are
taken as the metrics. From Table I, DCT and wavelet based
compressors need a large number of multiplications and mem-
ory units. The large number of multiplications are induced
by the multiplication between ECG signal vector and groups

Fig. 12. Comparison of the reconstructions of No. 103 record in European
ST-T database
sive sensing method is high while the computation burden is
heavy. The key features of ECG signal can also be observed in
DPCM based compressor (Fig. 11 (g)). However, we can nd
unexpected uctuations between R and S. DPCM quantizes
the error occurring estimating current ECG va');
INSERT INTO posts (postId,userId,title,body) VALUES (290,3252,'Pengda Huang (part 13)','lue. When ECG
waveform changes fast, such as in the segment between R and
S, DPCM is not able to keep tracking of the fast change. Thus,
the unexpected uctuations occur. Our method quantizes the
ECG amplitude change directly. Thus, our method is more
robust to the fast change. The similar phenomena can also be
observed in Fig. 12.

Fig. 13. MSE versus average number of bits per sample for records in
MIT-BIH arrhythmia database

Fig. 11 and Fig. 12 illustrate the reconstructed ECG wave-
form at a xed bit width of 8. Furthermore, we present the
normalized MSE of the reconstructed ECG at different bit
widths in Fig. 13 and Fig. 14 which curves are calculated from
the records in MIT-BIH Arrhythmia database and European
ST-T database respectively. From Fig. 13, wavelet based com-
pression induces the worst reconstruction accuracy. When 4
coefcients of LMS lters are not transmitted from a compres-
sor, MSE does not decrease with bit number increasing. When

Fig. 14. MSE versus average number of bits per sample for records in
European ST-T database
the coefcients are transmitted, MSE decreases signicantly
with bit width increasing. However, the coefcient updating
requires more bits. Our new method achieve the smallest MSE
at a given low bit rate. The advantage of the new method over
the existing ones can also be observed in Fig. 14.

B. Simulation in Double Sensors Based ECG Compression
In this subsection, we investigate the performance of double
sensors based ECG compression method. As discussed in
previous sections, the distribution of ECG signal from pri-
mary sensor is priorly known by the secondary sensor. Thus,
conditional quantization can be performed at the secondary
sensor. The quantization results are transmitted to the primary
sensor via a perfect channel. The primary sensor differentially
quantize ECG signal acquired by itself. The results of condi-
tional quantization and differential quantization are transmitted
to a remote data center. At the data center, the ECG signal
acquired by the primary sensor is rst reconstructed. Then,
reconstruction of the ECG signal from the secondary sensor
is performed.
The combination of differential quantization at primary
sensor and the conditional quantization at the secondary sensor
is called as hybrid quantization structure. For comparison, we
also consider other two quantization structures in the double
sensors based compression. First, differential quantization is
applied in both primary and secondary sensors. Second, differ-
ential quantization is taken at the primary sensor and uniform
quantization used at the secondary one. The average MSE
of the reconstructed ECG at both the primary and secondary
sensors is taken as the accuracy metric.
The results of the two sensors based ECG compression are
presented in Fig. 15. From the results, the proposed hybrid
quantization method outperforms other two, the conventional
quantization on original ECG signal (labeled as uniform
quantization) and the two-independent-differential ECG quan-
tization (labeled as twice diff. str). The advantage of the
proposed two sensors based compression scheme is caused by
the reduction of redundant information between ECG signal

of the joint optimization on saving bit rate can be clearly
observed.

V I . CONC LU S ION
We investigate the compression of ECG signal which is im-
portant for saving hardware and power consumption in health
telemonitoring systems. Different from the ECG compression
work in literature, compression scheme based on multiple ECG
sensors is co');
INSERT INTO posts (postId,userId,title,body) VALUES (291,3252,'Pengda Huang (part 14)','nsidered. Without loss of generality, we consider
an example in which there are two ECG sensors, a primary and
a secondary ECG sensor. At the primary one, we use a novel
differential structure to compress ECG signal which effectively
reduces the redundant
information between adjacent ECG
samples. At the secondary ECG sensor, conditional quantizer
is proposed to compress ECG signal which utilizes the inherent
connection between the shapes of ECG signals from the two
sensors. Experiments verify the advantage of our proposed
compression scheme both in complexity and reconstruction
accuracy.

R E F ER ENC E S

[1] Y. B. Lin, Y. W. Lin, C. M. Huang, C. Y. Chih, and P. Lin, Iottalk: A
management platform for recongurable sensor devices, IEEE Internet
of Things Journal, vol. PP, no. 99, pp. 11, 2017.
[2] P. Huang and Y. Pi, An improved location service scheme in urban
environments with the combination of gps and mobile stations, Wireless
Communications and Mobile Computing, vol. 14, no. 13, pp. 1287
1301, 2014. [Online]. Available: http://dx.doi.org/10.1002/wcm.2232
[3] , Wireless internet assisting satellite position in urban environ-
ments, in 2011 6th International ICST Conference on Communications
and Networking in China (CHINACOM), Aug 2011, pp. 262267.
[4] J. Pandey and B. Otis, A sub-100 w mics/ism band transmitter based
on injection-locking and frequency multiplication, Solid-State Circuits,
IEEE Journal of, vol. 46, no. 5, pp. 10491058, May 2011.
[5] J. Ma, T. Zhang, and M. Dong, A novel ecg data compression method
using adaptive fourier decomposition with security guarantee in e-health
applications, IEEE Journal of Biomedical and Health Informatics,
vol. 19, no. 3, pp. 986994, May 2015.
[6] T. Marisa, T. Niederhauser, A. Haeberlin, R. A. Wildhaber, R. Vogel,
M. Jacomet, and J. Goette, Bufferless compression of asynchronously
sampled ecg signals in cubic hermitian vector space, IEEE Transactions
on Biomedical Engineering, vol. 62, no. 12, pp. 28782887, Dec 2015.
[7] C. Deepu and Y. Lian, A joint QRS detection and data compression
scheme for wearable sensors, Biomedical Engineering, IEEE Transac-
tions on, vol. 62, no. 1, pp. 165175, Jan 2015.
[8] A. Bendifallah, R. Benzid, and M. Boulemden, Improved ECG com-
pression method using discrete cosine transform, Electronics Letters,
vol. 47, no. 2, pp. 8789, January 2011.
[9] A. Bilgin, M. Marcellin, and M. Altbach, Compression of electro-
cardiogram signals using JPEG2000, Consumer Electronics, IEEE
Transactions on, vol. 49, no. 4, pp. 833840, Nov 2003.
[10] Z. Lu, D. Y. Kim, and W. Pearlman, Wavelet compression of ECG sig-
nals by the set partitioning in hierarchical trees algorithm, Biomedical
Engineering, IEEE Transactions on, vol. 47, no. 7, pp. 849856, July
2000.
[11] Y. Zou, J. Han, S. Xuan, S. Huang, X. Weng, D. Fang, and X. Zeng, An
energy-efcient design for ECG recording and R-peak detection based
on wavelet transform, Circuits and Systems II: Express Briefs, IEEE
Transactions on, vol. 62, no. 2, pp. 119123, Feb 2015.

Fig. 15. Bit rate in ECG compression on double sensors

from the two sensors.

C. Simulation in Joint Bit Rate Optimization
Until now, the simulations are performed ');
INSERT INTO posts (postId,userId,title,body) VALUES (292,3252,'Pengda Huang (part 15)','in the precon-
dition that the sampling period is xed. In this subsection,
we investigate the performance of joint bit rate optimization
over quantization bit number and transmission period. As a
comparison, quantization results are transmitted at a period of
1/360s which is the same with the sampling period for the
records in MIT-BIH database. Via interpolation, the equivalent
sampling at 360H z is also performed on the data records for
European ST-T database.

Fig. 16.

Joint bit rate optimization

Fig. 16 plots MSE-bit rate curves with and without the
joint optimization. The two dimensional optimization is con-
strained by the bounds both in sampling period and quantiza-
tion bits per sample. These bounds, which are calculated in
Section IV-A, enable us guarantee key features of ECG signal
can be retained in the compression. From Fig. 16, advantage

[12] L. Polania, R. Carrillo, M. Blanco-Velasco, and K. Barner, Exploit-
ing prior knowledge in compressed sensing wireless ECG systems,
Biomedical and Health Informatics, IEEE Journal of, vol. 19, no. 2,
pp. 508519, March 2015.
[13] V. Cambareri, M. Mangia, F. Pareschi, R. Rovatti, and G. Setti, A
case study in low-complexity ecg signal encoding: How compressing is
compressed sensing? IEEE Signal Processing Letters, vol. 22, no. 10,
pp. 17431747, Oct 2015.
[14] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst, Com-
pressed sensing for real-time energy-efcient ecg compression on wire-
less body sensor nodes, IEEE Transactions on Biomedical Engineering,
vol. 58, no. 9, pp. 24562466, Sept 2011.
[15] U. E. Ruttimann and H. V. Pipberger, Compression of the ECG by pre-
diction or interpolation and entropy encoding, Biomedical Engineering,
IEEE Transactions on, vol. BME-26, no. 11, pp. 613623, Nov 1979.
[16] C.-C. Sun and S.-C. Tai, Beat-based ECG compression using gain-
shape vector quantization, Biomedical Engineering, IEEE Transactions
on, vol. 52, no. 11, pp. 18821888, Nov 2005.
[17] S.-L. Chen and J.-G. Wang, VLSI implementation of low-power cost-
efcient lossless ECG encoder design for wireless healthcare monitoring
application, Electronics Letters, vol. 49, no. 2, pp. 9193, January 2013.
[18] G. Einarsson, An improved implementation of predictive coding com-
pression, Communications, IEEE Transactions on, vol. 39, no. 2, pp.
169171, Feb 1991.
[19] S. Lloyd, Least squares quantization in PCM, Information Theory,
IEEE Transactions on, vol. 28, no. 2, pp. 129137, Mar 1982.
[20] P. Huang and D. Rajan, Estimation of centralized spectrum sensing
overhead for cognitive radio networks, in 2014 IEEE 25th Annual
International Symposium on Personal, Indoor, and Mobile Radio Com-
munication (PIMRC), Sept 2014, pp. 659663.
[21] , Bounds on the overhead of spectrum sensing in cognitive radio,
in 2014 IEEE Global Communications Conference, Dec 2014, pp. 846
850.
[22] P. Huang, Y. Du, and Y. Li, Stability analysis and hardware resource
optimization in channel emulator design, IEEE Transactions on Circuits
and Systems I: Regular Papers, vol. 63, no. 7, pp. 10891100, July 2016.
[23] P. Huang, W. Wang, and Y. Pi, Estimation on channel state feedback
overhead lower bound with consideration in compression scheme and
feedback period, IEEE Transactions on Communications, vol. 65, no. 3,
p');
INSERT INTO posts (postId,userId,title,body) VALUES (293,3252,'Pengda Huang (part 16)','p. 12191233, March 2017.
[24] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. C.
Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and
H. E. Stanley, PhysioBank, PhysioToolkit, and PhysioNet: Components
of a new research resource for complex physiologic signals, Circu-
lation, vol. 101, no. 23, pp. e215e220, 2000 (June 13), circulation
Electronic Pages: http://circ.ahajournals.org/cgi/content/full/101/23/e215
PMID:1085218; doi: 10.1161/01.CIR.101.23.e215.
[25] , Physiobank, physiotoolkit, and physionet, Circulation, vol.
101, no. 23, pp.
e215e220, 2000.
[Online]. Available: http:
//circ.ahajournals.org/content/101/23/e215

');
INSERT INTO posts (postId,userId,title,body) VALUES (294,736,'Bag-of-Words Method Applied to Accelerometer Measurements for the Purpose of Classication and Energy Estimation','Kevin M. Amaral
PhD. Student
Computer Science
University of Massachusetts Boston

Ping Chen, PhD.
Associate Professor
Computer Science and Engineering

University of Massachusetts Boston

Scott Crouter, PhD.
Assistant Professor
Exercise Physiology
University of Tennessee-Knoxville

Wei Ding, PhD.
Associate Professor
Computer Science
University of Massachusetts Boston

April 2017

1 Abstract

Accelerometer measurements are the prime type of sensor information most
think of when seeking to measure physical activity. On the market, there are
many tness measuring devices which aim to track calories burned and steps
counted through the use of accelerometers. These measurements, though good
enough for the average consumer, are noisy and unreliable in terms of the pre-
cision of measurement needed in a scientic setting. The contribution of this
paper is an innovative and highly accurate regression method which uses an in-
termediary two-stage classication step to better direct the regression of energy
expenditure values from accelerometer counts.
We show that through an additional unsupervised layer of intermediate fea-
ture construction, we can leverage latent patterns within accelerometer counts to
provide better grounds for activity classication than expert-constructed time-
series features. For this, our approach utilizes a mathematical model originat-
ing in natural language processing, the bag-of-words model, that has in the
past years been appearing in diverse disciplines outside of the natural language
processing eld such as image processing. Further emphasizing the natural lan-
guage connection to stochastics, we use a gaussian mixture model to learn the
dictionary upon which the bag-of-words model is built. Moreover, we show that
with the addition of these features, were able to improve regression root mean-

squared error of energy expenditure by approximately 1.4 units over existing
state-of-the-art methods.

Introduction

2.1 Background and Related Work

In 2005, Crouter et. al.
introduce the two-regression model which alternates
between a quadratic regression model and a linear regression model based on
the coecients of variations of each bout.
[3] This novel approach broke the
overall problem ob jective into two key parts: rst, separating instances by their
variability into two groupings based on their coecients of variation; second,
applying to each grouping a regression model which is more appropriate for
instances of that variability.
In 2012, Trost et. al. was able to improve physical activity classication
accuracy as well as low root mean squared-error (RMSE) in energy expenditure
estimation with an Articial Neural Network (ANN) model. [7]
In that same year, Mu et. al., revisited the two-regression model of Crouter
et. al. and extended it to a number of regression models, one per each activ-
ity type.
[5] The data used in this study including each activity bout therein
was structured rather variably, which made it analogous to a free-living data
collection. This method utilized distance metric learning methods to learn the
underlying block structure of variable-length activity bouts.
In 2014, Staudenmayer et. al. expanded on the eld with another ANN
model which they applied to their own dataset. [6] However, their classication
procedure was targeting learned activity types, as opposed to expert-dened
types. They produced these types through clustering based on their signal
activity levels.
In 2015, Montoye et. al. did an analysis of accelerometer placement for the
purpose of energy expenditure estimation and found in their results that the
thigh-mounted accelerometer produced the most accurate measurements of all
considered mount-points. [4]
Bastion et. al.');
INSERT INTO posts (postId,userId,title,body) VALUES (295,736,'Bag-of-Words Method Applied to Accelerometer Measurements for the Purpose of Classication and Energy Estimation (part 2)',' published an evaluation of cutting-edge methods outside of
the rigid laboratory setting and conrmed the activity classication commu-
nitys suspicions that existing methods would not perform well in the free-living
setting. [1]

3 Methods

For our experiments, we utilized a subset of the dataset used in [5] whose activ-
ities most-closely resembled those of [7]. In total, one hundred and eighty-four
(184) child participants data were used. For each of these participants, one
bout of lying resting for up to thirty (30) minutes with a median time of sev-
enteen (17) minutes. All other activities were performed for up to twelve (12)
minutes with a median time of four (4) minutes.

Sedentary

Light Household and Games

Moderate-Vigorous Household and Sports

Walk

Run

Lying Rest
Playing Computer Games
Reading
Light Cleaning
Sweeping
Workout Video
Wall Ball
Playing Catch
Brisk Track Walking
Slow Track Walking
Walking Course
Track Running

Table 1: Activity Classes and the types of activities performed within them.

In Table 1, we list the types of activities which have been included in our
experiment.
In the left column, we have the activity classes of which each
activity bout in the dataset only corresponds to one. On the right, are the more
specic activity types that each class consists of.
In Table 2, the number of bouts associated with each activity class is listed,
as well as the total number of intervals in each class.

Sed. LHH MtV Walk Run

Bouts

259

116

79

Intervals

16475

2505

1570

150

3775

23

485

Table 2: Dataset Summary

3.1 Classication Model

Articial neural networks are the state of the art method for activity classi-
cation. Trost et. al. used in their work a feed-forward neural network with a
single hidden layer to predict MET values directly [7]. Staudenmayer et. al.
also use an articial neural network as their model [6], in the rst step to predict
the physical activity type and then afterwards separately to predict the MET
values. Our method improves on the ideas of the two-regression framework by
expanding the number of regression models to one per each activity type and
leverages the model from Staudenmayers and Trosts works as our frameworks
nal classication component.
Our classication model is a three-stage framework which consists of a clus-
tering phase over the activity windows, a bag-of-words construction phase for
each unique activity bout, and a neural network classication phase over the
new bag of words features.

The Bag-of-Words structure within the framework brings with it its repre-
sentation power from the eld of Natural Language Processing, and integrating
it into our activity classication and energy expenditure estimation framework
bridges the gap between these two disciplines.

Figure 1: Classication Framework Diagram

3.1.1 Time Series Features - Atom Construction

The standard feature construction seen in the state of the art for physical ac-
tivity time series has been to use percentile features. These feature');
INSERT INTO posts (postId,userId,title,body) VALUES (296,736,'Bag-of-Words Method Applied to Accelerometer Measurements for the Purpose of Classication and Energy Estimation (part 3)','s represent
the time series signal by their moments, more specically their 10th, 25th, 50th,
75th, and 90th percentiles. [6] [7]. We will start with these features and include
lag-k autocorrelation features in our initial feature construction phase.
For the instance construction, we favor segmentation of the signal into 12-
second blocks. This allows us to best manage the inconsistent bout lengths in
our dataset.
In the work of Staudenmayer et. al., their activity bouts were
all of a xed-length of 10 minutes. Trost et. al.s dataset considered for xed-
length bouts of 2 minutes, however they opted to increase their data resolution
by further partitioning their bouts into 10, 15, 20, 30, and 60 seconds for their
experiments. In line with Trost et. al.s work, our narrower windows allow us

to tightly capture the volatile patterns of child physical activity, whereas wider
windows would be too long.
In either case, those works have a data setting which we do not have: xed-
length activity bouts. In later subsections of Section 3, we show how were able
to overcome this challenge within our dataset through our intermediate feature
construction phase to produce a xed-length data instances.

3.1.2 Clustering Phase - Atom Classing

In Staudenmayer et. al., they cluster activity instances based on their signals to
produce their activity classes. [6] However, we do not use the clustering phase
alone to determine physical activity classes. We reject the idea that classifying
the signal alone will give us the true activity class. Each performance of an
activity diers and any one moment spent idle or performing the activity in a
non-standard way will greatly aect the signal as a whole.
Again, we consider brief windows of 12-seconds to be characteristic of atomic
micro-performances within an activity. This is in line with previous work in Mu
et. al. [5] in which we considered the block structure of the timeseries signal. By
contrast, however, we are considering much smaller blocks which are expected
to lose their homogeneity with the rest of the signal, as opposed to the 1-minute
windows used in that paper. One minute of an activity may look like any other
minute of the same activity but as we choose this ner resolution, each 12-second
window will be more distinguished from other block units in the same activity.
By considering these very brief local acts, we can better classify the activity
as a whole. Henceforth, we will call these micro-performances atoms as they
represent our smallest considered unit of activity.
These atoms types must be learned latently. While we have some high-level
idea of what types of atoms we should be able to nd within the accelerometer
measurements, such as jumping, taking a step, climbing a stair, etc., it is not
clear which atoms best describe the space of accelerometer counts over the
types of activities were considering.
Its expected that the truly descriptive
atom classes are abstract spatial patterns which we can not dene empirically.
As such, we seek to identify these atom classes through clustering. The
model weve chosen to represent our clusters is a Gaussian Mixture model. In
statistics, mixture models best model a distribution for which there are dis-
tinct subdistributions which constitute the whole probability space [2].
In a
Gaussian mixture, we assume that each and every subpopulation in our sample
distribution is modelled at least approximately by a multivariate Gaussian.
Using variational Bayesian methods to approximate the Guassian mixture
over our sample, we learn the most-likely set of subpopulations or clusters that
our 12-second windows fall into. The associated distributions of these subpop-
ulations are each associat');
INSERT INTO posts (postId,userId,title,body) VALUES (297,736,'Bag-of-Words Method Applied to Accelerometer Measurements for the Purpose of Classication and Energy Estimation (part 4)','ed with a unique atom class.



zn

xn





Figure 2: Graphical representation of the Bayesian mixture of Gaussians model

3.1.3 Bag of Words Phase - Latent Feature Construction

A bag is a mathematical structure likened to that of the mathematical set except
that it allows duplicate elements. It has a higher representation power than the
set in that each element in the bag has an associated count or frequency. When
used as a collection of words, it represents word frequency within a sentence or
document in natural language processing and has found use in other elds of
machine learning. We seek to apply the bag-of-words model to our context in
the following way.
With each 12-second window of an activity bout assigned to a unique cluster,
we now have a basis for constructing an activity signature for the entire bout.
If we take each atom class to be a word, each bout can be seen as a sentence
composed of words. Intuitively, the idea follows from the concept of the atoms
in and of themselves, which are short meaningful chunks of the whole activity.
Looking at them independent from the activity doesnt give us any indication of
what activity is being performed. This is the same relationship between words
and sentences.
For the bag-of-words construction, we must select a dictionary. The dic-
tionary denes which words may appear in the bag structure. We must also
determine if we are going to use word counts or word frequency. Not all activ-
ities are of the same length in the same way that not all sentences are of the
same length. As such, we will use a bag-of-words model over term frequency.
The Guassian mixture acts as our dictionary where each cluster is a word.
For any one activity bout, the frequency associated with each word in its bag-
of-words is the ratio of how many atoms in the bout belong to that words
associated cluster.
In the bags representation, we include words with zero
frequency. This along with the xed-length dictionary allows us to build xed-
length vector representations of the bag-of-words model.
This allows us to leverage technologies we werent able to before. Before
this step, we had many activity bouts of various lengths; each instance in our
data would have variably many observations and would not reside in a xed-
dimensional vector space. As a result, we were not able to use classication
models which depended on those conditions directly.
However, now we have produced exactly those conditions. At the end of
this phase, we have converted our data to a xed-dimensional vector space with

exactly one bag-of-word vector associated with each bout.

3.1.4 Classication on Latent Features Phase

We apply the same feed-forward neural network model used in previous works
to the newly constructed bag-of-word features.
The neural network is applied to the bag-of-word vector with each of its
components being nodes in the input layer. For the hidden layer, we use an
ane layer of 25 nodes. This layer includes a bias vector (cid:126)b whose components
bi function as the bias terms for each node in the layer. The layer also includes
a weight matrix W whose components wi,j function as the weights associa');
INSERT INTO posts (postId,userId,title,body) VALUES (298,736,'Bag-of-Words Method Applied to Accelerometer Measurements for the Purpose of Classication and Energy Estimation (part 5)','ted
with the edges in the neural network. For the hidden layers activation function,
the hyperbolic tangent function was used.
For the output layer, we use softmax to give us a categorical output value
from the neural network. This output is our models class prediction.
The model was trained using the MATLAB Neural Network Toolbox which
uses the Levenberg-Marquardt algorithm for training neural networks.

3.2 Regression Model

In our regression stage, we learn a least-squares linear regression model. As
inputs to the model, we use the atoms with their original time-series features,
as well as the activity class prediction of the bout it belongs to from the Classi-
cation Stage. This is a carry-over from previous work in which we justied that
including the class prediction of an activity increases the accuracy of estimation
of energy expenditure. We also include as input the Bag-of-Words features as-
sociated with the bout each atom belongs to. The reasoning behind this is that
Bag-of-Words features carry higher-level knowledge of the moments surrounding
each atom which thereby improves the energy expenditure estimation for each
atom in the bout.
As output from this model, we get a low-error estimation of MET. We can
then aggregate the MET predictions over all atoms in a bout to get the energy
expenditure of the bout as a whole.

4 Results

As can be seen in Table 4, during the classication stage we manage to get com-
petitive accuracies with our competing methods. However, the ma jor success
in our model comes from our acceptable classication misses: run-class atoms
are never classied as sedentary-class atoms. Table 3 shows that relatively few
misclassications happen between distant classes when they happen at all. This
implies that the only misclassications that occur are happening at boundary
or outlier cases.
As for our regression results, we provide the standard least-squares linear
regression model on the data by itself as a baseline. This emphasizes the eect

Sed.
16455
90
20
25
LHH MtV Walk Run02020
310
2085
240
1290
2040
3685
250135
350

Sed.
LHH
MtV
Walk
Run

Table 3: Confusion Matrix as atoms for Classication Phase

Sed.
Sed.
99.88
LHH 3.59
1.27
MtV
0.66
Walk
Run
0.00

LHH MtV Walk Run
0.00
0.00
0.00
0.12
83.23
12.38
0.80
0.00
0.00
1.27
82.17
15.29
1.06
97.62
0.66
0.00
0.00
0.00
27.84
72.16

Table 4: Confusion Matrix as percentages for Classication Phase

of each aspect of our regression stage. Table 5 shows that each additional el-
ement to the model has a signicant impact on overall RMSE. In fact, simply
by including the activity class prediction, we beat the state of the arts regres-
sion model in RMSE by nearly 0.5 units. Including our Bag-of-Word features
increased accuracy by another 0.1 units.
From Table 6, we can see that our model has an in-class RMSE advantage
over all other methods. We do not simply beat the methods overall, but in re-
gressing any type of activity, our model performs with the least error. Especially
signicant is the 1.1 unit decrease in RMSE from the ANN in the Run-class and
the 1.2 unit decrease in RMSE from the ANN in the Moderate-to-Vigorous-
class. These activity classes contain the highest error rates for all models as
they are the most dicult to estimate. Our model');
INSERT INTO posts (postId,userId,title,body) VALUES (299,736,'Bag-of-Words Method Applied to Accelerometer Measurements for the Purpose of Classication and Energy Estimation (part 6)',' improves greatly on these
dicult classes.

Linear Regression on Raw Features

Linear Regression w/ Class Prediction

Linear Regression w/ Class Prediction and BoW Features

Articial Neural Network

RMSE
2.3690
0.9548
0.8502
1.4402

Table 5: Root Mean Squared Error for MET estimates of each Regression Model

LR-RF
LR+CP

LR+CP+BoWF

ANN

Sed.
2.0105
0.2284
0.1798
0.3999

LHH MtV Walk Run
4.1593
2.6670
3.3990
2.7549
2.5591
1.4612
1.8695
1.4094
1.3477
1.5964
1.3494
2.0146
3.6695
2.1308
2.7789
2.2715

Table 6: RMSE for MET estimates of each Regression Model by Activity Class

5 Conclusion

We presented a Classication-Regression framework for predicting activity classes
and estimating energy expenditure from time-series data collected from hip
mounted accelerometers. Our approach of utilizing an unsupervised intermedi-
ate feature construction layer has been shown to generate meaningful and useful
knowledge that contributes to high classication accuracy and lower regression
error. Integrating the Bag-of-Words model into our representation was shown
to have a signicant impact on our results over those of other methods. We
further show that our results exceed that of the state-of-the-art method.

References

[1] Thomas Bastian, Aurelia Maire, Julien Dugas, Abbas Ataya, Clement Vil-
lars, Florence Gris, Emilie Perrin, Yanis Caritu, Maeva Doron, Stephane
Blanc, Pierre Jallon, and Chantal Simon. Automatic identication of phys-
ical activity types and sedentary behaviors from triaxial accelerometer:
laboratory-based calibrations are not enough. Journal of Applied Physi-
ology, 118(6):716722, 2015.

[2] Christopher M. Bishop.
Springer, 2006.

Pattern Recognition and Machine Learning.

[3] Scott E. Crouter, Kurt G. Clowers, and David R. Bassett. A novel method
for using accelerometer data to predict energy expenditure. Journal of Ap-
plied Physiology, 100(4):13241331, 2006.

[4] AH Montoye, Lanay M Mudd, Subir Biswas, and Karin A Pfeier. Energy
expenditure prediction using raw accelerometer data in simulated free-living.
Medicine '||'&'||' Science in Sports '||'&'||' Exercise, 47(8):17351746, 2015.

[5] Y. Mu, H. Z. Lo, W. Ding, K. Amaral, and S. E. Crouter. Bipart: Learning
block structure for activity detection. IEEE Transactions on Know ledge and
Data Engineering, 26(10):23972409, Oct 2014.

[6] J. Staudenmayer, D. Pober, S. Crouter, D. Bassett, and P. Freedson. An
articial neural network to estimate physical activity energy expenditure

and identify physical activity type from an accelerometer. J. Appl. Physiol.,
107(4):13001307, Oct 2009.

[7] S. G. Trost, W. K. Wong, K. A. Pfeier, and Y. Zheng. Articial neural
networks to predict activity type and energy expenditure in youth. Med Sci
Sports Exerc, 44(9):18011809, Sep 2012.

10

');
INSERT INTO posts (postId,userId,title,body) VALUES (300,9,'Generalized Satisability Problems via Operator Assignments','Albert Atserias1

Phokion G. Kolaitis2

Simone Severini3

1 Universitat Polit`ecnica de Catalunya
2 University of California Santa Cruz and IBM ResearchAlmaden
3 University College London and Shanghai Jiao Tong University

April 7, 2017

Abstract

Schaefer introduced a framework for generalized satisability problems on the Boolean
domain and characterized the computational complexity of such problems. We investi-
gate an algebraization of Schaefers framework in which the Fourier transform is used
to represent constraints by multilinear polynomials in a unique way. The polynomial
representation of constraints gives rise to a relaxation of the notion of satisability
in which the values to variables are linear operators on some Hilbert space. For the
case of constraints given by a system of linear equations over the two-element eld,
this relaxation has received considerable attention in the foundations of quantum me-
chanics, where such constructions as the Mermin-Peres magic square show that there
are systems that have no solutions in the Boolean domain, but have solutions via op-
erator assignments on some nite-dimensional Hilbert space. We obtain a complete
characterization of the classes of Boolean relations for which there is a gap between
satisability in the Boolean domain and the relaxation of satisability via operator
assignments. To establish our main result, we adapt the notion of primitive-positive
denability (pp-denability) to our setting, a notion that has been used extensively
in the study of constraint satisfaction problems. Here, we show that pp-denability
gives rise to gadget reductions that preserve satisability gaps. We also present several
additional applications of this method. In particular and perhaps surprisingly, we show
that the relaxed notion of pp-denability in which the quantied variables are allowed
to range over operator assignments gives no additional expressive power in dening
Boolean relations.

1

Introduction and Summary of Results

In 1978, Schaefer [22] classied the computational complexity of generalized satisability
problems. Each class A of Boolean relations gives rise to the generalized satisability problem
SAT(A). An instance of SAT(A) is a conjunction of relations from A such that each conjunct
has a tuple of variables as arguments; the question is whether or not there is an assignment
of Boolean values to the variables, so that, for each conjunct, the resulting tuple of Boolean
values belongs to the underlying relation. Schaefers main result is a dichotomy theorem
for the computational complexity of SAT(A), namely, depending on A, either SAT(A)
is NP-complete or SAT(A) is solvable in polynomial time. Schaefers dichotomy theorem
provided a unifying explanation for the NP-completeness of many well-known variants of
Boolean satisability, such as POSITIVE 1-IN-3 SAT and MONOTONE 3SAT; moreover,
it became the catalyst for numerous subsequent investigations, including the pursuit of a
dichotomy theorem for constraints satisfaction problems, a pursuit that became known as
the Feder-Vardi Conjecture [9]');
INSERT INTO posts (postId,userId,title,body) VALUES (301,9,'Generalized Satisability Problems via Operator Assignments (part 2)','.
Every Boolean relation can be identied with its characteristic function, which, via the
Fourier transform, can be represented as a multilinear polynomial (i.e., a polynomial in
which each variable has degree at most one) in a unique way. Moreover, in carrying out
this transformation, the truth values false and true are typically represented by +1 and 1,
instead of 0 and 1. For example, it is easy to see that the multilinear polynomial representing
2 (1 + x + y  xy). The multilinear polynomial
the conjunction x  y of two variables x and y is 1
representation of Boolean relations makes it possible to consider relaxations of satisability
in which the variables take values in some suitable space, instead of the two-element Boolean
algebra. Such relaxations have been considered in the foundations of physics several decades
ago, where they have played a role in singling out the dierences between classical theory
and quantum theory.
In particular, it has been shown that there is a system of linear
equations over the two-element eld that has no solutions over {+1, 1}, but the system
of the associated multilinear polynomials has a solution in which the variables are assigned
linear operators on a Hilbert space of dimension four. The Mermin-Peres magic square
[16, 17, 20] is the most well known example of such a system. These constructions give
smal l proofs of the celebrated Kochen-Specker Theorem [8] on the impossibility to explain
quantum mechanics via hidden-variables [2]. More recently, systems of linear equations with
this relaxed notion of solvability have been studied under the name of binary constraint
systems, and tight connections have been established between solvability and the existence
of perfect strategies in non-local games that make use of entanglement [6, 7].
A Boolean relation is ane if it is the set of solutions of a system of linear equations
over the two-element eld. The collection LIN of all ane relations is prominent in Schae-
fers dichotomy theorem, as it is one of the main classes A of Boolean relations for which
SAT(A) is solvable in polynomial time. The discussion in the preceding paragraph shows
that SAT(LIN) has instances that are unsatisable in the Boolean domain, but are satisable
when linear operators on a Hilbert space are assigned to variables (for simplicity, from now
on we will use the term operator assignments for such assignments). Which other classes
of Boolean relations exhibit such a gap between satisability in the Boolean domain and

the relaxation of satisability via operator assignments? As a matter of fact, this question
bifurcates into two separate questions, depending on whether the relaxation allows linear op-
erators on Hilbert spaces of arbitrary (nite or innite) dimension or only on Hilbert spaces
of nite dimension.
In a recent breakthrough paper, Slofstra [24] showed that these two
questions are dierent for LIN by establishing the existence of systems of linear equations
that are satisable by operator assignments on some innite-dimensional Hilbert space, but
are not satisable by operator assignments on any nite-dimensional Hilbert space.
In a
related vein, Ji [15] showed that a 2CNF-formula is satisable in the Boolean domain if and
only if it is satisable by an operator assignment in some nite-dimensional Hilbert space.
Moreover, Ji showed that the same holds true for Horn formulas. Note that 2SAT, HORN
SAT, and DUAL HORN SAT also feature prominently in Schaefers dichotomy theorem as,
together with SAT(LIN), which from now on we will denote by LIN SAT, they constitute the
main tractable cases of generalized satisability problems (the other tractable cases are the
trivial cases of SAT(A), where A is a class of 0-valid relations or a class of 1-valid relations,
i.e., Boolean relatio');
INSERT INTO posts (postId,userId,title,body) VALUES (302,9,'Generalized Satisability Problems via Operator Assignments (part 3)','ns that contain the tuple consisting entirely of 0s or, respectively, the
tuple consisting entirely of 1s).
In this paper, we completely characterize the classes A of Boolean relations for which
SAT(A) exhibits a gap between satisability in the Boolean domain and satisability via
operator assignments. Clearly, if every relation in A is 0-valid or every relation in A is
1-valid, then there is no gap, as every constraint is satised by assigning to every variable
the identity operator or its negation, respectively. Beyond this, we rst generalize and ex-
tend Jis results [15] by showing that if  is a class of Boolean relations such that every
relation in A is bijunctive1 , or every relation in A is Horn, or every relation in A is dual
Horn2 , then there is no gap whatsoever; this means that an instance of SAT(A) is satisable
in the Boolean domain if and only if it is satisable by an operator assignment on some
nite-dimensional Hilbert space if and only if is satisable by an operator assignment on
some arbitrary Hilbert space. In contrast, we show that for all other classes A of Boolean
relations, SAT(A) exhibits a two-level gap: there are instances of SAT(A) that are not
satisable in the Boolean domain, but are satisable by an operator assignment on some
nite-dimensional Hilbert space; moreover, there are instances of SAT(A) that are not satis-
able by an operator assignment on any nite-dimensional Hilbert space, but are satisable
by an operator assignment on some (innite-dimensional) Hilbert space.
The proof of this result uses several dierent ingredients. First, we use the substitution
method [7] to show that there is no satisability gap for classes of relations that are bijunc-
tive, Horn, and dual Horn. This gives a dierent proof of Jis results [15], which were for
nite-dimensional Hilbert spaces, but also shows that, for such classes of relations, there is
no dierence between satisability by linear operators on nite-dimensional Hilbert spaces
and satisability by linear operators on arbitrary Hilbert spaces. The main tool for proving
the existence of a two-level gap for the remaining classes of Boolean relations is the notion

1A Boolean relation is bijunctive if it is the set of satisfying assignments of a 2CNF-formula.
2A Boolean relation is Horn (dual Horn ) if it is the set of satisfying assignments of a Horn (dual Horn)
formula.

of pp-denability, that is, denability via primitive-positive formulas, which are existential
rst-order formulas having a conjunction of (positive) atoms as their quantier-free part.
In the past, primitive-positive formulas have been used to design polynomial-time reduc-
tions between decision problems; in fact, this is one of the main techniques in the proof of
Schaefers dichotomy theorem. Here, we show that primitive-positive formulas can also be
used to design gap-preserving reductions, that is, reductions that preserve the gap between
satisability on the Boolean domain and satisability by operator assignments. To prove the
existence of a two-level gap for classes of Boolean relations we combine gap-preserving re-
ductions with the two-level gap for LIN discussed earlier (i.e., the results of Mermin [16, 17],
Peres [16], and Slofstra [24]) and with results about Posts lattice of clones on the Boolean
domain [21].
We also give two additional applications of pp-denability. First, we consider an extension
of pp-denability in which the existential quantiers may range over linear operators on some
nite-dimensional Hilbert space. At rst sight, it appears that new Boolean relations may
be pp-denable in the extended sense from a given set of Boolean relations. We show,
however, that this is not the case. Specically, by analyzing closure operations on sets of
linear operato');
INSERT INTO posts (postId,userId,title,body) VALUES (303,9,'Generalized Satisability Problems via Operator Assignments (part 4)','rs, we show that if a Boolean relation is pp-denable in the extended sense from
other Boolean relations, then it is also pp-denable from the same relations. In other words,
for Boolean relations, this extension of pp-denability is not more powerful than standard
pp-denability. Second, we apply pp-denability to the problem of quantum realizability
of contextuality scenarios. Recently, Fritz [12] used Slofstras results [24] to resolve two
problems raised by Acin et al. in [1]. Using pp-denability and Slofstras results, we obtain
new proofs of Fritzs results that have the additional feature that the parameters involved
are optimal.

2 Denitions and Technical Background

2.1 Notation
For an integer n, we write [n] for the set {1, . . . , n}. We use mainly the +1, 1 representation
of the Boolean domain (+1 for false and 1 for true). We write {1} for the set
{+1, 1}. If a denotes a tuple of length r we write a1 , . . . , ar to denote its r components. If
a is such a tuple and f is a function that has a1 , . . . , ar in its domain, we write f (a) to denote
the tuple (f (a1 ), . . . , f (ar )). We write T and F for the full and empty Boolean relations,
respectively. The letters stand for true and false. Their arity is unspecied by the notation
and will be made clear by the context.

2.2 Linear Operators and Polynomials Thereof

Let V be a complex vector space. A linear operator on V is a linear map from V to V .
The linear operator that is the identity on V is denoted by I , and the linear operator
that is identically 0 is denoted by 0. The pointwise addition of two linear operators A

and B is denoted by A + B , the composition of two linear operators A and B is denoted
by AB , and the pointwise scaling of a linear operator A by a scalar c  C is denoted
by cA. All these are linear operators. As a result, if C(cid:104)X1 , . . . , Xn (cid:105) denotes the ring of
polynomials with complex coecients and non-commuting variables in X1 , . . . , Xn , then for
a polynomial P (X1 , . . . , Xn ) in C(cid:104)X1 , . . . , Xn (cid:105) and linear operators A1 , . . . , An on V , the
notation P (A1 , . . . , An ) is explained. If A1 , . . . , An pairwise commute, i.e., AiAj = Aj Ai for
all i, j  {1, . . . , n}, then the notation is explained even for a polynomial in C[X1 , . . . , Xn ],
the ring of polynomials with commuting variables in X1 , . . . , Xn .
Let V and W be complex vector spaces. Let A be a linear operator on V and let B be
a linear operator on W . We say that A and B are similar if there exists an invertible linear
map C : V  W such that A = CBC 1 . Let A1 , . . . , An and B1 , . . . , Bn be linear operators
on V and W , respectively. We say that A1 , . . . , An and B1 , . . . , Bn are simultaneously similar
if there exists an invertible linear map C : V  W such that Ai = CBiC 1 holds for all
i  [n]. The following simple fact with an equally simple proof will be used multiple times.

Lemma 1. Let V and W be complex vector spaces, and let P (X1 , . . . , Xn ) be a polynomial
in C(cid:104)X1 , . . . , Xn (cid:105). If A1 , . . . , An and B1 , . . . , Bn are simultaneously similar linear operators
on V and W , respectively, then so are P (A1 , . . . , An ) and P (B1 , . . . , Bn ).
length of the sequence . Let P (X1 , . . . , Xn ) = (cid:80)
(cid:81)||
Proof. We write [n] for the set of nite sequences with components in [n], and || for the
i=1 Xi , where only nitely
[n] c
many of the coecients c are non-zero. Let C : V  W be an invertible linear map
holds for every j  [n]. Note that for every   [n] of length (cid:96) we have (cid:81)(cid:96)
wi');
INSERT INTO posts (postId,userId,title,body) VALUES (304,9,'Generalized Satisability Problems via Operator Assignments (part 5)','tnessing that A1 , . . . , An and B1 , . . . , Bn are simultaneously similar; thus Aj = CBj C 1
(cid:1)C 1 , and linearity
(cid:1)C 1 . It follows that P (A1 , . . . , An ) = (cid:80)
[n] cC (cid:0)(cid:81)||
C (cid:0)(cid:81)(cid:96)
i=1 (CBi C 1 ) =
i=1 Bi
i=1 Bi
gives P (A1 , . . . , An ) = C P (B1 , . . . , Bn )C 1 .

2.3 Unique Multilinear Polynomial Representations

A polynomial P (X1 , . . . , Xn ) is called multilinear if it has individual degree at most one on
each variable. Each function f : {1}n  C has a unique representation as a multilin-
(cid:89)
(cid:88)
ear polynomial in C[X1 , . . . , Xn ] given by the Fourier or Walsh-Hadamard transform [18].
Explicitly:
f (S )
Pf (X1 , . . . , Xn ) =
(cid:89)
(cid:88)
iS
S[n]2n
iS
a{1}n
The polynomial represents f in the sense that Pf (a) = f (a) holds for every a  {1}n . If the
range of f is a subset of R, then each f (S ) is indeed a real number. The Convolution Formula
describes the Fourier coecients of pointwise products f g of functions f , g : {1}n  C. It

f (S ) =

where

f (a)

ai .

Xi ,

(1)

(2)

states that

(3)

f (S )g(ST )

(cid:88)
(cid:99)f g(S ) =
T [n]
for every S  [n], where ST denotes symmetric dierence; i.e. ST = (S \ T )  (T \ S ).
We give an example of use of the uniqueness of the Fourier transform that will be useful
later on. We begin by recalling some notation and terminology. A literal
is a Boolean
variable x or its negation x. The literals x and x are said to be complementary of each
other, and x is their underlying variable. If (cid:96) is a literal, then (cid:96) denotes its complementary
literal. The sign sg((cid:96)) of (cid:96) is dened as follows: sg((cid:96)) = 1 if (cid:96) = x, and sg((cid:96)) = 1 if (cid:96) = x,
where x is its underlying variable. Clearly, sg((cid:96)) = sg((cid:96)).
A clause is a disjunction of literals. Let C = ((cid:96)1      (cid:96)r ) be a clause. In the 1 represen-
tation of Boolean values, the clause C represents the relation {1}r \ {(sg((cid:96)1 ), . . . , sg((cid:96)r ))},
which will be denoted by RC . The indicator function of the clause C = ((cid:96)1      (cid:96)r ) is
the Boolean function from {1}r  {1} that maps the tuple (sg((cid:96)1 ), . . . , sg((cid:96)r )) to +1
and every other tuple to 1. We write PC (X1 , . . . , Xr ) to denote the unique multilinear
polynomial representation of the indicator function of the clause C .
Lemma 2. Let C = ((cid:96)1      (cid:96)r ) be a clause on r dierent variables. Then, over the ring
of polynomials C[X1 , . . . , Xr ], the fol lowing identity holds.
(cid:16)
(cid:17)  1.
r(cid:89)
i=1
Proof. Let RC = {1}r \ {(sg((cid:96)1 ), . . . , sg((cid:96)r ))} be the Boolean relation represented by C .
Since the right-hand side of equation (4) is a multilinear polynomial and its left-hand side
is the unique multilinear polynomial that agrees with the indicator function of RC on {1},
it suces to check that the right-hand side also agrees with the indicator function of RC on
{1}r . In other words, we claim that for every (a1 ,');
INSERT INTO posts (postId,userId,title,body) VALUES (305,9,'Generalized Satisability Problems via Operator Assignments (part 6)',' . . . , ar )  {1}r , the right-hand side
evaluates to 1 if the truth-assignment (a1 , . . . , ar ) satises the clause C , and it evaluates
that aj = sg((cid:96)j ). It follows that 1 + sg((cid:96)j )aj = 0 and so (cid:81)r
to 1, otherwise.
Assume that (a1 , . . . , ar ) satises the clause C . Then there is some j  {1, . . . , r} such
i=1 (1 + sg((cid:96)i )ai ) = 0, which,
in turn, implies that PC (a1 , . . . , ar ) = 1. Assume that (a1 , . . . , ar ) does not satisfy the
i  {1, . . . , r}, we have that 1 + sg((cid:96)i )ai = 2 and so (cid:81)r
clause C . Then, for every i  {1, . . . , r}, we have that ai = sg((cid:96)i ). Consequently, for every
i=1 (1 + sg((cid:96)i )ai ) = 2r , which, in turn,
implies that PC (a1 , . . . , ar ) = 1. This completes the proof of the claim.

PC (X1 , . . . , Xr ) = 21r

1 + sg((cid:96)i )Xi

(4)

2.4 Hilbert Space

A Hilbert space is a complex vector space with an inner product whose norm induces a
complete metric. All Hilbert spaces of nite dimension d are isomorphic to Cd with the
standard complex inner product. In particular, this means that after the choice of a basis,
we can identify the linear operators on a d-dimensional Hilbert space with the matrices

in Cdd . Composition of operators becomes matrix multiplication. A matrix A is Hermitian
if it is equal to its conjugate transpose A . A diagonal matrix is one all whose o-diagonal
entries are 0. A matrix A in unitary if AA = AA = I , where I is the identity matrix. Two
matrices A and B commute if AB = BA, and a collection of matrices A1 , . . . , Ar pairwise
commute if AiAj = Aj Ai for all i, j  [r].
For the basics of general Hilbert spaces and their linear operators we refer the reader
to Halmos monograph [14]. We need from it the concepts of bounded linear operator and
of adjoint A of a densely dened linear operator A. Two operators A and B commute
if AB = BA. A sequence of operators A1 , . . . , Ar pairwise commute if AiAj = Aj Ai for
all i, j  [r]. A linear operator A is called normal if it commutes with its adjoint A ; i.e.,
AA = AA. A linear operator is called self-adjoint if A = A. A linear map from a Hilbert
space H1 to another Hilbert space H2 is called unitary if it preserves norms.
We also make elementary use of general L2 - and L -spaces. Let (, M, ) be a measure
space. Then L2 (, ) denotes the collection of square integrable measurable functions, up
to almost everywhere equality. Also L (, ) denotes the collection of essentially bounded
measurable functions, up to almost everywhere equality. All measure-theoretic terms in
these denitions refer to . See [11] for denitions.

2.5 Constraint Languages, Instances, Value and Satisability
A Boolean constraint language A is a collection of relations over the Boolean domain {1}.
Let V = {X1 , . . . , Xn} be a set of variables. An instance I on the variables V over the
constraint language A is a nite collection of pairs
I = ((Z1 , R1 ), . . . , (Zm , Rm ))
where each Ri is a relation from A and Zi = (Zi,1 , . . . , Zi,ri ) is a tuple of variables from V
or constants from {1}, where ri is the arity of Ri . Each pair (Zi , Ri ) is called a constraint,
and each Zi is called its constraint-scope. A Boolean assignment is a mapping f assigning
a Boolean value ai  {1} to each variable Xi , and assigning 1 and +1 to the constants
1 and +1, respectively. We say that the assignment satises the i-th constraint if the
tuple f (Zi ) = (f (Zi,1 )');
INSERT INTO posts (postId,userId,title,body) VALUES (306,9,'Generalized Satisability Problems via Operator Assignments (part 7)',', . . . , f (Zi,ri )) belongs to Ri . The value of f on I is the fraction of
constraints that are satised by f . The value of I , denoted by  (I ), is the maximum value
over all Boolean assignments. We say that I is satisable in the Boolean domain if there is
a Boolean assignment that satises all constraints; equivalently, if  (I ) = 1.

(5)

2.6 Operator Assignments and Satisability via Operators
Let X1 , . . . , Xn be n variables, and let H be a Hilbert space. An operator assignment for
X1 , . . . , Xn over H is an assignment of a bounded linear operator on H to each variable,
f : X1 , . . . , Xn (cid:55) A1 , . . . , An , such that the following conditions hold:
1. Aj is self-adjoint for every j  [n],
j = I for every j  [n].
2. A2

If S is a subset of {X1 , . . . , Xn}, we say that the operator assignment A1 , . . . , An pairwise
commutes on S if in addition it satises Aj Ak = AkAj for every Xj and Xk in the set S .
If it pairwise commutes on the whole set {X1 , . . . , Xn}, we say that the assignment ful ly
commutes.
Let A be a Boolean constraint language, let I be an instance over A, with n variables
X1 , . . . , Xn as in (5), and let H be a Hilbert space. An operator assignment for I over H
is an operator assignment f : X1 , . . . , Xn (cid:55) A1 , . . . , An for the variables X1 , . . . , Xn that
pairwise commutes on the set of variables of each constraint scope Zi in I ; explicitly
for every Xj and Xk in Zi , for every i  {1, . . . , m}.
(6)
Aj Ak = AkAj
We also require that f maps the constant 1 and +1 to I and I , respectively, where I is
the identity operator on H. We say that the assignment f satises the i-th constraint if
PRi (f (Zi )) = PRi (f (Zi,1 ), . . . , f (Zi,ri )) = I ,
where PRi denotes the unique multilinear polynomial representation of indicator function
of the relation Ri , i.e., the function that maps each tuple in Ri to 1, and each tuple
in its complement {1}ri \ Ri to +1. Note that since f (Zi,1 ), . . . , f (Zi,ri ) are required to
commute by denition, this notation is unambiguous despite the fact that PRi is dened as
a polynomial in commuting variables. The value of f on I is the fraction of constraints
that are satised by f ; note that this quantity takes one of a nite set of values in the set
{0, 1/m, 2/m, . . . , (m  1)/m, 1}. The value of I over H is the maximum value over all
operator assignments for I over H. We say that f satises I if it satises all constraints.
In that case we also say that f is a satisfying operator assignment for I over H.
The nite-dimensional value of I , denoted by   (I ), is the maximum of its value over all
nite-dimensional Hilbert spaces. The value of I , denoted by   (I ), is the maximum of its
value over all Hilbert spaces. We say that an instance I is satisable via nite-dimensional
operator assignments, or satisable via fd-operators for short, if   (I ) = 1. We say that I
is satisable via operator assignments, or satisable via operators for short, if   (I ) = 1.

(7)

3 The Strong Spectral Theorem

The Spectral Theorem plays an important role in linear algebra and functional analysis.
It has also been used in the foundations of quantum mechanics (for some recent uses see
[7, 15]). We will make a similar use of it, but we will also need the version of this theorem
for innite-dimensional Hilbert spaces. In this section we discuss the statement, both for
nite- and innite-dimensional Hilbert spaces, as well as one of its important applications
that we encapsulate in a lemma for later reuse.

3.1 Statement

In its most basic form, the Spectral Theorem for complex matrices states that every Hermi-
tian matrix is unitarily equivalent to a diagonal matrix. Explicitly: if A is a d  d Hermitian

matrix, then there exist a unitary matrix U and a diagonal matrix E such t');
INSERT INTO posts (postId,userId,title,body) VALUES (307,9,'Generalized Satisability Problems via Operator Assignments (part 8)','hat A = U 1EU .
In its strong form, the Strong Spectral Theorem (SST) applies to sets of pairwise commuting
Hermitian matrices and is stated as follows.
Theorem 1 (Strong Spectral Theorem; nite-dimensional case). Let A1 , . . . , Ar be d  d
Hermitian matrices, for some positive integer d. If A1 , . . . , Ar pairwise commute, then there
exists a unitary matrix U and diagonal matrices E1 , . . . , Er such that Ai = U 1EiU for every
i  [r].

This form of the SST will be enough to discuss satisability via fd-operators. For operator
assignments over arbitrary Hilbert spaces, we need to appeal to the most general form of
the SST in which the role of diagonal matrices is played by multiplication operators on an
L2 (, )-space. These are dened as follows.
Let V be a complex function space; a complex vector space of functions mapping indices
from an index set X to C. A multiplication operator of V is a linear operator whose value
at a function f : X  C in V is given by pointwise multiplication by a xed function
a : X  C. In symbols, the multiplication operator given by a is
for each x  X.

(Ta (f ))(x) = a(x)f (x)

(8)

In its weak form, the general Spectral Theorem states that any normal bounded linear
operator on a Hilbert space is unitarily equivalent to a multiplication operator on an L2 -
space. We need the following strong version of the Spectral Theorem that states that the
same is true for a collection of such operators, simultaneously through the same unitary
transformation, provided they commute. The statement we use is a direct consequence of
Theorem 1.47 in Follands monograph [10].

Theorem 2 (Strong Spectral Theorem; general case). Let A1 , . . . , An be normal bounded
linear operators on a Hilbert space H. If A1 , . . . , Ar pairwise commute, then there exist a
measure space (, M, ), a unitary map U : H  L2 (, ), and functions a1 , . . . , ar 
L (, ) such that Ai = U 1Tai U for every i  [r].
The special case in which H has nite dimension d, the measure space is actually a nite
set of cardinality d with the counting measure, and thus L2 (, ) is isomorphic to Cd with
the usual complex inner product.

3.2 An Oft-Used Application

The following lemma encapsulates a frequently used application of the Strong Spectral The-
orem.
It states that whenever a set of polynomial equations entail another polynomial
equation over the Boolean domain, then the entailment holds as well for fully commuting
operator assignments.
Lemma 3. Let X1 , . . . , Xr be variables, let Q1 , . . . , Qm , Q be polynomials in C[X1 , . . . , Xr ],
and let H be a Hilbert space.
If every Boolean assignment that satises the equations

Q1 =    = Qm = 0 also satises the equation Q = 0, then every ful ly commuting oper-
ator assignment over H that satises the equations Q1 =    = Qm = 0 also satises the
equation Q = 0.

Although the same proof applies to all Hilbert spaces, the proof of the nite-dimensional
case can be made more elementary. Since for certain applications only the nite-dimensional
case of the lemma is relevant, we split the proof accordingly into cases.
Proof of Lemma 3; nite-dimensional case. Assume H has nite dimension d. Since all
Hilbert spaces of dimension d are isometrically isomorphic to Cd , let us assume without
loss of generality that H = Cd . In such a case, a self-adjoint bounded linear operator is just
a Hermitian d  d matrix, and the composition of linear operators is matrix multiplication.
Assume the hypotheses of the lemma and let A1 , . . . , Ar be Hermitian d  d matrices.
Assume that A1 , . . . , Ar make a fully commuting operator assignment for X1 , . . . , Xr such
that the equations Q1 =    = Qm = 0 ');
INSERT INTO posts (postId,userId,title,body) VALUES (308,9,'Generalized Satisability Problems via Operator Assignments (part 9)','are satised. The matrices A1 , . . . , Ar pairwise
commute, so the Strong Spectral Theorem (i.e. Theorem 1) applies to them. Thus, there
exist a unitary matrix U and diagonal d  d matrices E1 , . . . , Em such that Ai = U 1EiU for
every i  [r]. Equivalently, U AiU 1 = Ei . From A2
i = I we conclude E 2
i = I . Hence, if ai (j )
denotes the j -th diagonal entry of Ei , then ai (j )2 = 1 for all j  [d]. Thus ai (j )  {1}
for all j  [d]. The conditions of Lemma 1 apply, so Qk (A1 , . . . , Ar ) and Qk (E1 , . . . , Er ) are
similar matrices for each k  [m]. Since Qk (A1 , . . . , Ar ) = 0 and the unique matrix that
is similar to the null matrix is the null matrix itself, we conclude that Qk (E1 , . . . , Er ) = 0.
Now, Ei is the diagonal matrix that has the vector (ai (1), . . . , ai (d)) in the diagonal, so
Qk (a1 (j ), . . . , ar (j )) = 0 for all j  [d]. Since ai (j ) is in {1} for each i  [r] and j  [d],
the hypothesis of the lemma says that also Q(a1 (j ), . . . , ar (j )) = 0 for all j  [d]. Thus
Q(E1 , . . . , Er ) = 0, and another application of Lemma 1 shows that Q(A1 , . . . , Ar ) = 0, as
was to be proved.

The proof for the general case follows the same structure as the proof of the nite-
dimensional case, using Theorem 2 in place of Theorem 1. Other than taking care of nul l
sets of exceptions, there are no further dierences in the two proofs. At a later stage we
will nd an application of the SST whose proof for the innite-dimensional case does require
some new ingredients. For now, let us ll in the details of the null-set-of-exceptions argument
as a warm-up.

Proof of Lemma 3; general case. Assume the hypotheses of the lemma and let A1 , . . . , Ar be
bounded self-adjoint linear operators on H. Suppose that A1 , . . . , Ar make a fully commuting
operator assignment for X1 , . . . , Xr such that the equations Q1 =    = Qm = 0 are satised.
The operators A1 , . . . , Ar pairwise commute, and since they are self-adjoint they are also
normal, so the Strong Spectral Theorem (i.e. Theorem 2) applies to them. Thus, there exist
a measure space (, M, ), a unitary map U : H  L2 (, ) and functions a1 , . . . , ar 
L (, ) such that, for the multiplication operators Ei = Tai of L2 (, ), the relations
Ai = U 1EiU hold for every i  [r]. Equivalently, U AiU 1 = Ei . From A2
i = I we conclude
i = I . Hence, ai ()2 = 1 for almost all   ; i.e. formally, ({   : ai ()2 (cid:54)=
E 2

10

1}) = 0. Thus ai ()  {1} for almost all   . The conditions of Lemma 1 apply,
thus Qk (A1 , . . . , Ar ) and Qk (E1 , . . . , Er ) are similar linear operators for each k  [m]. Since
Qk (A1 , . . . , Ar ) = 0 and the unique linear operator that is similar to the null operator is
the null operator itself, we conclude that Qk (E1 , . . . , Er ) = 0. Now, Ei is the multiplication
operator given by the function ai , so Qk (a1 (), . . . , ar ()) = 0 for almost all   . Since
for almost all    the component ai () is in {1} for each i  [r], the hypothesis of the
lemma says that also Q(a1 (), . . . , ar ()) = 0 for almost all   . Thus Q(E1 , . . . , Er ) = 0,
and another application of Lemma 1 shows that Q(A1 , . . . , Ar ) = 0, as was to be proved.

4 Reductions through Primitive Positive Formulas

Let A be a Boolean constraint language, let r be a positive integer, and let x1 , . . . , xr be
variables ranging over the Boolean domain {1}. A primitive positive formula, or pp-formula
for short, is a formula of the form
(x1 , . . . , xr ) = y1    ys (R1 (z1 )      Rm (zm ))

(9)

where each Ri is a relation in A and each zi is an ri -tuple of variables or constants from
{x1 , . . . , xr }  {y1 , . . . , ys}  {1}, where ri is the arity of Ri . A relation R  {1}r is
pp-denable from A if there exists a pp-formula (x1 , . . . , xr ) such that
R = {(a1 , . . . , ar )  {1}r : (x1/a1 , . . . , xr /ar ) is true in A}.

(10)

A Boolean');
INSERT INTO posts (postId,userId,title,body) VALUES (309,9,'Generalized Satisability Problems via Operator Assignments (part 10)',' constraint language A is pp-denable from another Boolean constraint language
B if every relation in A is pp-denable from B . Whenever the constants +1 and 1 do not
appear in the pp-formulas, we speak of pp-formulas and pp-denability without constants
or, also, without parameters.
In the following we show that if A is pp-denable from B , then every instance I over
A can be translated into an instance J over B in such a way that the satisfying operator
assignments for I lift to satisfying operator assignments for J . We make this precise.

4.1 The Basic Construction

Let A and B be two Boolean constraint languages and assume that every relation in A is
pp-denable from B . For R in A, let
R (x1 , . . . , xr ) = y1    yt (S1 (w1 )      Sm (wm ))

(11)

be the pp-formula that denes R from B , where S1 , . . . , Sm are relations from B , and
w1 , . . . , wm are tuples of variables or constants in {x1 , . . . , xr }  {y1 , . . . , yt}  {1} of ap-
propriate lengths. For every instance I of A we construct an instance J of B as follows.
Consider a constraint (Z, R) in I , where Z = (Z1 , . . . , Zr ) is a tuple of variables of I
or constants in {1}. In addition to the variables in Z , in J we add new fresh variables
Y1 , . . . , Yt for the quantied variables y1 , . . . , yt in R . We also add one constraint (Wj , Sj ) for

11

each j  [m], where Wj is the tuple of variables and constants obtained from wj by replacing
the variables in x1 , . . . , xr by the corresponding components Z1 , . . . , Zr of Z , replacing any
yi -variable by the corresponding Yi , and leaving all constants untouched. We do this for
each constraint in I one by one. The collection of variables Z1 , . . . , Zr , Y1 , . . . , Yt that are
introduced by the constraint (Z, R) of I is referred to as the block of (Z, R) in J . Note that
two blocks of dierent constraints may intersect, but only on the variables of I .
This construction is referred to as a gadget reduction in the literature. Its main property
for satisability in the Boolean domain is the following straightforward fact:
Lemma 4. I is satisable in the Boolean domain if and only if J is.

We ommit its very easy proof. Our goal in the rest of this section is to show that one direction
of this basic property of gadget reductions is also true for satisability via operators, for both
nite- and innite-dimensional Hilbert spaces, and that the other direction is almost true in
a sense we will make precise in due time.

4.2 Correctness: Operator Solutions Lift

The following lemma shows that the left-to-right direction in Lemma 4 also holds for satisa-
bility via operators: satisfying operator assignments for I can be lifted to satisfying operator
assignments for J , over the same Hilbert space.
Lemma 5. Let I and J be as above and let H be a Hilbert space. For every f that is a
satisfying operator assignment for I over H, there exists g that extends f and is a satisfying
operator assignment for J over H. Moreover, g is pairwise commuting on each block of J .

As in the proof of Lemma 3 we split into cases.

Proof of Lemma 5, nite-dimensiona');
INSERT INTO posts (postId,userId,title,body) VALUES (310,9,'Generalized Satisability Problems via Operator Assignments (part 11)','l case. As in the proof of the nite-dimensional case of
Lemma 3, we may assume that H = Cd for some positive integer d, and that A1 , . . . , An are
Hermitian d  d matrices that make a satisfying operator assignment f for I . We need to
dene Hermitian matrices for the new variables of J that were introduced by its construction.
We dene these matrices simultaneously for all variables Y1 , . . . , Yt that come from the same
constraint (Z, R) of I .
By renaming the entries in Z if necessary, let us assume without loss of generality that
the variables in Z are X1 , . . . , Xr , where r is the arity of R. By the commutativity condition
of satisfying operator assignments, the matrices A1 , . . . , Ar pairwise commute. As each Ai is
Hermitian, the Strong Spectral Theorem applies to them. Thus, there exist a unitary matrix
U and diagonal d  d matrices E1 , . . . , Er such that the relations Ai = U 1EiU hold for each
i  [r]. Equivalently, U AiU 1 = Ei . From A2
i = I we conclude E 2
i = I . Hence, if ai (j )
denotes the j -th diagonal entry of Ei , then ai (j )2 = 1 for all j  [d]. Thus ai (j )  {1} for
all j  [d]. The conditions of Lemma 1 apply, thus PR (A1 , . . . , Ar ) and PR (E1 , . . . , Er ) are
similar matrices. Since PR (A1 , . . . , Ar ) = I and the unique matrix that is similar to I
is I itself, we conclude that PR (E1 , . . . , Er ) = I . Now, Ei is the diagonal matrix that
has the vector (ai (1), . . . , ai (d)) in the diagonal, so PR (a1 (j ), . . . , ar (j )) = 1 for all j  [d].

12

Thus the tuple a(j ) = (a1 (j ), . . . , ar (j )) belongs to the relation R for all j  [d]. Now we are
ready to dene the matrices for the variables Y1 , . . . , Yt .
For each j  [d], let b(j ) = (b1 (j ), . . . , bt (j ))  {1}t be a tuple of witnesses to the
existentially quantied variables in R (x1/a1 (j ), . . . , xr /ar (j )); such a vector of witnesses
must exist since the tuple a(j ) belongs to R and R denes R. Let Fk be the diagonal
matrix that has the vector (bk (1), . . . , bk (d)) in the diagonal, and let Yk be assigned the
matrix Bk = U 1FkU . Since U is unitary, each such matrix is Hermitian and squares
to the identity since bk (j )  {1} for all j  [d]. Moreover, E1 , . . . , Er , F1 , . . . , Ft pairwise
commute since they are diagonal matrices; thus A1 , . . . , Ar , B1 , . . . , Bt also pairwise commute
since they are simultaneously similar via U . Moreover, as each atomic formula in the matrix
of R is satised by the mapping sending xi (cid:55) ai (j ) and yi (cid:55) bi (j ) for all j  [d], another
application of Lemma 1 shows that the matrices that are assigned to the variables of this
atomic formula make the corresponding indicator polynomial evaluate to I . This means
that the assignment to the X and Y -variables makes a satisfying operator assignment for the
constraints of J that come from the constraint (Z, R) of I . As dierent constraints from I
produce their own sets of Y -variables, these denitions of assignments do not conict with
one another, and the proof of the lemma is complete.

The proof for the general case requires some new ingredients. Besides the need to take
care of null sets of exceptions as in the proof of Lemma 3, a ');
INSERT INTO posts (postId,userId,title,body) VALUES (311,9,'Generalized Satisability Problems via Operator Assignments (part 12)','new complication arises from
the need to build the operators for the new variables that are introduced by the reduction.
Concretely we need to make sure that the functions of witnesses, in contraposition to the
nite tuples of witnesses in the nite-dimensional case, are bounded and measurable. We go
carefully through the argument.

Proof of Lemma 5, general case. Assume that A1 , . . . , An are bounded self-adjoint linear op-
erators on H for the variables of I . Suppose that the operators A1 , . . . , Ar make a valid satis-
fying operator assignment for I . We need to dene bounded self-adjoint linear operators for
the new variables of J that were introduced by the construction. We dene these operators
simultaneously for all variables Y1 , . . . , Yt that come from the same constraint (Z, R) of I .
By renaming the components of Z if necessary, assume without loss of generality that
the variables in Z are X1 , . . . , Xr , where r is the arity of R. By the commutativity condition
of satisfying operator assignments, the operators A1 , . . . , Ar pairwise commute. As each
Ai is self-adjoint, it is also normal, and the Strong Spectral Theorem (c.f. Theorem 2)
applies. Thus, there exist a measure space (, M, ), a unitary map U : H  L2 (, )
and functions a1 , . . . , ar  L (, ) such that, for the multiplication operators Ei = Tai of
L2 (, ), the relations Ai = U 1EiU hold for each i  [r]. Equivalently, U AiU 1 = Ei .
i = I . Hence, ai ()2 = 1 for almost all   ; i.e., formally
From A2
i = I we conclude E 2
({   : ai ()2 (cid:54)= 1}) = 0. Thus, ai ()  {1} for almost all   . The conditions of
Lemma 1 apply, thus PR (A1 , . . . , Ar ) and PR (E1 , . . . , Er ) are similar linear operators. Since
PR (A1 , . . . , Ar ) = I and the unique linear operator that is similar to I is I itself, we
conclude that PR (E1 , . . . , Er ) = I . Now, Ei is the multiplication operator given by ai , and
ai ()  {1} for almost all   , so PR (a1 (), . . . , ar ()) = 1 for almost all   . Thus

13

Q =

Qi,ai

the tuple a() = (a1 (), . . . , ar ()) belongs to the relation R for almost all   . Now we
are ready to dene the operators for the variables Y1 , . . . , Yt .
For each    for which the tuple a() belongs to R, let b() = (b1 (), . . . , bt ()) 
{1}t be the lexicographical ly smal lest tuple of witnesses to the existentially quantied vari-
ables in R (x1/a1 (), . . . , xr /ar ()); such a vector of witnesses must exist since R denes R,
and the lexicographically smallest exists because R is nite. For every other   , dene
b() = (b1 (), . . . , bt ()) = (0, . . . , 0).
Note that each function bk :   C is bounded since its range is in {1, 0, 1}. We claim
that such functions of witnesses bk are also measurable functions of (, M, ). This will follow
from the fact that a1 , . . . , ar are measurable functions themselves, the fact that R is a nite
relation, and the choice of a denite tuple of witnesses of each   ; the lexicographically
smallest if a() is in R, or the all-zero tuple otherwise. We discuss the details.
Since R is nite, the event Q = {   : bk () = }, for xed   {+1, 0, 1}, can be
expressed as a nite Boolean combination of events of the form Qi, = {   : ai () =  },
(cid:17)
(cid:16) (cid:92)
(cid:91)
where i  [r] and   {1}. Here is how: If  (cid:54)= 0, then
aR:
i[r ]
b(a)k=
where b(a) denotes the lexicographically smallest tuple of witnesses in {1}t for the quanti-
ed variables in R (x1/a1 , . . . , xr /ar ). If  = 0, then Q is the complement of this set. Each
Qi, is a measurable set in the measure space (, M, ) since ai is a measurable function and
Qi, = a1
i (B1/4 ( )), where B1/4 ( ) denotes the complex open ball of radius 1/4 centered at
 , which is a Borel set in the standard topology of C. Since the range of bk is in the nite set
{1, 0, 1}, the preimage b1
k (S ) of each B');
INSERT INTO posts (postId,userId,title,body) VALUES (312,9,'Generalized Satisability Problems via Operator Assignments (part 13)','orel subset S of C is expressed as a nite Boolean
combination of measurable sets, and is thus measurable in (, M, ).
We just proved that each bk is bounded and measurable, so its equivalence class under
almost everywhere equality is represented in L (, ). We may assume without loss of
generality that bk is its own representative; else modify it on a set of measure zero in order
to achieve so. Let Fk = Tbk be the multiplication operator given by bk and let Yk be assigned
the linear operator Bk = U 1FkU , which is bounded because bk is bounded and U is unitary.
Also because U is unitary, each such operator is self-adjoint and squares to the identity since
bk ()  {1} for almost all   . Moreover, E1 , . . . , Er , F1 , . . . , Ft pairwise commute since
they are multiplication operators; thus A1 , . . . , Ar , B1 , . . . , Bt pairwise commute since they
are simultaneously similar via U . Moreover, as each atomic formula in the matrix of R is
satised by the mapping sending xi (cid:55) ai () and yi (cid:55) bi () for almost all   , another
application of Lemma 1 shows that the operators that are assigned to the variables of this
atomic formula make the corresponding indicator polynomial evaluate to I . This means
that the assignment to the X and Y -variables makes a satisfying operator assigment for the
constraints of J that come from the constraint (Z, R) in I . As dierent constraints from
I produce their own sets of Y -variables, these denitions of assignments are not in conict
with each other, and the proof of the lemma is complete.

(12)

14

4.3 The Extended Construction
We proved so far that satisfying operator assignments for I lift to satisfying operator as-
signments for J . We do not know if the converse is true. One could try to just take the
restriction of the satisfying assignment for J to the variables of I , but there is little chance
that this will work because there is no guarantee that the operators that are assigned to any
two variables that appear together in a constraint of I will commute. Instead of trying to
modify the assignment, we modify the instance J . Let us discuss a slightly modied version
of J , over a very minor extension of the constraint language B , that still allows lifting of
solutions, and for which the naif pro jection works for the backward direction. Let us stress
now that we plan to use this modied construction over a minor extension of the constraint
language merely as a technical device to get other results.
In the following, let T denote the full binary Boolean relation; i.e., T = {1}2 . Observe
that the indicator polynomial PT (X1 , X2 ) of the relation T is just the constant 1; the
letter T stands for true.
Let A and B be the constraint languages such that A is pp-denable from B . Let I and
J be the instances over A and B as dened above. The modied version of J will be an
instance over the expanded constraint language B  {T}. We denote it J and it is dened
as follows: the variables and the constraints of J are dened as in J , but we also add all
the binary constraints of the form ((Xi , Xj ), T), ((Xi , Yk ), T) or ((Yk , Y(cid:96) ), T), for every four
dierent variables Xi , Xj , Yk and Y(cid:96) that come from the same block in J .

4.4 Correctness: Operator Solutions Lift and also Pro ject
We argue that in this new construction, satisfying assignments not only lift from I to J ,
but also pro ject from J to I .
Lemma 6. Let I and J be as above and let H be a Hilbert space. Then the fol lowing
assertions are true.
1. For every f that is a satisfying operator assignment for I over H, ther');
INSERT INTO posts (postId,userId,title,body) VALUES (313,9,'Generalized Satisability Problems via Operator Assignments (part 14)','e exists g that
extends f and is a satisfying operator assignment for J over H,
2. For every g that is a satisfying operator assignment for J over H, the restriction f of
g to the variables of I is a satisfying operator assignment for I over H.

Proof. Statement 1 follows from Lemma 5: Fix f that is a satisfying operator assignment
for I and let g be given by Lemma 5. This is also an assignment for the variables of J .
The constraints of J that are already in J are of course satised by g . Next consider an
additional constraint of the form ((Xi , Xj ), T), ((Xi , Yk ), T) or ((Yk , Y(cid:96) ), T), for variables Xi ,
Xj , Yk and Y(cid:96) coming from the same block in J . By the moreover clause in Lemma 5,
the operators Ai , Aj , Bk and B(cid:96) associated to Xi , Xj , Yk and Y(cid:96) by g pairwise commute.
Moreover, the associated polynomial constraints PT (Ai , Aj ) = I , PT (Ai , Bk ) = I and
PT (Bk , B(cid:96) ) = I are trivial (i.e., void) since the indicator polynomial PT (X1 , X2 ) of T is
just the constant 1.

15

For statement 2, x g that is a satisfying operator assignment for J over H, and let f
be the restriction of g to the variables of I . Since g satises J , for every two variables Xi
and Xj that appear together in a constraint (Z, R) of I , the associated operators g(Xi ) and
g(Xj ) commute since Xi and Xj appear in the same block of J . Hence f (Xi ) and f (Xj )
commute. We still need to show that the polynomial constraint PR (f (Z )) = I is satised
for every constraint (Z, R) of I . To do so, we use Lemma 3 on an appropriately dened
system of polynomial equations.
Let r be the arity of R and let R be the pp-formula as in (11) that denes R from B . The
polynomials we dene have variables X1 , . . . , Xr , Y1 , . . . , Yt , Z1 , Z+1 that correspond to the
variables and constants in (11). For every k  [m], let Qk be the polynomial PSk (Wk ) + 1, so
that the equation Qk = 0 ensures PSk (Wk ) = 1, where PSk is the characteristic polynomial
of Sk , and Wk is the tuple of components from X1 , . . . , Xr , Y1 , . . . , Ys , Z1 , Z+1 that appear
in the atom Sk (wk ) of (11). Here we use Xi and Yj in place of xi and yj , respectively, and
Z1 and Z+1 in place of the constants 1 and +1, respectively. Let also Qm+1 and Qm+2
be the polynomials Z1 + 1 and Z+1  1, so that the equations Qm+1 = Qm+2 = 0 ensure
that Z1 = 1 and Z+1 = +1. Finally, let Q be the polynomial PR (X1 , . . . , Xr ) + 1, where
PR is the characteristic polynomial of R. It follows from the denitions that every Boolean
assignment that satises all equations Q1 =    = Qm+2 = 0 also satises Q = 0. Thus
Lemma 3 applies, and since g extended to g(Z1 ) = I and g(Z+1 ) = I satises all equations
Q1 =    = Qm+2 = 0, it also satises Q = 0. It follows that PR (f (Z )) = PR (g(Z )) = I ,
as was to be proved.

5 Satisability Gaps via Operator Assignments
Let A be a Boolean constraint language and let I be an instance over A. It is easy to see
that the following inequalities hold:
 (I )    (I )    (I ).
(13)
Indeed, the rst inequality holds because if we interpret the eld of complex numbers C as
a 1-dimensional Hilbert space, then the only solutions to the equation X 2 = 1 are X = 1
and X = +1. The second inequality is a direct consequence of the denitions. For the same
reason, if I is satisable in the Boolean domain, then it is satisable via fd-operators, and
if it is satisable via fd-operators, then it is satisable via operators. The converses are, in
general, not true; however, nding counterexamples is a non-trivial task. For the Boolean
constraint language LIN of ane relations, counterexamples are given by Mermins magic
square [16, 17] for the rst case, and by Slofstras recent construction [24] for the second
case. These will be discussed at some length in due time. In the rest of this section, we
characterize the Boolean constraint languages that exhibit such gaps.
We distinguish three types of gaps. Specically');
INSERT INTO posts (postId,userId,title,body) VALUES (314,9,'Generalized Satisability Problems via Operator Assignments (part 15)',', we say that an instance I witnesses
1. a satisability gap of the rst kind if  (I ) < 1 and   (I ) = 1;
2. a satisability gap of the second kind if  (I ) < 1 and   (I ) = 1;

16

3. a satisability gap of the third kind if   (I ) < 1 and   (I ) = 1.
As a mnemonic rule, count the number of stars  that appear in the dening inequalities in
1, 2 or 3 to recall what kind the gap is.
We say that a Boolean constraint language A has a satisability gap of the i-th kind,
i = 1, 2, 3, if there is at least one instance I over A that witnesses such a gap. Clearly, a gap
of the rst kind or a gap of the third kind implies a gap of the second kind. In other words,
if A has no gap of the second kind, then A has no gap of the rst kind and no gap of the
third kind. A priori no other relationships seem to hold. We show that, in a precise sense,
either A has no gaps of any kind or A has a gap of every kind. Recall from Section 4 that
T denotes the full binary Boolean relation; i.e. T = {1}2 . We are now ready to state and
prove the main result of this section.

Theorem 3. Let A be a Boolean constraint language. Then the fol lowing statements are
equivalent.

1. A does not have a satisability gap of the rst kind.

2. A does not have a satisability gap of the second kind.
3. A  {T} does not have a satisability gap of the third kind,
4. A is 0-valid, or A is 1-valid, or A is bijunctive, or A is Horn, or A is dual Horn.

The proof of Theorem 3 has two main parts. In the rst part, we show that if A satises
at least one of the conditions in the fourth statement, then A has no satisability gaps of the
rst kind or the second kind, and A  {T} has no satisability gaps of the third kind. In the
second part, we show that, in all other cases, A has satisability gaps of the rst kind and
the second kind, and A  {T} has satisability gaps of the third kind. The ingredients in the
proof of the second part are the existence of gaps of all three kinds for LIN, results about
Posts lattice [21], and gap-preserving reductions that use the results about pp-denability
established in Section 4.

5.1 No Gaps of Any Kind

Assume that A satises at least one of the conditions in the fourth statement in Theorem 3.
First, we observe that the full relation T is 0-valid, 1-valid, bijunctive, Horn, and dual
Horn.
Indeed, T is obviously 0-valid and 1-valid. Moreover, it is bijunctive, Horn, and
dual Horn because it is equal to the set of satisfying assignments of the Boolean formula
(x  x)  (y  y), which is bijunctive, Horn, and dual Horn. Therefore, to prove that the
fourth statement in Theorem 3 implies the other three statement, it suces to prove that if
A satises at least one of the conditions in the fourth statement, then A has no gaps of any
kind. Towards this goal, we argue by cases.
We start with the trivial cases in which A is 0-valid or 1-valid. If an instance I of A
contains a constraint of the form (Z, F), where F is an empty relation (of some arity), then
I is not satisable by any operator assignment. Otherwise, I is satisable in the Boolean

17

domain, hence it is satisable by assigning the identity operator I to every variable, if A is
0-valid, or by assigning the operator I to every variable, if A is 1-valid.
Next, we have to show that if A is bijunctive or Horn or dual Horn, then A has no gaps
of any kind. As discussed earlier, it suces to show that A does not have a gap of the second
kind (since a gap of the rst kind or a gap of the third kind implies a gap of the second
kind).
Ji [15] proved that if I is a 2SAT instance or a HORN SAT instance that is satisable via
fd-operators, then I is also satisable in the Boolean domain. In other words, Ji showed that
2SAT and HORN SAT have no gaps of the rst kin');
INSERT INTO posts (postId,userId,title,body) VALUES (315,9,'Generalized Satisability Problems via Operator Assignments (part 16)','d. This is quite close to what we have
to prove, but there are two dierences. First, a constraint language A of Boolean relations
is bijunctive if every relation in A is the set of satisfying assignments of a 2CNF-formula,
but this formula need not be a 2-clause. Similarly, A is Horn (dual Horn) if every relation
in A is the set of satisfying assignments of a Horn (dual Horn) formula, but this formula
need not be a Horn (dual Horn) clause. This, however, is a minor complication that can
be handled with some additional arguments, the details of which will be provided later on.
Second, at rst glance, Jis proof for 2SAT and HORN SAT does not seem to extend to
operator assignments of arbitrary (nite or innite) dimension. The reason for this is that
Jis argument relies on the existence of eigenvalues and associated orthogonal eigenspaces
for the linear operators, which are not guaranteed to exist in the innite-dimensional case,
even for self-adjoint bounded linear operators. Note however that in our case we have the
additional requirement that the operators satisfy A2 = I , and in such a case their eigenvalues
and associated eigenspaces can be reinstated. This observation could perhaps be used to give
a proof along the lines of Jis that 2SAT and HORN SAT have no gaps of the second kind.
However, we prefer to give an alternative and more direct proof that does not rely at all on
the existence of eigenvalues. Our proof is based on the manipulation of non-commutative
polynomial identities, a method that has been called the substitution method (see, e.g., [7]).
Lemma 7. Let I be a 2SAT instance or a HORN SAT instance or a DUAL HORN SAT
instance. Then the fol lowing statements are equivalent.
1. I is satisable in the Boolean domain;
2. I is satisable via fd-operators;
3. I is satisable via operators.

We split the proof into two: one for 2SAT and another one for HORN SAT; the proof for
DUAL HORN SAT is analogous to the proof for HORN SAT, and it is omitted.
Proof of Lemma 7 for 2SAT. Let I be a 2CNF-formula. The implications 1 = 2 and
2 = 3 follow from the denitions. To prove the implication 3 = 1, assume that f is a
satisfying operator assigment for I over a (nite-dimensional or innite-dimensional) Hilbert
space H, and, towards a contradiction, assume that I is unsatisable in the Boolean domain.
We will make use of the well-known characterization of unsatisable in the Boolean domain
2SAT instances in terms of a reachability property of their associated implication graph.
For I , the implication graph is the directed graph G that has one vertex for each literal x

18

or x of every variable x in I , and two directed edges for each clause ((cid:96)1  (cid:96)2 ) of I , one
edge from (cid:96)1 to (cid:96)2 , and another one from (cid:96)2 to (cid:96)1 . The well-known characterization states
that I is unsatisable in the Boolean domain if and only if there exists a variable x and
two directed paths in G, one from the variable x to the literal x, and another one from
the literal x to the variable x (see, e.g., [19]). Accordingly, let (cid:96)1 , . . . , (cid:96)r and m1 , . . . , ms be
literals such that x, (cid:96)1 , . . . , (cid:96)r , x and x, m1 , . . . , ms , x are the vertices in the paths from x
to x and from x to x, respectively, ');
INSERT INTO posts (postId,userId,title,body) VALUES (316,9,'Generalized Satisability Problems via Operator Assignments (part 17)','in the order they are traversed.
The existence of the path x, (cid:96)1 , . . . , (cid:96)r , x from the variable x to the literal x in the
implication graph G means that the clauses
(x  (cid:96)1 ), ((cid:96)1  (cid:96)2 ), . . . , ((cid:96)r1  (cid:96)r ), ((cid:96)r  x)
(14)
are clauses of the instance I . Symmetrically, the existence of the path x, m1 , . . . , ms , x
from the literal x to the variable x in the implication graph G means that the clauses
(x  m1 ), (m1  m2 ), . . . , (ms1  ms ), (ms  x)
are clauses of the instance I .
In the case of satisability in the Boolean domain, one reasons that the instance I
is unsatisable, because if it were satisable by some truth assignment, then the path of
implications from x to x forces x to be set to false, while the path of implications from x
to x forces x to be set to true. In what follows, we will show that, with some care, essentially
the same reasoning can be carried out for operator assignments that satisfy the instance I .
Extend the operator assignment f to all literals by setting f ((cid:96)) = sg((cid:96))f (x), where x the
variable underlying (cid:96). Since f is a quantum satisfying assignment for I , Lemma 2 implies
that

(15)

(I  f (x))(I + f ((cid:96)1 )) = 0
(I  f ((cid:96)i ))(I + f ((cid:96)i+1 )) = 0,
(I  f ((cid:96)r ))(I  f (x)) = 0

1  i  r  1.

We now claim that

(I  f (x))(I + f ((cid:96)i )) = 0,

1  i  r.

(16)
(17)
(18)

(19)

We prove the claim by induction on i. For i = 1, what we need is just equation (16). By
induction, assume now that

(I  f (x))(I + f ((cid:96)i1 )) = 0
holds for some i with 2  i  r  1. By (17), we have that
(I  f ((cid:96)i1 ))(I + f ((cid:96)i )) = 0.

holds. First, by multiplying equation (20) from the right by (I + f ((cid:96)i )), we get
(I  f (x))(1 + f ((cid:96)i1 ))(1 + f ((cid:96)i )) = 0

(20)

(21)

(22)

19

(25)

(26)

(23)

(24)

Second, by multiplying equation (21) from the left by (I  f (x)), we get
(I  f (x))(1  f ((cid:96)i1 ))(1 + f ((cid:96)i )) = 0
By adding equations (22) and (23), we obtain
(I  f (x))(I + f ((cid:96)i )) = 0,
as desired. In particular, by considering the case i = r, we get
(I  f (x))(I + f ((cid:96)r )) = 0,
which, after multiplying out the left-hand side, becomes
I + f ((cid:96)r )  f (x)  f (x)f ((cid:96)r ) = 0.
Furthermore, by multiplying out the left-hand side of equation (18), we get
I  f (x)  f ((cid:96)r ) + f ((cid:96)r )f (x) = 0.
(27)
Since the variable x and the literal (cid:96)r appear in the same clause of the instance I , namely, the
clause ((cid:96)r  x), we have that f (x)f ((cid:96)r ) = f ((cid:96)r )f (x). Therefore, by adding equations (26)
and (27), we get that 2I  2f (x) = 0, which implies that f (x) = I .
An entirely symmetric argument using the path from x to x, instead of the path from
x to x, gives f (x) = I , which contradicts the previous nding that f (x) = I .
Proof of Lemma 7 for HORN SAT. Let I be a Horn formula. As with the proof for 2SAT,
the only non-trivial direction is 3 = 1. To prove the implication 3 = 1, assume that
f is a satisfying operator assigment for I over a (nite-dimensional or innite-dimensional)
Hilbert space H, and, towards a contradiction, assume that I is unsatisable in the Boolean
domain. As in the proof for 2SAT, let f be extended to all literals by f ((cid:96)) = sg((cid:96))f (x), where
x is the');
INSERT INTO posts (postId,userId,title,body) VALUES (317,9,'Generalized Satisability Problems via Operator Assignments (part 18)',' variable underlying x. We will make use of the characterization of unsatisable in
the Boolean domain Horn instances in terms of unit resolution. For this, we need to rst
introduce some terminology and notation. If C and C (cid:48) are two clauses such that C contains
a literal (cid:96) and C (cid:48) contains the complementary literal (cid:96) of (cid:96), then the resolution rule produces
in one step the resolvent clause D that is the disjunction of all literals in the premises C
and C (cid:48) other than (cid:96) and (cid:96). The unit resolution rule is the special case of the resolution rule
in which (at least) one of the clauses C and C (cid:48) is a single literal. It is well known (see, e.g.,
[23]) that a Horn formula I is unsatisable if and only if there is a unit resolution derivation
of the empty clause from the clauses of I , i.e., there is a sequence C1 , . . . , Cm of clauses such
that, for each i  {1, . . . , m}, we have that Ci is one of the clauses of I or Ci is obtained
from earlier clauses Cj and Ck in the sequence via the unit resolution rule. Clearly, in a
unit resolution derivation of the empty clause, the last application of the unit resolution rule
involves two clauses each of which is the complementary literal of the other.
In what follows, we will show that a unit resolution derivation can be simulated by a
sequence of equations involving operator assignments. We begin by formulating and proving
the following claim.

20

Claim 1: Let ((cid:96)1      (cid:96)r ) be clause and let (cid:96)j be the complementary literal of some literal
(cid:96)j in that clause. If f satises both the clause ((cid:96)1      (cid:96)r ) and the literal (cid:96)j , then f also
satises the resolvent ((cid:96)1      (cid:96)j1  (cid:96)j+1      (cid:96)r ) of ((cid:96)1      (cid:96)r ) and (cid:96)j ; equivalently,
the operators {f ((cid:96)i ) : i (cid:54)= j } pairwise commute and
j1(cid:89)
r(cid:89)
(I + f ((cid:96)i ))
(I + f ((cid:96)i )) = 0.
i=1
i=j+1

(28)

Observe that for the unit resolution rule, as is the case here, the resolvent is always
a subclause of one of the premises.
In particular, since f satises both premises, all the
operators involved in the premises commute, and so do the ones involved in the resolvent
clause. To complete the proof of the claim observe that, since f satises both the clause
((cid:96)1      (cid:96)r ) and the literal (cid:96)j , the corresponding operators commute, and the identity of
r(cid:89)
polynomials in commuting variables of Lemma 2 implies that
(I + f ((cid:96)i )) = 0
i=1 (I + f ((cid:96)i )) from the left, and by (cid:81)r
By multiplying equation (30) by (cid:81)j1
i=1
(I  f ((cid:96)j )) = 0.
i=j+1 (I + f ((cid:96)i ))
from the right, we get(cid:32)j1(cid:89)
(cid:32) r(cid:89)
(cid:33)
(cid:33)
(I + f ((cid:96)i ))
(I + f ((cid:96)i ))
i=1
i=j+1

(I  f ((cid:96)j ))

= 0.

(29)

(30)

(31)

By adding equations (29) and (31), we get (28), which completes the proof of Claim 1.
Consider now a unit resolution derivation C1 , . . . , Cm of the empty clause from the clauses
of I . Since the operator assignment f satises every clause of I , we can apply Claim 1
repeatedly and, by induction, show that f satises each clause in this derivation. Since Cm
is the empty clause, it must have been derived via the unit resolution rule from two earlier
clauses each of which is the complementary literal of the other, say, (cid:96) and (cid:96). So, we must
have f ((cid:96)) = I and f ((cid:96)) = I , which is a contradiction since f ((cid:96)) = f ((cid:96)).

In what follows, we will use Lemma 7 to show that if A is bijunctive or Horn or dual
Horn, then A has no gaps of any kind.
Assume that A is bijunctive. Note that we cannot apply Lemma 7 directly to conclude
that A');
INSERT INTO posts (postId,userId,title,body) VALUES (318,9,'Generalized Satisability Problems via Operator Assignments (part 19)',' has no gaps of any kind, because the relations in the constraint-language A are
dened by conjunctions of 2-clauses, but need not be dened by individual 2-clauses.
In
order to be able to apply Lemma 7, we rst need to verify the following claim. Assume that
(Z, R) is a constraint in which R is a relation in A dened by a conjunction C1      Cm ,
where each Ci is a 2-clause on the variables in Z . Then a satisfying operator assignment
for the instance consisting of the single constraint (Z, R) will also satisfy each of the 2-
clause constraints (W1 , C1 ), . . . , (Wr , Cr ) individually, where Wi = (Zci , Zdi ) is the tuple of

21

components of Z = (Z1 , . . . , Zr ) that appear in Ci . To prove this claim, rst note that
the commutativity condition on the operators assigned to the variables in Wi is guaranteed
by the commutativity condition on the variables in Z . Thus, we just need to check that
the characteristic polynomial of Ci evaluates to I , and to do so we use Lemma 3 for an
appropriately dened system of polynomial equations. In the remaining, x i  [m].
Our polynomials have variables X1 , . . . , Xr . Let Q1 be the polynomial PR (X1 , . . . , Xr )+1,
so that the equation Q1 = 0 ensures PR (X1 , . . . , Xr ) = 1, where PR is the characteristic
polynomial of R. Let Q be the polynomial PCi (Xci , Xdi ) + 1, so that the equation Q = 0
ensures PCi (Xci , Xdi ) = 1, where PCi is the characteristic polynomial of Ci , and ci and di are
the indices of the components of Z in Wi . Then, every Boolean assignment that satises the
equation Q1 = 0 belongs to R, from which it follows that the Boolean assignment satises
the conjunct Ci in the bijunctive denition of R, and hence it also satises the equation
Q = 0. Thus, Lemma 3 applies and every operator assignment that satises PR (Z ) = I
also satises PCi (W ) = I , as was to be proved.
We are now ready to complete the proof that if A is bijunctive, then A has no gaps. Let
I be an instance over A that is satisable via operators. The preceding paragraph shows
that the 2SAT instance that results from replacing each constraint in the instance I by its
dening conjunction of 2-clauses is also satisable via operators. By Lemma 7, this 2SAT
instance is also satisable in the Boolean domain. But then I itself is satisable in the
Boolean domain, as was to be shown.
If A is Horn or dual Horn, then the proof is entirely analogous.

5.2 Background on Posts Lattice

Before we start with the second part in the proof of Theorem 3, we need to introduce some
basic terminology and basic results from universal algebra; we devote this section to that.
Let R  {1}r be a Boolean relation of arity r and let f : {1}m  {1} be a Boolean
operation of arity m. The relation R is invariant under f if, for all sequences of m many
r-tuples (a1,1 , . . . , a1,r ), . . . , (am,1 , . . . , am,r ) in {1}r , the following holds:

if (a1,1 , . . . , a1,r ), . . . , (am,1 , . . . , am,r ) are tuples in R,
then (f (a1,1 , . . . , am,1 ), . . . , f (a1,r , . . . , am,r )) is also a tuple in ');
INSERT INTO posts (postId,userId,title,body) VALUES (319,9,'Generalized Satisability Problems via Operator Assignments (part 20)','R.

(32)

Note that the tuple in the second line is obtained by applying the m-ary operation f to the
m many tuples in the rst line componentwise. If A is a Boolean constraint language, we
say that A is invariant under f if every relation in A is invariant under f . Whenever A is
invariant under f we also say that f is a closure operation of A.
The importance of the closure operations of a constraint language stems from the fact
that they completely determine the relations that are pp-denable from it. This semantic
characterization of the syntactic notion of pp-denability was discovered by Geiger [13] and,
independently, Bodnarchuk et al. [3], for all constraint languages of arbitrary but nite
domain. Here we state the special case of this characterization for the Boolean domain,
since only this special case is needed in our applications.

22

Theorem 4 ([13, 3]). Let A be a Boolean constraint language and let R be a Boolean relation.
The fol lowing statements are equivalent:

1. R is pp-denable from A by a pp-formula without constants,

2. R is invariant under al l Boolean closure operations of A.

In the following we refer to Theorem 4 as Geigers Theorem.
Recall from Section 4 that a pp-formula without constants is one in which the constants
+1 and 1 do not appear in its quantier-free part of the formula. Although it will not
be used until a later section, it is worth pointing out here that a similar characterization
of pp-denability with constants exists.
Indeed, it is easy to see that Geigers Theorem
implies that a Boolean relation R is pp-denable from the Boolean constraint language A
by a pp-formula with constants if and only if R is invariant under all idempotent Boolean
closure operations of A, or equivalently, invariant under all Boolean closure operations of the
Boolean constraint language A+ that is obtained from A by adding the two unary singleton
relations {+1} and {1}; i.e., A+ = A  {{+1}, {1}}. We return to the issue of denability
with constants in Section 7.
For every set F of Boolean operations, let Inv(F ) denote the set of all Boolean relations
that are invariant under all operations in F . Conversely, for every set of Boolean relations A,
let Pol(A) denote the set of all Boolean operations under which all relations in A are invariant.
Geigers Theorem implies that the mappings A (cid:55) Pol(A) and F (cid:55) Inv(F ) are the lower and
upper adjoints of a Galois connection [8] between the partially ordered set of sets of Boolean
relations ordered by inclusion, and the partially ordered set of sets of Boolean operations,
also ordered by inclusion.
Note that for every constraint language A, the set Pol(A) contains all projection oper-
ations : all operations f : {1}r  {1} for which there exists an index i  [r] such that
f (x1 , . . . , xr ) = xi for all (x1 , . . . , xr )  {1}r . Also, Pol(A) is closed under compositions :
if f : {1}s  {1} and g1 , . . . , gs : {1}r  {1} are operations from Pol(A), then the
operation h = f  (g1 , . . . , gs ) dened by h(x1 , . . . , xr ) = f (g1 (x1 , . . . , xr ), . . . , gs (x1 , . . . , xr ))
for all (x1 , . . . , xr )  {1}r is also in Pol(A). Any set of relations that contains all pro jection
operations and that is closed under compositions is called a clone.
Post [21] analyzed the collection of all clones of Boolean operations and completely de-
termined the inclusions between them. In particular, he showed that this collection forms a
lattice under inclusion, which is known as Posts lattice. In denoting clones in Posts lattice,
we will follow the notation and terminology used by Bohler et al. [4]. The lattice is repre-
sented by the diagram in Figure 1, which is also borrowed from [4] (we thank Steen Reith
for allowing us to reproduce the diagram here).
Each circle in the diagram of Figure 1 represents a clone of Boolean operations, and a
line b');
INSERT INTO posts (postId,userId,title,body) VALUES (320,9,'Generalized Satisability Problems via Operator Assignments (part 21)','etween two circles denotes inclusion of the clone of the lower circle into the clone of
the upper circle. Post showed that every clone of Boolean operations is represented in the
diagram. Post also identied a nite basis of operations for each clone, which means that
the clone is the smallest class of operations that contains the operations in the basis and all
the pro jections operations, and that is closed under composition. For our application, we

23

Figure 1: Graph of all Boolean clones (diagram by Steen Reith).

24

R1R0BFR2MM1M0M2S20S30S0S202S302S02S201S301S01S200S300S00S21S31S1S212S312S12S211S311S11S210S310S10DD1D2LL1L0L2L3VV1V0V2EE0E1E2II1I0I2N2N
{false}
{true}
{(x  y)  (x  z )  (y  z )}

I2
I0
I1
D2

{x  y}
{x  y}
{x  y  z}
{x}

E2
V2
L2
N2

Figure 2: Bases of some selected clones from Figure 1. Here , ,  and  denote Boolean
conjunction, Boolean disjunction, Boolean negation, and Boolean exclusive or, respectively.

need only the bases for the eight clones called I2 , I0 , I1 , D2 , E2 , V2 , L2 and N2 . These are
listed in the table in Figure 2.
The nal ingredient we need from Posts lattice is a characterization of the tractable
Boolean constraint languages from Schaefers Theorem in terms of their closure operations.

Theorem 5 (see Section 1.1 in [5]). Let A be a Boolean constraint language. The fol lowing
statements hold.

1. A is 0-valid if and only if A is invariant under the constant false operation.

2. A is 1-valid if and only if A is invariant under the constant true operation.
3. A is bijunctive if and only if A is invariant under (x  y)  (x  z )  (y  z ).
4. A is Horn if and only if A is invariant under x  y .
5. A is dual Horn if and only if A is invariant under x  y .
6. A is ane if and only if A is invariant under x  y  z .

For the connection with Posts lattice, note that, by Figure 2, the six conditions listed on
the right of the entries 1 through 6 in Theorem 5 are equivalent to Pol(A) containing the
clones I0 , I1 , D2 , E2 , V2 and E2 , respectively.

5.3 Gaps of Every Kind

We are ready to proceed with the second part in the proof of Theorem 3. Assume that A
satises none of the conditions in the fourth statement in Theorem 3, i.e., A is not 0-valid,
A is not 1-valid, A is not bijunctive, A is not Horn, and A is not dual Horn. We will show
that A has a satisability gap of the rst kind (hence, A also has a satisability gap of the
second kind) and A  {T} has a satisabiilty gap of the third kind.
As a stepping stone, we will use the known fact that LIN has gaps of every kind. We
now discuss the proof of this fact and give the appropriate references to the literature.
every such equation is a parity equation of the form (cid:81)r
Recall that LIN is the class of all ane relations, i.e., Boolean relations that are the set of
solutions of a system of linear equations over the two-element eld. In the 1-representation,
i=1 xi = y , where y  {1}.

25

Mermin [16, 17] considered the following system M of parity equations:

X11X12X13 = 1
X21X22X23 = 1
X31X32X33 = 1

X11X21X31 = 1
X12X22X32 = 1
X13X23X33 = 1.

(33)

Graphically, this system of equations can be represented by a square, where each equation
on the left of (33) comes from a row, and each equation on the right of (33) comes from a
column.

X11 X12 X13

+1

X21 X22 X23

+1

X31 X32 X33
+1 +1 1

+1

It is easy to see that this system of equations has no solutions in the Boolean domain.
Indeed, by multiplying the left-hand sides of all equations, we get 1 because every variable
Xij occurs twice in the system and X 2
ij = 1. At the same time, by multiplying the right-hand
sides of all equations, we get 1, hence the system has no solutions in the Boolean domain.
Observe, however, that this argument used the assumption that variables commute pairwise,
even if they do not appear in the same equation. Thus, this argument does not go through if
one assumes only that variables occurring in the same equation commute pairwise. Mermin
[16, 17] showed that');
INSERT INTO posts (postId,userId,title,body) VALUES (321,9,'Generalized Satisability Problems via Operator Assignments (part 22)',' the system M has a solution consisting of linear operators on a Hilbert
space of dimension four. Thus, in our terminology, Mermin established the following result.
Theorem 6 ([16, 17]). M witnesses a satisability gap of the rst kind for LIN.

Cleve and Mittal [7, Theorem 1] have shown that a system of parity equations has a
solution consisting of linear operators on a nite-dimensional Hilbert space if and only if
there is a perfect strategy in a certain non-local game in the tensor-product model. Cleve,
Liu, and Slofstra [6, Theorem 4] have shown that a system of parity equations has a solution
consisting of linear operators on a (nite-dimensional or innite-dimensional) Hilbert space if
and only if there is a perfect strategy in a certain non-local game in the commuting-operator
model. Slofstra [24] obtained a breakthrough result that has numerous consequences about
these models.
In particular, Corollary 3.2 in Slofstras paper [24] asserts that there is a
system S of parity equations whose associated non-local game has a perfect strategy in
the commuting-operator model, but not in the tensor-product model. Thus, by combining
Theorem 1 in [7], Theorem 4 in [6], and Corollary 3.2 in [24], we obtain the following result.
Theorem 7 ([6, 7, 24]). S witnesses a satisability gap of the third kind for LIN.

26

LIN has a rather special place among all classes of Boolean relations that are not 0-valid,
are not 1-valid, are not bijunctive, are not Horn, and are not dual Horn. This special role
is captured by the next lemma, which follows from Posts analysis of the lattice of clones of
Boolean functions from Section 5.2.

Lemma 8. Let A be a Boolean constraint language. If A is not 0-valid, not 1-valid, not
bijunctive, not Horn, and not dual Horn, then LIN is pp-denable from A.

Proof. Assume that A is a Boolean constraint language satisfying the hypothesis of Lemma 8.
We consider the clone Pol(A) and distinguish several cases using Posts lattice.
If Pol(A) is the smallest clone I2 in Posts lattice, then Pol(A) contains only the pro jection
functions; hence, every Boolean relation is closed under every function in Pol(A). Geigers
Theorem implies that every Boolean relation and, in particular, every relation in LIN, is
pp-denable from A (and, in fact, it is pp-denable without using constants).
If Pol(A) is not the smallest clone I2 in Posts lattice, then it must contain one of the
seven minimal clones I0 , I1 , D2 , E2 , V2 , L2 , N2 that contain I2 . Recall that these clones have
bases of operations as described in Figure 2. Since A is not i-valid, where i = 0, 1, and since
the clone Ii is generated by the constant function ci (x) = i, it must be the case that Pol(A)
does not contain the clone I0 or the clone I1 . Since A is not bijunctive, there is a relation in
A that is not closed under the majority function ma j(x, y , z ) = (x  y)  (x  z )  (y  z ).
Since the clone D2 is generated by the function ma j(x, y , z ), it must be the case that that
Pol(A) does not contain the clone D2 . Since A is not Horn, there is a relation in A that
is not closed under the function and(x, y) = x  y . Since the clone E2 is generated by the
function and(x, y), it must be the case that Pol(A) does not contain the clone E2 . Since A is
not dual Horn, there is a relation in A that is not closed under the function or(x, y) = x  y .
Since the clone V2 is generated by the function or(x, y), it must be the case that that Pol(A)
does not contain the clone V2 .
The preceding analysis shows that there are just two possibilities: Pol(A) contains the
clone L2 or Pol(A) contains the clone N2 . Assume rst that Pol(A) contains the clone L2 .
Since L2 is generated by the exclusive or function (x, ');
INSERT INTO posts (postId,userId,title,body) VALUES (322,9,'Generalized Satisability Problems via Operator Assignments (part 23)','y , z ) = x  y  z and since a relation is
ane if and only if it is closed under the function , Geigers Theorem implies that a relation
is pp-denable without constants from A if and only if it is an ane relation. Thus, LIN
is pp-denable from A (and, in fact, it is pp-denable without constants). Finally, assume
that Pol(A) contains the clone N2 . Since Pol(A) is generated by the function not(x) = x,
Geigers Theorem implies that a relation is pp-denable without constants from A if and
only if it is it is closed under the function not(x). In particular, for every n  1 and for
i = 0, 1, the ane relation that is the set of solutions of the equation x1 +    + x2n = i mod 2
is pp-denable without constants from A. By using the constant 0 in these equations, we
have that for every n  1 and for every i = 0, 1, the ane relation that is the set of solutions
of the equation x1 +    + x2n1 = i mod 2 is pp-denable from A (recall that pp-denitions
allow constants). It follows that LIN is pp-denable from A.

The nal lemma in this section asserts that reductions based on pp-denitions preserve
satisability gaps upwards.

27

Lemma 9. Let B and C be Boolean constraint languages such that B is pp-denable from C .

1. If B has a satisability gap of the rst kind, then so does C .
2. If B has a satisability gap of the third kind, then so does C  {T}.
Proof. For the rst part, assume that B is pp-denable from C and that I is an instance that
witnesses a satisability gap of the rst kind for B . Thus, I is satisable via fd-operators,
but is not satisable in the Boolean domain. Let J be the instance over C as dened in
Section 4.1. On the one hand, by Lemma 5, the instance J is also satisable via fd-operator.
On the other hand, by Lemma 4, the instance J is also not satisable in the Boolean domain.
Thus, J witnesses a satisability gap of the rst kind for C .
For the second part, assume that B is pp-denable from C and that I is an instance that
witnesses a satisability gap of the third kind for B . Thus, I is satisable via operators,
but it is not satisable via fd-operators. Let J be the instance over C  {T} as dened in
Section 4.3. By Lemma 6, the instance J is satisable via operators, but it is not satisable
via fd-operators. Thus, J witnesses a satisability gap of the third kind for C  {T}.

We now have all the machinery needed to put everything together.
Let A be a Boolean constraint language that is not 0-valid, not 1-valid, not bijunctive,
not Horn, and not dual Horn. By Lemma 8, we have that LIN is pp-denable from A. Since,
by Theorem 6, LIN has a satisability gap of the rst kind, the rst part of Lemma 9 implies
that A has a satisability gap of the rst kind. Since, by Theorem 7, LIN has a satisability
gap of the third kind, the second part of Lemma 9 implies that A has a satisability gap of
the third kind. The proof of Theorem 3 is now complete.

6 Further Applications

In this section we discuss two applications of the results from Sections 4 and 5. The rst
application is about classication theorems in the style of Schaefer. The second application
builds on Slofstras results to answer some open questions from ');
INSERT INTO posts (postId,userId,title,body) VALUES (323,9,'Generalized Satisability Problems via Operator Assignments (part 24)','[1] on the quantum realiz-
ability of contextuality scenarios. While these open questions were solved earlier by Fritz
also using Slofstras results (see [12]), our alternative perspective may still add some value
since, as we will see, we obtain improved, and indeed optimal, parameters.

6.1 Dichotomy Theorems

For a Boolean constraint language A, let SAT(A) denote the following decision problem:
Given an instance I over A, is I satisable in the Boolean domain?
Similarly, let SAT (A) and SAT (A) be the versions of the problem in which the questions
are whether I is satisable via an operator assignment on a nite-dimensional Hilbert space,
or on an arbitrary Hilbert space, respectively. We say that a problem poly-m-reduces to

28

another if there is a polynomial-time computable function that transforms instances of the
rst problem into instances of the second in such a way that the answer is preserved.
Recall that T denotes the full binary Boolean relation {1}2 . The construction in Sec-
tion 4.3 and Lemma 6 give the following:
Lemma 10. Let A and B be Boolean constraint languages and let A(cid:48) = A  {T} and
B (cid:48) = B  {T}. If A is pp-denable from B , then
1. SAT(A(cid:48) ) poly-m-reduces to SAT(B (cid:48) ), SAT (B (cid:48) ), and SAT (B (cid:48) ),
2. SAT (A(cid:48) ) poly-m-reduces to SAT (B (cid:48) ).
3. SAT (A(cid:48) ) poly-m-reduces to SAT (B (cid:48) ).
Slofstras Corollary 3.3 in [24] in combination with Theorem 4 in [6] gives the undecid-
ability of SAT (LIN) which, from now on we denote by LIN SAT .
Theorem 8 ([24],[6]). LIN SAT is undecidable.
In combination with Lemmas 7, 10, and 8, we get the following dichotomy theorem:
Theorem 9. Let A be a Boolean constraint language and let A(cid:48) = A  {T}. Then, exactly
one of the fol lowing holds:
1. SAT (A(cid:48) ) is decidable in polynomial time,
2. SAT (A(cid:48) ) is undecidable.
Moreover, the rst case holds if and only if A is 1-valid, or A is 0-valid, or A is bijunctive,
or A is Horn, or A is dual Horn.
Proof. If A is 1-valid, 0-valid, bijunctive, Horn, or dual Horn, then A(cid:48) is also of the same
type; indeed T is both 1-valid and 0-valid, and it is also bijunctive, Horn and dual Horn
since it is dened by the empty conjunction of any kind of clauses. Thus SAT (A(cid:48) ) is the
same problem as SAT(A(cid:48) ) by Lemma 7, which is solvable in polynomial time.
If on the contrary A is neither 1-valid, nor 0-valid, nor bijunctive, nor Horn, nor dual
Horn, then Lemma 8 applies and LIN has a pp-denition from A. In such a case Lemma 10
applies and SAT (LIN(cid:48) ) reduces to SAT (A(cid:48) ), where LIN(cid:48) denotes LIN  {T}. Since every
instance of LIN SAT is also an instance of SAT (LIN(cid:48) ), the undecidability of SAT (A(cid:48) )
follows from Theorem 8.
Note that, in case 2, Theorem 9 states that SAT (A(cid:48) ) is undecidable but it says nothing
about SAT (A). Luckily, in most cases it is possible to infer the undecidability of SAT (A)
from the undecidability of SAT (A(cid:48) ). This is the case, for example, for both
3SAT = {{1}3 \ {(a1 , a2 , a3 )} : a1 , a2 , a3  {1}},
3LIN = {{(a1 , a2 , a3 )  {1}3 : a1a2a3 = b} : b  {1}}.
In the following we write 3LIN SAT and 3LIN SAT to denote the problems SAT (A)
and SAT (A) for A = 3LIN. Similarly, we use 3SAT and 3SAT to denote SAT (A) and
SAT (A) for A = 3SAT.

29

Theorem 10. 3LIN SAT and 3SAT are both undecidable.

Proof. Let A be the Boolean constraint language of 3LIN or 3SAT. It follows from Theorem 9
that SAT (A(cid:48) ) is undecidable. Now we reduce this problem to SAT (A). Take any instance
I over A(cid:48) and replace each constraint of the type ((Z1 , Z2 ), T) by an equation Z1Z2Y = 1
in the case of 3LIN, and a clause Z1  Z2  Y in the case of 3SAT, where Y is a fresh
variable not used anywhere else in the instance. Let J be the resulting instance. If f is
a satisfying operator assi');
INSERT INTO posts (postId,userId,title,body) VALUES (324,9,'Generalized Satisability Problems via Operator Assignments (part 25)','gnment for I , then we claim that an appropriate extension g of f
is a satisfying operator assignment for J . For 3LIN, set g(Y ) = f (Z2 )f (Z1 ). For 3SAT,
set g(Y ) = I . To see that this works, rst note that g(Z1 ) = f (Z1 ) and g(Z2 ) = f (Z2 )
commute since they appear together in a constraint of I . Thus, in both cases g(Z1 ), g(Z2 ) and
g(Y ) pairwise commute. Moreover, in the 3LIN case the assignment g(Y ) = f (Z2 )f (Z1 )
is chosen so that the equation g(Z1 )g(Z2 )g(Y ) = I is satised; to check this, multiply
g(Y ) = f (Z2 )f (Z1 ) by g(Z1 )g(Z2 ) from the right and use g(Z2 )f (Z2 ) = f (Z2 )2 = I and
g(Z1 )f (Z1 ) = f (Z1 )2 = I . Also, in the 3SAT case the assignment g(Y ) = I annihilates
the product in the expression of the characteristic polynomial of the clause Z1  Z2  Y in
see Lemma 2, which makes the characteristic polynomial evaluate to I regardless of what
g(Z1 ) and g(Z2 ) are. Thus, the new constraints are satised by g and the claim is proved.
Conversely, if g is a satisfying operator assignment for J , then the restriction of g to
the variables of I is a satisfying operator assignment for I , just because the commutativity
of f (Z1 ) and f (Z2 ) is enforced by the fact that they appear together in the constraint
Z1Z2Y = I or Z1  Z2  Y of J , and because the characteristic polynomial of T is the
constant 1.

The same construction and argument that we used in Theorem 10 starting at a gap
instance over the constraint language 3SAT  {T} gives a gap instance over 3SAT that will
be useful later on.

Corollary 1. There is an instance over the Boolean constraint language 3SAT that witnesses
a satisability gap of the third kind; it is satisable via operator assignments over some Hilbert
space but not over a nite-dimensional Hilbert space.
For the problems SAT (A(cid:48) ), a trichotomy theorem can be proved: 1) polynomial-time
solvable vs 2) polynomial-time equivalent to SAT (LIN(cid:48) ) vs 3) both SAT (LIN(cid:48) )-hard and
NP-hard. Unfortunately, whether SAT (LIN(cid:48) ) or SAT (LIN) are polynomial-time solvable,
NP-hard or undecidable is an open problem.

6.2 Quantum Realizability of Contextuality Scenarios
that (cid:83)
We follow the terminology in the paper by Acn, Fritz, Leverrier and Sainz [1]. A contextuality
scenario is a hypergraph H with set V (H ) of vertices and set E (H )  2V (H ) of edges such
eE (H ) e = V (H ). Given a contextuality scenario H , a quantum model for it is,
informally, an assignment of probabilities to the vertices of H that are reproduced as the
observation probabilities of a collection of pro jective measurements associated to the edges

30

of H , when the measurements are applied to some quantum state. When a contextuality
scenario has at least one quantum model, one says that H allows quantum models. As argued
in [1], this can be equivalently stated formally, without any reference to measurements or
quantum states, as follows.
We say that a contextuality scenario H al lows a quantum model, or is quantum realizable,
if there exists a Hilbert space H and an assignment of bounded linear operators Pv on H to
each vertex v in V (H ) in such a way that:
1. Pv is self-adjoint,
3. (cid:80)
v = Pv for each v  V (H ),
2. P 2
ve Pv = I for each e  E (H ).
Note that 1 and 2 together say that each Pv is an orthogonal pro jection operator3 , and 3 says
that the pro jection operators associated to the vertices of each edge resolve the identity. In [1]
the question was raised whether there exist contextuality scenarios that are quantum realiz-
able but only over innite-dimensional Hilbert spaces (see Problem 8.3.2 in [1]). A related
computational question was also raised: Is it decidable whether a contextuality scenario
given as input allows a quantum');
INSERT INTO posts (postId,userId,title,body) VALUES (325,9,'Generalized Satisability Problems via Operator Assignments (part 26)',' state? (see Conjecture 8.3.3 in [1]). Following the notation
in [1], this problem is called ALLOWS-QUANTUM. The restriction of the problem in which
the input hypergraph has edges of cardinality at most k we call k -ALLOWS-QUANTUM.
See [1] for a discussion on why these problems are important, and their relationship to Connes
Embedding Conjecture in functional analysis.
Soon after Slofstra published his results, both questions raised in [1] were answered
by Fritz by reduction from Slofstras Theorems 7 and 8 (see [12]).
In particular, Fritz
proved that ALLOWS-QUANTUM is undecidable. In the following we illustrate the methods
developped in the previous sections to give alternative proofs of these results. As a bonus,
our proof also gives optimal parameters; we get hypergraphs with edges of size at most 3 that
separate innite-dimensional realizability from nite-dimensional realizability, and we show
that already 3-ALLOWS-QUANTUM is undecidable. In contrast, Fritz reduction incurs
an exponential loss in the size of the edges of the hypergraphs with respect to the arity
of the constraints in Slofstras result, which is a priori not bounded, and the best it can
achieve is size 4 anyway. Moreover, as we will see, our 3 in the maximum size of the edges is
optimal since it turns out that 2-ALLOWS-QUANTUM is decidable (and even solvable in
polynomial time).
Next we show how our methods can be used to answer these questions. First, notice that
there is a clear similarity between the requirements 1, 2 and 3 in the denition of quantum
realization of H and the requirements that an operator assignment for a collection of variables
{Xv : v  V (H )} associated to the vertices of H must satisfy. For one thing, if we dene
Av = I  2Pv for every v  V (H ), then each Av is a bounded self-adjoint linear operator
3Acn et al. refer to orthogonal pro jection operators as pro jections, and so we will to avoid confusion with
the fact that two orthogonal pro jection operators P and Q could fail to satisfy P Q (cid:54)= 0. It may also be worth
pointing out that linear-algebraic pro jection operators of this section are unrelated to the universal-algebraic
pro jection operations from Section 5.2.

31

such that A2
v = I . Moreover, the fact that the pro jections associated to an edge of H resolve
the identity implies that they pairwise commute. Thus, the operators Av associated to the
vertices of e also pairwise commute for every edge e of H . This means that the assignment
Xv (cid:55) Av thus dened is a valid operator assignment to any instance with constraint scopes
However, the condition (cid:80)
given by the hyperedges of H .
2 (Av  I ) = I implied by condition 3 through the inverseve
2 (Av  I ) does not correspond directly to a constraint of the form
transformation Pv = 1
PR (Av : v  e) = I for any Boolean relation R. This means that we cannot interpret the
quantum realizability problem directly as an instance of a satisability problem via operator
assignments over a Boolean constraint language. However, as it turns out, the problem that
we called 3-ALLOWS-QUANTUM is literally the same as the arbitrary Hilbert space version
1-IN-3 SAT of the problem called 1-IN-3 SAT by Ji4 . Ji proved that 3SAT reduces to
1-IN-3 SAT , and in view of Theorem 10, the question arises whether 3SAT also reduces
to 1-IN-3 SAT , or to 3-ALLOWS-QUANTUM, which is the same. We show that it does.
Before we can do it, though, we need the following lemma that Ji proved for nite-
dimensional Hilbert spaces (see Lemma 5 in [15]), and that we prove for all Hilbert spaces:
Lemma 11. Let H a Hilbert space. For every two projection operators P1 and P2 of H that
commute, there exist projection operators Q1 , Q2 , Q3 and Q4 of H such that

');
INSERT INTO posts (postId,userId,title,body) VALUES (326,9,'Generalized Satisability Problems via Operator Assignments (part 27)','P1 + Q1 + Q4 = I
P2 + Q2 + Q4 = I
Q1 + Q2 + Q3 = I .
Conversely, if P1 , P2 , Q1 , Q2 , Q3 , Q4 are projection operators of H that satisfy these equations,
then P1 and P2 commute.

Proof. To prove the rst claim, consider the pp-formula
(Z1 , Z2 ) = U1U2U3U4 (R1/3 (Z1 , U1 , U4 )  R1/3 (Z2 , U2 , U4 )  R1/3 (U1 , U2 , U3 )),
(34)
where R1/3 = {(1, +1, +1), (+1, 1, +1), (+1, +1  1)}. It is straightforward to check that
this formula denes the full binary Boolean relation T = {1}2 . Now, let I be the instance
((Z1 , Z2 ), T) and let J be the instance obtained from I as in Section 4.1. Let f be dened
by f (Z1 ) = 1  2P1 and f (Z2 ) = 1  2P2 . Since P1 and P2 commute and the characteristic
polynomial of T is the constant 1, the assignment f is a satisfying operator assigment for
the instance ((Z1 , Z2 ), T). By Lemma 5, there exists g that extends f and is a satisfying
4There is an unfortunate clash in notation in that the problem 1-IN-3 SAT studied by Ji [15] is not
the same as the problem that we would call SAT (1-IN-3 SAT), where 1-IN-3 SAT is the Boolean relation
{{(1, +1, +1), (+1, 1, +1), (+1, +1, 1)}}. Note that P1-IN-3 SAT (X1 , X2 , X3 ) = 3
4 X1X2X3 + 1
4 X1X2 +
4 X1  1
4 X2  1
4 X1X3  14 X3 + 1
4 X2X3 + 1
4 , so the dierence is that, even though the characteristic polynomial
equation P1-IN-3 SAT (X1 , X2 , X3 ) = I is satised by an operator assignment if and only if the resolution of
2 (1  X1 ) + 1
2 (1  X2 ) + 1
2 (1  X3 ) = I is satised by the same operator assignment,
the identity equation 1
2 (1X1 )+ 1
2 (1X2 )+ 1
2 (1X3 ) are by no means the same.
the two polynomials P1-IN-3 SAT (X1 , X2 , X3 ) and 1

32

operator assignment for J over H. Moreover, g is pairwise commuting on each block of
J . Take Qi = (1  g(Ui )/2 for i = 1, 2, 3, 4. Then Q1 , . . . , Q4 are pro jection operators,
and P1 , P2 , Q1 , . . . , Q4 pairwise commute. We claim that they satisfy the equations in the
lemma. To see this we apply Lemma 3. Since the equation PR1/3 (Z1 , U1 , U4 ) = 1 entails
the equation (1  Z1 )/2 + (1  U1 )/2 + (1  U4 )/2 = 1 over the Boolean domain {1}, and
at the same time P1 , Q1 , Q4 pairwise commute, the equation PR1/3 (g(Z1 ), g(U1 ), g(U4 )) = I
implies P1 + Q1 + Q4 = I by Lemma 3. For the other two equations, the argument is the
same.
For the converse, we use the following easy to verify identities discovered via a computer
search by Ji (see the proof of Lemma 5 in [15]):
[P1 + Q1 + Q4  I , P1 + Q1 + Q3 ] = [P1 , Q3 ] + [Q4 , Q3 ]
[P2 + Q2 + Q4  I , P1 ] = [P1 , P2 ] + [P1 , Q2 ]
[Q1 + Q2 + Q3  I , P1 + Q4 ] = [Q2 , P1 ] + [Q3 , P1 ] + [Q3 , Q4 ],
where [X, Y ] denotes the commutator polynomial X Y  Y X . Note that the equations in
the lemma imply that the left-hand sides are all 0. On the other hand, using the identity
[X, Y ] + [Y , X ] = 0, the sum of the right-hand sides is [P1 , P2 ]. This gives [P1 , P2 ] = 0 and
thus P1 and P2 commute.
Lemma 12. 3SAT poly-m-reduces to 3-ALLOWS-QUANTUM.

Proof. Schaefer proved that 3SAT is pp-denable from the constraint language given by
the single relation R1/3 = {(1, +1, +1), (+1, 1, +1), (+1, +1, 1)}. If in addition to R1/3
we allow also the relations R1/2 = {(1, +1), (+1, 1)} and R1/1 = {1}, then the pp-
denition can be assumed to have the property that each atom ');
INSERT INTO posts (postId,userId,title,body) VALUES (327,9,'Generalized Satisability Problems via Operator Assignments (part 28)','involves dierent variables
and no constants. For example, an atom of the form R1/3 (X, X, Z ) can be replaced by
R1/3 (X, Y , Z )  R1/2 (X, Y (cid:48) )  R1/2 (Y (cid:48) , Y ), where Y and Y (cid:48) are fresh quantied variables that
do not appear anywhere else in the formula.
We use this for the construction in Section 4.3. Let I be a 3SAT instance and let J
be the instance over the Boolean constraint language A = {R1/3 , R1/2 , R1/1 , T} given by the
construction in Section 4.3, using the pp-denition of 3SAT from A. Starting at J we produce
an instance of 3-ALLOWS-QUANTUM as follows: Each variable in J becomes a vertex
in the hypergraph. Each constraint of the type ((Z1 , Z2 , Z3 ), R1/3 ) becomes a hyperedge
{Z1 , Z2 , Z3}, each constraint of the type ((Z1 , Z2 ), R1/2 ) becomes a hyperedge {Z1 , Z2}, each
constraint of the type (Z, R1/1 ) becomes a singleton hyperedge {Z }, and each constraint
of the type ((Z1 , Z2 ), T) introduces four fresh vertices U1 , U2 , U3 , U4 and three hyperedges
{Z1 , U1 , U4}, {Z2 , U2 , U4} and {U1 , U2 , U3} in correspondance with the equations of Lemma 11
with Z1 , Z2 playing the role of P1 , P2 , and U1 , U2 , U3 , U4 playing the role of Q1 , Q2 , Q3 , Q4 .
Let H be the hypergraph that results from this construction. We claim that for every
Hilbert space H, the instance I is satisable via operator assignments over H if and only if
the hypergraph H is quantum realizable over H.
In the forward direction, let f be a satisfying operator assignment for I over H. By
Lemma 6, there is a g that extends f and is a satisfying operator assignment for J over H.

33

Recall now that each vertex of H is indeed a variable of J , or an additional vertex of the
type U1 , U2 , U3 , U4 introduced by a constraint of the form ((Z1 , Z2 ), T). For each v of the rst
type, let Pv be the pro jection operator given by (1  g(v))/2. For each v of the second type,
let Pv be the pro jection given by Lemma 11 for the pro jection assignment P1 = PZ1 and
P2 = PZ2 with U1 , U2 , U3 , U4 corresponding to Q1 , Q2 , Q3 , Q4 . Note that P1 and P2 commute,
since Z1 and Z2 appear together in ((Z1 , Z2 ), T) and hence g(Z1 ) and g(Z2 ) commute, so the
lemma applies. We claim that this assignment of operators does the job.
We just need to check that the pro jection operators resolve the identity on every edge
of H . For edges of the type {Z1 , Z2 , Z3} introduced by a constraint ((Z1 , Z2 , Z3 ), R1/3 ) we
show this with an application of Lemma 3: the equation PR1/3 (Z1 , Z2 , Z3 ) = 1 entails the
equation (1Z1 )/2+(1Z2 )/2+(1Z3 )/2 = 1 over the Boolean domain {1}, and therefore,
since g(Z1 ), g(Z2 ), g(Z3 ) pairwise commute, the equation PR1/3 (g(Z1 ), g(Z2 ), g(Z3 )) = I
implies PZ1 + PZ2 + PZ3 = I by Lemma 3. For edges of the types {Z1 , Z2} or {Z } introduced
by constraints of the types ((Z1 , Z2 ), R1/2 ) or (Z, R1/2 ), respectively, the argument is the
same. Finally, for the three edges that come from a constraint of the form ((Z1 , Z2 ), T), the
claim follows from Lemma 11. This completes');
INSERT INTO posts (postId,userId,title,body) VALUES (328,9,'Generalized Satisability Problems via Operator Assignments (part 29)',' one direction of the reduction.
For the other direction, let v (cid:55) Pv be an assignment of pro jection operators of H that
witnesses that H is quantum realizable. Recall again that each vertex v of H is a variable
of J , or an additional vertex of the type U1 , U2 , U3 , U4 coming from a T-constraint. For
each v of the rst type, let Av = I  2Pv . Each Av is a self-adjoint bounded linear operator
that squares to the identity. Moreover, any two variables of J that appear together in a
constraint that is not a T-constraint appear together as vertices in some edge of H . Therefore
the corresponding operators belong to the resolution of the identity of that edge, and a set of
pro jection operators that resolve the identity are pairwise orthogonal and hence commute.
Also, for any two variables of J that appear together in a constraint of the form ((Z1 , Z2 ), T),
the corresponding operators commute thanks to the conversely clause in Lemma 11. Thus,
the only thing left to do is checking that each constraint of J is satised.
For constraints of the type ((Z1 , Z2 , Z3 ), R1/3 ) this follows also from an application of
Lemma 3: the equation (1  Z1 )/2 + (I  Z2 )/2 + (I  Z3 )/2 = 1 entails the equation
PR1/3 (Z1 , Z2 , Z3 ) = 1 over the Boolean domain {1}, and since AZ1 , AZ2 , AZ3 pairwise
commute, the equation PZ1 + PZ2 + PZ3 = I implies PR1/3 (AZ1 , AZ2 , AZ3 ) = I by Lemma 3.
For constraints of the type ((Z1 , Z2 ), R1/2 ) and (Z, R1/1 ) the argument is the same.

In combination with Theorem 10 we get the following.

Corollary 2. 3-ALLOWS-QUANTUM and ALLOWS-QUANTUM are undecidable.

The same construction as in Lemma 12 starting from Corollary 1 gives the next.

Corollary 3. There exists a hypergraph with edges of size at most three that is quantum
realizable on some Hilbert space, but not on a nite-dimensional Hilbert space.

It was mentioned earlier that 2-ALLOWS-QUANTUM is decidable in polynomial time.
One way to see this is by arguing that a hypergraph with edges of size two (i.e. a graph) is
quantum realizable if and only if it is bipartite. Another is by reduction to 2SAT , which is

34

decidable in polynomial time by Theorem 9. A close look reveals that, indeed, both proofs
are the same.

Theorem 11. 2-ALLOWS-QUANTUM is decidable in polynomial time.
Proof. We reduce to 2SAT . Given a hypergraph H , build the 2SAT instance that has
one variable Xv for each vertex in V (H ), two clauses Xu  Xv and Xu  Xv for every
edge {u, v}  E (H ), and one unit clause Xu for each singleton edge {u} in E (H ).
It
is straightforward to check that this reduction works through the usual conversion from
pro jection operators to involutions Pv (cid:55) 1  2Pv , and the usual conversion from involutions
to pro jection operators Av (cid:55) (1  Av )/2.

7 Closure Operations

In this section we develop a generalization of the concept of closure operation from Section 5.2
for sets of operator assignments. For every Boolean r-ary relation R, let R denote the set
of fully commuting r-variable operator assignments over nite-dimensional Hilbert spaces
that satisfy the equation PR (X1 , . . . , Xr ) = I . We show that every closure operation for R
gives a suitable closure operation for R . As an application, we show that the set of Boolean
relations that are pp-denable from a Boolean constraint language is not enlarged when we
allow the existential quantiers to range over operator assignments.
7.1 Closure Operations and pp-Denitions
Let A be a Boolean constraint language and let R be a Boolean relation of arity r. Let
 = R1 (z1 )      Rm (zm ) be a conjunction of atoms with relations from A; i.e. each Ri
is a relation from A, and each zi denotes a tuple of the appropriate arity made o');
INSERT INTO posts (postId,userId,title,body) VALUES (329,9,'Generalized Satisability Problems via Operator Assignments (part 30)','f rst-
order variables or constants in {1}. Each such formula can be thought of as an instance
over A. Concretely, it can be thought of as the instance I = ((Z1 , R1 ), . . . , (Zm , Rm )), where
each Zi is obtained from the corresponding zi by replacing each rst-order variable x by a
correponding variable X , and leaving all constants untouched.
Let H be a nite-dimensional Hilbert space. We say that R is pp -denable from A over H
if there is a pp-formula (x1 , . . . , xr ) = y1    ys ((x1 , . . . , xr , y1 , . . . , ys )) over A, where 
is a conjunction as above, such that, for every a1 , . . . , ar  {1}, the tuple (a1 , . . . , ar ) is in
R if and only if the instance

(35)
(x1/a1 , . . . , xr /ar , y1/Y1 , . . . , ys/Ys )
is satisable via operator assignments over H. We say that R is pp -denable from A if it is
pp -denable from A over a nite-dimensional Hilbert space. One of the goals of this section
is to prove the following conservativity theorem:

Theorem 12. Let A be a Boolean constraint language and let R be a Boolean relation. If
R is pp -denable from A, then R is pp-denable from A.

35

In order to prove this we need to develop the concept of closure operation for sets of oper-
ator assignments. Let r be a positive integer. A relation of operator assignments of arity r is
a set of fully commuting operator assignments for a xed set of r variables X1 , . . . , Xr . Note
that we do not require that all operator assignments come from the same Hilbert space. The
relation is called Boolean if all assignments in it come from a Hilbert space of dimension 1;
i.e., from C. If H is a Hilbert space and R  {1}r is a Boolean relation of arity r, we write
RH for the set of fully commuting operator assignments for X1 , . . . , Xr over H that satisfy
the polynomial equation PR (X1 , . . . , Xr ) = I , where PR is the characteristic polynomial
of R. We write R for the union of RH over all nite-dimensional Hilbert spaces. If A is a
set of Boolean relations, dene A = {R : R  A}.
Let H1 , . . . , Hm and H be Hilbert spaces, and let f be a function that takes as inputs m
many linear operators, one on each Hi , and produces as output a linear operator on H. We
say that f is an operation if the following conditions are satised.
1. If A1 , . . . , Am are 1-variable operator assignments over H1 , . . . , Hm , then f (A1 , . . . , Am )
is a one-variable operator assignment over H.
2. If (A1,1 , A1,2 ), . . . , (Am,1 , Am,2 ) are commuting 2-variable operator assignments over
H1 , . . . , Hm , then (f (A1,1 , . . . , Am,1 ), f (A1,2 , . . . , Am,2 )) is a commuting two-variable
operator assignment over H.

Let R be a relation of operator assignments of arity r and let F be a collection of operations
as above. We say that R is invariant under F if for each f  F the following additional
condition is also satised.

3. If (A1,1 , . . . , A1,r ), . . . , (Am,1 , . . . , Am,r ) are fully commuting r-variable operator assign-
ments over H1 , . . . , Hm , respectively, and (Ai,1 , . . . , Ai,r ) belongs to R for every i  [m],
then (f (A1,1 , . . . , Am,1 ), . . . , f (A1,r , . . . , Am,r )) is ');
INSERT INTO posts (postId,userId,title,body) VALUES (330,9,'Generalized Satisability Problems via Operator Assignments (part 31)','a fully commuting r-variable operator
assignment over H and belongs to R.

If A is a set of relations of operator assignments, we say that A is invariant under F if every
relation in A is invariant under F . We also say that F is a closure operation of A. A Boolean
closure operation of A is one in which the dimensions of all Hilbert spaces involved are 1;
i.e., they are C. Before we prove the main technical result of this section, we work out a
motivating example.

7.2 Example: LIN
In this section we study whether R for R = LIN has some closure operation. In the 0-1-
representation of Boolean values, the function (X1 , X2 , X3 ) (cid:55) X1  X2  X3 is a Boolean
closure operation of LIN. In the 1-representation of Boolean values, this is (X1 , X2 , X3 ) (cid:55)
X1X2X3 . It is tempting to think that the map (X1 , X2 , X3 ) (cid:55) X1X2X3 applied to linear
operators on a Hilbert space could already be a closure operation for LIN . However, the
solution to the Mermin-Peres magic square equations (33) is a counterexample: each row
equation is a parity equation with even right-hand side that is satised, but the composition

36

(36)

of columns by the operation X1X2X3 gives an operator assignment that satises a parity
equation with odd right-hand side.
It turns out that the correct way of generalizing the Boolean closure operation is not
by taking ordinary products, but Kronecker products. Let F be the function that takes
any three linear operators X1 , X2 , X3 over the same nite-dimensional Hilbert space and is
dened by
F (X1 , X2 , X3 ) = X1  X2  X3 .
Now let (A1 , . . . , Ar ), (B1 , . . . , Br ) and (C1 , . . . , Cr ) be three fully commuting r-variable op-
erator assignments over a nite-dimensional Hilbert space, say Cd . We think of all operators
as matrices. Take Di = F (Ai , Bi , Ci ) for i = 1, . . . , r . These are Hermitian matrices since
the operations of conjugate transposition and Kronecker product commute. Also
DiDj = (AiAj )  (BiBj )  (CiCj ) = (Aj Ai )  (Bj Bi )  (Cj Ci ) = DjDi
(37)
i )  (C 2
i )  (B 2
so D1 , D2 , D3 pairwise commute. Equation (37) also gives D2
i = (A2
i ) =
I  I  I = I , so (D1 , . . . , Dr ) is a fully commuting r-variable operator assignment. Next
we consider a relation in LIN, say R = {(a1 , . . . , ar )  {1}r : a1    ar = b}, with b  {1}.
Note that its characteristic polynomial is PR (X1 , . . . , Xr ) = b  X1    Xr . We show that if
PR (A1 , . . . , Ar ) = PR (B1 , . . . , Br ) = PR (C1 , . . . , Cr ) = I , then also PR (D1 , . . . , Dr ) = I .
(cid:33)
(cid:33)
(cid:33)
(cid:32) r(cid:89)
(cid:32) r(cid:89)
(cid:32) r(cid:89)
r(cid:89)
We have
i=1
i=1
i=1
i=1
Hence PR (D1 , . . . , Dr ) = b2I = I . This shows that F is a closure operation of LIN .
One consequence of the existence of F as a closure operation of LIN is that the binary
OR relation OR2 = {1}2 \ {(+1, +1)} is not pp -denable from LIN.
Theorem 13. OR2 is not pp -denable from LIN.
Note that this follows from the more general statement in Theorem 12 since it is known
that the Boolean relation OR2 is not pp-denable from LIN. Indeed, OR2 is not closed under
the (idempotent) Boolean closure operation (X1 , X2 , X3 ) (cid:55) X1X2X3 of LIN, since (1, 1),
(+1, 1) and (1, +1) are all three in the relation OR2 , but (+1, +1) is not in OR2 . The
undenability of OR2 from LIN by a pp-formula (with or without constants) follows from
the easy direction in Geigers Theorem 4. Since we prove Theorem 12 below, we omit a proof
of Theorem 13 at this point.

= (bI )  (bI )  (bI ) = ');
INSERT INTO posts (postId,userId,title,body) VALUES (331,9,'Generalized Satisability Problems via Operator Assignments (part 32)','b3I = bI .

Di =



Ai



Bi

Ci

(38)

7.3 Generalization

We show that every Boolean closure operation gives a closure operation for relations of
operator assignments over nite-dimensional Hilbert spaces.
In the following, if Xi is a
linear operator on a Hilbert space, X 0
i and X 1
i are to be interpreted as the identity operator
and Xi itself, respectively. If S is a set, we write S (i) for the 0-1-indicator of the fact that i
is in S ; i.e. S (i) = 1 if i is in S , and S (i) = 0 if i is not in S .

37

Theorem 14. Let A be a Boolean constraint language and let f : {1}m  {1} be a
Boolean closure operation of A. Then the function on linear operators on nite-dimensional
(cid:88)
(cid:79)
Hilbert spaces dened by
i[m]
S[m]
is a closure operation of A . Moreover, F (a1I , . . . , amI ) = f (a1 , . . . , am )I holds for every
(a1 , . . . , am )  {1}m .

F (X1 , . . . , Xm ) =

X S (i)
f (S )

(39)

i.e., it satises conditions 1 and 2 in the
Proof. First we show that F is an operation;
deniton of operation. Let X1 , . . . , Xm be 1-variable operator assignments over H1 , . . . , Hm .
(cid:32)(cid:79)
(cid:33)
In particular, X1 , . . . , Xm are all self-adjoint linear operators. Thus, for S  [m] we have
(cid:79)
(cid:79)
i[m]
i[m]
i[m]

(X 
i )S (i) =

(Xi )S (i) .

(Xi )S (i)

(40)

X S (i)
,

(41)

X T (i)
=

X U (i)
X U (i)
(X 2
i )V (i)

F (X1 , . . . , Xm )2 =

From this it follows that F (X1 , . . . , Xm ) is self-adjoint since each f (S ) is a real number. Next
we want to show that F (X1 , . . . , Xm )2 = I . First note that for S, T  [m], their symmetric
(cid:32)(cid:79)
(cid:33)(cid:32)(cid:79)
(cid:33)
(cid:32)(cid:79)
(cid:33)(cid:32)(cid:79)
(cid:33)
(cid:32)(cid:79)
(cid:33)
dierence U = ST and their intersection V = S  T , we have
i[m]
i[m]
i[m]
i[m]
i[m]
i = I for all i  [m]. Now we can expand
where the last equality follows from the fact that X 2
(cid:33)
(cid:33)(cid:32)(cid:79)
(cid:32)(cid:79)
(cid:88)
(cid:88)
F (X1 , . . . , Xm )2 as follows
(cid:32)(cid:79)
(cid:33)
f (S ) f (T )
X S (i)
(cid:88)
(cid:88)T [m]
S[m]
i[m]
i[m]
(cid:33)
(cid:32)(cid:79)
f (S ) f (SU )
(cid:88)
(cid:88)
i[m]
S[m]
U [m]
(cid:33) (cid:88)
(cid:32)(cid:79)
f (S ) f (SU )
(cid:88)
U [m]
S[m]
i[m]
X U (i)i
U [m]
i[m]
S[m]
By the Convolution Formula (3) we have(cid:88)
f (S ) f (SU ) = (cid:98)f 2 (U ).
S[m]

f (S ) f (SU ).

X U (i)
X U (i)
X T (i)
=

(46)

(42)

(43)

(44)

(45)

38

(cid:26) 1
Since the range of f is {1}, the function f 2 is identically 1, from which it follows that
(cid:98)f 2 (U ) =
if U = 
if U (cid:54)= (cid:32)(cid:79)
(cid:33)
by the uniqueness of the Fourier transform. Back into (45), this gives
i[m]
as was to be proved. Finally, if S  [m] and (X1 , Y1 ), . . . , (Xm , Ym ) are such that Xi and Yi
(cid:33)
(cid:33)(cid:32)(cid:79)
(cid:32)(cid:79)
commute for every i  [m], then
(cid:79)
(cid:79)
i[m]
i[m]
i[m]
i[m]

(cid:33)(cid:32)(cid:79)
i[m]

(cid:32)(cid:79)
i[m]

F (X1 , . . . , Xm )2 =

(XiYi )S (i) =

(YiXi )S (i) =

X S (i)
.

Y S (i)
=

(i)
X

= I

(49)

(50)

(47)

(48)

X S (i)
=

Y S (i)
(cid:33)

It follows that F (X1 , . . . , Xm ) and F (Y1 , . . . , Ym ) commute. This completes the proof that
F is an operation.
Next we show that for every relation R in A, the operator assignment relation R is
invariant under F . Let r be the arity of R and let PR (X1 , . . . , Xr ) be the characteristic
polynomial of R. Let (A1,1 , . . . , A1,r ), . . . , (Am,1 , . . . , Am,r ) be r-variable operator assignments
over nite-dimensional Hilbert spaces H1 , . . . , Hm . We may assume that Hi = Cdi where di
is the dimension of Hi . From now on we switch to the language of matrices.
Assume that all the assignments (A1,1 , . . . , A1,r ), . . . , (Am,1 , . . . , Am,r ) are in R . In par-
ticular, each sequence Ai,1 , . . . , Ai,r is a fully commuting assignment of Hermitian matrices
and PR (Ai,1 , . . . , Ai,r ) = I . The Strong Spectral Theorem (i.e. Theorem 1) applies, so
Ai,1 , . . . , Ai');
INSERT INTO posts (postId,userId,title,body) VALUES (332,9,'Generalized Satisability Problems via Operator Assignments (part 33)',',r simultaneously diagonalize. Let Ui be a unitary matrix of Hi that achieves
that, and let Di,j = U Ai,j U  for j  [r] be the resulting diagonal matrices. From A2
i,j = I
and U U = U U  = I we conclude that D2
i,j = I and hence each entry in the diagonal of Di,j
is +1 or 1. For c  [di ], let Di,j (c) denote the entry in position c of the diagonal of Di,j . The
hypotheses of Lemma 1 apply to the pairs (Ai,1 , Di,1 ), . . . , (Ai,r , Di,r ), so PR (Ai,1 , . . . , Ai,r )
and PR (Di,j , . . . , Di,r ) are similar matrices. As PR (Ai,1 , . . . , Ai,r ) = I , and the only matrix
that is similar to I is I itself, we get PR (Di,1 , . . . , Di,r ) = I . In particular
PR (Di,1 (c), . . . , Di,r (c)) = 1
for every c  [di ]. This will be of use later.
do so by showing that (cid:88)
(cid:89)
Our next goal is to show that PR (F (A1,1 , . . . , Am,1 ), . . . , F (A1,r , . . . , Am,r )) = I and we
F (A1,j , . . . , Am,j ) = I .
jT
T [r ]

R(T )

(51)

(52)

39

=

Ui

(54)

For xed T  [r], let AT = (cid:81)
jT F (A1,j , . . . , Am,j ) be the matrix product appearing in the
left-hand side of (52). Let t = |T |. By rst expanding on the denition of F and then
(cid:89)
(cid:88)
(cid:79)
(cid:88)
(cid:89)
(cid:89)
(cid:79)
distributing the product over the sum we get
f (S )
f (S (t))
(Ai,j )S (i) =
(Ai,j )S (t)(i) .
AT =
(53)
(cid:78)
For xed T  [r] and S : T  2[m] , let BT ,S = (cid:81)
jT
jT
jT
i[m]
S[m]
i[m]
S :T 2[m]
appearing in the right-hand side of (53). By distributing (cid:81) over (cid:78) and applying Ai,j =
i (Ai,j )S (t)(i) be the matrix product(cid:32)(cid:89)
(cid:33)S (t)(i)
(cid:33)S (t)(i)
(cid:32)(cid:89)
(cid:32)
(cid:33)
U 
i Di,j Ui in (53) we get
(cid:79)
(cid:79)
U 
(U 
i Di,j Ui )
Di,jjT
jT
i[m]
i[m]
(cid:32) (cid:88)
(cid:33)S (t)(i)(cid:33)
(cid:32)(cid:89)
(cid:79)
(cid:89)
Hence
AT = U 
f (S (j ))
for U = (cid:78)
Di,j
U,
jT
jT
i[m]
Di,j is a di  di diagonal matrix, M is a d  d diagonal matrix with d = (cid:81)
S :T 2[m]
i[m] Ui . Let M denote the matrix sitting within U  and U in line (55). As each
i[m] di . We think of
the entries in the diagonal of M as indexed by tuples c = (c1 , . . . , cm ) from [d1 ]      [dm ].
(cid:88)
(cid:89)
(cid:89)
(cid:89)
Let M (c) denote the entry in position c of the diagonal of M . Then
M (c) =
jT
jT
i[m]
S :T 2[m]
(cid:89)
(cid:89)
(cid:89)
(cid:88)
Factoring back the product over j  T , the right-hand side in (56) reads
f (S )
(Di,j (ci ))S (i) =
jT
jT
i[m]
S[m]
equations (56) and (57) give M (c) = (cid:81)
For xed j  [r] and c  [d1 ]      [dm ], let Xj,c = f (D1,j (c1 ), . . . , Dm,j (cm )) so that
jT Xj,c . From (51) and the fact that f is a Boolean
(cid:88)
(cid:89)
(cid:88)
closure operator of R, the tuple (X1,c , . . . , Xr,c ) belongs to the relation R. Thus
Xj,c = PR (X1,c , . . . , Xr,c ) = 1.
R(T )
R(T )M (c) =
Since this holds for every diagonal entry of M , we get (cid:80)
jT
T [r ]
T [r ]
(cid:32) (cid:88)
(cid:33)
(cid:88)
together, the left-hand side of our goal (52) evaluates to
T [r ]
T [r ]

R(T )M = I . Putting it all

U = U  (I )U = I .

R(T )U M U = U 

(Di,j (ci ))S (t)(i) .

f (D1,j (c1 ), . . . , Dm,j (cm )).

(57)

f (S (t))

(58)

(59)

(5');
INSERT INTO posts (postId,userId,title,body) VALUES (333,9,'Generalized Satisability Problems via Operator Assignments (part 34)','5)

(56)

BT ,S =

T [r]

R(T )M

40

This gives (52) as desired.
(cid:32)(cid:89)
(cid:33)
In order to prove the moreover clause of the theorem, observe that if S  [m] and
(a1 , . . . , am )  {1}m , then (cid:79)
(aiI )S (i) =
ai
I ,
and in the right-hand side the identity matrix has dimension d  d for d = (cid:81)
iS
i[m]
where in the left hand side the identity matrices have dimensions d1 , . . . , dm , respectively,
(cid:33)
(cid:32) (cid:88)
(cid:89)
i[m] di . It
follows that
f (S )
I = f (a1 , . . . , am )I .
iS
S[m]

F (a1I , . . . , amI ) =

ai

(60)

(61)

This completes the proof of the theorem.

7.4 Finale

Before we prove Theorem 12, we need the following straightforward fact about the role of
constants in pp-denitions.

Lemma 13. Let A be a Boolean constraint language, let R be Boolean a relation, and let
A+ = A  {{+1}, {1}}. The fol lowing two statements hold.
1. R is pp-denable from A if and only if it is pp-denable without constants from A+ .
2. R is pp -denable from A if and only if it is pp -denable without constants from A+ .

Proof. In both cases, for the only if  part it suces to replace each occurrence of a constant
in the quantier-free part of the pp-formula by a new existentially quantied variable Z , and
force it to belong to the corresponding new unary relation in A+ by an additional conjunct: if
Z replaces the constant 1, we force Z to belong {1} by a new conjunct, and it Z replaces
the constant +1, we force it to belong {+1} by a new conjunct. In both cases too, the if 
part follows from the reverse construction: replace each occurrence of a variable that appears
within the scope of one of the new unary relations in A+ by the corresponding constant, and
remove the conjuncts that involve the new unary relations. That these transformations are
correct follows directly from the denitions and the fact that both I and I commute with
any operator.

We are ready to prove Theorem 12.
Proof of Theorem 12. Assume R is pp -denable from A. By Lemma 13, the relation R is
also pp -denable without constants from A+ = A  {{+1}, {1}}. Let r be the arity of R
and let (x1 , . . . , xr ) be the pp-formula without constants that pp -denes R from A+ . By
Geigers Theorem 4 and Lemma 13 it suces to show that R is invariant under all Boolean
closure operations of A+ .
Let f : {1}m  {1} be a Boolean closure operation of A+ . By Theorem 14, the
function F is a closure operation of A+ 
. Let (a1,1 , . . . , a1,r ), . . . , (am,1 , . . . , am,r ) be tuples in

41

R and let aj = f (a1,j , . . . , am,j ) for every j  [m]. We need to show that (a1 , . . . , ar ) is also
in R. Let (x1 , . . . , xr , y1 , . . . , ys ) be the quantier-free part of  and consider the instance
over A+ that is given by

(x1/ai,1 , . . . , xr /ai,r , y1/Y1 , . . . , ys/Ys )

(62)

as described in the begining of this section. Since the tuple (ai,1 , . . . , ai,r ) is in R and
 pp -denes R, the instance in (62) is satisable via operator assignments over a nite-
dimensional Hilbert space for every i  [m]. Let Bi,1 , . . . , Bi,s be such a satisfying operator
assignment for every i  [m]. Since I and I commute with any operator, this means that
ai,1I , . . . , ai,r I , Bi,1 , . . . , Bi,s is a satisfying operator assignment of

(63)
(x1/X1 , . . . , xr /Xr , y1/Y1 , . . . , ys/Ys )
for every i  [m]. Let Aj = F (a1,j I , . . . , am,j I ) and Bj = F (B1,j , . . . , Bm,j ). As F is a closure
operation of A+ 
, the tuple A1 , . . . , Ar , B1 , . . . , Bs is a satisfying operator assignment for (63).
Moreover, from the moreover clause in Theorem 14 we know that Aj = f (a1,j , . . . , am,j )I =
aj I for every j  [m]. Thus, the instance

(x1/a1 , . . . , xr /ar , y1/Y1 , . . . , ys/Ys )

(64)

is satisable via operator assignments over a nite-dimensional Hilbert space; the nite-
dimensional operator assignment B1 , . . . , Bs satises it. As  pp');
INSERT INTO posts (postId,userId,title,body) VALUES (334,9,'Generalized Satisability Problems via Operator Assignments (part 35)',' -denes R, it follows that
(a1 , . . . , ar ) is in R, as was to be shown.

Acknowledgments. We are grateful to Heribert Vollmer for sharing with us Steen Reiths
diagram of Posts lattice (Figure 1). This work was initiated and part of the research was
carried out while all three authors were in residence at the Simons Institute for the Theory
of Computing during the fall of 2016, where they participated in the program on Logical
Structures in Computation. The research of Albert Atserias was partially funded by the
European Research Council (ERC) under the European Unions Horizon 2020 research and
innovation programme, grant agreement ERC-2014-CoG 648276 (AUTAR), and by MINECO
through TIN2013-48031-C4-1-P (TASSAT2); the research of Simone Severini was partially
funded by The Royal Society, Engineering and Physical Sciences Research Council (EPSRC),
and the National Natural Science Foundation of China (NSFC).

References

[1] Antonio Acn, Tobias Fritz, Anthony Leverrier, and Ana Belen Sainz. A combinatorial
approach to nonlocality and contextuality. Communications in Mathematical Physics,
2(334):533628, 2015.

42

[2] John S Bell. On the problem of hidden variables in quantum mechanics. Reviews of
Modern Physics, 38(3):447, 1966.

[3] V. G. Bodnarchuk, L. A. Kaluzhnin, V. N. Kotov, and B. A. Romov. Galois theory for
Post algebras. I. Cybernetics, 5(3):243252, 1969.

[4] Elmar Bohler, Nadia Creignou, Steen Reith, and Heribert Vollmer. Playing with
boolean blocks, part I: Posts lattice with applications to complexity theory. In ACM
SIGACT-Newsletter, 2003.

[5] Elmar Bohler, Nadia Creignou, Steen Reith, and Heribert Vollmer. Playing with
boolean blocks, part II: Constraint satisfaction problems. In ACM SIGACT-Newsletter,
2004.

[6] Richard Cleve, Li Liu, and William Slofstra. Perfect commuting-operator strategies for
linear system games. arXiv preprint arXiv:1606.02278, 2016.

[7] Richard Cleve and Ra jat Mittal. Characterization of binary constraint system games. In
Automata, Languages, and Programming - 41st International Col loquium, ICALP 2014,
Copenhagen, Denmark, July 8-11, 2014, Proceedings, Part I, pages 320331, 2014.

[8] Brian A. Davey and Hilary A. Priestley. Introduction to lattices and Order. Cambridge
University Press, 2002.

[9] Tomas Feder and Moshe Y Vardi. The computational structure of monotone monadic
SNP and constraint satisfaction: A study through datalog and group theory. SIAM
Journal on Computing, 28(1):57104, 1998.

[10] Gerald B. Folland. A Course in Abstract Harmonic Analysis. Studies in Advanced
Mathematics. Taylor '||'&'||' Francis, 1994.

[11] Gerald B. Folland. Real Analysis: Modern Techniques and Their Applications. Pure
and Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts. Wiley,
2013.

[12] Tobias Fritz. Quantum logic is undecidable. arXiv preprint arXiv:1607.05870, 2016.

[13] David Geiger. Closed systems of functions and predicates. Pacic journal of mathe-
matics, 27(1):95100, 1968.

[14] Paul R. Halmos. Introduction to Hilbert Space and the Theo');
INSERT INTO posts (postId,userId,title,body) VALUES (335,9,'Generalized Satisability Problems via Operator Assignments (part 36)','ry of Spectral Multiplicity.
Benediction Classics, 2016.

[15] Zhengfeng Ji. Binary constraint system games and locally commutative reductions.
arXiv preprint arXiv:1310.3794, 2013.

[16] N. David Mermin. Simple unied form for the ma jor no-hidden-variables theorems.
Physical Review Letters, 65(27):3373, 1990.

43

[17] N. David Mermin. Hidden variables and the two theorems of John Bell. Reviews of
Modern Physics, 65(3):803, 1993.

[18] Ryan ODonnell. Analysis of Boolean Functions. Cambridge University Press, 2014.

[19] Christos H Papadimitriou. Computational complexity. Addison Wesley, 1994.

[20] Asher Peres. Incompatible results of quantum measurements. Physics Letters A, 151(3-
4):107108, 1990.

[21] Emil L Post. The Two-Valued Iterative Systems of Mathematical Logic, volume 5 of
Annals of Mathematical Studies. Princeton University Press, 1941.

[22] Thomas J. Schaefer. The complexity of satisability problems. In Proceedings of the
10th Annual ACM Symposium on Theory of Computing, May 1-3, 1978, San Diego,
California, USA, pages 216226, 1978.

[23] Uwe Schoning. Logic for computer scientists. Springer Science '||'&'||' Business Media, 2008.

[24] William Slofstra. Tsirelsons problem and an embedding theorem for groups arising
from non-local games. arXiv preprint arXiv:1606.03140, 2016.

44

');
INSERT INTO posts (postId,userId,title,body) VALUES (336,5511,'Sample title','Abstract  -  This  paper  describes  the  design  and 
simulation of an 8-bit dedicated processor for calculating the 
Sine  and  Cosine  of  an  Angle  using  CORDIC  Algorithm 
(COordinate  Rotation  DIgital  Computer),  a  simple  and 
efficient algorithm to calculate hyperbolic and trigonometric 
functions.  We  have  proposed  a  dedicated  processor  system, 
modeled  by  writing  appropriate  programs  in  VHDL,  for 
calculating  the  Sine  and  Cosine  of  an  angle.  System 
simulation  was  carried  out  using  ModelSim  6.3f  and  Xilinx 
ISE Design Suite 12.3. A maximum frequency of 81.353 MHz 
was  reached  with  a  minimum  period  of  12.292  ns.  126  (3%) 
slices  were  used.  This  paper  attempts  to  survey  the  existing 
CORDIC  algorithm  with  an  eye  towards  implementation  in 
Field  Programmable  Gate  Arrays 
(FPGAs).  A  brief 
description  of  the  theory  behind  the  algorithm  and  the 
derivation  of  the  Sine  and  Cosine  of  an  angle  using  the 
CORDIC  algorithm  has  been  presented.  The  system  can  be 
implemented  using  Spartan3  XC3S400  with  Xilinx  ISE  12.3 
and VHDL.  Keywords  -  CORDIC,  VHDL,  dedicated  processor, 
datapath, finite state machine.  

Proceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)  
IEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 
Design and Simulation of an 8-bit Dedicated Processor for calculating the Sine 
and Cosine of an Angle using the CORDIC Algorithm  
Aman Chadha1,a, Divya Jyoti2,b and M. G. Bhatia3,c 
1,2 Thadomal Shahani Engineering College, Bandra (W), Mumbai, INDIA 
3 Ameya Centre for Robotics and Embedded Technology, Andheri (W), Mumbai, INDIA 
a aman.x64@gmail.com, b dj.rajdev@gmail.com, c mgbhatia@acret.in  
these technologies into a tool for exploring and evaluating micro-architectural  designs 
[4].  Because  of 
their advantage of real-time in-circuit reconfigurability, FPGAs based  processors  are  flexible,  programmable  and  reliable 
[1].  Thus,  higher  speeds  can  be  achieved  by  these 
customized hardware  solutions at competitive costs. Also, 
various  simple  and  hardware-efficient  algorithms  exist 
which  map  well  onto  these  chips  and  can  be  used  to 
enhance  speed  and  flexibility  while  performing  the 
desired signal processing tasks [1],[2],[3]. One  such  simple  and  hardware-efficient  algorithm  is 
COordinate  Rotation  DIgital  Computer  (CORDIC)  [5]. 
Primarily  developed  for  real-time  airborne  computations, 
it  uses  a  unique  computing  technique  highly  suitable  for 
solving  the  trigonometric  relationships  involved  in  plane 
co-ordinate  rotation  and  conversion  from  rectangular  to 
polar  form.  John  Walther  extended  the  basic  CORDIC 
theory  to  provide  solution  to  and  implement  a  diverse 
range  of  functions  [7].  It  comprises  a  special  serial 
arithmetic  unit  having 
three  shift 
registers, 
three 
adders/subtractors,  Look-Up  Table  (LUT)  and  special 
interconnections.  Using  a  prescribed  sequence  of 
conditional  additions  or  subtractions, 
the  CORDIC 
arithmetic  unit  can  be  designed  to  solve  either  of  the 
following equation');
INSERT INTO posts (postId,userId,title,body) VALUES (337,5511,'Sample_title (part 2)','s: Y 
 K Ycos =
 K Xcos X 
Where, K is a constant.  By making  slight  adjustments  to  the  initial  conditions 
and  the  LUT  values,  it  can  be  used  to  efficiently 
implement 
trigonometric, 
hyperbolic, 
exponential 
functions,  coordinate  transformations  etc.  using  the  same 
hardware.  Since  it  uses  only  shift-add  arithmetic,  the 
VLSI  implementation  of  such  an  algorithm  is  easily 
achievable [4]. 
 Over  the  years,  the  field  of  Digital  Signal  Processing 
(DSP) 
has 
been 
essentially 
dominated 
by 
Microprocessors.  This  is  mainly  because  of  the  fact  that 
they provide designers with the advantages of single cycle 
multiply-accumulate 
instruction  as  well  as  special 
addressing  modes  [4].  Although  these  processors  are 
cheap  and  flexible,  they  are  relatively  less  time-efficient 
when  it  comes  to  performing  certain  resource-intensive 
signal  processing  tasks,  e.g.,  Image  Compression,  Digital 
Communication  and  Video  Processing.  However  as  a 
direct  consequence  of  rapid  advancements  in  the  field  of 
VLSI  and  IC  design,  special  purpose  processors  with 
custom-architectures  are  designed  to  perform  certain 
specific  tasks.  They  need  fewer  resources  and  are  less 
complex 
than 
their  general  purpose  counterparts. 
Instructions  for  performing  a  task  are  hardwired  into  the 
processor  itself,  i.e.,  the  program  is  built  right  into  the 
microprocessor circuit itself [2]. Due to this, the execution 
time  of  the  program  is  considerably  less  than  that  if  the 
instructions  are  stored  in  memory.  Emerging  high  level 
hardware  description  and  synthesis 
technologies 
in 
conjunction  with  Field  Programmable  Gate  Arrays 
(FPGAs)  have  significantly  lowered  the  threshold  for 
hardware  development  as  opportunities  exist  to  integrate 
 
The  CORDIC  algorithm  is  an  iterative  technique 
based  on  the  rotation  of  a  vector  which  allows  many 
transcendental  and 
trigonometric 
functions 
to  be 
calculated.  The  key  aspect  of  this  method  is  that  it  is 
achieved  using  only  shifts,  additions/subtractions  and 
table  look-ups  which  map  well  into  hardware  and  are 
ideal for FPGA implementation. The CORDIC algorithms 
presented in this paper are well known in the research and 
super-computing circles. 
II.  CORDIC ALGORITHM 

I.  INTRODUCTION 

 Xsin 
 Ysin 


(1) 

1 

 

-2i

A  = 
A.  Algorithm Fundamentals 
Vector  rotation 
the 
to  obtain 
the  first  step 
is 
trigonometric  functions.  It  can  also  be  used  for  polar  to 
rectangular  and  vice-versa  conversions, 
for  vector 
magnitude,  and  as  a  building  block  in  certain  transforms 
such  as  the  Discrete  Fourier  Transform  (DFT)  and 
Discrete  Cosine  Transform  (DCT).  The  algorithm  is 
derived from Givens [6] rotation as follows: 

Proceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) 
IEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 system or treated as part of a system processing gain. That 
product  approaches  0.6073  as  the  number  of  iterations 
reaches  infinity.  Therefore,  the  rotation  algorithm  has  a 
gain, An   1.65. The  exact  gain  depends  on  the  number  of 
iterations, and follows the following equation: 
1 + 2The  angle  of  a  composite  rotation  is  realized  by  the 
sequence  of  the  directions  of  the  elementary  rotations. 
That  sequence  can  be  repr');
INSERT INTO posts (postId,userId,title,body) VALUES (338,5511,'Sample_title (part 3)','esented  by  a  decision  vector. 
The  set  of  all  possible  decision  vectors  is  an  angular 
measurement  system  based  on  binary  arctangents. 
Conversions  between  this  angular  system  and  any  other 
can  easily  be  accomplished  using  a  LUT.  A  better 
conversion  method  uses  an  additional  adder-subtractor 
that  accumulates  the  elementary  rotation  angles  post 
iteration.  The  angle  accumulator  adds  a  third  difference 
equation to the CORDIC algorithm: )2
 z  
 d tan

(6) 
i + 1i
As  discussed  above,  when  the  angle  is  in  the 
arctangent  base,  this  extra  element  is  not  needed.  The 
CORDIC  rotator  is  normally  operated  in  one  of  two 
modes, i.e., the Rotation mode and the Vectoring mode.  
  
B.  Rotation Mode 
(5) 



z

 



The first mode of operation, called rotation by Volder 
[5],[4], rotates the input vector by a specified angle (given 
as an argument). Here, the angle accumulator is initialized 
with  the  desired  rotation  angle.  The  rotation  decision 
based on the sign of the residual angle is made to diminish 
the  magnitude  of 
the  residual  angle 
in 
the  angle 
accumulator.  If  the  input angle  is already expressed  in  the 
binary  arctangent  base,  the  angle  accumulator  is  not 
needed [4],[1]. The equations for this are: 

x  
i 1y  
i 1z
i + 1
=
 x   y   z  


-i

-i

 y d 2

i
 x d 2

i
 d tan

 



2

(7) 

(2) 
 
Fig. 1.  Illustration of the CORDIC algorithm 
In Fig. 1, the diagonal blue line is at an angle 
1  above 
the  horizontal.  The  diagonal  red  line  is  actually  the  blue 
line  rotated  anti-clockwise  by  an  angle. The  new X  and 
Y values are related to the old X and Y values as follows: x  cos  
\'  
  sin


 x\'  
  cos  
  sin

+
For CORDIC, the  final angle 
2  the angle whose sine 
or cosine we want to calculate and initial angle 
1  is set to 
a convenient value such as 0. Rather than rotating from 
1  
to 
2  in  one  full  sweep,  we  move  in  steps  with  careful 
choice of step values. Rearranging (2) gives us: ]
x\'
 cos
x 
 y tan


[
 x tan
y\' =  cos
y 


Restricting  the  rotation  angles  such  that  tan =    2-i, 
transforms  the  multiplication  by  the  tangent  term  to  a 
simple  shift  operation  [1]. Arbitrary  angles  of  rotation  are 
obtained  by  successively  performing  smaller  elementary 
rotations.  If  i,  the  decision  at  each  iteration,  is  which 
direction  to  rotate  rather  than  whether  to  rotate  or  not, 
then  cos(i)  is  constant  as  cos(i)  =  cos(-i).  Then  the 
iterative rotation can be expressed as: 

(3) 

x  
i 1y  
i 1
=


 K x  
i

 K y  
i



 y d 2

i
 x d 2

i

-i

-i






(4) 

Where, 
-i

K cos(tan 2 )  = i
1+2

-i

=

-i

1+2

) 1


id  =  1  Removing 
iterative 
the 
the  scale  constant  from 
equations  yields  a  shift-add  algorithm  for  vector  rotation. 
The  product  of  the  Ki\'s  can  be  applied  elsewhere  in  the 

2 

Where, 

 

d

1   if z  < 0
 
= 
+1 otherwise

y sin z ]
 A [x cos z
x  
00n
x sin z ]
 A [y cos z
y  +0nz  = 0
A  = n

1 + 2

-2i

0
C.  Vectoring Mode 
 

(8) 

In the vectoring mode, the CORDIC rotator rotates the 
input  vector  through  whatever  angle  is  necessary  to  align 
the  result  vector  with  the  x  axis.  The  result  of  the 

Proceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) 
IEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 
vectoring  operation  is  a  rotation  angle  and  the  scaled 
m');
INSERT INTO posts (postId,userId,title,body) VALUES (339,5511,'Sample_title (part 4)','agnitude i.e. the x component of the original vector. The 
vectoring  function  works  by  seeking  to  minimize  the  y 
component  of  the  residual  vector  at  each  rotation.  The 
sign  of  the  residual  y  component  is  used  to  determine 
which direction  to  rotate next. When  initialized with zero, 
accumulator  contains  the  traversed  angle  at  the  end  of  the 
iterations [4]. The equations in this mode are: 

x  
i 1y  
i 1z
i + 1
=
 x   y   z  


-i

-i

 y d 2

i
 x d 2

i
 d tan

 



2

(9) 

Where, 

i

= 


+1   if y  < 0-1 otherwise

Then: 

x  y  
=
2
 A x  + y0
 0
0

z  = z  + tan0

-1




1 + 2
x
-2i





0

(10) 

A  = 
The  CORDIC  rotation  and  vectoring  algorithms  as 
stated  are  limited  to  rotation  angles  between  -/2  and  /2. 
For  composite  rotation  angles 
larger 
than  /2,  an 
additional  rotation  is  required  [1]. Volder  [4] describes an 
initial rotation of  /2. This gives the correction iteration: 
  d y
x\'
=  
y\' =  d x


(11) 



 

d

Where, 

z\' = z + d

+1   if y < 0
= 
-1 otherwise

There 
rotation. 
initial 
this 
for 
is  no  growth 
Alternatively,  an  initial  rotation  of  either    or  0  can  be 
made,  avoiding 
the  reassignment  of 
the  x  and  y 
components  to  the  rotator  elements.  Again,  there  is  no 
growth due to the initial rotation: x\'
 d
y
y\' =  d

z           if d = 1


z -    if d =  1


-1    if x < 0
= 
+1 otherwise

Both  reduction  forms  assume  a modulo  2  representation 
of  the  input  angle.  The  second  reduction  may  be  more 
convenient  when  wiring  is  restricted,  as  is  often  the  case 
with FPGAs. 

Where, 

(12) 

z\' = 

 

 

i

D.  Evaluation of Sine and Cosine using CORDIC 
In  rotational  mode  the  sine  and  cosine  of  the  input 
angle  can  be  computed  simultaneously.  Setting  the  y 
component of the  input vector to zero reduces the rotation 
mode result to: 

 

(13) 

 A x cos z
x  
nn
 A x sin z
y  
0nIf  x0  is  equal  to  1/An,  the  rotation  produces  the 
unscaled  sine  and  cosine  of  the  angle  argument,  z0.  Very 
often,  the  sine  and  cosine  values  modulate  a  magnitude 
value. Using other techniques (e.g., a LUT) requires a pair 
of  multipliers  to  obtain  the  required  modulation.  The 
algorithm  performs  the  multiply  as  part  of  the  rotation 
operation,  and  therefore  eliminates  the  need  for  a  pair  of 
explicit  multipliers.  The  output  of  the  CORDIC  rotator  is 
scaled  by  the  rotator  gain.  If  the  gain  is  not  acceptable,  a 
single  multiply  by  the  reciprocal  of  the  gain  constant 
placed  before  the  CORDIC  rotator  will  yield  unscaled 
results [1].  E.  Advantages 
  Number 
hardware 
in 
required 
gates 
of 
implementation  on  an  FPGA,  are minimum.  Thus, 
hardware  complexity  is  greatly  reduced  compared 
to  other  processors  such  as  DSP  multipliers. 
Hence, it is relatively simple in design.  
  Due  to  reduced  hardware  requirement,  cost  of  a 
CORDIC  hardware  implementation  is  less  as  only 
shift registers, adders and look-up table (ROM) are 
required. 
  Delay  involved during processing  is comparable  to 
that of a division or square-rooting operation.  
  No  multiplication  and  only  addition,  subtraction 
and  bit-shifting  operation  ensures  simple  VLSI 
implementation.  
  Either  if  there  is  an  absence  of  a  hardware 
multiplier (e.g. microcontroller, microprocessor) or 
there is a necessity to optimize the number of logic 
gates  (e.g.  FPGA),  CORDIC  is  the  preferred 
choice [4]. 
F.  Applications 
  The  algorithm  was  basically  developed  to  offer 
digital  solutions  to  the  problems  of  real-time 
navigation in B-58 ');
INSERT INTO posts (postId,userId,title,body) VALUES (340,5511,'Sample_title (part 5)','bomber [5].  
  This  algorithm 
in  8087  Math 
finds  use 
coprocessor,  the  HP-35  calculator  [8],  radar  signal 
processors [8] and robotics.  
  CORDIC algorithm has also been described for the 
calculation  of  DFT,  DHT,  Chirp  Z-transforms, 
filtering, Singular value decomposition and solving 
linear systems [4].  

3 

Proceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) 
IEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 
  Most  calculators,  especially  the  ones  built  by 
Instruments  and  Hewlett-Packard  use 
Texas 
CORDIC 
algorithm 
for 
calculation 
of 
transcendental functions.  

III.  SYSTEM ARCHITECTURE 

 

In  this  paper,  the  FPGA  implementation  of  simple  8-
bit dedicated processor  for calculating  the  sine and cosine 
of  an  angle  using  CORDIC  Algorithm  is  presented.  The 
processor  was  implemented  by  using  Xilinx  ISE  Design 
Suite  12.3  and  VHDL.  Fig.  2  shows  functional  block 
diagram of our 8-bit processor.  It mainly consists of an 8-
bit  multiplexers,  registers,  arithmetic  logic  unit  (ALU), 
tri-state  buffer,  comparator,  and  control  unit.  The  logic 
circuit  for  dedicated  microprocessor  is  divided  into  two 
parts: the datapath unit and control unit [9].  
Input  of  two  registers  can  be  either  from  an  external 
data  input  or  from  the  output  of  ALU  unit.  Two  control 
signals  ln_X  and  ln_Y  select  which  of  two  sources  are  to 
be  loaded  into  registers.  Two  control  signals  XLoad  and 
YLoad  load  a  value  into  respective  registers.  Bottom 
multiplier determines the source of two operands of ALU. 
This  allows  the  selection  of  one  of  the  two  subtraction 
operations X-Y  or Y-X.  A  comparator  unit  is  used  to  test 
condition  of  equal  to  or  greater  than  and  it  accordingly 
generates  status  signals.  Tristate  buffer  is  used  for 
outputting result from register X. 
Fig. 3.  A simple, general datapath circuit for the dedicated 
microprocessor 

B.  Control Unit 
Fig.  4  shows  the  block  diagram  of  control  unit  and 
Fig. 5 shows the corresponding state diagram.  
Fig. 3.  Block diagram of the control unit 

Fig. 2.  Functional block diagram of the 8-bit processor 
A.  Datapath Unit 
Datapath  is  responsible  for  the  actual  execution  of  all 
data  operations  performed  by  the  dedicated  processor  [9]. 
Fig.  3  shows  the  datapath  unit  for  the  8-bit  dedicated 
processor. 

  
Fig. 4.  State diagram of the control unit 
Control  signals  are  generated  by  the  control  unit 
which  is  modelled  as  a  finite  state  machine  with  6  states 
say  S0-S5.  There  are  9  control  signals  which  form  control 
word  and  control  the  operation  of  datapath,  as  per  the 
following table: 

4 

Proceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) 
IEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 
  TABLE I 
CONTROL SIGNAL STATUS DURING DIFFERENT STATES 

Fig. 7.  Datapath unit simulation 

 

 

The  following  table  shows  the  synthesis  report  of  the 
datapath unit: 

 

ln_Y 
1 
0 
0 
0 
0 
1 

ln_X 
1 
0 
0 
0 
0 
1 

XLoad 

1 
0 
1 
0 
0 
0 

YLoad 
1 
0 
0 
1 
0 
0 

 
XY 
0 
0 
1 
0 
0 
0 

Clear 
0 
0 
0 
0 
0 
1 
 

 

ln_X 
1 
0 
0 
0 
0 
1 

OE 
0 
0 
0 
0 
1 
0 

Done 
0 
0 
0 
0 
1 
0 

State 
ln_Y  ALU(0,1,2) 
S0 
1 
101 
S1 
101 
0 
S2 
101 
0 
S3 
101 
0 
S4 
0 
101 
S5 
101 
1       
If  Reset  =  1  the');
INSERT INTO posts (postId,userId,title,body) VALUES (341,5511,'Sample_title (part 6)','n  state  S5  occurs.  In  this  state  
registers are initialized to 0 by asserting the Clear signal. 
If  Reset  =  0  then  at  rising  edge  of  clock,  the  state  is 
upgraded  from  S5  to  state  S0.  During  the  state  S0  two 
inputs  are  loaded  in  two  registers.  After  completion  of 
state S0, state S1 is reached. In this state output of registers 
is  checked  in  comparator  for  equality  and  greater  than 
conditions. If both values are same, state S4 occurs else S2 
or  S3  will  continue  depending  on  status  of  signal  neq1. 
State S1 is repeated again. Default state is S5. 
IV.  IMPLEMENTATION AND VERIFICATION 

All  the  units  in  dedicated  processor  were  designed. 
These  units  were  described  in  VHDL-modules  and 
synthesized  using  ISE  Design  Suite  12.3.  ModelSim 
simulator  was  used  to  verify  the  functionalities  of  each 
unit.  Finally  all  the  units  were  combined  together  and 
once  again  tested  by  using  ModelSim  simulator.  Fig.  6 
shows  the  RTL  schematic  of  the  CORDIC  processor 
generated from Xilinx ISE. 

Fig. 6.  RTL schematic of the CORDIC processor 

 
A.  Datapath Unit 
Simulation result of datapath is shown in Fig. 7.  

 

5 

  TABLE II 
SYNTHESIS REPORT OF THE DATAPATH UNIT 

Number of Slices 
Maximum Frequency 
Minimum Period  

61 (31%) 
114.05 MHz 
8.76 ns 
 

 

 

 
Simulation result of the control unit is shown in Fig. 8. 
  
B.  Control Unit  

Fig. 8.  Control unit simulation 

The  following  table  shows  the  synthesis  report  of  the 
control unit: 

  TABLE III 
SYNTHESIS REPORT OF THE CONTROL UNIT 

Number of Slices 
Maximum Frequency 
Minimum Period    C.  Dedicated CORDIC Processor 
4 (2%) 
264.34 MHz 
3.78 ns 
 

 

 
 

Once 
the  datapath  unit  and  control  unit  were 
simulated,  they were  combined  and  a  dedicated  processor 
was  constructed.  Simulation  shows 
the  CORDIC 
calculation  operation  of  the  Sine  and  Cosine  of  an  angle. 
Simulation  result  for  the  dedicated  processor  is  as  shown 
in Fig. 9. 

Fig. 9.  Test-Bench waveforms indicating the Sine and Cosine output, 
obtained after the CORDIC Core Simulation 

Proceedings of the 2011 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC) 
IEEE Xplore: CFB1120J-ART; ISBN: 978-1-61284-694-1; Print Version: CFB1120J-PRT; ISBN: 978-1-61284-766-5 
The  following  table  shows  the  synthesis  report  of  the 
8-bit dedicated processor: 

  TABLE IV 
SYNTHESIS REPORT OF THE 8-BIT DEDICATED PROCESSOR 

Logic Utilization 
Number of Slices 
Number of Slice Flip Flops 
Number of 4 input LUTs 
Number of bonded IOBs 
Number of GCLKs 

Used  Available  Utilization 
3% 
3584 
126 
0% 
7168 
58 
3% 
7168 
238 
28 
141 
19% 
12% 
8 
1 

 
For a Speed Grade of -5, the minimum period required 
is 12.292 ns, which corresponds  to a maximum  frequency 
of  81.353  MHz.  The  minimum  input  arrival  time  before 
clock  and  maximum  output  required  time  after  clock  are 
9.657 ns and 8.133 ns respectively. 
The  following  table  shows  the  comparison  between 
the  actual  Sine  and  Cosine  values  and  the  ones  obtained 
from Test-Bench analysis in VHDL. 

  TABLE V 
COMPARISON OF SUCCESSIVE ANGLE ROTATION VALUES 
Angle (A)  Rotation 
0.000000 
5 
10 
0.000000 
15 
0.000000 
20 
0.000000 
5 
0.523599 
10 
0.523599 
0.523599 
15 
20 
0.523599 
5 
1.000000 
10 
1.000000 
15 
1.000000 
1.000000 
20 
5 
3.');
INSERT INTO posts (postId,userId,title,body) VALUES (342,5511,'Sample_title (part 7)','141593 
10 
3.141593 
15 
3.141593 
3.141593 
20  
Angle (A)  Rotation 
5 
0.000000 
0.000000 
10 
15 
0.000000 
20 
0.000000 
5 
0.523599 
10 
0.523599 
0.523599 
15 
20 
0.523599 
5 
1.000000 
10 
1.000000 
15 
1.000000 
1.000000 
20 
5 
3.141593 
10 
3.141593 
15 
3.141593 
3.141593 
20 
sin(A) 
(Actual) 
0.00000000 
0.00000000 
0.00000000 
0.00000000 
0.50000000 
0.50000000 
0.50000000 
0.50000000 
0.84147098 
0.84147098 
0.84147098 
0.84147098 
0.00000000 
0.00000000 
0.00000000 
0.00000000 

cos(A) 
(Actual) 
1.00000000 
1.00000000 
1.00000000 
1.00000000 
0.86602540 
0.86602540 
0.86602540 
0.86602540 
0.54030231 
0.54030231 
0.54030231 
0.54030231 
-1.0000000 
-1.0000000 
-1.0000000 
-1.00000000 

sin(A) 
(Test-Bench) 
0.01483516 
0.00117259 
0.00001292 
-0.00000043 
0.48362630 
0.49892865 
0.50003905 
0.50000106 
0.80881306 
0.84080033 
0.84149350 
0.84147186 
-0.01483516 
-0.00117259 
-0.00001292 
0.00000043 

cos(A) 
(Test-Bench) 
0.99988995 
0.99999931 
1.00000000 
1.00000000 
0.87527459 
0.86664307 
0.86600286 
0.86602479 
0.58806584 
0.54134537 
0.54026724 
0.54030094 
-0.99988995 
-0.99999931 
-1.00000000 
-1.00000000 

Error 
-1.4835e-002 
-1.1725e-003 
-1.2922e-005 
4.2874e-007 
1.6373e-002 
1.0713e-003 
-3.9047e-005 
-1.0561e-006 
3.2657e-002 
6.7065e-004 
-2.2515e-005 
-8.7478e-007 
1.4835e-002 
1.1725e-003 
1.2922e-005 
-4.2874e-007 

Error 
1.1004e-004 
6.8748e-007 
8.3498e-011 
9.2037e-014 
-9.2491e-003 
-6.1766e-004 
2.2545e-005 
6.0975e-007 
-4.7763e-002 
-1.0430e-003 
3.5067e-005 
1.3623e-006 
-1.1004e-004 
-6.8748e-007 
-8.3498e-011 
-9.2037e-014 

V.  CONCLUSION 
  We  have  successfully  simulated  an  8-bit  dedicated 
processor  for calculating  the Sine and Cosine of an angle, 
on  ModelSim  simulator  using  the  VHDL  language.  Our 
processor  has  six  main  components  namely,  control  unit, 
multiplexer  unit,  ALU  unit,  register  unit,  tristate  buffer 
unit  and  comparator.  Our  dedicated  processor  has  a 
maximum  frequency  of  81.353  MHz  was  reached  with  a 
minimum period of 12.292 ns. 126 (3%) slices were used. 
Our  System  can  be  implemented  on  Xilinx  Spartan  3 
XC3S400  using  ISE  Design  Suite  12.3  and  VHDL 
language.  Our  dedicated  processor  has  a  distinct 
advantage  over  a  general  purpose  processor,  since  it 
repeatedly  performs  same  task  its  design  is more  efficient 
and consumes less resources and is less time intensive.  REFERENCES 
[1]  R.  Andraka,  A  survey  of  CORDIC  algorithms  for  FPGA 
based  computers,  Proceedings  of  the  1998  ACM/SIGDA 
sixth  international symposium on Field programmable gate 
arrays, pp. 191  200. 
[2]  V.  Sharma,  FPGA  Implementation  of  EEAS  CORDIC 
based  Sine  and  Cosine  Generator,  M.  Tech  Thesis,  Dept. 
of  Electronics  and  Communication  Engineering,  Thapar 
University, Patiala, 2009. 
[3]  S.  Panda,  Performance  Analysis  and  Design  of  a  Discrete 
Cosine Transform Processor using CORDIC Algorithm, M. 
Tech  Thesis,  Dept.  of  Electronics  and  Communication 
Engineering, NIT Rourkela, Rourkela, Orissa, 2010. 
[4]  R.  K.  Jain,  B.  Tech  The');
INSERT INTO posts (postId,userId,title,body) VALUES (343,5511,'Sample_title (part 8)','sis,  Design  and  FPGA 
Implementation  of  CORDIC-based  8-point  1D  DCT 
Processor, NIT Rourkela, Rourkela, Orissa, 2011. 
[5]  J.  Volder,  "The  CORDIC  Trigonometric  Computing 
Technique,"  IRE  Transactions  on  Electronic  Computing, 
Vol EC-8, Sept 1959, pp. 330-334. 
[6]  F.  Ling,  Givens  rotation  based  least  squares  lattice  and 
IEEE  Transactions  on  Signal 
related  algorithms, 
Processing, Jul 1991, pp. 1541  1551. 
[7]  J.  S.  Walther,  "A  unified  algorithm  for  elementary 
functions,"  Proceedings  of  the  Spring  Joint  Computer 
Conference, 1971, pp. 379-385. 
[8]  R.  Andraka.  "Building  a  High  Performance  Bit-Serial 
Processor  in  an  FPGA,"  Proceedings  of  Design  SuperCon, 
Jan 1996, pp. 1-2. 
Digital  Logic  and  Microprocessor 
[9]  E. O. Hwang,  
Design  with  VHDL,  Thomson/Nelson,  2006,  pp.  379-413, 
pp. 290-311. 

 

6 

');
INSERT INTO posts (postId,userId,title,body) VALUES (344,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework ','Clment Moulin-Frier 
SPECS Lab 
Universitat Pompeu Fabra 
Barcelona, Spain 
Email: 
clement.moulinfrier@gmail.com 
Mart Sanchez-Fibla 
SPECS Lab 
Universitat Pompeu Fabra 
Barcelona, Spain 
Email: santmarti@gmail.com 

Jordi-Ysard Puigb 
SPECS Lab 
Universitat Pompeu Fabra 
Barcelona, Spain 
Email:  
jordiysard.puigbo@upf.edu 

Xerxes D. Arsiwalla 
SPECS Lab 
Universitat Pompeu Fabra 
Barcelona, Spain 
Email: 
x.d.arsiwalla@gmail.com Paul FMJ Verschure 
SPECS Lab 
Universitat Pompeu Fabra '||'&'||' 
ICREA 
Barcelona, Spain 96678-2391 
Email: 
paul.verschure@upf.edu 

 

AbstractIn  this  paper,  we  argue  that  the  future  of  Artificial 
Intelligence  research  resides  in  two  keywords:  integration  and 
embodiment.  We  support  this  claim  by  analyzing  the  recent 
advances  of  the  field.  Regarding  integration,  we  note  that  the 
most  impactful  recent  contributions  have  been  made  possible 
through  the  integration  of  recent  Machine  Learning  methods 
(based  in  particular  on  Deep  Learning  and  Recurrent  Neural 
Networks)  with  more  traditional  ones  (e.g.  Monte-Carlo  tree 
search,  goal  babbling  exploration  or  addressable  memory 
systems).  Regarding  embodiment,  we  note  that  the  traditional 
benchmark  tasks  (e.g.  visual  classification    or  board  games)  are 
becoming  obsolete  as  state-of-the-art 
learning  algorithms 
approach  or  even  surpass  human  performance  in  most  of  them, 
having  recently  encouraged  the  development  of  first-person  3D 
game  platforms  embedding  realistic  physics.  Building  upon  this 
analysis,  we  first  propose  an  embodied  cognitive  architecture 
integrating heterogenous sub-fields of Artificial Intelligence into a 
unified  framework.  We  demonstrate  the  utility  of  our  approach 
by showing how major contributions of the field can be expressed 
within 
the  proposed 
framework.  We 
then  claim 
that 
benchmarking  environments  need  to  reproduce  ecologically-valid 
conditions  for  bootstrapping  the  acquisition  of  increasingly 
complex  cognitive  skills  through  the  concept  of  a  cognitive  arms 
race between embodied agents.  
Index  TermsCognitive  Architectures,  Embodied  Artificial 
Intelligence,  Evolutionary  Arms  Race,  Unified  Theories  of 
Cognition.  

I.  INTRODUCTION 

In  recent  years,  research  in  Artificial  Intelligence  has  been 
primarily  dominated  by  impressive  advances  in  Machine 
Learning,  with  a  strong  emphasis  on  the  so-called  Deep 
Learning framework. It has allowed considerable achievements 
such  as  human-level  performance  in  visual  classification  [1] 
and  description  [2],  in  Atari  video  games  [3]  and  even  in  the 
highly  complex  game  of Go  [4]. The Deep Learning  approach 
is characterized by supposing very minimal prior on the task to 
be  solved,  compensating  this  lack  of  prior  knowledge  by 
feeding  the  learning  algorithm  with  an  extremely  high  amount 
of training data, while hiding the intermediary representations.   However,  it  is  important  noting  that  the  most  important 
contributions of Deep Learning for  Artificial Intelligence often 
owe  their  success  in  part  to  their  integration  with  other  types 
of  learning  algorithms.  For  example,  the  AlphaGo  program 
which  defeated  the  world  champions  in  the  famously  complex 
game  of  Go  [4],  is  based  on  the  integration  of  Deep 
Reinforcement  Learning  with  a  Monte-Carlo  tree  search 
algorithm.  Without  the  tree  search  addition,  AlphaGo  still 
outperforms  previous  machine  performances  but  is  unable  to 
beat  high-level  human  players. Another  example  can be  found 
in  the  original Deep Q-Learni');
INSERT INTO posts (postId,userId,title,body) VALUES (345,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 2)','ng  algorithm  (DQN, Mnih  et  al., 
2015),  achieving  very  poor  performance  in  some  Atari  games 
where  the  reward  is  considerably  sparse  and  delayed  (e.g. 
Montezuma  Revenge).  Solving  such  tasks  has  required  the 

Paper submitted to the ICDL-Epirob 2017 conference. Pre-review version. 

integration  of  DQN  with  intrinsically-motivated  learning 
algorithms for novelty detection [5], or goal babbling [6].  A drastically different approach has also received considerable 
attention,  arguing  that  deep  learning  systems  are  not  able  to 
solve  key  aspects of  human  cognition  [7]. The approach states 
that  human  cognition  relies  on  building  causal  models  of  the 
world  through  combinatorial  processes  to  rapidly  acquire 
knowledge  and  generalize  it  to  new  tasks  and  situations.  This 
has  led  to  important  contributions  through  model-based 
Bayesian  learning  algorithms,  which  surpass  deep  learning 
approaches  in  visual  classification  tasks,  while  displaying 
powerful  generalization  abilities  in  one-shot  training  [8].  This 
solution  however,  comes  at  a  cost:  the  underlying  algorithm 
requires  a  priori  knowledge  about  the  primitives  to  learn  from 
and  about  how  to  compose  them  to  build  increasingly  abstract 
categories.  An  assumption  of  such  models  is  that  learning 
should  be  grounded  in  intuitive  theories  of  physics  and 
psychology, supporting and enriching acquired knowledge  [7], 
as supported by infant behavioral data [9]. Considering 
intuitive  physics  and 
the  pre-existence  of 
psychology  engines  as an  inductive bias  for Machine Learning 
is far from being a  trivial assumption. It immediately raises the 
question: where does such knowledge come from and how is it 
shaped  through  evolutionary,  developmental  and  cultural 
processes? All  the  aforementioned  approaches  are  lacking  this 
fundamental  component  shaping  intelligence  in  the  biological 
world,  namely  embodiment.  Playing  Atari  video  games, 
complex  board  games  or  classifying  visual  images  at  a  human 
level  are  considerable  milestones  of  Artificial  Intelligence 
research.  Yet,  in  contrast,  biological  cognitive  systems  are 
intrinsically  shaped  by 
their  physical  nature.  They  are 
embodied  within  a  dynamical  environment  and  strongly 
coupled  with  other  physical  and  cognitive  systems  through 
complex feedback loops operating at different scales: physical, 
sensorimotor,  cognitive,  social,  cultural  and  evolutionary. 
Nevertheless,  many  recent  Artificial  Intelligence  benchmarks 
have  focused  on  solving  video  games  or  board  games, 
adopting  a  third-person  view  and  relying  on  a  discrete  set  of 
actions  with  no  or  poor  environmental  dynamics.  A  few 
interesting  software  tools have however  recently been  released 
to provide more realistic benchmarking environments. This for 
example,  is  the  case  of Project Malmo  [10] which provides an 
API  to  control  characters  in  the  MineCraft  video  game,  an 
open-ended  environment  with  complex  physical  and 
environmental  dynamics;  or  Deepmind  Lab  [11],  allowing  the 
creation  of  rich  3D  environments  with  similar  features. 
Another  example  is  OpenAI  Gym  [12],  providing  access  to  a 
variety  of  simulation  environments  for  the  benchmarking  of 
learning  algorithms,  specially  reinforcement  learning  based. 
Such  complex  environments  are  becoming  necessary  to 
validate  the  full  potential  of  modern  Artificial  Intelligence 
research,  in  an  era  where  human  performance  is  being 
achieved  on  an  increasing  number  of  traditional  benchmarks. 
There  is also a  renewed  interest  for multi-agent b');
INSERT INTO posts (postId,userId,title,body) VALUES (346,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 3)','enchmarks  in 
light  of  the  recent  advances  in  the  field,  solving  social  tasks 

such as  the prisoner dilemma  [13] and studying  the emergence 
of cooperation and competition among agents [14].  The  above  examples  emphasize  two  important  challenges  in 
modern  Artificial  Intelligence.  Firstly,  there  is  a  need  for  a 
framework  providing  a  principled 
unified 
integrative 
methodology  for  organizing 
the 
interactions  of  various 
subfields  (e.g.  planning  and  decision  making,  abstraction, 
classification,  reinforcement  learning,  sensorimotor  control  or 
exploration).  Secondly,  Artificial  Intelligence  is  arriving  at  a 
level  of  maturation  where  more  realistic  benchmarking 
environments  are  required,  for  two  reasons:  validating  the  full 
potential  of  the  state-of-the-art  artificial  cognitive  systems,  as 
well  as  understanding  the  role  of  environmental  complexity  in 
the shaping of cognitive complexity.  In  this  paper,  we  first  propose  an  embodied  cognitive 
architecture  structuring  the  main  sub-fields  of  Artificial 
Intelligence  research  into  an  integrated  framework.  We 
demonstrate  the utility of our approach by showing how major 
contributions of the field can be expressed within the proposed 
framework,  providing  a  powerful  tool  for  their  conceptual 
description  and  comparison.  Then  we  argue 
that 
the 
complexity  of  a  cognitive  agent  strongly  depends  on  the 
complexity  of  the  environment  it  lives  in.  We  propose  the 
concept  of  a  cognitive  arms  race,  where  an  ecology  of 
embodied  cognitive  agents  interact  in  a  dynamic  environment 
reproducing  ecologically-valid  conditions  and  driving  them  to 
acquire  increasingly  complex  cognitive  abilities  in  a  positive 
feedback loop. 

II.  AN INTEGRATED COGNITIVE ARCHITECTURE FOR EMBODIED 
ARTIFICIAL INTELLIGENCE 

Considering  an 
to 
integrative  and  embodied  approach 
Artificial  Intelligence  requires  dealing  with  heterogeneous 
aspects  of  cognition,  where  low-level  interaction  with  the 
environment  interacts bidirectionally with high-level reasoning 
abilities.  This  reflects  an  historical  challenge  in  formalizing 
how  cognitive  functions  arise  in  an  individual  agent  from  the 
interaction  of  interconnected  information  processing  modules 
structured  in  a  cognitive  architecture  [15],  [16].  On  one  hand, 
top-down  approaches  mostly  rely  on  methods  from  Symbolic 
Artificial  Intelligence  (from  the  General  Problem  Solver   [17] 
to  Soar  [18]  or  ACT-R  [19]  and  their  follow-up),  where  a 
complex  representation  of  a  task  is  recursively  decomposed 
the  other  hand,  bottom-up 
into  simpler  elements.  On 
approaches 
instead  emphasize 
lower-level  sensory-motor 
control  loops  as  a  starting  point  of  behavioral  complexity, 
which  can  be  further  extended  by  combining  multiple  control 
loops  together,  as  implemented  in  behavior-based  robotics 
intelligence  without 
[20] 
(sometimes 
referred 
as 
representation  [21]).  These  two  approaches  thus  reflect 
different  aspects  of  cognition:  high-level  symbolic  reasoning 
for the former and low-level embodied behaviors for the latter. 
However,  both  aspects  ar');
INSERT INTO posts (postId,userId,title,body) VALUES (347,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 4)','e  of  equal  importance when  it  comes 
to defining a unified theory of cognition. It is therefore a major 
challenge  of  cognitive  science  to  unify  both  approaches  into  a 
single  theory, where  (a)  reactive  control  allows  an  initial  level 

Paper submitted to the ICDL-Epirob 2017 conference. Pre-review version. 

of  complexity  in  the  interaction  between  an  embodied  agent 
and  its  environment  and  (b)  this  interaction  provides  the  basis 
for  learning  higher-level  representations  and  for  sequencing 
them in a causal way for top-down goal-oriented control. 
For  this  aim,  we  adopt  the  principles  of  the  Distributed 
Adaptive  Control  (DAC)  theory  of  the  mind  and  brain  [22], 
[23].  Besides  its  biological  grounding,  DAC  is  an  adequate 
modeling framework for integrating heterogeneous concepts of 
Artificial  Intelligence  and  Machine  Learning  into  a  coherent 
cognitive  architecture,  for  two  reasons:  (a)  it  integrates  the 
principles of both the aforementioned bottom-up and top-down 
approaches  into  a  coherent  information  processing  circuit;  (b) 
it  is  agnostic  to  the  actual  implementation  of  each  of  its 
functional modules. Over  the  last  fifteen  years, DAC  has been 
applied  to  a  variety  of  complex  and  embodied  benchmark 
tasks,  for  example  foraging  [22],  [24]  or  social  humanoid 
robot control [16], [25].  

A.  The  DAC-EAI  cognitive  architecture:  Distributed 
Adaptive Control for Embodied Artificial Intelligence 

DAC  posits  that  cognition  is  based  on  the  interaction  of 
interconnected  control  loops  operating  at  different  levels  of 
abstraction  (Figure  1).  The  functional  modules  constituting 
the  architecture  are  usually  described 
in  biological  or 
psychological terms (see e.g. [26]). Here we propose instead to 
describe  them  in  purely  computational  term,  with  the  aim  of 
facilitating  the  description  of  existing  Artificial  Intelligence 
systems  within 
this  unified 
framework.  We  call 
this 
the  architecture  DAC-EAI:  Distributive 
instantiation  of 
Adaptive Control for Embodied Artificial Intelligence.  
Figure  1:  The  DAC-EAI  architecture  allows  a  coherent 
organization  of  heterogenous  subfields  of  Artificial  Intelligence. 
DAC-EAI  stands  for Distributed Adaptive Control  for Embodied 

Machine  Learning.  It  is  composed  of  three  layers  operating  in 
parallel  and  at  different  levels  of  abstraction.  See  text  for  detail, 
where each module name is referred with italics. 

The  first  level,  called  the  Somatic  layer,  corresponds  to  the 
embodiment  of  the  agent  within  its  environment.  It  includes 
the  sensors  and  actuators,  as  well  internal  variables  to  be 
regulated  (e.g.  energy  or  safety  levels).  The  self-regulation  of 
these  internal  variables  occurs  in  the  Reactive  layer  and 
extends  the  aforementioned  behavior-based  approaches  (e.g. 
the  Subsumption  architecture  [20])  with  drive  reduction 
mechanisms  through  predefined  sensorimotor  control  loops 
(i.e.  reflexes).  In  Figure  1,  this  corresponds  to  the  mapping 
from  the  Sensing  to  the  Motor  Control');
INSERT INTO posts (postId,userId,title,body) VALUES (348,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 5)','  module  through  Self 
Regulation. The Reactive layer offers several advantages when 
analyzed  from  the  embodied  artificial  intelligence  perspective 
of  this  paper.  First,  reward  is  traditionally  considered  in 
Machine  Learning  as  a  scalar  value  associated  with  external 
states of  the environment. DAC proposes instead that it should 
derive  from  the 
internal  dynamics  of  multiple  internal 
variables  modulated  by 
the  body-environment  real-time 
interaction,  providing  an  embodied  notion  of  reward  in 
cognitive  agents.    Second,  the  Reactive  layer  generates  a  first 
level  of  behavioral  complexity  through  the  interaction  of 
predefined  sensorimotor  control  loops  for self-regulation. This 
provides a notion of embodied  inductive bias bootstrapping  an 
structuring  learning  processes  in  the  upper  levels  of  the 
architecture.  This  is  a  departure  from  the  model-based 
approaches mentioned  in  the  introduction  [7], where  inductive 
biases  are  instead  considered  as  intuitive  core  knowledge  on 
the form of a pre-existent physics and psychology engine. 
Behavior  generated  in  the  Reactive  layer  bootstraps  learning 
processes  for  acquiring  a  state  space  of  the  agent-environment 
interaction in the Adaptive layer. The Representation Learning 
module  receives  input  from  Sensing  to  form  increasingly 
abstract  representations.  For  example,  unsupervised  learning 
methods  such  as  deep  autoencoders  [27]  could  be  a  possible 
implementation  of  this module. The  resulting abstract states of 
the  world  are  mapped  to  their  associated  values  through  the 
Value Prediction module, informed by the internal states of the 
agent from Self Regulation. This allows the inference of action 
policies  maximizing  value  through  Action  Selection,  a  typical 
reinforcement  learning  problem  [28].  An  interesting  point  is 
that  the  self-regulation  of  multiple  internal  variables  in  the 
Reactive  layer  requires  the  agent  to  switch  between  different 
action  policies  (differentiating  e.g.  between  situation  of  low 
energy vs. low safety). In the current framework, such a switch 
is  controlled  higher  up  the  architecture  through  the  Goal 
Selection and Planning module (see below).   
The  state  space  acquired  in  the  Adaptive  layer  then  supports 
the  acquisition  of  higher-level  cognitive  abilities  such  as  goal 
selection,  memory  and  planning  in  the  Contextual  layer.  The 
abstract  representations  acquired  in  Representation  Learning 
through  Relational  Learning.  The 
are 
linked 
together 
availability  of  abstract  representations  in  possibly  multiple 
modalities  provides  the  substrate  for  causal  and  compositional 
linking.  Several  state-of-the-art  methods  are  of  interest  for 
learning  such  relations,  such  as Bayesian program  learning  [8] 
or  Long  Short  Term  Memory  neural  network  (LSTM,  [29]). 

Paper submitted to the ICDL-Epirob 2017 conference. Pre-review version. 

Based  on  these  higher-level  representations,  Goal  Selection 
forms the basis of goal-oriented behavior by selecting valuable 
states  to  be  reached,  where  value  is  provided  by  the  Value 
Prediction  module. 
Intrinsically-motivated  methods 
maximizing  learning  progress  can  be  applied   here  for  an 
efficient  exploration  of  the  environment  [30].  The  selected 
goals  are  reached  through  Planning,  where  any  adaptive 
method  of  this  field  can  be  applied  [31].  The  resulting  action 
plans,  learned  from  ');
INSERT INTO posts (postId,userId,title,body) VALUES (349,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 6)','action-state-value  tuples  generated  by  the 
Adaptive  layer,  propagate  down  the  architecture  to  modulate 
behavior.  Finally,  an  addressable memory  system  registers  the 
activity of the Contextual layer, allowing the persistence of the 
agent  experience  over  the  long  term  for  lifelong  learning 
abilities  [32].  In  psychological  terms,  this  memory  system  is 
analog to an autobiographical memory. 
These  high-level  cognitive  processes 
in 
turn  modulate 
behavior  at  lower  levels  via  top-down  pathways  shaped  by 
behavioral  feedback.  The  control  flow  is  therefore  distributed 
within  the  architecture,  both  from  bottom-up  and  top-down 
interactions between layers, as well as from lateral information 
processing into the subsequent layers. 

B.  Expressing  existing  Machine  Learning  systems  within 
the DAC-EAI framework 

We  now  demonstrate  the  generality  of  the  proposed  DAC-
EML  architecture  by  describing  how  well-known  Artificial 
Intelligence  systems  can  be  conceptually  described  as  sub-
parts of the DAC-EAI architecture (Figure 2).  
We start with behavior-based robotics [20], implementing a set 
of  reactive  controllers  through  low-level  coupling  between 
sensors to effectors . Within the proposed framework, there are 
described  as  the  lower  part  of  the  architecture,   spanning  the 
Somatic  and  Reactive  layers  (Figure  2B).  However,  those 
approaches  are  not  considering  the  self  regulation  of  internal 
variables  but  instead  of  exteroceptive  variables,  such  as  light 
quantity for example.  
In  contrast, 
top-down  robotic  planning  algorithms  [33] 
correspond  to  the  right  column  (Action)  of  the  DAC-EAI 
architecture:  spanning  from  Planning  to  Action  Selection  and 
Motor  Control,  where  the  current  state  of  the  system  is 
typically 
provided 
by 
pre-processed 
sensory-related 
information  along  the  Reactive  or  Adaptive  layers  (Figure 
2C).    More  recent  Deep  Reinforcement  Learning  methods, 
such  as  the  original  Deep  Q-Learning  algorithm  (DQN,  [3]) 
typically  span  over  all 
the  Adaptive 
layer,  They  use 
convolutional  deep  networks  learning  abstract  representation 
from  pixel-level  sensing  of  video game  frames, Q-learning  for 
predicting  the  cumulated  value  of  the  resulting  states  and 
competition  among  discrete  actions  as  an  action  selection 

process  (Figure  2D).  Still,  there  is  no  real  motor  control  in 
this system, given that most available benchmarks operate on a 
limited  set  of  discrete  (up-down-left-right)  or  continuous 
(forward speed,  rotation speed) actions. Not shown on   Figure 
2,  classical  reinforcement  learning  [28]  relies  on  the  same 
architecture  as  Figure  2D,  however  not  addressing  the 
representation  learning  problem,  since  the  state  space  is 
usually  pre-defined  in  these  studies  (often  considering  a  grid 
world). 
Several  extensions  based  on  the  DQN  algorithm  exist.  For 
example,  intrinsically-motivated  deep  reinforcement  learning 
[6]  extends  it  with  a  goal  selection  mechanism  (Figure  2E). 
This  extension  allows  solving  tasks  with  delayed  and  sparse 
reward (e.g. Montezuma Revenge) by encouraging exploratory 
behaviors.  AlphaGo  also  relies  on  a  Deep  Reinforcement 
Learning method  (hence  spanning  the Adaptive  layer  as  in  the 
last  examples),  coupled  with  a  Monte-Carlo  tree  search 
algorithm  which  can  be  conceived  as  a  planning  process  (see 
also  [34]), as represented in Figure 2F.  
Another  recent work,  adopting  a  drastically opposite approach 
as  compared  to  end-to-end  deep  learning,  addresses  the 
problem  of  learning  highly  abstract  concepts  from  the 
perspective  of  the  human  ability  to  perform  one-shot  learning. 
The  resulting  model,  called  B');
INSERT INTO posts (postId,userId,title,body) VALUES (350,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 7)','ayesian  Program  Learning  [8], 
relies on a priori knowledge about  the primitives  to  learn  from 
and  about  how  to  compose  them  to  build  increasingly  abstract 
categories.  In  this  sense,  it  is  described  within  the  DAC-EAI 
framework as addressing  the pattern  recognition problem  from 
the  perspective  of  relational  learning,  where  primitives  are 
causally  linked  for  composing  increasingly  abstract  categories 
(Figure 2G).  
Finally,  the Differentiable Neural Computer  [35],  successor of 
the  Neural  Turing  Machine  [36],  couples  a  neural  controller 
(e.g.  based  on  a  LSTM)  with  a  content-addressable  memory. 
The  whole  system  is  fully  differentiable  and  is  consequently 
optimizable  through  gradient  descent.  It  can  solve  problems 
requiring  some  levels  of  sequential  reasoning  such  has  path 
planning  in  a  subway  network  or  performing  inferences  in  a 
family  tree.  In  DAC-EAI  terms,  we  describe  it  as  an 
implementation  of  the  higher  part  of  the  architecture,  where 
causal  relations  are  learned  from  experience  and  selectively 
stored  in  an  addressable  memory,  which  can  further  by 
accessed for reasoning or planning operations (Figure 2H).   An  interesting  challenge  with  such  an  integrative  approach  is 
therefore  to express a wide range of Artificial systems within a 
unified 
framework, 
facilitating 
their  description 
and 
comparison in conceptual terms.  

 

Paper submitted to the ICDL-Epirob 2017 conference. Pre-review version. 
Figure  2:  The  DAC-EAI  architecture  allows  a  conceptual  description  of  many  Artificial  Intelligence  systems  within  a  unified 
framework.  A)  The  complete  DAC-EAI  architecture  (see  Figure  1  for  a  larger  version).  The  other  subfigures  (B  to  E)  show  conceptual 
descriptions  of  different  Artificial  Intelligence  systems  within  the  DAC-EAI  framework.  B):  Behavior-based  Robotics  [20].  C)  Top-down 
robotic  planning  [33].  D)  Deep  Q-Learning  [3].  E)  Intrinsically-Motivated  Deep  Reinforcement  Learning    [6].  F)  AlphaGo  [4].  G)  Bayesian 
Program Learning [8]. H) Differentiable Neural Computer [35]. 

III.  THE COGNITIVE ARMS RACE: REPRODUCING 
ECOLOGICALLY-VALID CONDITIONS FOR DEVELOPING 
COGNITIVE COMPLEXITY  

A  general-purpose  cognitive  architecture 
for  Artificial 
Intelligence,  as  the  one  proposed  in  the  previous  section,  
tackles  the  challenge  of  general-purpose  intelligence  with  the 
aim  of  addressing  any  kind  of  task.  Traditional  benchmarks, 
mostly  based  on  datasets  or  on  idealized  reinforcement 
learning  tasks,  are  progressively  becoming  obsolete  in  this 
respect.  There  are  two  reasons  for  this.  The  first  one  is  that 
state-of-the-art  learning  algorithms  are  now  achieving  human 
performance  in  an  increasing  number  of  these  traditional 
benchmarks  (e.g.  visual  classification,  video  or  board');
INSERT INTO posts (postId,userId,title,body) VALUES (351,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 8)','  games). 
The  second  reason  is  that  the  development  of  complex 
cognitive  systems  is  likely  to  depend  on  the  complexity  of  the 
environment  they  evolve  in1.  For  these  two  reasons,  Machine 
Learning  benchmarks  have  recently  evolved  toward  first-

1  See  also  https://deepmind.com/blog/open-sourcing-deepmind-lab/:  It  is 
possible  that  a  large  fraction  of  animal  and  human  intelligence  is  a  direct 
consequence  of  the  richness  of  our  environment,  and  unlikely  to  arise 
without it. 

person  3D  game  platforms  embedding  realistic  physics  [10], 
[11] and likely to become the new standards in the field. It  is  therefore  fundamental  to  figure  out what  properties of  the 
environment  act  as  driving  forces  for  the  development  of 
complex cognitive abilities in embodied agents. We propose in 
this  paper  the  concept  of  a  cognitive  arms  race  as  a 
fundamental  driving  force  catalyzing  the  development  of 
cognitive  complexity.  The  aim  is  to  reproduce  ecologically-
valid  conditions  among  embodied  agents  forcing  them  to 
continuously  improve  their  cognitive  abilities  in  a  dynamic 
multi-agent  environment.  In  natural  science,  the  concept  of  an 
evolutionary  arms  race  has  been  defined  as  follows:  an 
adaptation  in  one  lineage  (e.g.  predators)  may  change  the 
selection  pressure  on  another  lineage  (e.g.  prey),  giving  rise 
to  a  counter-adaptation  [37].  This  process  produces  the 
conditions  of  a  positive  feedback  loop  where  one  lineage 
pushes  the  other  to  better  adapt  and  vice  versa.  We  propose 
that  such  a  positive  feedback  loop  is  a  key  driving  force  for 
achieving  an  important  step  towards  the  development  of 
machine general intelligence.  A  first  step  for  achieving  this  objective  is  the  computational 
modeling  of  two  populations  of  embodied  cognitive  agents, 

 

preys  and  predators,  each  agent  being  driven  by  the  cognitive 
architecture  proposed  in  the  previous  section.  Basic  survival 
behaviors  are  implemented  as  sensorimotor  control  loops 
operating  in  the  Reactive  layer,  where  predators  hunt  preys, 
while  preys  escape  predators  and  are  attracted  to  other  food 
sources.  Since  these  agents  adapt  to  environmental  constraints 
through  learning processes occurring  in  the upper  levels of  the 
architecture,  they  will  reciprocally  adapt  to  each  other.  A 
cognitive  adaptation  (in  term  of  learning)  of  members  of  one 
population  will  perturb  the  equilibrium  attained  by  the  others 
for self-regulating  their own internal variables, forcing them to 
re-adapt  in  consequence.  This  will  provide  an  adequate  setup 
for studying the conditions of entering in a cognitive arms race 
between  populations,  where  both  reciprocally  improve  their 
cognitive abilities against each other. 
It  is  interesting  to  note  that  there  exist  precursors  of  this 
concept  in  the  recent  literature  under  a  quite  different  angle. 
An');
INSERT INTO posts (postId,userId,title,body) VALUES (352,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 9)','  interesting  example  is  a  Generative  Adversarial  Network 
[38],  where  a  pattern  generator  and  a  pattern  discriminator 
compete  and  adapt  against  each  other. Another  example  is  the 
AlphaGo  program  [4]  which  was  partly  trained  by  playing 
games  against  itself,  consequently  improving  its  performance 
in  an  iterative  way.  Both  these  systems  owe  their  success  in 
part  to  their  ability  to  enter  in  a  positive  feedback  loop  of 
performance improvement.  

IV.  CONCLUSION 

Building  upon  recent  advances  in  Artificial  Intelligence    and 
Machine Learning, we  have proposed  in  this paper a cognitive 
architecture,  called  DAC-EAI,  allowing 
the  conceptual 
description  of  many  Artificial  Intelligence  systems  within  a 
unified  framework.  Then  we  have  proposed  the  concept  of  a 
cognitive  arms  between  embodied  agent  population  as  a 
potentially  powerful  driving  force  for  the  development  of 
cognitive complexity. 
We  believe  that  these  two  research  directions,  summarized  by 
the  keywords  integration  and  embodiment,  are  key  challenges 
for  leveraging  the  recent  advances  of  the  field  toward  the 
achievement  of  General  Artificial  Intelligence.  This  ambitious 
objective  requires  integrating  all  aspects  of  cognition  (from 
low-level  sensorimotor  coupling 
to  high-level  symbolic 
reasoning)  within  an  embodied  cognitive  architecture,  as  well 
as  to  consider  the  strongly  non-stationary  nature  of  ecological 
conditions  shaping  the  cognitive  development  of  biological 
organisms.  
The main lesson of our integrative effort at the cognitive  level, 
as  summarized  in  Figure  2,  is  that  powerful  algorithms  and 
control systems are existing which,  taken  together, span all the 
relevant  aspects  of  cognition  required  to  solve  the  problem  of 
General  Artificial  Intelligence2.  We  see  however  that  there  is 
still  a  considerable  amount  of  work  to  be  done  in  order  to 
integrate  all  the  existing  sub-parts  into  a  coherent  and 
complete  cognitive  system.  This  effort  is  central  to  the 
research  program  of  our  group  and  we  have  already 
demonstrated our  ability  to  implement complete version of  the 
architecture (see [16], [24] for our most recent contributions). 

2 But this does not mean those aspects are sufficient to solve the problem.  

As  we  already  noted  in  previous  publications  [15],  [26],  [39], 
[40],  there  is  however  a  missing  ingredient  in  these  systems 
preventing  them  to  being  considered  at  the  same  level  as 
animal  intelligence:  they  are  not  facing  the  constraint  of  the 
massively  multi-agent  world  in  which  biological  systems 
evolve.  We  propose  here  that  a  key  constraint  imposed  by  a 
multi-agent world  is  the  emergence of positive  feedback  loops 
between  competing  agent  populations,  forcing 
to 
them 
continuously adapt against each');
INSERT INTO posts (postId,userId,title,body) VALUES (353,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 10)',' other.  
Our  approach  facing  several  important  challenges.  The  first 
one  is  to  leverage  the  recent advances  in  robotics and machine 
learning 
toward 
the  achievement  of  general  artificial 
intelligence, based on  the principled methodology provided by 
the  DAC  framework.  The  second  one  is  to  provide  a  unified 
theory  of  cognition  [41]  able  to  bridge  the  gap  between 
computational  and  biological  science.  The  third  one  is  to 
understand  the  emergence  of  general  intelligence  within  its 
ecological  substrate,  i.e.  the  dynamical  aspect  of  coupled 
physical and cognitive systems. 

 ACKNOWLEDGMENT 

Work  supported  by  ERCs  CDAC  project:  "Role  of 
in  Adaptive  Behavior" 
Consciousness 
(ERC-2013-ADG 
341196)  '||'&'||'  EU 
project  Socialising  Sensori-Motor 
Contingencies 
(socSMC-641321H2020-FETPROACT-
2014). 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

V.  REFERENCES 

O.  Russakovsky,  J.  Deng,  H.  Su,  J.  Krause,  S.  Satheesh,  S. Ma,  Z. 
Huang,  A.  Karpathy,  A.  Khosla,  M.  Bernstein,  A.  C.  Berg,  and  L. 
Fei-Fei, ImageNet Large Scale Visual Recognition Challenge,  Int. 
J. Comput. Vis., vol. 115, no. 3, pp. 211252, 2015. 
A. Karpathy  and L. Fei-Fei, Deep Visual-Semantic Alignments for 
Generating  Image  Descriptions,  in  Proceedings  of  the  IEEE 
Conference  on  Computer  Vision  and  Pattern  Recognition ,  2015, 
pp. 31283137. 
V. Mnih, K. Kavukcuoglu, D.  Silver, A. A. Rusu,  J. Veness, M. G. 
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, 
S.  Petersen,  C.  Beattie,  A.  Sadik,  I.  Antonoglou,  H.  King,  D. 
Kumaran, D. Wierstra, S. Legg, D. Hassabis, others, S. Petersen, C. 
Beattie,  A.  Sadik,  I.  Antonoglou,  H.  King,  D.  Kumaran,  D. 
Wierstra,  S.  Legg,  and  D.  Hassabis,  Human -level  control  through 
deep  reinforcement  learning,  Nature,  vol. 518, no. 7540, pp. 529
533, Feb. 2015. 
D.  Silver, A. Huang, C.  J. Maddison, A. Guez, L. Sifre, G. van den 
Driessche,  J.  Schrittwieser,  I.  Antonoglou,  V.  Panneershelvam,  M. 
Lanctot,  S.  Dieleman,  D.  Grewe,  J.  Nham,  N.  Kalchbrenner,  I. 
Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and 
D. Hassabis,  Mastering  the game of Go with deep neural networks 
and  tree  search,  Nature,  vol.  529,  no.  7587,  pp.  484489,  Jan. 
2016. 
M.  Bellemare,  S.  Srinivasan,  G.  Ostrovski,  T.  Schaul,  D.  Saxton, 
and  R.  Munos,  Unifying  count-based  exploration  and  intrinsic 
motivation,  in  Advances  in  Neural  Information  Processing 
Systems, 2016, pp. 14711479. 
T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, 
Hierarchical  deep  reinforcement  learning:  Integrating  temporal 
arXiv 
Prepr. 
abstraction 
and 
intrinsic  motivation, 
arXiv1604.06057, 2016. 
B.  M.  Lake,  T.  D.  Ullman,  J.  B.  Tenenbaum,  and  S.  J.  Gershman, 
Building  Machines  That  Learn  and  Think   Like  People,  Behav. 
Brain Sci., Nov. 2017. 
B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, Human -level 
concept  learning  through  probabilistic program  induction,  Science 
(80-. )., vol. 350, no. 6266, pp. 13321338, Dec. 2015. 

S. M. LaValle, Planning Algorithms. 2006. 
Z.  Chen  and  B.  Liu,  Lifelong  Machine  Learning');
INSERT INTO posts (postId,userId,title,body) VALUES (354,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 11)',' .  Morgan  '||'&'||' 
Claypool Publishers, 2016. 
J.-C.  Latombe,  Robot  motion  planning ,  vol.  124.  Springer  Science 
'||'&'||' Business Media, 2012. 
X.  Guo,  S.  Singh,  H.  Lee,  R.  L.  Lewis,  and  X.  Wang,   Deep 
learning  for  real-time  Atari  game  play  using  offline  Monte-Carlo 
in  Advances 
in  neural 
information 
tree  search  planning, 
processing systems, 2014, pp. 33383346. 
A.  Graves,  G.  Wayne,  M.  Reynolds,  T.  Harley,  I.  Danihelka,  A. 
Grabska-Barwiska,  S.  G.  Colmenarejo,  E.  Grefenstette,  T. 
Ramalho,  and  others,  Hybrid  computing  using  a  neural  network 
with dynamic external memory, Nature, pp. 14764687, 2016. 
A. Graves, G. Wayne, and I. Danihelka, Neural Turing Machines, 
Arxiv, vol. abs/1410.5, 2014. 
R.  Dawkins  and  J.  R.  Krebs,  Arms  races  between  and  within 
species.,  Proc.  R.  Soc.  Lond.  B.  Biol.  Sci. ,  vol.  205,  no.  1161,  pp. 
489511, 1979. 
I.  Goodfellow,  J.  Pouget-Abadie,  M.  Mirza,  B.  Xu,  D.  Warde-
Farley,  S.  Ozair,  A.  Courville,  and  Y.   Bengio,  Generative 
adversarial  nets,  in  Advances  in  neural  information  processing 
systems, 2014, pp. 26722680. 
X. D. Arsiwalla, I. Herreros, C. Moulin-Frier, M. Sanchez, and P. F. 
M.  J.  Verschure,  Is  Consciousness  a  Control  Process?,  in 
International  Conference  of  the  Catalan  Association  for  Artificial 
Intelligence, 2016, pp. 233238. 
C.  Moulin-Frier  and  P.  F.  M.  J.  Verschure,  Two  possible  driving 
the  evolution  of  animal  communication. 
forces  supporting 
Comment 
on 
Towards 
a  Computational  Comparative 
Neuroprimatology:  Framing  the  language-ready  brain  by  Michael 
A. Arbib, Phys. Life Rev., 2016. 
A. Newell, Unified  theories of  cognition . Harvard University Press, 
1990. 

[31] 
[32] 

[33] 

[34] 

[35] 

[36] 

[37] 

[38] 

[39] 

[40] 

[41] 

 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

[19] 

[20] 

[21] 

[22] 

[23] 

[24] 

[25] 

[26] 

[27] 

[28] 

[29] 

[30] 

A.  E.  Stahl  and  L.  Feigenson,  Observing  the  unexpected  enhances 
infants  learning  and  exploration,  Science  (80-.  ).,  vol.  348,  no. 
6230, pp. 9194, 2015. 
M.  Johnson,  K.  Hofmann,  T.  Hutton,  and  D.  Bignell,  The Malmo 
Platform  for  Artificial  Intelligence  Experimentat ion,  Int.  Jt.  Conf. 
Artif. Intell., pp. 42464247, 2016. 
C. Beattie,  J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. 
Kttler,  A.  Lefrancq,  S.  Green,  V.  Valds,  A.  Sadik,  and  others, 
DeepMind Lab, arXiv Prepr. arXiv1612.03801 , 2016. 
G. Brockman, V. Cheung, L. Pettersson,  J. Schneider, J. Schulman, 
J.  Tang,  and  W.  Zaremba,  OpenAI  Gym,  in  arXiv  preprint 
arXiv:1606.01540, 2016. 
J.  Z.  Leibo,  V.  Zambaldi,  M.  Lanctot,  J.  Marecki,  and  T.  Graepel, 
Multi-agent  Reinforcement  Learning 
in  Sequential  Social 
Dilemmas, Feb. 2017. 
A.  Tampuu,  T.  Matiisen,  D.  Kodelja,  I.  Kuzovkin,  K.  Korjus,  J. 
Aru,  J.  Aru,  and  R.  Vicente,  Multiagent  Cooperation  and 
Competition with Deep Reinforcement Learning, Nov. 2015.  
C.  Moulin-Frier,  X.  D.  Arsiwalla,  J.-Y.  Puigb,  M.  Snchez-Fibla, 
A.  Duff,  and  P.  F.  M.  J.  Verschure,  Top-Down  and  Bottom-Up 
Interactions  between  Low-Level  Reactive  Control  and  Symbolic 
Rule  Learning  in  Embodied  Agents,  in  Proceedings  of  the 
Workshop  on  Cognitive  Computation:  Integrat ing  neural  and 
symbolic  approaches.  30th  Annual  Conference  on  Neural 
Information Processing Systems (NIPS 2016) , 2016. 
C. Moulin-Frier, T. Fischer, M. Petit, G. Pointeau,  J.-Y. Puigbo, U. 
Pattacini,  S.  C.  Low,  D. Camilleri,  P. Nguyen, M. Hoffmann, H.  J. 
Chang,  M.  Zambelli,  A.-L.');
INSERT INTO posts (postId,userId,title,body) VALUES (355,4803,'Embodied Artificial Intelligence through  Distributed Adaptive Control: An Integrated  Framework  (part 12)','  Mealier,  A.  Damianou,  G.  Metta,  T. 
Prescott,  Y.  Demiris,  P.-F.  Dominey,  and  P.  Verschure,  DAC-h3: 
A  Proactive  Robot  Cognitive  Architecture  to  Acquire  and  Express 
Knowledge About  the World and the Self, Submitt. to IEEE Trans. 
Cogn. Dev. Syst., 2017. 
A.  Newell,  J.  C.  Shaw,  and  H.  A.  Simon,  Report  on  a  general 
problem-solving program, IFIP Congr., pp. 256264, 1959. 
J.  E.  Laird,  A.  Newell,  and  P.  S.  Rosenbloom,  SOAR:  An 
architecture  for  general  intelligence,  Artificial  Intelligence,  vol. 
33, no. 1. pp. 164, 1987. 
J.  R.  Anderson,  The  Architecture  of Cognition . Harvard University 
Press, 1983. 
R.  Brooks,  A  robust  layered  control  system  for  a  mobile  robot, 
IEEE J. Robot. Autom., vol. 2, no. 1, pp. 1423, 1986. 
R.  A.  Brooks,  Intelligence  without  representation,  Artif.  Intell., 
vol. 47, no. 13, pp. 139159, 1991. 
P.  F.  M.  J.  Verschure,  T.  Voegtlin,  and  R.  J.  Douglas, 
Environmentally  mediated  synergy  between  perception  and 
behaviour  in  mobile  robots,  Nature,  vol.  425,  no.  6958,  pp.  620
624, 2003. 
P.  F.  M.  J.  Verschure,  C.  M.  A.  Pennartz,  and  G.  Pezzulo,  The 
why, what, where, when  and  how  of  goal-directed  choice: neuronal 
and  computational  principles,  Philos.  Trans.  R.  Soc.  B  Biol.  Sci. , 
vol. 369, no. 1655, p. 20130483, 2014.  
G. Maffei,  D.  Santos-Pata,  E. Marcos, M.  Snchez-Fibla,  and  P.  F. 
M.  J.  Verschure,  An  embodied  biologically  constrained  model  of 
foraging:  From  classical  and  operant  conditioning  to  adaptive  real-
world behavior in DAC-X, Neural Networks, 2015. 
V.  Vouloutsi,  M.  Blancas,  R.  Zucca,  P.  Omedas,  D.  Reidsma,  D. 
Davison,  and  others,  Towards  a  Synthetic  Tutor  Assistant:  The 
EASEL  Project  and  its  Architecture,  in  International  Conference 
on Living Machines, 2016, pp. 353364. 
P.  F.  M.  J.  Verschure,  Synthetic  consciousness:  the  distributed 
adaptive  control  perspective,  Philos.  Trans. R. Soc. Lond. B. Biol. 
Sci., vol. 371, no. 1701, pp. 263275, 2016. 
G.  E.  Hinton  and  R.  R.  Salakhutdinov,  Reducing 
the 
Dimensionality  of  Data  with  Neural  Networks,  Science  (80-.  )., 
vol. 313, no. 5786, 2006. 
R.  S.  Sutton  and  A.  G.  Barto,  Reinforcement  Learning:  an 
introduction. The MIT Press, 1998. 
S.  Hochreiter  and  J.  Schmidhuber,  Long  Short -Term  Memory, 
Neural Comput., vol. 9, no. 8, pp. 17351780, Nov. 1997. 
A. Baranes  and P.-Y. Oudeyer, Active Learning of Inverse Models 
with  Intrinsically  Motivated  Goal  Exploration  in  Robots,  Rob. 
Auton. Syst., vol. 61, no. 1, pp. 4973, 2013. 

');
INSERT INTO posts (postId,userId,title,body) VALUES (356,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com    ','Abstract Biometric  authentication  systems  that  make  use  of 
signature  verification  methods  often  render  optimum 
performance  only  under 
limited  and 
restricted 
conditions.  Such  methods  utilize  several 
training 
samples  so  as  to  achieve  high  accuracy.  Moreover,  
several constraints are  imposed on  the end-user  so  that 
the  system  may  work  optimally,  and  as  expected.  For 
example,  the user  is made  to sign within a small box,  in 
order  to  limit  their  signature  to  a  predefined  set  of 
dimensions,  thus  eliminating  scaling.  Moreover,  the 
angular  rotation  with  respect 
to 
the  referenced 
signature  that  will  be  inadvertently  introduced  as 
human  error,  hampers  performance  of  biometric 
signature  verification  systems.  To  eliminate 
this, 
traditionally, a user  is asked  to sign  exactly on  top of a 
reference  line.  In  this  paper,  we  propose  a  robust 
system  that  optimizes  the  signature  obtained  from  the 
user  for a  large range of variation  in Rotation-Scaling-
Translation  (RST)  and  resolves  these  error  parameters 
in  the  user  signature  according  to  the  reference 
signature stored in the database. Keywords:  rotation;  scaling; 
translation;  RST; 
image registration; signature verification. 1.  Introduction  
The  aim  o f  a  biometric  verificat ion  system  is   to 
determine  if  a  person  is  who  he/she  purports  to  be, 
based  on  one  or  more  intrinsic,  physical  or  behaviora l 
attributes.  This  trait  or  biometric  attribute  can  be  the 
signature,  voice,  iris,  face,  fingerpr int,  hand  geometry 
etc.  
A  simple  biometric  system  has  a  sensor  module,  a 
feature  extraction  module,  a  matching  module  and  a 
decision  making  module.  The  sensor  module  acquires 
the  biometric  data  of  an  individual.  In  this  case,  the 
digital  pen  tablet  functions  as  the  sensor.  In  the  feature 
extraction  module,  the  acquired  biometric  data  is 
processed to extract a feature set that represents the data. 
For  example,  the  position  and  orientation  of  certain 
specific  points  in  a  signature  image  are  extracted  in  the 
feature  extraction  module  o f  a  signature  authentication 
system.  In  the  matching  module,  the  extracted  feature  
 
set  is  compared  against  that  of  the  template  by 
generating a matching score. In this module, the number  
of  matching  points  between  the  acquired  and  reference 
signatures  are  determined,  and  a  matching  score  is 
obtained.  Decision-making  involves  either  verification 
or  identification.  In  the  decision-making  module,  the 
user\'s  claimed  identity  is  either  accepted  or  rejected 
based  on 
the  matching  score, 
i.e.,  verification. 
Alternately,  the system may  identify a user based on  the 
matching scores, i.e., identification [1],[11]. 
Signature  recogni tion  is  one  of  the  oldest  biometric 
authentication  methods,  with  wide-spread 
legal 
acceptance.  Handwritten  signatures  are  commonly  used 
to  approbate  the  contents  of  a  document  or  to 
authenticate a  financial  transaction [1]. A  trivial method 
of  signature  verificat ion  is  visual  inspection.  A  manual 
comparison  of  the  two  signatures  is  done  and  the  given 
signature  is  accepted  if  it  is  sufficient ly  similar  to  the 
reference  signature,  for  example,  on  a  credit-card.   In 
most  scenarios,  where  a  signature  is  used  as  the  means 
of authentication, no verificat ion takes place at all due to 
the  entire  process  being  excessively  t ime  intensive');
INSERT INTO posts (postId,userId,title,body) VALUES (357,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com     (part 2)','  and 
demanding. An automated signature verification process 
will  help  improve  the  current  situation  and  thus, 
eliminate  fraud.  Well-known biometric methods  include 
iris,  retina,  face  and  fingerprint  based  identification  and 
verification.  Even  though  human  features  such  as  iris, 
retina and  fingerprints do not change over time and have 
low  intra-class  variation,  i.e.,  the  variations  in  the 
respective  biometric  attribute  are  low,  special  and 
relatively  expensive  hardware  is  needed  for  data 
acquisit ion  in  such  systems. An  important  advantage  of 
signatures 
as 
the  human 
trait 
for  biometric 
authentication over other attributes is their  long standing 
tradition  in  many  commonly  encountered  verification 
tasks.  In  other  words,  signature  verificat ion  is  already 
accepted  by  the  general  public.  In  addi tion,  it  is  also 
relatively  less  expensive  than  the  other  biometric 
methods [1],[2]. 
The  difficult ies  associated  with  signature  verification 
systems due to the extensive  intra-class variations, make 
signature  verification  a  difficult  pattern  recognit ion 
problem.  Examples  of  the  various  alterations  observed 
in  the  signature of  an  individua l have been  illustrated  in 
Fig.  1. 

Aman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1419ISSN:2229-6093 

 
Figure 1: Intra-class variations, i.e. variations 
in the signature of an individual 
Depending  on 
the  data  acquisit ion  method, 
automatic  signature verificat ion  can be  divided  into  two 
main  types:  off-line  and  on-line  signature  verification. 
The  most  accurate  systems  almost  always 
take 
advantage  of  dynamic 
features 
like  acceleration, 
velocity  and  the  difference  between  up  and  down 
strokes  [3].  This  class  of  so lut ions  is  called  on-line 
signature  verification.  However  in  the  most  common 
real-world  scenarios,  because  such  systems  require  the 
observation  and  recording  off  the  signing  process,  this 
information  is  not  readily  available.  This  is  the  main 
reason,  why  static  signature  analysis  is  still  in  focus  of 
many  researchers.  On-line  signature  verificat ion  uses 
special  hardware,  such  as  a  digit izing  tablet  or  a 
pressure  sensitive  pen,  to  record  the  pen  movements 
during  writ ing.  In  addition  to  shape,  the  dynamics  of 
writing  are  also  captured  in  on-line  signatures, which  is 
not   present  in  the  2-D  representation  of  the  signature 
and  hence  it  is  difficult  to  forge.  Off-line  methods   do 
not  require special acquisit ion hardware, just a pen and a 
paper,  and  are  therefore  less  invasive  and  more  user 
friendly.  In  the  past  decade  a  bunch  of  solutions   has 
been  introduced,  to  overcome  the  limitations  of  off-line  
signature  verification  and  to  compensate  for  the  loss  of 
accuracy  [2],[3].  In  off-line  signature  verification,  the 
signature is available on a document which is scanned to 
obtain  its  digital  image.  In  all  applications  where 
handwritten  signatures  currently  serve  as  means  o f 
authentication,  automatic  signature  verification  can  be 
used  such  as  cashing  a  check,  signing  a  credit  card 
transaction  or  authenticating  a 
legal  document. 
Basically,  any  system  that  uses  a  password  can  instead 
use  an  on-line  signature  for  access.  The  advantages  are 
such  systems  are  obvious   a  signature  is more difficult 
to  steal  or  guess  than  a  password  and  is  also  easier  to 
remember for the user.  
However,  the  high  level  of  intra-class  variations  in 
signatures,  as  shown  in  Fig.  1,  hinder  the  performance 
of  signature  verifica');
INSERT INTO posts (postId,userId,title,body) VALUES (358,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com     (part 3)','tion  systems  and  thus  minimize  the 

accuracy  of  such  systems.  Hence,  to  reduce  errors  and 
the inefficiency problems associated with these systems, 
the  intra-class  variat ions  in  the  signatures  need  to  be 
minimized.   This  involves  eliminating  or  reducing  the 
rotation,  scaling  and  translation  factors  between  the 
reference and the test signature images. Fig. 2 shows the 
diagram  o f  a  typical  signature  verification  system  with 
rotation, scaling and  translation (RST) cancellation. The 
reference  image within  the  database  and  the  user  image 
act  as  inputs  to  the  system.  Feature  extraction  is  done 
from  the  reference  signature  which  describes  certain 
characteristics of  the  signature  and  stored  as  a  template. 
For  verification,  the  same  features  are  extracted  from 
the test signature and compared to the template. 
 
Figure 2: A typical signature verification 
system with RST  cancellation 
It  should  be  noted  that  a  dist inct  advantage  of  the 
proposed  system,  illustrated  in  Fig. 2,  is  that  it  does not 
require mult iple signature reference samples  for training 
in  order  to  achieve  high  levels  of  accuracy.  Previous 
work  by  researchers  has  witnessed  the  use  of  affine  
transformation  for  calculating  the  angular  rotation 
between two images, the scaling and translation [4]-[7].  
In  this  paper,  we  propose  the  use  of  the  concept  of 
correlation to identify the rotation and a simple cropping 
method  to  eliminate  scaling  and  translation,  thereby 
creating  an  optimum  template  after  subjecting  the  user 
image to RST correction.  2. Idea of the proposed solution 
  

The  foremost  concern  is  fetching  the  angle  of 
rotation  between  the  user  and  the  reference  images.  In 
order  to  achieve  this,  the  concept   of  correlation  is 
deployed.  The 
term  correlat ion 
is  a  statistica l 
measure,  which  refers  to  a  process  for  establishing 
whether  or  not   relat ionships  exist  between 
two 
variables  [8] .  The  maximum  value   of  cross-correlation 
between  the  original,  i.e.,  the  reference  image  and  the 
user  image  is  found  by  means  of  repetit ive  iterations 
invo lving 
the  cross-correlation 
the  calculation  of 
between the two  images  in quest ion.   
The  proposed  algorithm  essent ially  finds  the  cross-
correlat ion  between  origina l  image  and  the  user  image.  
If  X(m,  n)  is  reference  image  and  Y(m,  n)  is  the  user 

Aman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1420ISSN:2229-6093mn



 
0

(1) 

image  then  the  cross-correlat ion  r  between  X  and  Y  is 
given by the fo llowing equat ion: (
) (rX
mn
m n
Minimum  value  o f  r  ind icates  dissimilarity  of 
images  and  for  the  same  image  (autocorrelat ion)  it will 
have  a  peak  value  so  as  to  indicate  maximum 
correlat ion. X0  and Y0  represent mean of  Image X  and 
Y respectively.  
We  use  normalized  cross-correlat ion  to  simplify 
analys is  and  comparisons  of  coefficient  values 
corresponding  to  the  respective  angular  values.  Min-
max  normalizat ion  is  the  procedure  used  to  obtain 
normalized 
[9].  Min-max 
cross-correlat ion 
normalizat ion  preserves  the  relat ionships  among  the 
original  data  values.  The  normalizat ion  operation 
transforms  the  data  into  a  new  range,  generally  [0,  1].  
Given  a  data  set  xi,  such  that  i  =  1,  2,  .  .  ,  n,  the 
normalized value x  is given by the fo llowing equation: x
min(
ix
) min(max(
i
The  second  aim  is  to  deal  with  the  translat ion 
associated  with  the  images.  This  is  achieved  by  a 
simple');
INSERT INTO posts (postId,userId,title,body) VALUES (359,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com     (part 4)','  cropping  technique.  Init ially,  we  calculate  the 
number  o f  rows  and  columns  bordering  the  signature 
pixels  within  the  image.  The  image  devo id  of  these 
rows  and  co lumns  is  extracted.  The  result  is  an  image 
consist ing  of  only  the  signature  pixels.  Add it iona l 
background surrounding the  image  is thus eliminated. 
Third  factor  is  the  scaling  between  the  two  images. 
For  calculat ion  of  the  scaling  factor,  the  cropped 
images  obtained  dur ing  translat ion  are  utilized.  The 
size  o f  the  reference  image  divided  by  the  size  of  the 
user  image gives the scaling ratio. 
The proposed solut ion  is  illustrated by Fig. 3. 

(2) 

\'

 
 

Figure 3: Schematic block diagram of the 
proposed system 
3. Implementation steps 3.1. Image acquisition and pre-processing  
Our  image   acqu isit ion  is  inherent ly  simple  and does 
not  employ  any  special  illuminat ion.  The  system 
implemented  here  uses  a  digital  pen  tablet,  namely,  
WACOM  Bamboo  [10],  as  the  data-capturing  device.  
The pen has  a  touch  sensit ive  switch  in  its  t ip  such  that 

only  pen-down  samples  (i.e., when  the  pen  touches  the 
paper)  are  recorded.  The  database  consists  of  a  set  of 
signature  samples  of  90  people.  For  each  person,  there 
are  9  test  images  and  1  training  or  reference  image  in 
the  database.  Upon  signature  acqu isit ion,  the  next   step 
is  co lour  normalizat ion  and  binarizat ion.  Colour 
normalizat ion  is  the  conversion  o f  the  image  from  the 
RGB  form  to  the  corresponding  Grayscale  image.  
Binarizat ion  is  the  conversion  of  this  grayscale  image 
to  an  image  cons ist ing  of  two  luminance  elements, 
namely,  black  and  white.  On  complet ion  o f  the  image 
acquisit ion  and  pre-processing  stage,  the  resultant 
image  thus  obtained,  becomes  ready  for  the  corrective 
phases: rotation, scaling and translat ion cancellat ion.  
 
 Figure 4: Wacom Bamboo Digital Pen Tablet 

  
3.2. Rotation correction  While  co llect ing  signature  samples,  it was observed 
that  users  gave  consecut ive  samples  having  angu lar  
variat ions  approximately  from  60  to  +60.  Hence, 
before  feature  extraction,  the  user  image  should  be 
aligned  with  the  reference  image.  For  simplicity  in 
computing  rotation  angle,  we  choose  to  align  the 
reference  image  with  the  trial  image  fetched  from  the 
user, i.e., the user  image.  
The  preprocessed  reference  image  is  cropped  in 
order  to  extract  only  the  signature  pixels  without  any 
addit ional  background  and  used 
for  all 
further 
computations.  In  order  to  make  the  program  time 
efficient  and  less  resource  intensive,  two  stages  of 
rotation  correction  are  applied.  The  first  stage  is 
designed  to  offer  a  relat ively  lower  reso lut ion  o f  5  so  
as  to  offer  an  approximate  value  of  the  angle  o f 
rotation.  In  contrast,  the  second  stage  is  designed  for  a 
comparat ively  higher  resolut ion. Within  a  range  of  +3 
to  3  of  the  approximate  value,  a  reso lut ion  of  1  is 
selected for a more precise value of the rotation angle.  
After  pre-processing,  the  user  image  is  then  rotated 
by  5  within  the  range  o f  60  to  +60  in  successive 
iterat ions.  Cross-correlat ion  values  between 
the 
reference  image  and  the  user  image  are  recorded  on 
complet ion  o f  each  iteration  of  the  rotation  process. 
The  maximum  cross-correlat ion  value  refers  to  the 
correct angle o f rotation within a 5 range, further, after 
the  approximate  angle  value  is  obtained,   +3  or  3  o f 

Aman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1421ISSN:2229-6093this  angle  can  be  inspected  for  maximum  correlation 
value  which  corresponds  ');
INSERT INTO posts (postId,userId,title,body) VALUES (360,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com     (part 5)','to  angle  of  rotation  accurate 
to up to 1. The user  image  is rotated by the negat ive o f 
the  angle  thus  obtained,  and  then  subjected  to  feature 
extraction. Thus, rotation cancellat ion is achieved.  
The steps  invo lved  in the rotation correction process 
can be summarized as fo llows: 
1)  Obtain user image and the reference image. 
2)  Carry  out  pre-processing  by  converting  both 
images 
performing 
and 
grayscale 
to 
normalization. 
3)  Trim  the  reference  signature  to  remove  any 
excess background; this will act as the template. 
4)  Starting  with  the angle as  60,  in  increments of  
5, 
record  normalized  correlation  values 
between pre-processed reference  image and user 
image. 
5) 
If angle is less than or equal to 60, go to step 4. 
6)  Maximum  correlation  value  corresponds 
to 
angle  within  a  5  range.  Let  this  angular  value 
be x. 
7)  Starting with  the angle as  (x  3),  in  increments 
of  1,  record  normalized  correlation  values 
between  the  preprocessed  reference  image  and  
the user image. 
If  angle  is  less  than  or  equal  to  (x  +  3),  go  to 
step 7. 
9)  Correct  the  user  image  by  the  obtained  angle 
and proceed for further correction, if required. 
Fig.   5 
shows  a 
reference 
image  and 
the 
corresponding  image rotated by 20.9. 
8) 
Figure 5: Reference image and the 
correponding rotated image 
 

3.3. Scaling correction  
An  end  user  o ften  modifies  his/her  signature 
according  to  the  size  of  signing  box.  For  smaller  
spaces,  the  signature  may  be  compressed,  for  no  space 
limitation,  the  sign  may  be  enlarged.  Thus,  before 
extraction  o f  feature  points,  it  is  essent ial  that  any 
scaling,  if present  in  the  test sample,  be removed. Upon 
trimming  both  images,  the  ratio  of  height  gives  Y 

scaling and ratio o f width gives X scaling. However,  to 
resize  the  user  image  and  make  it  the  same  size  as  the 
registered  image,  either  of  the  scaling  ratios  can  be 
used.  For  a  rotation  range  of  60  to  +60,  height  was 
observed  to  vary  significant ly  as  compared  to  the 
length.  Hence,  Y  scaling  was  chosen  as  the  scaling 
ratio.  To  account  for  scaling,  the  above  mentioned 
cropping technique  is applied to both the user as well as 
reference  image.  Scaling  ratio  is  calculated  by  the 
fo llowing equat ion: 
Scaling ratio =  Size of the reference image
Size of the test image

(3) 

The  user  image  is  resized  as  per  the  obtained 
scaling  rat io  and  then  sent  to  the  feature  extraction 
segment.  Fig.  6  shows  a  reference  image  and  the 
corresponding  image  down  scaled  by  a  scaling  ratio  of 
1.4045. 
 
Figure 6: Reference image and the 
corresponding down-scaled image  

 
3.4. Translation correction  
On the apparatus used for taking signature  input, the 
user  is  free  to  sign  without  using  any  fixed  starting 
po int.  This  may  introduce  translat ion  in  X  and/or  Y 
direct ion,  having  a  maximum  value  equal  to  the  width 
or  height  of  the  signature  canvas  respect ively.   The 
boundary  condit ions  for  translation  error  are  computed 
assuming that the user starts to sign from the edge.  
This  problem  is  overcome  by  cropping  the  pre-
processed  reference  image  so  as  to  extract  only  the 
signature  pixels  without  any  addit ional  background. 
This  cropping  process  truncates  the  extra  background 
region by  trimming  the  image  canvas. Thus,  translation 
is  removed  completely.  For  representational  purposes, 
bottom  left corner of test image  is assumed  to be or igin.   
The  number  of  co lumns  from  left  and  number  of 
rows  from  the  bottom,  which  contain  no  black  pixels 
corresponding  to  the  actual ');
INSERT INTO posts (postId,userId,title,body) VALUES (361,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com     (part 6)',' signature,  i.e.,  which 
consist solely o f  image-background, are counted. These 
values  give  X 
translat ion 
and  Y 
translation 
respectively.  Fig.  7  shows  a  reference  image  and  the 

Aman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1422ISSN:2229-6093corresponding  image  translated  by  35px  along  X-Axis 
and 9px along Y-Axis. 
 

 
Figure 7: Reference image and the 
corresponding translated image 

  
3.5. Combined RotationScalingTranslation 
It  is  easy  to  manipulate  the  samples  to  get  pure 
rotation,  translat ion  and  scaling,  however,  for  actual 
signatures,  all  the  above  ment ioned  factors  are  altered 
simultaneous ly. Hence,  rotation,  translat ion  and  scaling  
corrections are applied  in the same order.  
Rotation  correction  precedes  translat ion  correction 
as  the  assumed  origin  at  bottom  left  corner  also  gets 
rotated  and  translation  effects  cannot  be  eliminated 
unless  the origin  is returned  to bottom  left as accurately 
as  possible.  Therefore,  rotation  correction  needs  to  be 
performed  first  as  the  scaling  ratio  calculated  by  the 
pure  scaling  method  is  not  consistent with  scaling  ratio 
of the rotated image, as shown  in Fig. 8.  
Conseque nt ly,  
scaling 
of 
the 
effect iveness 
correction  depends,  to  a  large  extent, on  the percentage 
error obtained dur ing rotation correction.  
 

 

 

Figure 8: Change in the width and height of the 
image before and after rotation correction 
After correcting the angle of rotation, the user  image 
pre-processed  copy  is  cropped  to  eliminate  translation 
and  the  image  so  obtained  is  a  case  of  pure  scaling 
which has been discussed above.  
Thus,  rotationscalingtranslat ion  cancellat ion  is 
achieved. 4. Results  
4.1. Rotation 
Results  obtained  for  pur e  rotation  have  been 
tabulated as follows: 

Table 1: Results obtained for pure rotation  

Signature 
Samples 
Sample 1 
Sample 2 
Sample 3 
Sample 4 
Sample 5 
Sample 6 
Sample 7 
Sample 8 
Sample 9 
Sample 10  

Actual 
Angle 
-60 
-48 
-20 
-6 
0 
4 
13 
27 
37 
59 

Detected 
Angle 
-60 
-48 
-20 
-6 
0 
4 
13 
27 
37 
60 

% 
Error 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1.69 
rrE%

Figure 9: Plot of % error values in case of pure 
rotation for various samples 

4.2. Scaling Results  obtained  for  pure  scaling  have  been 
tabulated as follows: 

Table 2: Results obtained for pure scaling  

Signature 
Samples 
Sample 1 
Sample 2 
Sample 3 
Sample 4 

Actual 
Scaling Ratio 
7.69 
5 
4 
2.17 

Detected 
Scaling Ratio 
10.55 
5.70 
4.22 
2.27 

% 
Error 
37.2 
14 
5.5 
4.6 

Aman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1423ISSN:2229-6093Signature 
Samples 
Sample 5 
Sample 6 
Sample 7 
Sample 8 
Sample 9 
Sample 10  

Actual 
Scaling Ratio 
1.28 
1 
0.63 
0.54 
0.48 
0.31 

Detected 
Scaling Ratio 
1.34 
1.05 
0.66 
0.57 
0.49 
0.33 

% 
Error 
4.7 
5 
4.8 
5.6 
2.1 
6.5 
rrE%
rrE%

Figure 10: Plot of % error values in case of 
pure scaling for various samples 

4.3. Translation Results  obtained  for  pur e 
tabulated as follows: 

translat ion  have  been 

Table 3: Results obtained for pure translation  

Signature 
Samples 
Sample 1 
Sample 2 
Sample 3 
Sample 4 
Sample 5 
Sample 6 
Sample 7 
Sample 8 
Sample 9 
Sample 10  

Actual 
Translation 
0,5 
5,5 
10,0 
15,10 
0,25 
25,25 
25,50 
50,50 
50,100 
150,150 

Recovered 
Translation 
0,5 
5,5 
10,0 
15,10 
0,25 
25,25 
25,50 
50,50 
50,100 
150,150 

% 
Error 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 

Signature 
Samples 

Sample 1 
Sample 2 
Sample 3 
Sample 4 
Sample 5  
  

 

Figure 11: Plot of % error values in case of 
pure translation 
4.4. Rotation-Scaling-Translation Results  obtained  upon  combining  rotation,  scaling 
and translat ion  have been tabulated as fo llows: 

Table 4: Results obtained on com');
INSERT INTO posts (postId,userId,title,body) VALUES (362,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com     (part 7)','bining 
rotation, scaling and translation 
Detected 
Actual 
Parameters 
Parameters 
Rotation  Scaling  Rotation  Scaling 
1.90 
52 
1.67 
50 
1.39 
10 
1.33 
12 
1.25 
34 
1.11 
31 
-40 
0.91 
-42 
0.94 
0.82 
-32 
0.8 
-30 

 
 

 

Figure 12: User image before RST  correction 
(top); reference image (bottom-left) and the 
user image after RST  Correction (bottom-right) 

Aman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1424ISSN:2229-6093 
[5]  G.  Wo lberg  and  S.  Zokai,  Robust 
Image 
Registration Using Log-Polar Transform, Proceedings 
of   the  IEEE  International  Conference  on  Image 
Processing, Sep. 2000 pp. 1-2. [6]  Z.  Y.   Cohen,  Image  registration  and  object 
recognit ion  using  affine  invariants  and  convex  hulls,  
IEEE  Transactions  on  Image  Processing,  July  1999, 
pp. 1-3. [7]  N.  Chumchob  and  K.  Chen,  A  Robust  Affine 
Image  Registration  Method,  International  Journal  Of 
Numerical  Analysis  And Modeling, Volume  6, Number  
2, pp. 311-334. [8]  R. 
J.  Rummel,  Understanding  Correlat ion, 
Honolulu:   Depa rtment  of  Po lit ical  Science,  University 
of Hawaii, 1976. [9]  J.  Han,  M.  Kamber,  Data  mining:  concepts  and 
techniques, Morgan Kaufmann, 2006, pp. 70-72. [10]  WACOM  Bamboo  Digital 
www.wacom.co.in/bamboo, June 2011. [11]  Henk  C.  A.  van  T ilborg,  Encyclopedia  of 
cryptography and security, Springer, 2005, pp. 34-36. 

Tablet, 

Pen 

5. Conclusion 
The  system  has  been  designed  to  correct  variat ions 
in  angle  of  rotation,  in  the  range  o f  60  to  +60.  
However,  it  can  be  extended  to  cover  the  ent ire  360 
planar rotation, to design a fool proof system capable o f 
creating  an  optimum  template  after  RST  cancellation,  
even  in  a  situat ion where  the  input   pad  may  have  been 
inverted. On similar  lines,  the reso lut ion of rotation can 
be  improved  from  1  to  0.5  or  0.25  or  even  more, 
with  the  trade-off  being  increased  program  execution 
times.  
Pure  scaling  and  pure  translat ion  can  be  detected 
accurately  as  long  as signature pixels do  not go beyond 
the  defining  boundaries  o f  the  template.  For  the 
signatures  used,  a  maximum  translation  o f  200  pixels 
was  detected  along  X  and  Y  axes.  Maximum  scaling  
ratio  was  found  to  be  0.55.  However,  maximum 
variance  of  both  translat ion  and  scaling   may  show 
slight variations from one signature to another. 
For  combined  RST,  it was  experimentally  observed 
that  the  correlat ion  approach  tends  to  be  less  reliable 
with  significant  increase  or  decrease  in  the  scaling 
ratio.  Signature  images  used  for  testing  gave  optimum 
result  for  scaling  ratio,  i.e.,  within  0.67  to  1.33, 
however,  the  scaling  range giving  angle  and  translation 
accurately  may  increase  or  decrease  depending  on  the 
signature sample under test. 
Thus,  an  optimum  template  was  generated  by  the 
proposed system after subject ing the user  image to RST 
correction with respect to the reference  image.  6. References  [1]  A.  K.  Jain,  F.  D.  Griess  and  S.  D.  Connell,  On-
verificat ion,  Elsevier, 
line 
Pattern 
signature 
Recognit ion 35, 2002, pp. 2963-2972. [2]  M.  Mani  Ro ja  and  S.  Sawarkar,  A  Hybr id 
Approach  using  Majority  Voting 
for  Signature 
Recognit ion,  International  Conference  on  Electronics 
Computer Technology (ICECT) 2011, pp. 1-3. [3]  B.  Kovari,  I.  Albert  and  H.  Charaf,  A  Genera l 
Representation  for  Modeling  and  Benchmarking  Off-
line  Signature  Verifiers, ');
INSERT INTO posts (postId,userId,title,body) VALUES (363,205,'Thadomal Shahani Engineering College, Mumbai, India   aman.x64@gmail.com     (part 8)',' BME  Publicat ions,   12th 
WSEAS  International  Conference  on  Computers,  2008, 
p. 1 [4]  M.  Holia  and  V.  Thakar,  Image  registration  fo r 
recovering  affine  transformat ion  using  Nelder  Mead 
International 
Simplex  method 
for  optimization, 
Journal of Image Processing (IJIP), Volume 3, Issue 5, 
2009. pp. 218-221. 

Aman Chadha et al, Int. J. Comp. Tech. Appl., Vol 2 (5), 1419-1425IJCTA | SEPT-OCT 2011  Available online@www.ijcta.com1425ISSN:2229-6093');
INSERT INTO posts (postId,userId,title,body) VALUES (364,1285,'Secrecy-Energy Efciency Performance of','UAV-Enabled Communication Networks

Xiaohui Qi, Bin Li, Zheng Chu, Kaizhi Huang, and Hongbin Chen

Abstract

Recent researches show that unmanned aerial vehicle (UAV) can offer an efcient solution to achieve

wireless connectivity with high mobility and low cost. This letter investigates the secrecy energy efciency

(SEE) in a multi-tier UAV-enabled communication network via a threshold-based access scheme and

multi-antenna technique, where the UAV-enabled transmitters, legitimate receivers and eavesdroppers are

deployed randomly. In particular, we rst exploit the association probability of a randomly located receiver

and the activation probability of UAV-enabled transmitters. Then, we analyze the security, reliability, and

SEE of the UAV-enabled networks. Simulation results are provided to show the effect of the predetermined

access threshold on the reliability as well as security performance, and determine the optimal design

parameters for a given UAV-enabled network to maximize the SEE.

I . INTRODUCT ION

In recent years, unmanned aerial vehicle (UAV) is emerging as a novel paradigm in civil and military

applications, such as trafc monitoring, disaster rescue, and military reconnaissance [1]. In contrast to

a terrestrial transmitter, UAV, as a mobile transmitter, can provide a promising solution to complement

the capacity and coverage of terrestrial cellular systems, especially in extreme environments without

infrastructure [2]. On the other hand, information security is a critical issue facing national defense when

X. Qi and K. Huang are with the National Digital Switching System Engineering '||'&'||' Technological Research Center, Zhengzhou

450002, China (e-mails: seven66226067@163.com; huangkaizhi@tsinghua.org.cn).

B. Li is with the School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China (e-mail:

libin sun@bit.edu.cn).

Z. Chu is with the School of Science and Technology, Middlesex University, London, NW4 4BT, U.K. (e-mail:

z.chu@mdx.ac.uk).

H. Chen is with the Key Laboratory of Cognitive Radio and Information Processing, Guilin University of Electronic

Technology, Ministry of Education, 1 Jinji Road, Guilin 541004, China (e-mail: chbscut@guet.edu.cn).

April 7, 2017

DRAFT

2

people rely heavily on wireless network for transmitting private information [3], [4]. Toward this, the

use of UAV can offer new opportunities for security enhancement via a cooperative air-ground network.

However, the performance and operation of a UAV-enabled communication network is constrained by

the limited onboard energy. Therefore, the joint performance analysis of security and energy efciency

for UAV-enabled networks is urgently needed and is the emphasis of this work.

An overview of UAV-enable wireless communications was provided in [2], where the basic network-

ing architecture and main channel characteristics were portrayed, and the key design challenges were

discussed. Owing to high mobility, UAVs could also be deployed as mobile relays to provide wireless

connectivity between distant ground terminals whose direct links were severely blocked [5]. The authors

of [6] proposed an algorithm to allocate the time to different ground receivers based on the ying UAVs

position to maximize the minimum throughput. [7] modeled the locations of the UAV base stations

in a nite area as a uniform binomial point process and derived exact expression for the coverage

probability of a target receiver situated on the ground. The aforementioned works addressed the basic

networking architecture and optimization problem of the throughput. However, the information security

against eavesdropping attacks was not taken into account.

A very recent effort [8] considered physical layer security in a UAV-enabled ');
INSERT INTO posts (postId,userId,title,body) VALUES (365,1285,'Secrecy-Energy Efciency Performance of (part 2)','mobile relaying system

where the air-to-ground link was established. Note that the authors of [8] focused on the optimization

of transmit power, but not from the perspective of network analysis and deployment. They considered

neither the multi-UAV multi-eavesdropper wiretap scenarios, nor the random spatial positions of network

nodes. To the best of our knowledge, such work has not tried to design and analyze the secrecy energy

efciency (SEE) performance in UAV-enable communication networks, which motivates this work.

In this letter, we focus on the SEE in downlink UAV-enabled communication networks. Main contri-

butions of this letter are summarized as follows. 1) Modeling multi-antenna UAV-enabled transmitters,

receivers, and eavesdroppers as independent homogeneous Poisson point processes (HPPPs). By using the

threshold-based access scheme, a fundamental analysis framework for evaluating the SEE performance in

UAV-enabled communication networks is proposed; 2) The inuences on connection outage probability

(COP) and secrecy outage probability (SOP), caused by the predetermined access threshold and the

number of receivers served by each transmitter, are further analyzed in this scenario.
Notations: Boldface lowercase letter denotes vector. () , k  k, P{}, and E() denote the conjugate
transpose, Euclidean norm, probability, and expectation operation. (a, b) is the Gamma distribution with

shape parameter a and scale parameter b.

April 7, 2017

DRAFT

3

Fig. 1: A simplied system model for a 2-tier UAV-enabled communication network.

A. Network Descriptions

I I . DOWNL INK SY STEM MODEL

We consider a wireless system consisting of K -tier UAV-enabled transmitters, multiple legitimate

receivers, and multiple eavesdroppers, as shown in Fig. 1. Note that the UAVs of the kth tier are assumed
to be at the same height Hk for simplicity of exposition. Let  , {1, 2, . . . , K } denote the set of UAVs. In
the kth tier, each UAV-enabled transmitter equipped with Mk antennas can collect and transmit information

to the ground receivers. The number of receivers served in each transmitters resource block is k , and the

transmit power is Pk . The legitimate receivers and eavesdroppers are equipped with a single antenna. We
denote the set of UAVs in the kth tier, legitimate receivers, and eavesdroppers locations as k , u , and

E , which follow independent HPPPs with densities k , u , and E , respectively. According to Slivnyaks

theorem [9], the analysis can be performed at a typical legitimate receiver located at the origin. Compared

with interference, noise almost has no effect for legitimate receivers in multi-tier wireless networks [10].

Hence, we assume that the noises received by legitimate receivers and eavesdroppers are neglected.

In this letter, the system model has other three restraints:

 All the channels undergo independent and identically distributed quasi-static Rayleigh fading;

 Perfect channel state information (CSI) is available at each UAV-enabled transmitter;
 All the transmitters use precoding w = h/khk, where h is the corresponding channel.
In UAV-enabled communication networks, the received signal-interference-plus-noise ratio (SINR) of
the typical receiver o served by the UAV-enabled transmitter xk  k in the kth tier is given by
Pk hxk ,oklxk ,ok
SINRk
u =
I inter
o,k

(1)

April 7, 2017

DRAFT

4

where Pk hxk ,oklxk ,ok denotes the received power of the kth receiver, lxk ,o = qH 2
k + r2
xk ,o denotes the
distance between the serving transmitter xk and o, rxk ,o denotes the distance between xk and');
INSERT INTO posts (postId,userId,title,body) VALUES (366,1285,'Secrecy-Energy Efciency Performance of (part 3)',' o (the center
of the plane of k ), hxk ,o  (k , 1) stands for the array gain of the main channel, k = Mk  k + 1,
Pi go,iklyi ,ok represents receivers received
and klxk ,ok is the path loss [11]. I inter
o,k = Pi Pyio
i \xk
interference from all the active transmitters, where lyi ,o = qH 2
i + r2
yi ,o denotes the distance between the
transmitter yi and o, ryi ,o denotes the distance between yi and o , and go,i  (i , 1) is the array gain
of corresponding interference channel. The set of active transmitters in the ith tier is a thinning of i ,
act i , where P i
denoted by o
i with density o
i = P i
act denotes the activation probability of transmitters in

the ith tier.

We consider the non-colluding and passive eavesdropping scenario that each eavesdropper intercepts

the information signal of typical receiver independently without any attacks. In this case, we only pay

our attention to the eavesdropper that has the largest received SINR, which was commonly assumed [11].

Such an eavesdropper e is considered as the most malicious one and its received SINR can be expressed

as

xeE ( Pk hxe ,k klxe ,k k
xe ,k )
SINRk
e = max
I intra
xe ,k + I inter
where hxe ,k  exp(1) denotes the equivalent small-scale fading channel power gain for the received
SINR of eavesdropper xe  E , lxe ,k = qH 2
k + r2
xe ,k indicates the distance between the eavesdropper
xe and its target transmitter xk , and rxe ,k is the eavesdroppers horizontal distance from xk . I intra
xe ,k =
Pi gxe ,yi klxe ,yi k with gxe ,yi 
Pk gxe ,k klxe ,k k with gxe ,k   (k  1, 1) and I inter
xe ,k = Pi Pyio
i \xk
 (i , 1) are intra-cell and inter-cell interference, respectively [11]. lxe ,yi = qH 2
i + r2
is the distance
xe ,yi
between the eavesdropper xe and transmitter yi , and rxe ,yi is the eavesdroppers horizontal distance from
yi .

(2)

B. Secrecy mobile association scheme

In this subsection, we assume open access, i.e., a legitimate receiver is permitted to access any tiers

UAV-enabled transmitters. In addition, we consider a mobile association based on highest average received

signal power (ARSP), where a legitimate receiver is only allowed to associate with the UAV-enabled

transmitter providing the highest ARSP. For a legitimate receivers o, the ARSP related to the kth tier is
dened as Pk = Pkk l
xk ,o .

Following the idea of [12], the secure mobile association scheme is designed for improving the

security/reliability of downlink transmission in UAV-enabled communication networks. For the secure

April 7, 2017

DRAFT

mobile association scheme, the served transmitter broadcasts data only when the truncated ARSP at

receiver is larger than a predetermined access threshold  , i.e. lxk ,o  Rk = (Pkk / )
denotes the radius of the serving region. The following lemma provides the association probability.
 , where Rk

Lemma 1: The probability with which a typical legitimate receiver o associates with a transmitter in

(a)
the kth tier is given as
R2
k H 2
k +x2 )/2 > 1 |rxk )frxk
P ( Pkk (cid:16)H 2
xj (cid:17)/2
j +r2R0
Qj\k
Sk =
Pj j (H 2
exp" Pj
j (cid:17)#
exp" Pj\k
j (cid:');
INSERT INTO posts (postId,userId,title,body) VALUES (367,1285,'Secrecy-Energy Efciency Performance of (part 4)','16)( Pj,k j,k )2/
j (cid:16)( Pj,k j,k )2/
H 2
k H 2
j,k ( Pj,k j,k )2/
j,k ( Pj,k j,k )2/
Pj
Pj
where the step (a) can be easily recognized by the probability generating functional of HPPP [12],
, rxi is the receivers horizontal distance from transmitter xi , Pi,j = Pi /Pj ,
(x) = 2k xek x2
frxk
i,j = i/j , and i,j = i/j .
It is worth noting that a transmitter in the kth tier may be active when existing an associated receiver,

j (cid:17)#
kH 2
R2

(x) dx



(3)

and the activation probability of transmitter Bk is dened as [12]

P k
act = P (Bk associates with at least one receiver)
P (xu is not associated with Bk )#.
= 1  Eu " Qxuu
From (1) and (2), we know that the derivation for activation probability of transmitter is necessary, which

(4)

is given in Lemma 2.

Lemma 2: The activation probability of UAV-enabled transmitters in the kth tier is given by
act = 1  Eu " Qxuu
xj ,xu (cid:27)#
P (cid:26) Pkk
P k
Pj j l
< maxj
xu ,Bk
j (cid:17)xdx#
= 1  exp "2u
j (cid:16)( Pj,k j,k )2/
Rk
x2H 2
 Pj
(b)
RHkj (R2
j )
j (H 2
= 1  exp " e
# ,
j,k )2/
H 2
k ( Pj,k
k ( Pj,k
 P
 P
jK
jK
j ( Pj,k j,k )2/1
 Pj
 Pj
where the step (b) is derived following the basic nature of PPP, lxu ,Bk is the distance between xu and

j ( Pj,k j,k )2/

 e

j,k )2/

j )
H 2

(5)

Bk , and lxj ,xu is the distance between xu and xj .1

1For convenience, we interchangeably use exp(x) and e

x to denote the exponential function of x.

April 7, 2017

DRAFT

6

I I I . PER FORMANCE ANALY S IS

In this section, we analyze the SEE in UAV-enabled communication networks. In an effort to assess

the SEE, we rst derive the COP and the SOP in UAV-enabled networks.

When the legitimate receivers message cannot be decoded with error-free, the connection outage

occurs. The expression of COP in the kth tier is given in Theorem 1.

Theorem 1: When the typical receiver is associated with a UAV-enabled transmitter in the kth tier, its

COP can be expressed as

cop (k ) = P (cid:0)SINRk
(2k  1) ,
P k
u < 2k  1(cid:1) = FSINRkwhere k is the target channel capacity. The cumulative distribution function (CDF) of SINRk
u can be

(6)

given by

2/
Cj,k ( Pj,k )

Pk (cid:17)n
n! (cid:16) 
Sk Pk1
( ) = 1  kP mM (n) C ( m)F ( m) (P ml + 1)
FSINRk
n=0(cid:19)
(cid:18)eQk ( )R2
2(P mli)
2(P mli)
eQk ( )H 2
k R
k Hk
PP mli=0
j,k )2/
H 2
j ( Pj,k
 P[Qk ( )]i+1(P ml+1i)en
jmj = n}, uj,k = (cid:16)1 +  /( j,k Bj,k )(cid:17)1
where M (n) = { m = (m1 , m2 ,    , mn )T :
Pj=1
, C ( m) =
j !ml
Ql=1   Pjn

Dj (l)P
j (cid:16) Pj,k j,k(cid:17)2/
n!+ Pj
Qj (mj !(j !)mj ) , Qk ( ) = Pj
, F ( m) =k ) 2
 P ml+n
(2 ) P ml
(P 1
Pm=1 
m j
Cj,k = 2
 , uj,k (cid:1), Bi,j = Bi /Bj , Bi = pi/i , B  (a, b, z ) =
 , m  2
 B  (cid:0)j  m + 2


B  (j + 2
 ,uj,k )
 ,l 2R 1
z ta1 (1  t)b1 dt is the Beta function, and Dj (l) =.
((j +l1)!)1 (j 1)!

Proof : The CDF of SINRk
u can be obtained as
R2
kH 2
P (cid:18)hxk ,o >
Pkklxk ,ok |rxk ,o (cid:19) frxk ,o (x) dx I inter
R0
( ) = 1 
FSINRk
o,kRk
k1
dnLI inter
(s)
2yk (n!)1 (s)nS1
RHk
Pn=0
dy ,
o,kdsn
j,k )2/ (y2 H 2
k )
j ( Pj,k
 Pi (cid:16)1  (1 + sPiz )1(cid:17) zdz(cid:21) is obtained by the basic
exp (cid:20)R 
2o
(s) = Qi( Pi,k i,k )
 y
j ( Pj,k j,k )2/
x2
dn');
INSERT INTO posts (postId,userId,title,body) VALUES (368,1285,'Secrecy-Energy Efciency Performance of (part 5)','LI inter
 Pj
(s)
, s = P 1
nature of PPP, frxk ,o (x) = 2k x
k y , ando,k
is derived in
dsn
Sk
[12]. Substituting (8) into (6), we complete the proof.
Mathematically, the COP of a typical receiver can be given by Pcop = Pk

Sk P k
cop (k ).

where LI inter
o,k

= 1 

(7)

(8)

April 7, 2017

DRAFT

As such, when the eavesdroppers have a better channel than the access threshold, the secrecy outage

occurs to ensure the secrecy of those messages. As an important indicator of security, the expression of

SOP in the kth tier is given in Theorem 2.

Theorem 2. When the typical receiver is associated with a UAV-enabled transmitter in the kth tier, its

(9)

FSINRk
SOP can be given by
s  1(cid:17) ,
e (cid:16)2k  Rk
s (cid:17) = P (cid:16)log2 (cid:0)1 + SINRk
s (cid:17) = 1  FSINRk
sop (cid:16) Rk
e (cid:1) > k  Rk
P k
s (cid:17) and the CDF of SINRk
sop (cid:16) Rk
where Rk
s is the target secrecy rate of P k
e can be expressed as
i (cid:18)1(cid:16)1+ Pi y
Pk z (cid:17)i (cid:19)zdz
( ) = exp "2E ( + 1)(k1)
ydy#

 Pi R 
2o
Hi
RHkProof : The CDF of SINRk
e can be derived as follows:
Pk klxe ,k k (cid:19)#
( ) = EE " QxeE
P (cid:18)hxe ,k 
xe ,k )
 (I intra
xe ,k +I inter
FSINRk(c)
(s) ydy(cid:17) ,
= exp (cid:16)2E R 
(s) LI inter
LI intra
Hk
xe ,k
xe ,k
where the step (c) is achieved by the probability generating functional of HPPP E [12], the Laplace
i R 
Hi (1(1+sPi z )i )zdz
2o
 Pi
xe ,k and I intra
transform of I inter
xe ,k are given by LI inter
(s) = e
and LI intra
xe ,k
xe ,k
( + 1)(k 1) , respectively. Substituting (11) into (9), we can arrive at the nal result.
s (cid:17).
sop (cid:16) Rk
SkP k
Mathematically, the SOP of a typical receiver can be given by Psop = Pk
Due to the requirement of secure communication and the limitation of energy, SEE as an important
metric is used to evaluate the secrecy performance achieved with unit energy consumption. Similar to

(s) =

(11)

(10)

[13], SEE is dened as the ratio of the average secrecy rate at which the condential messages are

reliably and securely transmitted from the UAV-enabled transmitters to the intended receivers over the

total power consumption (bits/Joule). The following theorem provides the SEE achieved by the UAV-

enabled communication network.

where

Sk SEEk ,

Theorem 3. The SEE of the UAV-enabled communication networks is given by
SEE = Xk
sop (cid:16) Rs,k(cid:17)(cid:17) Rs,k
cop (k )(cid:1) (cid:16)1  P k
k (cid:0)1  P k
P totaldenotes the SEE for the kth tier. The total power consumption for a UAV-enabled transmitter in eachk (cid:16) k
Pt=1 (cid:16)t1
t + Mkt(cid:17)(cid:17) [13]. P 0
k + Pk
= P 0
k and k represent the static
channel is given by P total
SEEk =

(12)

(13)

April 7, 2017

DRAFT

8

k=4,k=1(T)
k=4,k=2(T)
k=4,k=4(T)
k=4,k=1(S)
k=4,k=2(S)
k=4,k=4(S)

0.8

0.7

0.6

0.5
O
0.4

0.3

0.2

0.1

k =4,k=1(T)
k =4,k=2(T)
k =4,k=4(T)
k =4,k=1(S)
k =4,k=2(S)
k =4,k=4(S)

0.35

0.3

0.25

0.2
O
0.15

0.1

0.05

 

-70

-60
-50
 (dBm)

-40

 

-70

-60
-50
 (dBm)

-40

(a) COP versus 

(b) SOP versus 

Fig. 2: The COP and SOP versus  with P1 = 15dBm and P2 = 5dBm. S denotes the simulation results

and T denotes the theoretical results.

hardware power consumption and the efciency of the power amplier for the kth tier, respectively. The
parameters k
t and t depends on the transceiver chains, coding and decoding, etc.

IV. NUMER ICAL RE SULT S

In this section, numerical results are provided to examine the COP and SOP for');
INSERT INTO posts (postId,userId,title,body) VALUES (369,1285,'Secrecy-Energy Efciency Performance of (part 6)',' a 2-tier UAV-enabled

networks. In addition, the impact of  and P1 on the SEE are also investigated. The validity of the

theoretical derivations are veried by the Monte Carlo simulation results. In the following results, we
assume  = 4, 1 = 1.5  105m2 , 2 = 3  105m2 , u = 6  106m2 , E = 2  105m2 , H1 = 13m,
2 = 0, k
1 = 4.8, k
H2 = 5m, 1 = 2 = 0.38, k
3 = 2.08k  108 , 1 = 1, 2 = 9.5  108 ,
1 = 4W, and P 0
3 = 6.25  108 , P 0
2 = 13.6W. All the simulation results shown in this section are
averaged over 100,000 Monte Carlo simulations.

Fig. 2 compares the SOP and COP in UAV-enabled networks with different k (k  {1, 2}). Intuitively,
the simulation results highly consistent with the theoretical results, which validates the accuracy of those

two analytical expressions derived. It is also observed that the COP increases with k and the SOP

decreases with k , which is mainly due to the fact that k not only increases the interference received

by eavesdroppers, but also increases the interference received by legitimate receivers. Furthermore, the

April 7, 2017

DRAFT

9
euJsi(E
0.01

0.008

0.006

0.004

0.002
-90

X: 21
Y: 66
Z: 0.009853

-80

-70

-60

 (dBm)

5

10

-50

-40

15

20

-30

25

P1 (dBm)

Fig. 3: SEE versus  and P1 , with 1 = 2 = 1, 1 = 4, 2 = 2 and P2 = 5dBm.

COP and SOP performances over different  are also shown in Fig. 2. Obviously, both SOP and COP

degrades with increasing  . This implies that the predetermined access threshold can affect both security

and reliability. This is due to both the association probability and the activation probability of UAV-enabled

transmitter degrades with increasing  .

Fig. 3 shows the inuences on SEE caused by  and P1 . From (12), we note that the SEE is not a

monotonous function of  and P1 . Consequently, the optimal value of SEE can be obtained by properly

designing  and P1 . In Fig. 3, the SEE reveals a maximum value for a given network with the optimal
pair of ( , P1 ) = (66, 21), which is marked in the gure.

V. CONCLU S ION

In this letter, we studied the SEE of downlink UAV-enabled networks, where the locations of network

nodes were characterized by independent HPPPs. To ensure the reliability and security of UAV-enabled

networks, both the threshold-based access scheme and multi-antenna technology were employed. The

security, reliability, and SEE of UAV-enabled networks was analyzed. Simulation results have revealed

that the reliability and security of UAV-enabled networks can be improved by using the threshold-based

access scheme, and the optimal value of SEE can be achieved by designing the transmit powers and

predetermined access threshold.

[1] Y. Zhou, N. Cheng, N. Lu, and X. S. Shen, Multi-UAV-aided networks: Aerial-ground cooperative vehicular networking

architecture, IEEE Veh. Technol. Mag., vol. 10, no. 4, pp. 36-44, Dec. 2015.

RE FERENCE S

April 7, 2017

DRAFT

10

[2] Y. Zeng, R. Zhang, and T. J. Lim, Wireless communications with unmanned aerial vehicles: Opportun');
INSERT INTO posts (postId,userId,title,body) VALUES (370,1285,'Secrecy-Energy Efciency Performance of (part 7)','ities and challenges,

IEEE Commun. Mag., vol. 54, no. 5, pp. 36-42, May 2016.

[3] Z. Chu, H. Xing, M. Johnston, and S. L. Goff, Secrecy rate optimizations for a MISO secrecy channel with multiple

multiantenna eavesdroppers, IEEE Trans. Wireless Commun., vol. 15, no. 1, pp. 283-297, Jan. 2016.

[4] B. Li, Z. Fei, and H. Chen, Robust articial noise-aided secure beamforming in wireless-powered non-regenerative relay

networks, IEEE Access, vol. 4, pp. 7921-7929, Nov. 2016.

[5] J. Lyu, Y. Zeng, R. Zhang, and T. J. Lim, Placement optimization of uav-mounted mobile base stations, IEEE Commun.

Lett., vol. 21, no. 3, pp. 604-607, Mar. 2017.

[6] J. Lyu, Y. Zeng, and R. Zhang, Cyclical multiple access in uav-aided communications: A throughput-delay tradeoff, IEEE

Wireless Commun. Lett., vol. 5, no. 6, pp. 600-603, Dec. 2016.

[7] V. V. C. Ravi and H. S. Dhillon, Downlink coverage probability in a nite network of unmanned aerial vehicle (UAV) base

stations, in Proc. IEEE SPAWC, Edinburgh, UK, Jul. 2016, pp. 1-5.

[8] Q. Wang, Z. Chen, W. Mei, and J. Fang, Improving physical layer security using UAV-enabled mobile relaying, IEEE

Wireless Commun. Lett., published online.

[9] D. B. Taylor, H. S. Dhillon, T. D. Novlan, and J. G. Andrews, Pairwise interaction processes for modeling cellular network

topology, in Proc. IEEE GLOBECOM, Anaheim, USA, Dec. 2012, pp. 4524-4529.

[10] H. S. Dhillon, R. K. Ganti, F. Baccelli, and J. G. Andrews, Modeling and analysis of K-tier downlink heterogeneous

cellular networks, IEEE J. Sel. Areas Commun., vol. 30, no. 3, pp. 550-560, Apr. 2012.

[11] Y. Deng, L. Wang, K.-K. Wong, A. Nallanathan, M. Elkashlan, and S. Lambotharan, Safeguarding massive MIMO aided

hetnets using physical layer security, in Proc. IEEE WCSP, Nanjing, China, Oct. 2015, pp. 1-5.

[12] H.-M. Wang, T.-X. Zheng, J. Y. D. Towsley, and M. H. Lee, Physical layer security in heterogeneous cellular networks,

IEEE Trans. Commun., vol. 64, no. 3, pp. 1204-1219, Mar. 2016.

[13] J. Ouyang, M. Lin, Y. Zou, W.-P. Zhu, and D. Massicotte, Secrecy energy efciency maximization in cognitive radio

networks, IEEE Access, vol. 5, pp. 2641-2650, Mar. 2017.

April 7, 2017

DRAFT

');
INSERT INTO posts (postId,userId,title,body) VALUES (371,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System','Raluca Necula
Faculty of Computer Science
Alexandru Ioan Cuza University
Iasi, Romania
Email: raluca.necula@info.uaic.ro

Mihaela Breaban
Faculty of Computer Science
Alexandru Ioan Cuza University
Iasi, Romania
Email: pmihaela@info.uaic.ro

Madalina Raschip
Faculty of Computer Science
Alexandru Ioan Cuza University
Iasi, Romania
Email: mionita@info.uaic.ro
12rA6 E.c 198047:ir
AbstractThe Dynamic Vehicle Routing Problem with Time
Windows (DVRPTW) is an extension of the well-known Vehicle
Routing Problem (VRP), which takes into account the dynamic
nature of the problem. This aspect requires the vehicle routes
to be updated in an ongoing manner as new customer requests
arrive in the system and must be incorporated into an evolving
schedule during the working day. Besides the vehicle capacity
constraint involved in the classical VRP, DVRPTW considers in
addition time windows, which are able to better capture real-
world situations. Despite this, so far, few studies have focused on
tackling this problem of greater practical importance. To this end,
this study devises for the resolution of DVRPTW, an ant colony
optimization based algorithm, which resorts to a joint solution
construction mechanism, able to construct in parallel the vehicle
routes. This method is coupled with a local search procedure,
aimed to further improve the solutions built by ants, and with an
insertion heuristics, which tries to reduce the number of vehicles
used to service the available customers. The experiments indicate
that the proposed algorithm is competitive and effective, and on
DVRPTW instances with a higher dynamicity level, it is able to
yield better results compared to existing ant-based approaches.

I . INTRODUCT ION

The Vehicle Routing Problem plays a central part in cap-
turing logistics and distribution operations, with an impact in
the economy, more prominent as the need for transportation
is increasingly growing. Thus, this class of problems has a
practical importance and has attracted a lot of research inter-
est over the years. Recently, the technological advancements
made possible the use of mobile devices to enable the direct
communication between the clients and the drivers, such that
a driver could dynamically change his plan while executing
his route. Also, the emergence of global positioning systems
allows a dispatcher to know the current position of a driver
and communicate him in a timely manner the next customer
to visit on his route. This leads to a VRP variant known as
Dynamic Vehicle Routing Problem (DVRP), also referred to
as online or real-time VRP.
As indicated in [1], where the authors perform a review
on VRP literature, more challenging problems such as DVRP
began being studied more intensively only recently, due to
their ability to capture real-world scenarios. Possible sources
of dynamism in DVRP include: customers requests, travel time
and demands, which can be dynamically revealed for a set
of known customers. In this paper, we considered that the

dynamic nature of DVRP is given by the customers requests,
which is the most common source of dynamism, as pointed
out in [2]. Therefore, DVRP allows customers requests to be
serviced in a real-time manner, while the drivers have already
started their routes, visiting in their way other customers. Thus,
the algorithm needs to adjust the ongoing vehicle routes, such
as to take into account these dynamic customer requests, while
maintaining the feasibility of solution.
The aim of this study is to design and analyse ant colony
optimization based algorithm');
INSERT INTO posts (postId,userId,title,body) VALUES (372,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 2)','s, that are able to achieve good
quality solutions for DVRPTW instances with a higher level of
dynamicity. To this end, we propose in this paper an ant colony
system (ACS) based algorithm, relying on a joint mechanism
for constructing the solutions, in which the vehicle and the next
customer to be added in its tour are simultaneously selected
during the transition step. Our approach is hybridized with a
local search procedure, consisting of two operators, relocate
and exchange, that are applied in an iterative manner, until
no further improvement is possible. Moreover, we integrated
in our method an insertion heuristics to better incorporate
unvisited customers in the existing tours, thus reducing the
number of vehicles needed to service all the customers.
The remainder of this paper is structured as follows. Section
II describes the static variant of the considered DVRPTW
problem. Section III presents a survey on existing literature
related to DVRPTW. In Section IV the ant colony system
optimization algorithm is summarized, as the underlying meta-
heuristic for our approach that will be described in Section
V. In Section VI the experimental study and the obtained
results will be presented. The paper concludes with Section
VII, which draws the nal remarks and offers future research
directions.

I I . THE VEH ICL E ROUT ING PROBL EM W ITH T IME
W INDOW S

In this section, we describe the (static) vehicle routing
problem with time windows (VRPTW), which is the static
variant of our considered DVRPTW problem.
As in the classical VRP or capacitated VRP, VRPTW
requires to nd a set of routes for each vehicle, such that
each customer, having a given demand, is visited exactly
once by a single vehicle and the total demand on the route

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings
OSERHU
does not exceed the vehicles capacity. The eet of vehicles
is assumed to be homogeneous, all the vehicles are located
at a single depot, where they start and end their route, and
each vehicle has a limited capacity. Besides this, VRPTW
brings the additional constraint of time windows associated
to a customer, such that a customer i must be serviced by a
vehicle within a time interval, dened as [ei , li ], where ei is
the earliest arrival time, and li is the latest arrival time. The
service at customer i cannot start before its earliest arrival
time, and if the vehicle arrives at the customer earlier than
ei , waiting occurs. Also, the service at each customer takes
an amount of time, denoted by si , required for the pickup
and/or delivery of goods or services. In case of hard time
windows constraints, the vehicle must arrive at the customers
not later than the latest arrival time, constraint that must be
satised for each customer within a vehicles tour in order to
maintain the solution feasible. There is also the possibility of
applying penalties if the services start after the allowed time
windows, which is the case of soft time windows. The depots
time window induces a constraint regarding the maximum total
route time, ');
INSERT INTO posts (postId,userId,title,body) VALUES (373,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 3)','meaning that each vehicle route must start and end
within the time window associated with the depot.
In this study, we consider hard time windows constraints
and the existence of a homogeneous eet of vehicles, that are
located at a single depot. Also, we assume that the primary
objective of VRPTW is to minimize the number of tours (vehi-
cles), whilst a second objective is to minimize the total traveled
distance needed to supply all customers in their required
hours. This means that the number of tours minimization
takes precedence over total distance minimization. In case of
solutions with the same number of vehicles, those solutions
of lower distance are preferred.
it can be noticed that VRPTW is
By its description,
more complicated than multiple traveling salesman problem
(mTSP), problem that we have tackled in our previous studies
[3], [4] with ant colony based methods, and for which we pro-
posed a benchmark1. Thus, mTSP can be seen as a relaxation
of VRPTW, if removing the time windows associated with the
customers, and assuming that vehicles have unlimited capacity,
which imposes no restriction on the number of customers to
be visited on a vehicles route.

I I I . RE LAT ED WORK

During the last decades, the VRP and his famous VRPTW
extension have been an intensive research area. Heuristic and
exact optimization approaches have been developed for the
VRPTW problem. Surveys of proposed techniques can be
found in [5], [6]. As the VRP and VRPTW problems are both
NP-hard and they generalize the traveling salesman problem
(TSP), ant-based methods were proposed for solving them. A
multi-ant colony system consisting in two colonies, one for
optimizing the number of vehicles and one for optimizing the
total travel time, was proposed for the VRPTW problem in
[7].

1 http://profs.info.uaic.ro/mtsplib/

Because of the recent technological advances, the research
on dynamic routing increased. Techniques ranging from lin-
ear programming to metaheuristics were designed to address
dynamism and uncertainty. A comprehensive review of appli-
cations and approaches for dynamic vehicle routing problems
is given in [2]. A taxonomy of papers concerning DVRP
problems was developed in [8]. Also, the interested reader
is referred to [9], for a survey of hybrid articial intelligence
algorithms, employed for solving DVRP problems.
Very few previous studies on using metaheuristics on
DVRPTW exist, that take into account both the dynamic nature
of VRP and the time windows as additional constraint. A
rst such study is due to [10], which presents a tabu search
algorithm that works on the DVRPTW, but with soft time
windows. A hybrid genetic approach based on two populations
of individuals which evolve concurrently are used to minimize
the total traveled distance, and respectively, the number of
unserviced customers, total lateness at customers locations
and temporal constraint violations in [11]. A metaheuristic
algorithm is developed in [12] for tackling two variants of
DVRPTW, which consider several real-world constraints: mul-
tiple time windows, customers priorities and vehicle-customer
constraints. In [13], the DVRPTW problem is decomposed into
several static VRPTW problems, which are then solved using
an improved large neighborhood search algorithm.
To the best of our knowledge, the study from [14] is the only
ant colony based approach found in the literature, that tackles
the DVRPTW problem. To this end, the authors propose an
ACO based method, denoted as MACS-DVRPTW, by extend-
ing to the dynamical case the state-of-the-art ant algorithm
[7] developed for VRPTW. This study was continued in [15],
where MACS-DVRPTW was applied to schedule the routes of
a eet of cars for a surveillance company. Recently, MACS-
DVRPTW was enhanced in [16] with two strategies for dealing
with DVRPTW problem in ');
INSERT INTO posts (postId,userId,title,body) VALUES (374,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 4)','which customers have different
priority levels.

IV. THE ANT COLONY SY S T EM

Ant Colony Optimization (ACO) is a nature-inspired meta-
heuristic, which follows the metaphor of real ants that succeed
in nding the shortest paths between their nest and food
sources. In case of ACO, each articial ant, referred henceforth
as ant, constructs solutions in a probabilistic manner by
iteratively adding components, until a complete solution to
the problem is obtained. When building a solution, an ant is
inuenced by two factors: 1) pheromone trails, updated dy-
namically at runtime, that reect the gained search experience
of ants, and 2) the heuristic information, which is a static
value, dependent upon the problem.
The algorithm presented in this study is based on the Ant
Colony System (ACS) [17], which is a successor of the Ant
System (AS) [18]. AS is the rst ACO algorithm proposed in
the literature, that was initially designed for solving instances
of TSP. ACS is an improvement over AS and it brings the
following changes: 1) a modied state transition rule, which
is more focused on exploitation, 2) the inclusion of a local

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings
OSERHU
pheromone update, which reduces the amount of pheromone
on edges visited by ants, in order to increase exploration
and prevent an early stagnation of search, and 3) the global
pheromone update is performed by a single ant, generally
being the best so far ant, which produced the best solution.

V. THE ALGOR ITHM INVE S T IGAT ED FOR DVRPTW

the
This section describes our approach designed for
DVRPTW problem, which is based on the ant colony system
metaheuristic, outlined in Section IV. To tackle the dynamic
nature of the problem, we adopted in our algorithm the
time slices and nodes commitment approach from the MACS-
DVRPTW ant based solver, introduced in [14]. Accordingly,
a working day of twd seconds is equally split into nts time
slices, the length of a time slice (tts ) being computed as:
tts = twd /nts . Thus, the initial DVRPTW problem is divided
into nts static problems, that will be solved consecutively.
In this context, we introduce the concept of current problem
conguration, which denotes one of these static problems,
dened by the subset of available nodes (customer requests).
We will denote by S best the currently optimal solution, which
is the best feasible solution including all the available customer
requests known so far.
is
Our approach, referred henceforth as DVRPTW-ACS,
composed of two parts, that will be presented in the next
subsections, and each part is running in a separate thread. The
core part of the algorithm, briey denoted as the planner,
is concerned with the time slices management and with the
dynamic nature of the problem. The other part is responsible
with the optimization task, by running the ACS based algo-
rithm on the current problem conguration.

A. The planner

Before the beginning of the working day, an initial feasible
solution (with respect to capacity, time window and vehicle
arrival time at the depot constraints), is built using the time-
oriented nearest neighbour heuristic, introduced in [19]. The
time-oriented nearest neighbour heuristic is a sequential tour
building algorithm, that takes into account both geographical
and temporal closeness of customers. This initial solution
denes the tentative tours for the vehicles, considering only a
priori nodes, known in the system from the beginning.
then
Initially, a vehicle starts its tour from the depot,
iteratively adds the closest (in terms of a me');
INSERT INTO posts (postId,userId,title,body) VALUES (375,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 5)','tric m) unvisited
customer, relative to the last customer of this tour, while
keeping feasible the tour under construction. The metric mij ,
which denes the closeness between i, the last customer on
the current emerging tour, and j , that refers to any unrouted
customer that could be visited next, is dened as:

mij = 0.4  dij + 0.4  Tij + 0.2  uij

(1)

In the previous equation, dij
is the distance between the
two customers, Tij means the time difference between the
beginning of service at customer j and the completion of
service at customer i, and is expressed as: Tij = bj  (bi + si ),
and uij represents the urgency of delivery to customer j ,

dened as: uij = lj  (bi + si + dij ), where bj denotes
the beginning of service at customer j , being computed as:
bj = max(ej , bi + si + dij ), assuming that the vehicle travels
to the next customer j , as soon as it has nished the service
at the current customer i.
Each time when there are still remaining unrouted cus-
tomers, that cannot be feasibly appended to the current tour,
a new tour, that starts from the depot, is added to incorporate
them.
The obtained solution is used to initialize the best so
far solution, S best , and to compute the value of the initial
pheromone level, 0 , as:

0 = (nav  LN N )1

(2)

where nav is the number of available nodes (customers) and
LN N denotes the total traveled distance of this solution.
When the working day begins, a timer is started which keeps
track of the current time, so that when the working day is over,
the algorithm will stop its execution. Also, the planner starts
the ant colony thread, which will perform its optimization task,
considering only the currently available customers.
Taking into account the current time, the planner repeatedly
veries whether a new time slice began. If this is the case, the
planner checks: 1) if there are new nodes that became available
in the meantime, during the last time slice, and 2) if there
are new nodes that must be committed. If one of these two
conditions is met, the planner stops the execution of the ACS
based algorithm, since the problem denition has changed.
The next node i, after the last committed node from a tour
of the best so far solution, S best , is marked as committed if
the following condition holds: bi  idxts  tts , where idxts is
the index of the current time slice, and the idxts  tts product
denotes the time when the current time slice ends. When a
node becomes committed, its location within the vehicles tour
cannot be changed anymore and it will be found at this position
in its tour in the nal solution to be returned by the algorithm
at the end of the working day.
After committing the necessary nodes from the tours of
S best , if condition 1) holds, the list of available customer
requests is updated and the new available nodes are added in
the tours of S best by adopting the insertion technique from I1
insertion heuristic [19]. The insertion heuristic starts by com-
puting for each unrouted node its best feasible insertion place
in the tours of S best acco');
INSERT INTO posts (postId,userId,title,body) VALUES (376,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 6)','rding to a criterion (c1 ). The score of
inserting a customer u between two adjacent customers i and
j on a route, is computed as: c1 (i, u, j ) = 0.1  c11 (i, u, j ) +
0.9  c12 (i, u, j ), where c11 (i, u, j ) = diu + duj  dij and
c12 (i, u, j ) = bju  bj , where bju denotes the new time of
beginning the service at customer j , after inserting customer
u in the route. For a given node, its best insertion position is
the one that achieves the minimum score for the c1 criterion.
Based on this, the best customer u to be inserted in the route
is selected to be the one that obtains the minimum value for c2
criterion, dened as: c2 (i, u, j ) = 2.0  d0u  c1 (i, u, j ), where
d0u denotes the distance between the depot and customer u.

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings
OSERHU
The insertion heuristic is applied in an iterated manner, until
no further feasible insertions are possible.
If there are still remaining available nodes which couldnt
be feasibly inserted in the existing tours of S best , a new
tour is added, which starts from the depot. The unrouted
nodes will be tried to be added in this tour by following
the same construction mechanism used in the time-oriented
nearest neighbour heuristic. This process continues until all the
available customer requests are incorporated in S best , meaning
that a complete feasible solution was obtained.
After the necessary commitments were made and the new
available nodes were added in the best so far solution, the
ant colony is restarted, performing its optimization task on
the new problem conguration. The algorithm stops after twd
seconds of execution, which marks the ending of the working
day. At this point in time, the best solution obtained so far is
returned as the output of DVRPTW-ACS, indicating the nal
tours to be traveled by each vehicle.

B. The ACS based algorithm

Our DVRPTW-ACS approach relies on the ACS algorithm
for performing the optimization task, such that all the available
customers are visited only once, while minimizing the number
of vehicles and the total traveled distance. An ants solution is
composed of a set of tours, each one designating the route to be
traveled by a vehicle. As mentioned in Section II, the formu-
lation of the considered VRPTW problem assumes a hierarchy
between these two objectives, namely the minimization of the
number of vehicles takes precedence over the minimization
of the traveled distance. This implies that a solution which
uses less vehicles will be preferred over a solution with more
vehicles, even if it has a smaller traveled distance.
Unlike the MACS-DVRPTW solver which resorts to two
colonies, ACS-VEI and ACS-TIME, each one trying to op-
timize a different objective, our algorithm uses one single
colony, which simultaneously optimizes the two objectives,
the number of vehicles and the total traveled distance. Conse-
quently, MACS-DVRPTW uses two pheromone trail matrices,
one for each colony, whereas our approach involves only one
pheromone trial matrix. Besides that, DVRPTW-ACS works
only with feasible solutions, in which all the known customers
are incorporated, which is different from MACS-DVRPTW,
where the ACS-VEI colony works also with unf');
INSERT INTO posts (postId,userId,title,body) VALUES (377,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 7)','easible solu-
tions.
At the rst run of the ACS based algorithm, the pheromone
trail matrix is initialized with 0 , computed as in equation (2).
Then, at subsequent runs, when the ant colony is restarted,
0 is computed like in equation (2), except for the fact that
LN N is replaced with Lbest , which denotes the total distance
of the tours from S best . In addition to this, we have adopted
the pheromone preservation strategy, employed also in one of
the variants of MACS-DVRPTW, which allows that a certain
amount of pheromone from the previous run of the ant colony
to be preserved in the current execution.
In the implementation of the algorithm, we resorted to
candidate lists, as indicated in [20], when solving instances of

TSP with ACS. A candidate list of size cl contains for a given
node, the most closest cl neighbours, arranged in increasing
order according to their distance from the considered node. In
our case, all the nodes part of the DVRPTW instance will be
considered when computing the candidate list of a node. Then,
in the transition step of ACS, when an ant located in node r
has to decide the next node to move to, it will rst examine
the candidate list of node r, choosing only those nodes that are
revealed. Only when all the available nodes from the candidate
list have already been visited, the search of the next customer
is extended on the rest of available nodes.
Initially, a vehicle is placed at the depot, where it starts
and ends its tour. In an iteration of the ACS based algorithm,
several ants build feasible solutions for the problem, that inte-
grate all the available customers and do not violate any of the
VRPTW constraints: capacity, time window and vehicle arrival
time at the depot. Later on during the algorithm execution,
when the planner starts to commit nodes, the committed parts
of S best are used to initialize the ants solutions, being copied
at exactly the same positions. Therefore, it may result that an
ants solution to be comprised from the beginning of more
than one tour and use more than one vehicle. In case there are
committed nodes, the solution construction will continue from
these added portions of tours, which will be next extended by
ants during the transition step.
Unlike the MACS-DVRPTW solver, in which the tours are
constructed sequentially and lled up with customers one by
one, in our approach the tours are built simultaneously and the
vehicles compete towards extending their partially tours with
a new available customer. In addition to this, we employed a
joint mechanism for building the vehicles tours, in which the
vehicle and the customer to be added in its tour, are selected at
the same time, during the state transition rule from the standard
ACS. Both the vehicle (v ) and the next customer to visit (s)
are chosen by iterating over the existing tours (vehicles) from
the set T and the candidate set C , comprised of available
customers, that have not been visited and committed yet, and
can be appended to the tour of v vehicle, while keeping the
solution feasible:
(v , s) = (arg max
vT sC
S,
where r represents the ants current position, being the last
customer on the tour denoted by v , rs is the pheromone trail
associated to edge rs, rs is the heuristic information corre-
sponding to edge rs and is computed as: 
rs = (1/mrs) ,
where mrs is dened as in equatio');
INSERT INTO posts (postId,userId,title,body) VALUES (378,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 8)','n (1). In addition,  and
 are parameters that reect the importance of pheromone,
respectively heuristic information, q0  [0, 1] is a parameter
and S is a random variable selected according to the following
probability distribution:
prs = (

if rand(0, 1) < q0

rs  
( 
rs ),

otherwise

otherwise

(3)

rs
rs
PwT PuC
0,

if s  C
ru


ru

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings
OSERHU
During the building process of an ants solution, when there
are remaining unrouted available customers that cannot be
feasibly added in the existing tours and the number of these
customers is less or equal to ten, we apply the same insertion
heuristic used by the planner to update S best with the new
available nodes. Its aim is to insert in the existing tours the
remaining available unvisited nodes. In this way, the insertion
heuristic is applied in order to prevent the addition of new
tours needed to incorporate these nodes, thus reducing the
number of vehicles used to service all the known customers.
After this step, if there are still remaining unvisited customers,
that are available, a new tour is added to incorporate them.
This process is repeated until the ants solution covers all the
available customers, meaning that a feasible complete solution
for the DVRPTW problem was obtained.
Once an ant moves from customer r to s, it updates the
pheromone level on the edge associated to it, by applying the
local pheromone update rule, as in the standard ACS:

rs = (1  )  rs +   rs

(4)

where   (0, 1) is a local pheromone decay parameter and
for the rs we have used the following setting, as indicated
in [17]: rs = 0 , where 0 is the initial pheromone level,
computed as in equation (2). The aim of the local pheromone
update is to make visited edges less desirable for subsequent
ants, enforcing diversity within the same iteration and thus
avoiding the situation in which all the ants construct very
similar tours.
After all the ants nished to construct their solution, the
best iteration ant is selected as the one that built the best
solution, either regarding the number of vehicles or the total
distance objective. Since the number of vehicles objective has
priority over the total distance objective, a solution which uses
less vehicles is always preferred over a solution with more
vehicles, even if it has a smaller total distance. The global best
ant, which corresponds to the best so far solution, is updated,
if necessary, based on the best iteration ant. Then it follows
the global pheromone update and like in the standard ACS, it
reinforces with additional pheromone the edges used by the
global best ant:

rs = (1  )  rs +   rs

(5)

if rs  best so far solution
otherwise

where
rs = (cid:26) (Lgb )1 ,
0,
where   (0, 1) is the pheromone decay parameter and Lgb
denotes the total traveled distance of the best so far solution,
obtained by the global best ant.
Since it is known from the literature that combining an ACO
algorithm with a local search phase can greatly improve its
performance, we integrated in DVRPTW-ACS a local search
procedure. This method consists of two multi-route operators,
relocate and exchange [21], which modify simultaneously
two different tours. The relocate operator removes one node
from a tour and reinserts it
into another tour, whilst
the

exchange operator swaps the position of two nodes from
two different tours.');
INSERT INTO posts (postId,userId,title,body) VALUES (379,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 9)',' These two operators are applied in an
iterated manner, meaning they are called repeatedly, until no
further improvement, regarding the number of vehicles or the
total distance, can be obtained. We applied the local search
procedure to the initial solution, produced by the time-oriented
nearest neighbour heuristic, and to the best solution obtained
in the current iteration (best iteration ant) of the algorithm.
According to the concept of commitment, which involves
that the position of committed nodes within a tour is xed, the
insertion heuristic and the local search operators are applied
only to positions that do not belong to the committed parts of
a tour. In case of insertion heuristic, this means that a node
cannot be inserted in front of a node that is committed, whilst
in case of the local search procedure, the positions eligible for
relocation and exchange will start from the rst uncommitted
node within a tour.
The ACS based algorithm iteratively performs its optimiza-
tion task, trying to improve S best in any of the two objectives
(number of vehicles and total distance), until it is stopped by
the planner, either because new nodes became available or
because there are nodes that must be committed.

V I . EX P ER IMENT S

A. Problem instances

The experimental analysis in this paper is conducted on
several DVRPTW instances, belonging to the benchmark in-
troduced in [14]. These problem instances were constructed
starting from the Solomons 100 customers static VRPTW
benchmark2, that was extended to the dynamical case, by spec-
ifying a dynamicity level of X %. To this end, the Solomons
benchmark was enhanced with an additional attribute, namely
the available time, which conveys the time when a customers
request is revealed to the system. Starting with this time, the
customer request is considered to be known and will be taken
into account by the algorithm.
A dynamicity of X % means that a percentage of X %
customer requests have a non-zero available time, these being
dynamic requests which are revealed during the working day.
The a-priori nodes (customer requests), that are known from
the beginning of the algorithm execution, have their available
time equal to 0. The problem instances from the DVRPTW
benchmark have specied different values for the dynamicity
level, ranging in the interval [0%..100%], with an increment
of 10%. The instances with 0% dynamicity are static VRPTW
problems taken from the Solomons benchmark, whilst the
instances with 100% dynamicity are the most dynamic ones.
The DVRPTW benchmark encompasses euclidian test prob-
lems, divided in six categories: R1, R2, C1, C2, RC1 and
RC2. The test instances in R1 and R2 categories have nodes
with randomly generated coordinates, the nodes from C1 and
C2 categories are clustered, whilst in RC1 and RC2 there is
a combination of randomly generated and clustered nodes.
Test problems of type 1 have a short scheduling horizon,

2 http://web.cba.neu.edu/ msolomon/problems.htm

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings
OSERHU
allowing only a few nodes to be visited on a vehicles route.
On the other hand, problems of type 2 have a long scheduling
horizon, such that more customers can be serviced by the same
vehicle, and thus fewer vehicles are needed to visit all the
customers. From each of these six categories, we have chosen
two problem instances and for each of it we have selected
three different values for dynamicity: 10%, 50% and 100%,
resulting in a total of 36 DVRPTW instances, that were used in
our experimental study. We have chosen different dynamicity
degrees in order to analyze the impact of the dynamicity on the
quality of solutions produced by the i');
INSERT INTO posts (postId,userId,title,body) VALUES (380,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 10)','nvestigated algorithms. In
each DVRPTW instance, the depot is considered to be the rst
entry in the list of given customers. Also, the eet of vehicles
is homogeneous and the travel time between two customers is
assumed to be equal with the distance between them.
The VRPTW problem instances from the Solomons bench-
mark dene a time window for each node, including the depot,
namely [e0 , l0 ], which denotes the scheduling horizon. At the
same time, the specications of the DVRPTW problem involve
a certain length of the working day, which establishes when
the algorithm will stop from its execution. To comply with
these two requirements, we scaled in our algorithm all time
related values, as it was done in the MACS-DVRPTW solver.
More precisely, for each customer request from a particular
DVRPTW instance,
the values for time windows, service
time, available time and distance between two customers, are
multiplied with the sv factor, computed as: sv = twd/(l0  e0).
In a similar manner, the results reported in Section VI-C for the
total traveled distance (T D) are obtained as: T D = T D/sv .
To allow the reproduction of results, the DVRPTW test
instances, used in the experimental part of this study, and the
Java source code of our DVRPTW-ACS algorithm are available
online3 .

B. Parameters setup

To allow a fair comparison of the two investigated algo-
rithms, DVRPTW-ACS and MACS-DVRPTW, we used the same
setting of the parameters, their values being taken from [14].
For the ACS based algorithms we have set: q0 = 0.9,  = 1.0,
 = 1.0,  = 0.9 and the number of ants was set to 10, whilst
for the dynamic scenario we have set: twd = 100 seconds
and nts = 50. Also, we have employed in both algorithms
the pheromone preservation strategy, by specifying  = 0.3 in
the equation: ij = (1  )   old
ij +   0 . Besides that, the
size of the candidate list, used in our algorithm, was set to
cl = 20. For both algorithms 30 runs were carried out on a
Linux server with 6 GB RAM memory, processor Intel Core
2 Duo P9xxx (Penryn Class Core 2) at 2.5 GHz.

C. Results

To assess the performance of the proposed algorithm, we
compared it with another ACO based approach from the
literature, namely MACS-DVRPTW, described in [14]. More
precisely, among the four variants of ACO based algorithms

3 http://profs.info.uaic.ro/mtsplib/vrp- extension/dvrptw/

employed for the MACS-DVRPTW solver, we have chosen
the ACS based algorithm with pheromone preservation (WPP)
and which uses no improved initial solution (IIS), constructed
before the start of the working day. We selected this variant
of algorithm, denoted as DVRP, 0.3 wpp, no IIS in [14],
since according to the results the authors reported in their
study, this variant achieves overall one of the best results on
DVRPTW instances with 50% dynamicity. For performing the
comparison of the investigated approaches, we stored for each
method the best so far solution and the number of feasible
solutions, computed during the working day.
Based on the 30 best so far solutions achieved at
the
end of each algorithm execution on all DVRPTW instances,
we report in Tables I to VI the average, minimum (best),
maximum (worst) and standard deviation values. These values
are computed in terms of the two objectives: number of
vehicles (abbreviated in tables as NV) and total
traveled
distance (abbreviated in tables as TD).
In addition to this, we also indicate the results obtained by
two approaches after performing 30 runs on static VRPTW
instance');
INSERT INTO posts (postId,userId,title,body) VALUES (381,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 11)','s. The static VRPTW instances correspond to the
considered DVRPTW instances, in which all the customer
requests are known beforehand and there is no node to be
revealed dynamically during the working day. The instances
from the DVRPTW benchmark were constructed such as to
still allow obtaining the optimal solution of the corresponding
static VRPTW instance. Therefore, the solutions produced for
the static instances will act as a reference point when assessing
the quality of solutions found by the investigated approaches
on dynamic instances. In this sense, we have computed the
decline in the solution quality, relative to the solution ob-
tained for the static instance, that will allow us to analyze
how much the solution deteriorates with the increase in the
dynamicity level. The decline measure is indicated in tables
as increase(%) and its value is computed for each of the two
objectives as: increasek = (minDk  minSk )/minSk  100,
k = 1, 2, where minDk is the minimum value obtained on
the dynamic instance for the k-th objective, and minSk is the
minimum value obtained on the static instance for the k-th
objective.
In the evaluation of the two algorithms, we take into account
the hierarchy between the two objectives, meaning that we aim
with priority minimum values for the number of vehicles, then
secondly we seek for smaller total distance values. In each
table, we highlighted with boldface the best (minimum) value
from a row, except for the instances where the statistical test
did not show statistically signicant differences. The end part
from the name of a DVRPTW instance reects its associated
dynamicity, such that 0.0 denotes static instances having 0%
dynamicity, 0.1 is for instances with a low dynamicity of
10%, 0.5 is for instances with 50% dynamicity, and 1.0
denotes instances having the maximum dynamicity of 100%.
Analyzing the best values obtained for the two objectives,
it can be stated that MACS-DVRPTW is a clear winner on
most of the static instances, except for C201-0.0 and C202-
0.0 instances, where our algorithm has a better average per-

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings
OSERHU
formance. Actually, on these instances, the minimum values
produced on C201-0.0 by both algorithms, and on C202-0.0 by
DVRPTW-ACS, coincide with the best known solution reported
in the literature4 . Also, on most of DVRPTW instances with
a low dynamicity of 10% and in case of a few instances
with higher dynamicity, such as R103-0.5, R104-0.5 and R104-
1.0, MACS-DVRPTW achieves better results than our approach
on both objectives. The advantage of our algorithm over
MACS-DVRPTW becomes apparent as the dynamicity level of
instances increases. Thus, on almost all instances of 50% and
100% dynamicity, DVRPTW-ACS manages to obtain superior
solutions with less vehicles and of lower total distance than
MACS-DVRPTW. The better performance of our algorithm
is more noticeable on C201-0.5, C202-0.5, C101-1.0, C102-
1.0 and C201-1.0 instances, where the values attained by
DVRPTW-ACS on both objectives are almost half the ones
obtained by MACS-DVRPTW.
The values recorded by the two approaches for the increase
measure reveal
the same good performance of DVRPTW-
ACS. More precisely,
it can be noticed that
the solution
quality on DVRPTW instances with higher dynamicity of
50% and 100% is not inuenced so much by the increase
in the dynamicity. Besides that, on instances with clustered
nodes such as C101-0.5, C101-1.0, C201-0.5 and C201-1.0,
DVRPTW-ACS is still ab');
INSERT INTO posts (postId,userId,title,body) VALUES (382,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 12)','le to attain the same good results as
obtained for the corresponding static instances, as indicated
by the zero values for the increase measure. In contrast to
this, MACS-DVRPTW yields on most instances higher values
for the increase measure. Also on some instances with higher
dynamicity such as R201-1.0, C201-0.5, C201-1.0, C202-1.0
and RC202-1.0, the value for increase measure in terms of
the number of vehicles is greater than or equal to 100, which
shows that the decline in the solution quality is signicant.
Also, compared to the average number of vehicles required
on static instances, MACS-DVRPTW uses in addition up to 2
vehicles on instances of 10% dynamicity, on instances of 50%
dynamicity it uses at most extra 6 vehicles, and on instances
of 100% dynamicity it uses at most extra 10 vehicles. On the
other hand, our approach requires on average on all DVRPTW
instances, regardless of their dynamicity level, up to 2 extra
vehicles. This aspect shows that our algorithm manages to
cope better on instances of higher dynamicity, in which fewer
customers are known in advance and more nodes are revealed
in an ongoing fashion during the working day.
Figure 1 illustrates the distribution of the results of the two
algorithms with respect to the total distance objective. Yel-
low boxplots correspond to the results obtained with MACS-
DVRPTW, and green ones to DVRPTW-ACS. The results are
grouped based on the instance category and dynamicity level.
Analyzing the boxplots, it can be noticed that our algorithm
wins on instances from C category for all the dynamicity
levels. Also, on R and RC instances with low dynamicity,
DVRPTW-ACS is surpassed. However, on the same R and
RC instances having 100% dynamicity, DVRPTW-ACS out-

4 https://goo.gl/YbMGJx

performs MACS-DVRPTW. Besides that, the boxplots reveal
that the variance of DVRPTW-ACS is smaller, which indicates
a more stable algorithm.
We conducted two statistical
the Mann-Whitney-
tests,
Wilcoxon nonparametric test and the Students t-test (we
performed 30 runs for each algorithm), to assess if these
differences in the performance of the two approaches are statis-
tically signicant or not. The results obtained for the dynamic
instances in terms of the total distance objective, indicate
statistically signicant differences on all DVRPTW instances,
excepting R202-1.0, RC102-0.5 and RC203-1.0. Also when
considering the number of vehicles objective, statistically
signicant differences are recorded on all dynamic instances,
except for R202-0.1 and RC102-0.5. On static instances, only
for C102-0.0 the two statistical tests do not show signicant
differences, in terms of the total distance objective. For the
number of vehicles objective, the results indicate statistically
signicant differences on all static instances.
Correlating the values obtained for the considered measures
by the two investigated approaches, it can be stated that our
algorithm achieves the best overall performance on both objec-
tives. Moreover, in some cases where DVRPTW-ACS does not
have such a good performance, the obtained values are close
to the ones attained by MACS-DVRPTW. This indicates that
our approach is competitive with the MACS-DVRPTW solver,
and on higher dynamicity instances DVRPTW-ACS is able to
yield better results.
The good performance of our approach can be ascribed
to its underlying joint mechanism employed in the transition
step of the ACS based algorithm, in which the vehicles tours
are constructed simultaneously. This offers better chances of
covering the available unvisited customers with the existing
vehicles, without needi');
INSERT INTO posts (postId,userId,title,body) VALUES (383,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 13)','ng to resort to additional vehicles. In
contrast to this, in MACS-DVRPTW the tours are constructed
sequentially and are lled up one by one with customers.
This aspect restricts the number of choices for adding an
unrouted customer, which may lead to using more vehicles.
Besides this, DVRPTW-ACS is coupled with a powerful inser-
tion heuristic, being among the best heuristics proposed by
Solomon for VRPTW. Our insertion procedure manages to
better incorporate unvisited available customers in the tours
of existing vehicles, by searching for each unvisited node the
best insertion position. In this way, all the unvisited nodes
have equal chances and compete for being inserted in best
position within a vehicles tour. In contrast to this, in MACS-
DVRPTW the nodes are inserted sequentially into their best
position, according to a partial order of the customers, which
are sorted in descending order based on their demand.
In terms of the computational cost, the comparison of the
number of feasible solutions obtained by the two approaches,
reveals that for most of the DVRPTW instances, MACS-
DVRPTW computes more feasible solution than our algorithm.
This may be attributed to the fact that DVRPTW-ACS was
implemented in Java, whilst the MACS-DVRPTW solver was
coded in C, which may induce more rapid execution times.
Also, the relocation and exchange operators from our local

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings

TABLE I
T H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R
NUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON R 1
I N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY

TABLE II
T H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R
NUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON R 2
I N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY

instance

measure

R103-0.0

R104-0.0

R103-0.1

R104-0.1

R103-0.5

R104-0.5

R103-1.0

R104-1.0

average
min
max
stdev
average
min
max
stdev
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)

MACS-DVRPTW
TD
NV
1087.923
10.933
10
1046.225
1148.068
11
23.397
0.254
979.076
10
944.92
10
1007.948
1015.742
1078.205
10.367
1026.682
10
1135.584
11
21.565
0.49-1.868
969.424
10
934.079
10
996.333
10
16.0310
-1.147
12.367
1153.761
1113.695
12
1199.753
13
20.675
0.49
6.449
20
976.452
9.133933.443
1013.922
10
24.099
0.346
-1.215
-10
1327.262
14.567
14
1279.144
1390.228
15
29.899
0.504
22.263
40
1035.714
9.8
955.57910
1092.117
0.407
29.13
-10
1.128

DVRPTW-ACS
TD
NV
1267.525
14
14
1246.521
1287.189
14
10.781072.18
10.8
10
1037.729
1107.054
11
0.407
16.243
1272.788
14
1248.78
14
1302.275
14
14.110
0.181
1059.812
10.533
10
1023.262
1108.842
11
17.035
0.507
-1.39414
1301.596
1245.081
14
1350.338
1422.676
-0.1161097.715
10.567
10
1054.123
1168.38
11
25.714
0.504
1.581301.524
14
14
1267.567
1355.115
14
18.1671.6881103.916
10.533
1054.419
10
11
1161.368
24.571
0.5071.608

instance

measure

R201-0.0

R202-0.0

R201-0.1

R202-0.1

R201-0.5

R202-0.5

R201-1.0

R202-1.0

average
min
max
stdev
average
min
max
stdev
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)

MACS-DVRPTW
NV
TD
977.568
3.767869.717
1154.8180.43
77.168
928.359846.2791019.080
39.238
1075.4
4.9
966.551143.90143.654
0.305
33.333
11.134
949.171
3.967
858.5551044.95846.793
0.183
1.4511262.015
5.933
1171.5211341.55345.987
0.254
34.701
66.667
1148.596
5.133
1046.7991233.39648.095
0.346
23.694
66.667
1811.948
7.21625.431
1995.43886.74
0.484
86.892
100
1249.129
5.433
1129.50');
INSERT INTO posts (postId,userId,title,body) VALUES (384,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 14)','86
1415.06
0.504
54.7
33.468
66.667

DVRPTW-ACS
TD
NV
1307.182
4.71239.081
1414.32547.917
0.466
1209.1361157.2971295.7920
27.184
4.533
1308.6531218.0041433.059
56.646
0.507-1.701
1196.2641163.7951255.84922.3270.5614
1365.7051323.1481433.617
25.786.7854
1229.8331166.3771349.382
34.8160.7851394.5334
1357.513
1450.42323.7319.5584
1247.4351195.3211323.84933.1323.286
OSERHU
search procedure are applied multiple times, and this involves
a higher computational cost, compared to the cross exchange
operator from MACS-DVRPTW, which is called only once.
This aspect indicates that the better performance of our algo-
rithm on DVRPTW instances with higher dynamicity is not
due to a greater number of feasible solutions that it computes.

Fig. 1. Distribution of total distance objective for dynamicity levels ranging
in {0.1, 0.5, 1}; yellow is for MACS-DVRPTW, green is for DVRPTW-ACS.
Results are reported for each of the three problem classes (C, R, RC)

V I I . CONCLU S ION S

In this paper, we have proposed an ACS based algorithm
to tackle the single-objective DVRPTW problem, which con-
siders a hierarchy among the two objectives, minimizing the
number of vehicles having priority over minimizing the trav-
eled distance. Experiments conducted on several DVRPTW
instances with different dynamicity levels, showed that our
algorithm is efcient and competitive with another ACO
based approach from the literature. Besides that, on DVRPTW
instances with a higher dynamicity, DVRPTW-ACS was able to
achieve better results than MACS-DVRPTW, in terms of both
objectives. Also, our algorithm proved to cope better on higher
dynamicity instances, since the decline in the solution quality
is not so signicant, compared to the solution obtained for
the corresponding static instances. This makes our approach
more suitable to be applied on DVRPTW instances with a
higher dynamicity, in which few customers requests are a
priori known, case more likely to occur in real-world scenarios.
Possible avenues for future work include solving this prob-

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings

TABLE III
T H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R
NUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON C 1
I N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY

TABLE IV
T H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R
NUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON C 2
I N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY

instance

measure

C101-0.0

C102-0.0

C101-0.1

C102-0.1

C101-0.5

C102-0.5

C101-1.0

C102-1.0

average
min
max
stdev
average
min
max
stdev
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)

MACS-DVRPTW DVRPTW-ACS
NV
TD
NV
TD
828.937
10
828.937
10
10
828.937
10
828.937
828.937
10
828.937
1000
10
10
864.627
857.087
10
10
834.639
829.133
10
10
902.735
899.3020
19.344
19.69
828.937
10
1003.766
11
828.937
10
1002.601
11
828.937
10
1003.904
1100.3970
10
20.95
845.559
10
974.14
12
830.814
10
916.369
12
890.275
10
1087.706
12
12.4990
38.088
-0.45810.521
20
10
828.937
16.667
1392.599
828.937
10
1293.749
16
828.937
10
1629.158
180
68.256
0.6610
56.073
60
868.538
10
1170.861
11.633
10
828.937
11
1040.89
932.31
10
1273.721
12
25.14961.062
0.49
-0.68325.54
10
828.937
10
2016.359');
INSERT INTO posts (postId,userId,title,body) VALUES (385,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 15)','
20.167
10
828.937
19
1879.521
828.937
10
2185.256
210
69.944
0.4610
126.739
90
881.348
10
1521.483
16.833
872.031
10
1457.996
16
10
888.646
17
1596.63
5.45536.923
0.3794.48
60
75.846
OSERHU
instance

measure

C201-0.0

C202-0.0

C201-0.1

C202-0.1

C201-0.5

C202-0.5

C201-1.0

C202-1.0

average
min
max
stdev
average
min
max
stdev
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)

MACS-DVRPTW
NV
TD
619.148
3.767591.557
675.3070.43
20.922
670.613
3.267605.298
734.5120.45
37.403
626.003622.535650.2370
9.014
33.333
5.237
4.033
689.57
626.809797.3249.843
0.183
3.554
33.333
6.133
899.03
814.2581122.56982.495
0.346
100
37.647
878.535
5.933767.05
1008.2775.38
0.254
66.667
26.723
1577.455
7.2331339.839
1831.838116.901
0.43
126.494
133.333
948.312
6.067
856.8787
1126.274
77.829
0.254
100
41.563

DVRPTW-ACS
TD
NV
591.5573
591.557
591.5570600.733591.557634.50
15.079
591.557591.557591.55700594.185591.557623.5317.96903
591.557
591.557591.55700606.8533
591.557
636.70517.3910591.5573
591.557
591.55700593.19591.5573
626.741
6.4220
lem from a multi-objective perspective and extending this
research to tackle dynamic rich vehicle routing problems,
that can incorporate more complex constraints and objectives,
found in real-life VRPs.

ACKNOW L EDGEMENT S

We would like to thank to the authors of MACS-DVRPTW
solver [14]
for providing us their source code,
that al-
lowed us to perform the comparison with our algorithm.
This work is partly funded from the European
Union's Horizon 2020 research and innovation
programme under grant agreement No 692178.

RE F ERENCE S

[1] K. Braekers, K. Ramaekers, and I. V. Nieuwenhuyse, The vehicle
routing problem: State of the art classication and review, Computers
'||'&'||' Industrial Engineering, vol. 99, pp. 300-313, 2016.
[2] V. Pillac, M. Gendreau, C. Gu eret, and A. Medaglia, A review of
dynamic vehicle routing problems, European Journal of Operational
Research, vol. 225(1), pp. 1-11, 2013.

[3] R. Necula, M. Breaban, and M. Raschip, Performance evaluation of
ant colony systems for the single-depot multiple traveling salesman
problem, in Proceedings of the 10th International Conference on Hybrid
Articial Intelligence Systems, vol. 9121, pp. 257-268, 2015.
[4] R. Necula, M. Breaban, and M. Raschip, Tackling the Bi-criteria Facet
of Multiple Traveling Salesman Problem with Ant Colony Systems, in
Proceedings of the 2015 IEEE 27th International Conference on Tools
with Articial Intelligence, pp. 873-880, 2015.
[5] J. F. Cordeau, G. Desaulniers, J. Desrosiers, M. M. Solomon, and F.
Soumis, The VRP with time windows, in P. Toth, D. Vigo (eds.) The
Vehicle Routing Problem, SIAM Monographs on Discrete Mathematics
and Applications, pp. 157-194, 2001.
[6] O. Br aysy, and M. Gendreau, Vehicle routing problem with time
windows, Part I: Route construction and local search algorithms,
Transportation Science, vol. 39(1), pp. 104-118, 2005.
[7] L. Gambardella, E. Taillard, and G. Agazzi, Macs-vrptw: a multiple
ant colony system for vehicle routing problems with time windows, in
D. Corne, M. Dorigo, F. Glover, D. Dasgupta, P. Moscato, R. Poli, KV.
Price (eds.) New ideas in optimization, ch 5, McGraw-Hill, pp. 63-76,
1999.
[8] H. N. Psaraftis, M. Wen, and C. A. Kontovas, Dynamic vehicle routing
problems: Three decades and counting, Networks, vol. 67(1), pp. 3-31,
2016.
[9] V. Ilin, D. Simi c, J. Tepi c, G. Stoj');
INSERT INTO posts (postId,userId,title,body) VALUES (386,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 16)','i c, and N. Sauli c, A Survey of
Hybrid Articial Intelligence Algorithms for Dynamic Vehicle Routing

2017 IEEE Congress on Evolutionary Computation (CEC) PREPRINT; the camera ready version will be published in IEEE proceedings

TABLE V
T H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R
NUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON RC 1
I N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY

TABLE VI
T H E VA LU E S O B TA I N ED BY MACS-DVRPTW AND DVRPTW-ACS F O R
NUM B ER O F V EH I C L E S AND TOTA L D I S TAN C E O B J EC T I V E S ON RC 2
I N S TAN C E S W I TH 0% , 1 0% , 5 0% AND 1 0 0% DYNAM I C I TY

instance

measure

RC101-0.0

RC102-0.0

RC101-0.1

RC102-0.1

RC101-0.5

RC102-0.5

RC101-1.0

RC102-1.0

average
min
max
stdev
average
min
max
stdev
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)

MACS-DVRPTW
TD
NV
1419.259
13.567
13
1353.045
1514.738
14
36.447
0.504
1316.636
11.967
1278.533
11
1383.166
12
0.183
24.018
1499.454
14.9
1438.998
14
1569.396
15
0.305
33.063
7.692
6.353
1361.773
11.167
1279.555
11
1460.245
12
0.379
46.071
0.0818.733
1947.191
1772.705
17
2124.249
20
85.384
0.828
31.016
30.769
1566.836
14
13
1471.19
1693.733
15
54.201
0.587
15.069
18.182
2219.684
20.867
19
2057.832
2453.04
23
86.721
0.819
52.089
46.154
1714.181
16.433
1613.665
15
17
1778.713
44.416
0.626
36.364
26.212

DVRPTW-ACS
TD
NV
1669.585
15.1
15
1649.803
1705.964
16
14.753
0.305
1538.879
13.767
1501.709
13
1589.491
14
21.2
0.43
1698.417
15.267
1666.823
15
1740.601
16
19.908
0.451.032
1563.956
13.667
1514.923
13
1629.863
14
22.929
0.4790.88
15.133
1733.718
1669.847
14
1781.071
16
28.038
0.434
1.215
-6.667
1580.218
13.933
13
1540.802
1651.335
15
28.635
0.365
2.6031720.723
15.1
15
1669.856
1781.512
16
26.007
0.305
1.2151602.576
13.9
1551.439
13
15
1704.936
32.527
0.4813.312

instance

measure

RC202-0.0

RC203-0.0

RC202-0.1

RC203-0.1

RC202-0.5

RC203-0.5

RC202-1.0

RC203-1.0

average
min
max
stdev
average
min
max
stdev
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)
average
min
max
stdev
increase(%)

MACS-DVRPTW
NV
TD
1004.3693
893.646
1134.0030
51.035
900.178822.186975.8090
35.982
1044.9174
952.876
1139.7190
48.588
33
6.628
899.799814.817997.52939.368-0.8961261.691
4.8331146.6
1438.76571.85
0.379
28.306
33.333
1123.38
4.333
1016.051235.29750.062
0.479
33.333
23.579
1688.338
6.71460.384
1825.70576.98
0.466
100
63.419
1231.181
5.8
1139.4776
1334.584
41.599
0.407
66.667
38.591

DVRPTW-ACS
TD
NV
1301.5514
1245.37
1366.97332.9261046.391011.5521112.6060
24.8571291.9091234.5511343.794
29.8130
-0.869
1076.276
3.9671019.385
1216.30341.685
0.183
-25
0.7741340.9591246.3761440.575
47.4830.0813.1
1247.3531122.8971392.911
0.305
67.217
11.007
-25
1364.969
3.9671277.912
1544.41260.915
0.183
2.613
-25
3.067
1243.4471097.1931345.215
0.254
60.904
-25
8.466
OSERHU
Problem, in Proceedings of the 10th International Conference on Hybrid
Articial Intelligence Systems, vol. 9121, pp. 644-655, 2015.
[10] M. Gendreau, F. Guertin, JY. Potvin, and E. Taillard, Parallel tabu
search for real-time vehicle routing and dispatching, Transportation
Science, vol. 33(');
INSERT INTO posts (postId,userId,title,body) VALUES (387,7685,'Tackling Dynamic Vehicle Routing Problem with Time Windows by means of Ant Colony System (part 17)','4), pp. 381-390, 1999.
[11] G. B. Alvarenga, R. M. de Abreu Silva, and G. R. Mateus, A hybrid
approach for the dynamic vehicle routing problem with time windows,
in Fifth International Conference on Hybrid Intelligent Systems, pp. 61-
67, 2005.
[12] J. de Armas, and B. Meli an-Batista, Constrained dynamic vehicle
routing problems with time windows, Soft Computing, vol. 19(9), pp.
2481-2498, 2015.
[13] L. Hong, An improved LNS algorithm for real-time vehicle routing
problem with time windows, Computers '||'&'||' Operations Research, vol.
39(2), pp. 151-163, 2012.
[14] B. van Veen, M. Emmerich, Z. Yang, T. B ack, and J. Kok, Ant
colony algorithms for the dynamic vehicle routing problem with time
windows, in Natural and Articial Computation in Engineering and
Medical Applications, vol. 7931, Springer, pp. 1-10, 2013.
[15] Z. Yang, J. P. van Osta, B. van Veen, R. van Krevelen, R. van Klaveren,
A. Stam, J. Kok, T. B ack, and M. Emmerich, Dynamic vehicle routing
with time windows in theory and practice, Natural Computing, pp. 1-16,
2016.
[16] Z. Yang, M. Emmerich, and T. B ack, Ant based solver for dynamic
vehicle routing problem with time windows and multiple priorities, in

2015 IEEE congress on evolutionary computation (CEC), pp. 2813-2819,
2015.
[17] M. Dorigo, and L. M. Gambardella, Ant Colony System: A Coop-
erative Learning Approach to the Traveling Salesman Problem, IEEE
Transactions on Evolutionary Computation, vol. 1(1), pp. 53-66, 1997.
[18] M. Dorigo, V. Maniezzo, and A. Colorni, The Ant System: Opti-
mization by a Colony of Cooperating Agents, IEEE Transactions on
Systems, Man, and Cybernetics, Part B, vol. 26(1), pp. 29-41, 1996.
[19] M. Solomon, Algorithms for the vehicle routing and scheduling prob-
lems with time window constraints, Operations Research, vol. 35(2),
pp. 254-265, 1987.
[20] L. M. Gambardella, and M. Dorigo, Solving symmetric and asymmetric
TSPs by ant colonies, in Proceedings of the 1996 IEEE International
Conference on Evolutionary Computation, pp. 622-627, 1996.
[21] M. W. P. Savelsbergh, The vehicle routing problem with time windows:
minimizing route duration, Informs Journal on Computing, vol. 4(2),
pp. 146-154, 1992.

');
INSERT INTO posts (postId,userId,title,body) VALUES (388,9938,'Vico-Greengard-Ferrando quadratures in the tensor solver for integral equations','V. Khrulkov1 , M. Rakhuba1 , I. Oseledets1,2

1Skolkovo Institute of Science and Technology, Russia
2 Institute of Numerical Mathematics, Russia

Abstract Convolution with Greens function of a dierential operator appears in a lot of
applications e.g. Lippmann-Schwinger integral equation. Algorithms for computing such are
usually non-trivial and require non-uniform mesh. However, recently Vico, Greengard and Fer-
rando developed method for computing convolution with smooth functions with compact support
with spectral accuracy, requiring nothing more than Fast Fourier Transform (FFT). Their ap-
proach is very suitable for the low-rank tensor implementation which we develop using Quantized
Tensor Train (QTT) decomposition.

1. INTRODUCTION

h(r(cid:48) ) =

(cid:90)
In this paper we propose an algorithm for computing an approximation to a convolution
g(r  r(cid:48) )(r)dr,
R3
where G is a continuous Greens function of some PDE. It has been shown recently in [1] that
discrete approximation to h yieding spectral accuracy for compactly supported smooth functions 
can be constructed using relatively simple idea. Idea is based on the fact that continuous Greens
 (R3 ). Apply-
function can be replaced by the truncated one, Fourier transform of which is in C
ing convolution theorem one gets answer as a Fourier type integral of a rapidly decreasing function
 (R3 ) which is then discretized on ner grid and evaluated using Discrete Fourier Transform
from C
(DFT). This scheme achieves spectral order of accuracy due to the superalgebraic approximation of
the continuous Fourier transform by DFT and has been shown to also give second order with small
constant for continuos functions. We implemented this scheme using data compression via Quan-
tized Tensor Train (QTT). Approach is straightforward  we compute discrete truncated Greens
function using formulas from [1] and convert it to QTT-format. Then we use standard algorithms
in QTT which allow for logarithmic storage with respect to the grid size and accuracy of approxi-
mation. Moreover, discrete convolution and solution of integral equations are then performed also
with logarithmic complexity by the algorithms described in [2],[3]. Paper is organized as follows:

 We briey describe the algorithm developed in Section 2
 We present necessary denitions and algorithms from TT theory in Section 3.1
 We describe TT implementation of the algorithm in Section 3.2
 We present numerical results in Section 4
The main reference for Vico-Greengard-Ferrando quadrature is the original paper [1], in the pre-
sentation of the algorithm we follow [4].

2. ALGORITHM
12rA5 A.c 196047:ir
We wil present the scheme described in [1] in a way which is most suitable for transitioning to
(cid:90)
TT-format. Let (r) be a smooth function such that supp   D = [0, L1 ]  [0, L2 ]  [0, L3 ] and we
are interested in computing
g(r  r(cid:48) )(r(cid:48) )dr(cid:48) ,
R3

h(r) =

(1)

2

R3

L = (cid:112)L2
where g(r) is a Greens function of some dierential operator. Let us assume that g(r) depends
only on r = |r|. If we seek restriction of the solution h(r) to D then |r  r (cid:48)
| in (1) doesnt exceed
(cid:90)
(cid:90)
3 . Thus if we replace g(r) by gL (r) = g(r) rect r
1 + L2
2 + L2
2L ,
g(r  r(cid:48) )(r(cid:48) )dr(cid:48) =
gL (r  r(cid:48) )(r(cid:48) )dr(cid:48) .
R3
Advantage of using gL (r) is that since it has compact support its Fourier transform GL (s) is in
C (R3');
INSERT INTO posts (postId,userId,title,body) VALUES (389,9938,'Vico-Greengard-Ferrando quadratures in the tensor solver for integral equations (part 2)',' ), and is straightforward to compute for many dierential operators. We mainly focus on
Helmholtz dierential operator 2 + k2 for which the following formula holds:
(cid:90)
(cid:17)
(cid:16) r
eikrg(r) =
4r
GL (s) =
R3
2L
= 1 + eiLk (cos Ls  i k
s sin Ls)
(k  s)(k + s)
(r) by (cid:98)(s) we obtain the nal formula:
(cid:17)3 (cid:90)
It is easy to check that GL (s) is indeed smooth and nonsingular. If we denote Fourier transform of
(cid:16) 1
eisr (cid:98)(s)GL (s)ds.
(3)
h(r) =
R3slightly oscillatory behavior of (cid:99)GL (s) zero padding by a factor of at least 3 is required (we will use
This integral is then discretized using trapezoidal rule and computed using DFT. However to cancel
factor 4 to keep grid size being a power of 2 - for the analysis see [4]). Suppose that domain D is
discretized using uniform grid with Ni nodes in corresponding dimensions and function  is sampled
on this grid yielding an array ij k i.e.

eisrdr

g(r) rect

(2)

ij k := (ih1 , jh2 , kh3 ),

hi =

Li
Ni

Algorithm then is summarized as following
1: Zero pad ijk by a factor of 4 and compute 3d FFT dening (cid:98)(s) for s = 
Algorithm 1 Basic Vico-Greengard-Ferrando quadrature
2: Evaluate GL (s) for L = (cid:112)L2
, s2
, s3
2 ( s1
3 for s dened above, and multiply elementwise by (cid:98)(s).
L3
L2
L1
{2Ni , . . . , 2Ni  1}.
1 + L2
2 + L2
3: Perform 3d IFFT on the array dened above and truncate the result keeping rst N1  N2  N3 entries,
obtaining approximation to h(r) on the grid.

), si 

hij k =

where

2.1. Convolution form of the algorithm
2N11(cid:88)
2N21(cid:88)
2N31(cid:88)
Let us write Algorithm 1 more explicitly. All steps together can be represented as follows
GL (s) (cid:98)(s)e2 i s1 i14N2 e2 i s3 k
4N1 e2 i s2 j
4N3 ,
4N1
4N2
4N3
s2=2N2
s1=2N1
s3=2N3
4N11(cid:88)
4N31(cid:88)
4N21(cid:88)
(cid:98)(s) =
i(cid:48) j (cid:48) k(cid:48) e
j (cid:48)=0
i(cid:48)=0
k(cid:48)=0
4N11(cid:88)
4N21(cid:88)
4N31(cid:88)
By plugging (5) into (4) and by changing the order of summation it is easy to see that
j (cid:48)=0
i(cid:48)=0
k(cid:48)=0

GM
ii(cid:48) ,jj (cid:48) ,kk(cid:48) i(cid:48) j (cid:48) k(cid:48) ,

2 i s2 j (cid:48)
4N2 e

2 i s1 i(cid:48)
4N1 e

2 i s3 k(cid:48)
4N3 .

hij k =

(4)

(5)

(6)

where

GM
ii(cid:48) ,jj (cid:48) ,kk(cid:48) =

2N31(cid:88)
s3=2N3

2N21(cid:88)
2N11(cid:88)14N1
4N2
4N3
s2=2N2
s1=2N1
 N2 , k (cid:48)
 N1 , j (cid:48)
Moreover, since i(cid:48) j (cid:48) k(cid:48) is 0 for i(cid:48)
 N3 (see step 1 of Algorithm 1) and we truncate
N31(cid:88)
N21(cid:88)
N11(cid:88)
the result, formula (6) simplies and nally:
j (cid:48)=0
i(cid:48)=0
k(cid:48)=0

GL (s)e2 i s1 (ii(cid:48) )
4N2 e2 i s3 (kk(cid:48) )
4N1 e2 i s2 (jj (cid:48) )
4N3

hij k =

GM
ii(cid:48) ,jj (cid:48) ,kk(cid:48) i(cid:48) j (cid:48) k(cid:48) .

.

(7)

(8)

We see that (8) takes the form of a discrete aperiodic convolution with discrete Greens function
GM (which we will call mol lied Greens function ). One can notice that to fully determine GM
it is sucient to run Algorithm 1 once for a special right hand side ij k = i0 j 0 k0 . Formula (8)
plays essential role in the further analysis. Multiplication by multilevel Toeplitz matrix generated
by GM can be performed with logarithmic complexity in QTT format as described in [2], and we
discuss neccessary denitions and algorithms in the next section.

3. LOW-RANK TENSOR APPROACH

3.1. TT and QTT formats
To understand the QTT format let us start with describing the TT-format, which is a nonlinear low-
paramentric representation of multidimensional arrays, called tensors. Tensor X  Cn1n2nd is
said to be in the TT-format if it represents as

Xi1 i2 ...id = X (1) (i1 )X (2) (i2 ) . . . X (d) (id ),
(9)
where X (k) (ik )  Crk1rk , r0 = rd = 1, ik = 1, . . . , nk . Matrices X (k) are called TT-cores and rk
are called TT-ranks. Notice that if r = maxk rk is small, then there is a signicant compression to
store X . Indeed, initial tensor requires storing');
INSERT INTO posts (postId,userId,title,body) VALUES (390,9938,'Vico-Greengard-Ferrando quadratures in the tensor solver for integral equations (part 3)',' nd parameters, while to store its TT-representation
only O(dnr2 ) parameters are needed.
In fact, one could use TT representation to store and to work with arising in Algorithm 1 3-
dimensional arrays. However we will use a more sophisticated approach called QTT format, which
allows for additional storage reduction compared to TT. QTT format is the following modication
of the TT format. First we assume that d = 3, ni = 2li , i = 1, 2, 3. Then each physical index i,
j , k is represented in the binary format, i.e.
i = i1 + 21 i2 +    + 2li1 id ,
and we have initial tensor ij k encoded as a (l1 + l2 + l3 )-dimensional array :

im = 0, 1, m = 1, . . . , d

ij k  i1 ...il1 j1 ...jl2 k1 ...kl3
TT decomposition of  is called the QTT decomposition. The storage of the QTT decomposition is
O(r2 (l1 + l2 + l3 )) = O(r2 log n), so if r = maxi ri is bounded, the total storage scales logarithmically.
In practice tensors of exact low-rank rarely occur. Typically one xes accuracy  and tries to nd
best approximation with this accuracy. It has been shown that in some applications ranks grow as
r = O(log 1 ),  > 1 [5, 6].
3.2. Translation of the algorithm to the QTT format
Computation of kernel GM . To use (8) we rst need to nd GM (7). For this purpose we run
Algorithm 1 for ij k = i0 j 0 k0 . Precomputations are done in the full format, in other words we
form the whole dense tensor GM and utilize TT-SVD algorithm [7] to nd its QTT representation.
TT-SVD algorithm is based on the computation of SVD decompositions of tensors reshaped into
full 2D matrices and therefore is quite expensive.
In principle one could use DFT in the QTT
format [8] to avoid forming full tensors. Unfortunately, we found that intermediate tensors arising
in Algorithm 1 are of large rank. We will address this problem in our future work.

4

Computation of . Tensor  can be already given in the QTT representation. This can happen,
e.g. if we are running a certain iterative process involving computation of convolution (1) and all
operations in this process are done within the QTT format. Otherwise, ij k can be approximated
with logarithmic complexity by using the cross approximation method [9], which adaptively samples
elements of a tensor. In this case we just need ij k be given as a function which returns value by
given 3 indices i, j, k .

Computation of convolution GM  . Next goal is to nd convolution of tensors GM and  (8).
The convolution can be considered as a multiplication of multilevel Toeplitz matrix generated by
GM and vector . Matrices can also be represented in the TT and by analogy in the QTT format.
(cid:88)
The denition is similar to that of TT-tensor: given matrix (operator) Ai1 ...id j1 ...jd , which acts on
vector Xj1 ...jd such that
Ai1 ...id j1 ...jd Xj1 ...jd .
Yi1 ...id =
j1 ...jd

its TT-decomposition is dened as
Ai1 ...id j1 ...jd = A(1) (i1 , j1 )A(2) (i2 , j2 ) . . . A(d) (id , jd ),
where A(k) (ik , jk )  CRk1Rk , R0 = Rd = 1, ik = 1, . . . , nk . QTT decomposition of 3D operator
GM
ii(cid:48) ,jj (cid:48) ,kk(cid:48) is dened by analogy with the QTT decomposition of a tensor  we quantize indices
i, j, k and i(cid:48) , j (cid:48) , k (cid:48) , group them pairwise and then compute TT decomposition:
1 ) . . . G(l1+2) (il1+2 , i(cid:48)
ii(cid:48) ,jj (cid:48) ,kk(cid:48) =G(1) (i1 , i(cid:48)
GM
l1+2 )
1 ) . . . G(l1+l2+4) (jl2+2 , j (cid:48)
G(l1+3) (j1 , j (cid:48)
l2+2 )
1');
INSERT INTO posts (postId,userId,title,body) VALUES (391,9938,'Vico-Greengard-Ferrando quadratures in the tensor solver for integral equations (part 4)',' ) . . . G(l1+l2+l3+6) (kl3+2 , i(cid:48)
G(l1+l2+5) (k1 , k (cid:48)
l3+2 ).
We use approach from [2] and analytically construct QTT representation of the induced multilevel
Toeplitz matrix GM
ii(cid:48) ,jj (cid:48) ,kk(cid:48) given QTT representation of GM
i,j,k . Then matrix-vector product (8)
can be done in dierent ways. We used optimization procedure AMEn (alternating minimal energy
method) [10, 11] which allows for rank adaptation compared to standard ALS (alternating least
squares) [12] optimization which works with the representation of a given size.

4. NUMERICAL EXPERIMENTS
Approximating GM using QTT. Firstly we show that using QTT representation greatly reduces
number of degrees of freedom (DOF) of GM . Suppose that tensor X is given in the QTT format
d(cid:88)
with ranks {r1 , . . . , rd}. Then it is easy to count total number of DOF of X :
i=2

DOF(X ) = 2(r1 + rd ) +

2ri1 ri .

By applying this formula to GM computed as described in 3.2 for various values of k (while keeping
L = 1) we obtained the following results (see Figure 1). This shows advantages of using QTT.
Solving scattering problems. To further test our approach we solve the Lippmann-Schwinger
(cid:90)
equation which is used for solving scattering problems:
eik|rr(cid:48) |
(r(cid:48) )dr(cid:48) = k2 q(r)inc ,
|r  r(cid:48)
(cid:90)
R3eik|rr(cid:48) |
(r(cid:48) )dr(cid:48) .
scat =
|r  r(cid:48)
R3We used rounding by  = 107 in our computations. Firstly we xed k = 1 and L = 32 and took
q(r) to be a 3D gaussian:
|rr(cid:48) |2
2a2

and then we nd

(r) + k2 q(r)

q(r) = e

(10)

(11)

5

(a)
(b)
Figure 1: DOF of GM in 2D (1a) and 3D (1b) for  = 107 and L = 1
Table 1: Relative error (err) and eective rank (erank) for dierent grid sizes and rounding errors . Results
are presented for two types of function q : Gaussian (11) and smoothed cube (12).

3D grid size

 = 103
 = 105
 = 107
 = 103
 = 105
 = 107

Gaussian

Smoothed cube

with r(cid:48) = (cid:0) L
2 , L
2 , L
(cid:1) , and a = L
10 , and

323
erank
14
24
30
22
29
29

err
8e-1
8e-1
8e-1
8e-1
8e-1
8e-1

643
erank
15
25
34
35
57
73

err
8e-2
4e-4
5e-6
8e-2
5e-4
2e-5

1283
erank
14
26
38
34
59
80

err
7e-2
4e-4
4e-6
6e-2
5e-4
6e-6

To solve arising systems in the QTT format we used AMEn [10, 11] , which allows for rank
adaptation. Taking the solution computed on a grid with size 2563 as a reference we measured

inc (x, y , z ) = eix .

(a) Section z = 16 .

(b) Section x = 17.5 .

Figure 2: Scattering on the Gaussian with standard deviation 3.2 in D = [0, 32 ]  [0, 32 ]  [0, 32 ] for
k = 1. Eective rank of the solution is equal to 39.

relative error of the solutions computed on smaller grids for various rounding parameters . Results
are given in the Table 1. We see that error is roughly equal to the  even for modest number of
grid nodes per wavelength.

28292102112121Dgridsize104105106107DOFUncompressedk=2k=22k=42k=62k=82252627281Dgridsize105106107DOFUncompressedk=2k=22k=42k=62k=82020406080100x020406080100y6.604.953.301.650.001.653.304.956.60020406080100y020406080100z1.000.750.500.250.000.250.500.751.001.25As a next experiment we performed the same computations for q(r) representing smoothed cube:
(cid:16) |rr(cid:48) |
(cid:17)8
(cid:1) and a = L
for r(cid:48) = (cid:0) L2 , L
2 , L
4 . For the results see Figure 3.
0.5
q(r) = e

6

(12)

(a) Section z = 16 .

(b) Section x = 17.5 .

Figure 3: Scattering on the smoothed cube of size 8 in D = [0, 32 ]  [0, 32 ]  [0, 32 ] for k = 1');
INSERT INTO posts (postId,userId,title,body) VALUES (392,9938,'Vico-Greengard-Ferrando quadratures in the tensor solver for integral equations (part 5)','. Eective
rank of the solution is equal to 81.

Scattering problems on quasiperiodic structures. As a nal test we took q(r) to be a periodic
grid of smoothed cubes (structures like this are extremely suitable for QTT computations). Namely,
denoting q(r) dened by formula (12) by qr(cid:48) ,a (r) we solve the equation (10) in the domain [0, 2] 
1(cid:88)
3(cid:88)
19(cid:88)
[0, 1]  [0, 10] on the grid 64  64  1024 for q(r) dened as
j=0
i=1
k=0

q(0.25+0.5j,0.25+0.5k,0.5i),0.1 (r)

q(r) =

and for k = 4 . inc in this case is a plane wave propagating in z -direction

Figure 4 demonstrates our results.

inc (x, y , z ) = e4 iz .

Figure 4: Scattering on the periodic cube structure in D = [0, 2]  [0, 1]  [0, 10] for k = 2 . Eective rank
of the solution is equal to 67. Section x = 1.

020406080100x020406080100y432101234020406080100y020406080100z2.752.201.651.100.550.000.551.101.6502468z0.00.5y2.41.81.20.60.00.61.21.82.47

Acknowledgements

This study was supported by the Ministry of Education and Science of the Russian Federation
(grant 14.756.31.0001), by RFBR grants 16-31-60095-mol-a-dk, 16-31-00372-mol-a and by Skoltech
NGP program.

REFERENCES
1. F. Vico, L. Greengard, and M. Ferrando, Fast convolution with free-space Greens functions,
Journal of Computational Physics, vol. 323, pp. 191203, 2016.
2. V. Kazeev, B. Khoromskij, and E. Tyrtyshnikov, Multilevel Toeplitz matrices generated by
tensor-structured vectors and convolution with logarithmic complexity, SIAM J. Sci. Com-
put., vol. 35, no. 3, pp. A1511A1536, 2013.
3. I. V. Oseledets, Approximation of 2d  2d matrices using tensor decomposition, SIAM J.
Matrix Anal. Appl., vol. 31, no. 4, pp. 21302145, 2010.
4. L. a. Klinteberg, D. S. Shamshirgar, and A.-K. Tornberg, Fast Ewald summation for free-space
Stokes potentials, arXiv preprint arXiv:1607.04808, 2016.
5. V. Kazeev and C. Schwab, Quantized tensor-structured nite elements for second-order ellip-
tic pdes in two dimensions, tech. rep., SAM research report 2015-24, ETH Zurich, 2015.
6. V. Kazeev, I. Oseledets, M. Rakhuba, and C. Schwab, QTT-nite-element approximation for
multiscale problems I: model problems in one dimension, Adv. Comp. Math., 2016.
7. I. V. Oseledets, Tensor-train decomposition, SIAM J. Sci. Comput., vol. 33, no. 5, pp. 2295
2317, 2011.
8. S. V. Dolgov, B. N. Khoromskij, and D. V. Savostyanov, Superfast Fourier transform using
QTT approximation, J. Fourier Anal. Appl., vol. 18, no. 5, pp. 915953, 2012.
9. I. V. Oseledets and E. E. Tyrtyshnikov, TT-cross approximation for multidimensional arrays,
Linear Algebra Appl., vol. 432, no. 1, pp. 7088, 2010.
10. S. V. Dolgov and D. V. Savostyanov, Alternating minimal energy methods for linear systems
in higher dimensions. Part I: SPD systems, arXiv preprint 1301.6068, 2013.
11. S. V. Dolgov and D. V. Savostyanov, Alternating minimal energy methods for linear systems
in higher dimensions. Part II: Faster algorithm and application to nonsymmetric systems,
arXiv preprint 1304.1222, 2013.
12. S. Holtz, T. Rohwedder, and R. Schneider, The alternating linear scheme for tensor opti-
mization in the tensor train format, SIAM J. Sci. Comput., vol. 34, no. 2, pp. A683A713,
2012.

');
INSERT INTO posts (postId,userId,title,body) VALUES (393,8261,'Sample title','                              Abstract    
Existing  memory  management  mechanisms  used  in  commodity 
computing  machines  typically  adopt  hardware  based  address 
interleaving  and  OS  directed  random  memory  allocation  to 
service  generic  application  requests.  These  conventional 
memory  management  mechanisms  are  challenged  by  contention 
at  multiple  memory  levels,  a  daunting  variety  of  workload 
behaviors,  and  an  increasingly  complicated  memory  hierarchy. 
Our  ISCA-41  paper  proposes  vertical  partitioning  to  eliminate 
shared  resource  contention  at  multiple  levels  in  the  memory 
hierarchy.  Combined  with  horizontal  memory  management 
policies,  our  framework  supports  a  flexible  policy  space  for 
tackling  diverse  application  needs  in  production  environment 
and is suitable for future heterogeneous memory systems. 
1. Introduction 
Efficiently  utilizing  shared  resources  in  the  memory  hierarchy 
such as Last Level Cache (LLC) and main memory  is at  the core 
of  constructing  high  performance  multi-core  machines.  To  date, 
the  most  common  mechanism  of  memory  and  cache  sharing 
used  in memory  controllers  in  commodity machines  is  based  on 
generic  address  interleaving,  where  the  physical  address  of  a 
memory request determines which LLC set and DRAM bank  the 
request  is  serviced.  Previous  studies  [1,3,7,8]  indicate  that  this 
simple 
approach 
can 
cause 
significant  memory/cache 
interference  as  multiple  threads  share  the  same  DRAM  banks 
and  cache  sets.  Additionally,  this  approach  is  entirely  oblivious 
to  application  and  architecture  characteristics,  thus  fail  to 
efficiently  use  memory  resource  on  modern  computing  systems 
with increasing diversity and heterogeneity. 
1.1 Challenges for Existing Memory Management   
Existing  memory  management  approaches  [6,8,9,11]  typically 
focus  on  horizontally  optimizing  a  single  level  in  the  memory 
hierarchy and have the following drawbacks:   
(1)  Contention  at  different  memory  hierarchy:  Shared 
resources  (i.e.,  LLC  and  DRAM)  by  multiple  threads  lead  to 
contention  at  multiple  levels  in  the  memory  hierarchy.  Past 
efforts  [7,8,9]  focus  on  horizontally  partitioning  and  managing 
LLC  or  DRAM  banks  to  minimize  contention  at  a  single  level. 
However,  the  contention  problem  has  never  been  addressed  for 
all  levels  in  the  memory  hierarchy  (except  per-core  private 
caches  that  do  not  suffer  from  inter-thread  interference)  at  the 
same  time.  To  completely  eliminate  interference  in  the  memory 
hierarchy,  a  new  approach  is  needed  to  vertically  combine 
contention elimination techniques on multiple levels. 
(2)  Single  policy  management:  Existing  memory  management 
in  operating  system  (OS)  is  largely  single  policy  based,  which 
fails  to  support  flexible  and  effective  memory  allocation  with 
respect  to  different  applications  sharing  and  capacity  needs.  As 
disparate  non-volatile  memory  technologies  are  emerging  and 
evolving  to  more  sophisticated  and  hybrid  memory  systems 
[10,17],  adaptive  and  reconfigurable  policies  are  needed  to 
manage  the  heterogeneity  in  terms  of  retention,  access  speed, 
fault  tolerance,  and  energy  efficiency.  In  such  heterogeneous 
memory  environments,  single  policy  management  can  result  i');
INSERT INTO posts (postId,userId,title,body) VALUES (394,8261,'Sample_title (part 2)','n 
significant resource underutilization.  
(3)  Application obliviousness:  Emerging   workloads   contain 

*  I  wish  to  extend  my  deep  thanks  to  Yong  Li  (Pitt  '||'&'||'  VMware  CA),  Prof. 
Chen Ding (Rochester) and Prof. Xiaodong Zhang (Ohio) for their efforts. 
The  first  step  work  is  published  in  ISCA-14  (Corresponding  author:  Lei  Liu). 
Title:  Going  Vertical  in  Memory  Management:  Handling  Multiplicity  by 
Multi-Policy.  
 
Figure  1.  Address  mapping  from  the  view  of  OS  and  three 
categories of color bits on a typical multicore machine. 
numerous  applications  and  exhibit  diverse  and  dynamic 
behaviors. Our results demonstrate  that servicing all applications 
using  one  simple  generic  policy  in  a  program-oblivious  way 
often  results  in  inter-program  perturbation,  resources  thrashing, 
poor  memory/cache  utilization,  and,  consequently,  degraded 
performance.  Therefore,  an 
intelligent  system 
that  can 
understand  application  behaviors  is  extremely  important, 
especially in complicated computing environments such as cloud 
and  data  center,  where  a  large  number  of  applications  are 
running and sharing memory resources [1,6,16,18]. 
1.2 Going Vertical in Memory Management 
Our  ISCA-2014  paper  proposes  a  novel  solution  to  meet  the 
aforementioned  challenges.  (1)  To  address  the  contention 
problem  in  the  entire  memory  space,  we  expose  architecture 
features  (i.e.  physical  address  mapping  for  cache  sets  and 
DRAM  banks)  to  OS  by  utilizing  different  types  of  addressing 
bits.  Based  on  these  architecture  features,  our  ISCA-2014  paper 
introduces Vertical Partitioning  (VP)  [10]  into modern OS. VP 
partitions  the  memory  hierarchy  vertically  through  cache  and 
DRAM  simultaneously  to  completely  eliminate  the  contention 
issue  for  all  susceptible  levels  in  the  memory  hierarchy.  (2) 
Combined with horizontal partitioning mechanisms (i.e, bank- or 
cache-only  partitioning),  we  group  the  memory  partitioning 
policies  into  several  categories  to  form  a  memory  management 
policy  space  for  diverse workloads  to  choose  from. This  enables 
flexible  and  customized  resource  allocation  that  meets  each 
individual applications memory and cache resource requirement. 
(3) By  dynamically  monitoring  applications  page-table  using  a 
low-overhead  algorithm, we  equip  the OS with  the  capability  of 
understanding  applications  memory  behavior  on  the  fly. 
Combining  all  these  components  and  a  large  set  of  experimental 
results  conducted  in  real  systems,  we  devise  an  intelligent 
memory  management  mechanism  that  can  choose  appropriate 
allocation policy for workloads with arbitrary characteristics.    
2. Solution 
2.1 Vertical and Horizontal Partitioning 
In  memory  controllers  (MC),  address  mapping  policies  are 
determined by platform and configurations. Typically, beside the 
bits that only index DRAM banks (B-bits) and LLC sets (C-bits), 
there  are  also  some  bits  that  denote  both  (noted  as  O-
bits).   Figure  1  illustrates  the  three  categories  of  coloring  bits on 
a  mainstream  machine  (Intel  i7-860  with  8GB  memory  and  64 
banks, B-bits: 21~22; C-bits: 16~18; O-bits: 14~15 [8,9,10]). 
   We  observe  that  by  consi');
INSERT INTO posts (postId,userId,title,body) VALUES (395,8261,'Sample_title (part 3)','dering  different  number  of  bits  of 
various  types  when  allocating  a  memory  page,  OS  can  exploit 
diverse  memory allocation  approaches. As  shown  in  Table  1, 
bank-only  partitioning  and  interleaving  based  page  allocation 
can  be  derived  by  considering  B-bits  when  allocating  a  page 
number  to  an  application. Particularly,  the O-bits  enable  vertical 
partitioning  that  partitions  both  LLC  sets  and  DRAM  banks 
vertically  through  the  memory  hierarchy.  We  further  derive 
several  sub-policies  (i.e. ,   A/B/C-VP)   by  choosing   different 

4/8-core 

4-core 

8-core 

8-core 

4/8-core 

B-VP 

C-VP 

A-VP 

O-bits {14~15} 

Target Cores 

Coloring Bits 
Policy  
Interleaving  B-bits {21~22} 
O-bits {14~15} 
Bank-Only  B-bits {21~22} 
O-bits {15} 

Description   Bank-Interleaving w/  
 random page allocation 
LLC  2 groups 
  Banks  8 groups 
     LLC  4 groups 
     Banks  4 groups 
     LLC  4 groups 
B-bits {22} + 
  Banks  8 groups 
O-bits {14~15} 
LLC  8 groups 
C-bits {16} +  
  Banks  4 groups 
O-bits {14~15} 
Table 1. Several representative partitioning policies 
combinations  of  O-,  B-  and  C-  bits  to  partition  memory/cache 
into different  quotas.   Each policy  represents  one  resource  usage 
characteristic  and  has  its  own  friendly  (suitable)  workloads, 
which performs better on this policy than on any other policies. 
2.2 Understanding Memory Features 
2.1.1 Key Observations and Classification   
Based  on  numerous  experiments  across  over  200  workloads  in 
real  system  (Figure  3),  we  find  that  compared  to  DRAM,  the 
amount of available cache resource has a much greater impact on 
application  performance.  Thus,  we  classify  applications  into 
four categories:  Core  Cache  Fitting  (CCF),  LLC  High  (LLCH), 
LLC  Middle  (LLCM)  and  LLC  Thrashing  (LLCT),  based  on 
their  performance drop caused  by  cache  quota  reduction  from 
8/8 (entire cache) to 1/8.  CCF applications (e.g., hmmer), do not 
degrade  significantly  when  using  fewer  LLC  resources  as  their 
working  sets  fit  into  the  L1  and  L2  private  caches.  LLCT 
applications,  such  as  libquantum,  are  also  insensitive  to  cache 
quotas,  but  due  to  cache  thrashing  behavior  rather  than  small 
working  set  sizes.  LLCH  applications  such  as  mcf  suffer  the 
worst  performance  degradations  from  reduced  cache  quotas  due 
to their large resource requirements. 
2.1.2 Dynamic Application Classification  
To enable online optimization, we need to classify an application 
based  on  its  run-time  characteristics.  We  found  the  number 
of hot  pages  (active  pages  used  in  a  particular  time  interval 
and can be  identified by  the access bit  in PTE) can reflect an 
applications  LLC  demand  due  to  the  DRAM  row-buffer 
locality.   Based  on  this  insight,  we  devise  a  classification 
algorithm  implemented  as  two  kernel  tasks,  JOB1  and  JOB2, 
which  sample  the  page  access  patterns  periodically.  JOB1  is 
responsible  for  collecting  the  number  of  hot  pages  by  clearing 
the  access_bit  and  examining  pages  with  access_bit  =1  at  the 
end  of  each  sampling  period.  JOB2  uses  an  array  of  page  access 
counters  to  record  the  number  of  accesses  for  each  page.  JOB2 
groups  the  counter  values  into  ranges  and  computes  a  weighted 
page  distribution  (WPD)  to  reflect  page  reference  locality.  Our 
algorithm  can  accurately  classify  applications  into  one  of  the 
four  categories  detailed  in  Section  2.1.1  by  comparing  ');
INSERT INTO posts (postId,userId,title,body) VALUES (396,8261,'Sample_title (part 4)','the 
number of hot pages and WPD with trained thresholds.  
2.3 Memory Allocation Policy Selection 
We  adopt  a  data  mining  approach  to  quantitatively  study  the 
impacts  of  various  memory  allocation  schemes  on  over  2000 
workloads.  We run  each  workload  with  different  policies  and 
record 
the  performance 
improvements  achieved  by 
the 
corresponding  policies.  Based  on  the  correlation  between  the 
classification  and  performance  gains  on  different  policies,  we 
create a set of rules to select the policy of vertical management. 
2.3.1 Partitioning Rules 
First, most workloads  that contain at  least one LLCT application 
perform  best  on  A/C-VP.  Second,  a  dominating percentage  of 
workloads  containing  LLCH  but  not  LLCT  perform  best  on 
bank-only  partitioning.    Third,  most  workloads  with  LLCM  but 
no LLCT or LLCH applications achieve best performance results 
with a modest cache  partitioning scheme such  as A-VP  and  B- 

Figure 2. Memory allocation policy decision tree (PDT) 

  

  

  

  
Figure  3.  Performance  improvement  of  various  polices  for  214 
workloads (A/B/C-VP are memory policies introduced in Table 1). VP. Based on  the  rules  and  their priorities  relative  to  each other, 
we generate  a memory management policy decision  tree  (PDT) 
shown  in  Figure  2.  The  PDT  is  useful  for  choosing  appropriate 
policies  for  diverse  workloads.  Moreover,  for  multi-threaded 
workloads, Park et al. [12] argues that a random page-interleaved 
allocation  scheme  (Table  1)  outperforms  other  schemes.  Thus, 
we add this policy to handle multi-threaded workloads in PDT. 
2.3.2 Coalescing Rules 
Despite  the  advantage  in  eliminating  interference,  a  pure 
partitioning  based  approach  is  not  always  preferable  since  it 
limits  the  available  resource  and  can  harm  the  performance  for 
resource  hungry  applications  (e.g.,  LLCH).  Thus,  we  extend  the 
PDT with  several coalescing  rules  that  can  be  used  to merge  the 
partitioned  resource  quotas  among  certain  types  of  applications. 
Using  the  data  mining  approach,  we  find  the  LLCH  and  LLCM 
applications  should  be coalesced  together  to  share  the  cache 
quota, while LLCT  and CCF  applications  should  be coalesced 
respectively to share a small cache quota. 
3. Experimental Results  
We  implement  PDT  with  partitioning  and  coalescing  rules  in 
Linux  kernel.  Extensive  experiments  show  that  our  framework 
outperforms  the  unmodified  Linux  kernel  and  achieves  up  to  11% 
performance  gains  over  prior  techniques.  Our  results  also  show 
that  higher  performance  can  be  achieved  by  adaptively  choosing 
workloads  best-fit  memory  policy.  Illustrated  in  Figure  3,  half 
of  the  workloads  fall  into  region  4,  meaning  their  performance 
degrade  in  shared  memory  conditions  where  cache  resource  is 
being  limited/partitioned, but bank partitioning can benefit  them. 
By contrast, some workloads  in region 1 can achieve above 10% 
performance gains via x-VP to eliminate interference. 
4. Future Impact  
DRAM  and  cache technology has  been undergoing remarkable 

changes.  In  contrast  to  the  fast-paced  changes  in  the  memory 
hierarchy, the legacy memory management strategies such as the 
order-based, 
interleaved  memory/cache  allocation  sch');
INSERT INTO posts (postId,userId,title,body) VALUES (397,8261,'Sample_title (part 5)','emes 
adopted 
largely 
in  commodity  OS  and  hardware  remain 
unchanged.  These  existing  strategies  manage  memory  resource 
blindly  in  that  they  are  not  aware  of  the  architecture  features 
and  applications  memory  characteristics,  leading  to  a  more 
generic but  less efficient approach. We develop a practical, cost-
effective  way  to  make  the  OS  conscious  about  running 
applications  and  the  underlying  architecture,  enabling  a  more 
adaptive  and  efficient  way  of  utilizing  memory  resources.  Our 
contributions may have  the  following  long-term  impacts on both 
academia and industry: 
4.1 Academic Impact 
(1)  New  insight  in  memory  optimization:  We  conduct  a 
comprehensive  study  in  page-coloring  based  partitioning.  We 
further  propose  a  new  vertical memory management mechanism 
to  control  the  entire  memory  hierarchy,  and  thus  eliminate  the 
memory  interference  in  the  entire  memory  hierarchy.  Our  key 
insight is the overlapped address bits (O-bits) in physical address 
mapping.  Using  O-bits,  we  enable  memory  and  cache  vertical 
management that significantly mitigates the memory interference 
issue.  Additionally,  our  idea  to  utilize  architecture  features  (O-, 
B-,  C-  address  mapping  bits)  enriches  memory  allocation 
policies  and  enlarges  the  space  of  memory  optimization.  Our 
study  brings  new  opportunities  and  design  patterns  in  the  areas 
of high performance computing and memory architecture. 
(2) New method  for  application-aware  computing/allocation: 
Numerous prior studies demonstrate  that many  important system 
optimizations  cannot  be  achieved without  leveraging  application 
behavior.  In  our  work,  we  devise  an  online  application 
classification method and  implement  it as Linux kernel modules. 
To  the  best  of  our  knowledge,  this  is  the  first  approach  that 
captures  dynamic  application  memory  and  cache  usage  patterns 
in  real  production  settings,  without  the  help  of  hardware  based 
performance  counters,  long-running  simulations,  or  pin-based 
profiling.  The  accuracy  of  our  approach  is  verified  by  off-line 
profiling. Our work opens a new path for researchers  to  leverage 
the knowledge of running workloads for system optimizations.  
(3)  New  perspective  for  designing  future  OS:  To  assist  the 
application-aware  policy  selection  process,  we  studied  a  large 
amount  of  workloads  running  on  different  memory  allocation 
policies.  Using  data  mining  to  analyze  the  results,  we  generate 
partitioning  and  coalescing  rules  used  to  appropriately  partition 
resources  while  allowing  non-interfering  programs  to  live 
together  for  resource  sharing.  Our  large  result  set  provides 
valuable  reference  for  studying 
the 
impact  of  memory 
scheduling/allocation methods on diverse workloads.  In the long 
term,  we  believe  that  our  approach,  including  (1)  and  (2),  will 
motivate  researchers ');
INSERT INTO posts (postId,userId,title,body) VALUES (398,8261,'Sample_title (part 6)',' in  the  related  fields  to  build  more 
intelligent  modern  OS  that  can  understand  and  learn  from 
application behavior  to and adapt  its own behaviors  to maximize 
resource utilization and performance.   
(4)  New  approach  for  hybrid  memory  management.  We 
restructured  the  conventional  memory  strategy  and  firstly 
introduced  the  Vertical  approach.  We  use  this  approach  for 
hybrid  DRAM-NVM  memory  management  and  devise  Memos 
[19,20]  in  Linux  kernel.  Our  first  step  results  show  memos  can 
benefit the hybrid memory performance and the NVM lifetime.  
4.2 Industry Impact    
The  benefits  of  our  proposed  vertical  memory  management  to 
industrial world are multifold. (1) It adds both cache and DRAM 
into  the  OS  management  pool,  and  thus  potentially  benefits  the 
overall  system  performance  by  simultaneously  reducing  cache 
and  DRAM  contention,  a  critical  problem  faced  by  many  cloud 
providers  such  as  Amazon,  Google  and  VMware.  (2)  It 
significantly enlarges  the memory management policy space and 

brings  greater  flexibility  for  diverse  application  needs  in 
commercial data center and production environments. Moreover, 
application memory access and usage patterns are captured using 
a  practical,  page-table  sampling  based  approach  that  only  adds  a 
very low overhead. (3) It helps reduce the energy cost and access 
latency  of  emerging  NVMs.  In  particular,  memory-partitioning 
techniques  is  more  effective  in  NVM  cases  where  row  buffer 
miss  latency  and  energy  is  larger.  (4)  It  segregates  applications 
with  high  latency-sensitivity  versus  those  with  bandwidth-
sensitivity  accesses  (e.g.,  stream-like  application),  thus  ensuring 
better  QoS  and  fairness.  Particularly,  our  partitioning  and 
coalescing  techniques  can  be  used  together  to  handle  dynamic 
workload  changes  in  production  environments,  thus  having  a 
profound  influence  on  efficient  resource  isolation,  virtualization 
and  consolidation,  which  are  critical  and  have  a  significant 
impact on the trend of moving to the cloud.    
    By  restructuring  the  buddy  system  in  Linux  kernel,  we 
implement  the  HVR  framework  as  an  all-in-one  solution  that 
combines  Horizontal,  Vertical  Partitioning  and  Random 
allocation  [12].  We  believe  our  prototype  demonstrates  the 
feasibility of  a more  intelligent memory management  strategy  in 
modern  OS  design  for  addressing  the  emerging  challenges  in 
future  complicated  computing  environments.  It  would  require  a 
minimal  effort  to  port  our  prototype  to  production  settings  to 
benefit diverse commercial workloads. 
References 
[1]  D.  Chandra,  F.  Guo,  S.  Kim,  and  Y.  Solihin.  Predicting  Inter-Thread  Cache 
Contention on a Chip Multi-Processor Architecture. In HPCA-2005. 
[2]  S. Cho  and L.  Jin. Managing  distributed,  shared L2  caches  through OS-level 
page allocation. In MICRO-2006. 
[3]  X.  Ding,  et.al.  SRM-buffer:  an  OS  buffer  management  technique  to  prevent 
last level cache from thrashing in multicores. In EuroSys-2011. 
[4] X. Ding, K. Wang, and X. Zhang. ULCC: a user-level  facility  for optimizing 
shared cache performance on multicores. In PPoPP-2011. 
[5]  M.K.  Jeong,  D.H.  Yoon,  D.  Sunwoo,  et.al.  Balancing  DRAM  locality  and 
parallelism in shared memory CMP systems. In HPCA-2012. 
[6]  Y.  Kim,  et.al.Thread  cluster  memory  scheduling:  Exploiting  differences  in 
memory access behavior. In MICRO-2010. 
[7]  J.  Lin,  et.al.  Gaining  insights  into multicore  cache  partitioning:  Bridging  the 
gap between simulation and real systems. In HPCA-2008. 
[8]  L.  Liu,  et.al.  A  software  memory  partition  approach  for  eliminati');
INSERT INTO posts (postId,userId,title,body) VALUES (399,8261,'Sample_title (part 7)','ng  bank-
level interference in multicore systems. In PACT-2012. 
[9]  L.  Liu  et.  al.  BPM/BPM+:  Software-based  Dynamic  Memory  Partitioning 
Mechanisms  for  Mitigating  DRAM  Bank-/Channel-level  Interferences 
in 
Multicore Systems. In ACM TACO-2014 
[10] L. Liu et. al. Going Vertical in Memory Management: Handling Multiplicity 
by Multi-policy. In ISCA-2014 
[11] W. Mi, X. Feng,  J. Xue,  and Y.  Jia. Software-hardware  cooperative DRAM 
bank partitioning for chip multiprocessors. In NPC-2010. 
[12]  H.  Park,  S.  Baek,  J.  Choi,  D.  Lee,  and  S.H.  Noh,  Regularities  considered 
harmful:  forcing  randomness  to memory  accesses  to  reduce  row  buffer  conflicts 
for multi-core, multi-bank systems. In ASPLOS-2013. 
[13]  S.  Rixner,  W.J.  Dally,  U.J.  Kapasi,  P.  Mattson,  and  J.D.  Owens.  Memory 
access scheduling. In ISCA-2000. 
[14]  L.  Soares,  et.al.  Reducing  the  harmful  effects  of  last-level  cache  polluters 
with an OS-level, software-only pollute buffer. In MICRO-2008. 
[15]  X.  Zhang,  S.  Dwarkadas,  and  K.  Shen.  Towards  practical  page  coloring-
based multicore cache management. In EuroSys-2009. 
[16]  S. Zhuravlev,  S. Blagodurov,  and A.  Fedorova. Addressing  shared  resource 
contention in multicore processors via scheduling. In ASPLOS-2010. 
[17]  J.  Lee  and  H.  Kim.  TAP:  A  TLP-aware  cache  management  policy  for  a 
CPU-GPU heterogeneous architecture. In HPCA-2012. 
[18] M. K. Qureshi et.al. Utility-based cache partitioning: A  low-overhead, high-
performance, runtime mechanism to partition shared caches. In MICRO-2006. 
[19]  L.  Liu,  H.  Yang,  M.  Xie  et.al.  Memos:  A  Full  Hierarchy  Hybrid  Memory 
Management Framework. In ICCD-2016. 
[20]  L.  Liu,  M.  Xie  and  H.  Yang.  Memos:  Revisiting  Hybrid  Memory 
Management in Modern Operating System. In arXiv:1703.07725. 

');
INSERT INTO posts (postId,userId,title,body) VALUES (400,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U','I .

IN TRODUC T ION

Over the past few years, there has been a growing interest in
ubiquitous health monitoring [1-3]. One of the areas which
beneted most from this surge in the interest is elderly care
[4][5]. Today, we see wearable devices that continuously mon-
itor vital health signs [6], track essential tremor in Parkinson
patients [7], generate alarms if there are any falls, and do more.
However, such devices present several challenges: they intrude
with users routine activities, they have to be worn all the time
even during sleep, and they have to be frequently recharged.
With the advances in wireless sensing, it has become possible
to track and localize human motion [8-10]. In this paper, we
explore if we can tap onto these advances to monitor three
basic elderly care activities; i.e., breathing, tremor and falls.
Breathing involves continuous inhale and exhale move-
ments and may be used to study the subjects physiological
state, her stress levels, or even emotions like fear and relief.
Tremor is a movement disorder in which the subject expe-
riences rhythmic shaking of a body part such as hands [7].
It is highly prevalent among older people and although not
life threatening, it causes great inconvenience in social and
daily life settings, such as writing and eating. Falls are also
highly common among the ageing population and detecting
them early is integral to effective interventions and subsequent
treatments.
Despite the ubiquitous nature of wireless networks, a
little work has been done on exploring their suitability for
health applications. This is concerning since the typical health

monitoring techniques are inconvenient; they require contact
with human body, and most of them are intrusive. For example,
current breath monitoring solutions require a chest band [6],
nasal probe [11], or pulse oximeter [12]. Technologies that
are more comfortable such as wrist bands tend to be erro-
neous and unreliable. Tremor and fall monitoring devices are
similarly inconvenient. However, the bigger concern is that
contact devices are not suitable for elderly health care. Using
a technology round the clock may be cumbersome or even
demeaning for the elderly. Worse, they may be in a condition
such as dementia where they cant remember to put on the
device once they have worn it off.
Related literature on target detection and localization fo-
cuses on both active and passive radar techniques. Active
radars employ dedicated transmitters, use high bandwidth and
often require complex antenna arrays to go along with them
[8]. In contrast, passive radars utilize the transmitted signals
in the environment such as WiFi and cellular signals for target
monitoring. Passive radars offer numerous advantages when
compared to active radars, i.e., they are low-cost and covert
due to their receive-only nature, they operate license free due
to no bandwidth requirements, and they offer better Doppler
resolution due to the possibility of higher integrations times.
To the best of the authors knowledge, this is the rst
comprehensive study on elderly-focused health care applica-
tions using passive WiFi sensing. Specically, this research
introduces a vital health wireless device (Wi-Vi) and makes
the following contributions:


Proposes a phase extra');
INSERT INTO posts (postId,userId,title,body) VALUES (401,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U','ction system in 2-D to de-
termine breathing rate, classify essential tremor and
detect falls.
Carries out extensive experiments to study the accu-
racy of elderly care applications in different environ-
ments and conditions.



A. Related Work
Multiple approaches have been proposed for human mo-
tion detection and other applications using passive sensing.
Kotaru et.al presented an indoor localization system using the
channel state information (CSI) and received signal strength
information (RSSI) [9]. Similar approaches have been used
for other applications such as keystroke identication [13], in-
home activity analysis [10], and virtual drawing [14]. Some
work has been done on using active radars for breathing
rate measurements. WiZ uses frequency modulated continuous
wave (FMCW) signal chirps to detect breathing rate in both
line-of-sight (LOS) and non line-of-sight (NLOS) scenarios

time is a function of total number of samples N and the
sampling frequency fs . Accordingly, the Doppler resolution
of the system is given as

d =

fs
.

(3)

Equation (3) shows that a long integration time is required to
achieve a ne Doppler resolution. For example, an integration
time of 5 seconds yields a Doppler resolution of 0.2Hz. This
poses two challenges to Vi-Wis target applications which
involve small scale body motions. First, due to the nature
of these applications, multiple Doppler shifts will emerge in
a single integration cycle resulting in an inherent ambiguity;
second, positive and negative Doppler shifts may be present in
a single cycle and thus cancel out the effects of one another.
In order to address these issues, Vi-Wis operations consist of
the following three steps:


Extract phase information off the reected surveillance
signals through cross-correlation.
Develop a 2-D system model to identify phase varia-
tions in two separate planes.
Analyze and process phase variations to determine
breathing rate, classify tremor and detect human falls.





In what follows, we present an elaborate approach to imple-
ment these steps.

A. Phase Extraction
Let a small scale movement in a single plane is given by m.
If the wavelength of transmitted signal is given by , then the
phase  associated with this movement can by described as

 =

2m


(4)

Since the reference and the surveillance antennas are spa-
tially separate, the small scale human motion is observed by
surveillance antenna only. It follows then that the surveillance
signal has an associated phase shift which is positive when
the movement is towards the antenna and negative when the
movement is away from it. Assuming that there is a single
reection at the surveillance antenna from the desired small
scale motion, the surveillance signal is given as

s[n] = Axsource [n +  ]ei2fd n ,

(5)

where xsource [n] is the source signal, fd is the Doppler shift,
and A is the signal amplitude. In the simplest case when
there is no movement, the Doppler shift is zero and the above
equation simplies to a delayed version of transmitted source
signal with no phase shift. When there is a movement towards
t');
INSERT INTO posts (postId,userId,title,body) VALUES (402,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U','he plane of surveillance signal, the Doppler shift increases,
resulting in a positive phase shift, and vice versa. Hence, by
tracking phase variations in the reected signal, we can also
track the movements in the environment.
If we divide the data into B batches and assume that the
time delay between reference and the surveillance signals is  ,

Fig. 1. Essential components of a passive radar system. Both surveillance and
reference antennas pick up signals from the access point, but the surveillance
signal contains an added Doppler shift due to human movement.

[8]. Other similar works include an ultra wide-band radar
with short transmit pulses to avoid reections from surround-
ing objects [15], a continuous wave (CW) radar for clinical
measurements [16], and a CW radar for indoor environment
[17]. Essential tremor monitoring is mostly unexplored in the
wireless domain and is typically done using electromyography
sensors, accelerometers and gyroscopes [18].
The rest of the paper is organized as follows. In section
2, the phase extraction theory is discussed followed by an
explanation of the 2-D system model and subsequent signal
processing in section 3. Section 4 elaborates on the imple-
mentation procedure and section 5 lists the key results and
limitations of this research.

I I . TH EORY O F O PERAT ION
The observed frequency shift due to relative motion of trans-
mitter and receiver is given by the well-know phenomenon of
Doppler frequency, and is written asc

fd = fo

cos(),

(1)

where fo is the carrier frequency of the transmitted signal, v
is the speed of relative motion, c is the speed of light, and  is
the angle of relative motion between transmitter and receiver.
In a passive radar sensing system (shown in Fig. 1), one or
multiple WiFi access points (APs) are used as transmitters,
and spatially directional high gain antennas with narrow beam-
widths are used to obtain the reference and the surveillance
signals. One approach to obtain target range and Doppler
information is by using the cross ambiguity function (CAF),
N(cid:88)
which can be represented in its discrete form as
n=0

r[n]s [n +  ]ei2fd

[ , f ] =

(2)
N ,

where r [n] is the reference signal, s[n] is the surveillance
signal,  is the path delay from access point to the surveillance
receiver, and N is the total number of samples in a single
window. For the system given in (2), the total integration

Access PointSurveillance AntennaReference Antennay (m)x (m)12123453Fig. 2. Displacement axes during vital health care activities. Dotted lines
show inactive axes for a specic activity.

then the cross correlation result in time domain for each batch
b, where b  {1, 2, ..., B }, is represented as
Nb(cid:88)
n=1

r [ib + n   ]s [ib + n],

y [b] =

(6)

where Nb is the number of samples in each batch, and ib
is the starting sample in each batch. In (6), (.) denotes the
conjugate operator on a complex number. We note that since
passive radars have a limited range resolution, therefore, 
can be set to the maximum time delay our system is likely to
encounter. If the sampling frequency is fs and the time delay
between the reference and surveillance signals is tlag , then');
INSERT INTO posts (postId,userId,title,body) VALUES (403,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U',' the
delay is given as

 =

(7)

fs
tlag

In typical small scale motions such as breathing, the time delay
is in the order of nano seconds (ns). Hence, we can assume  to
be zero in context of our applications. In order to get smoother
transitions in y [b] (and consequently in [b]), we consider an
x% overlap between consecutive batches. The exact choice of
x is governed by systems latency requirements. The phase of
each batch, [b], can now be found by taking inverse tangent
of real and imaginary parts of complex series y [b], i.e.,
[b] = tan1 (cid:60)(y [b])
(cid:61)(y [b])

(8)

[b] =

Although [b] encodes small scale body motions,
it also
captures reections off static objects such as furniture and
walls. In order to remove these time invariant phase shifts,
we normalize [b] to zero mean and unit variance, i.e.,
(cid:113) 1
(cid:80)B
[b]  b
i=bW +1 ([i]  b )2B(cid:88)
where W denotes the window length, and b is given as
i=bW +1

b =
W

(10)

[i].

(9)

I I I . PA S S IV E RADAR SY ST EM MOD EL IN 2 -D
Figure (2) shows the displacements involved in the three
activities of human breathing, essential tremor and human fall,
respectively. Human respiration involves movements along a
single axis which can be quantied by placing a surveillance
antenna perpendicular to human chest. In contrast, essential
tremor involves motion in two different planes which requires a

minimum of two antennas placed perpendicular to one another.
Finally, the motion involved in human fall is also along a single
axis but perpendicular to the breathing motion.
In order to detect phase variations in these applications, Vi-
Wi uses two surveillance antennas surv1 and surv2 with the
following polar coordinate geometry:

surv1 :  =
2

,  = 0,

surv2 :  =

(12)

(11)

,  =

where  denotes the angles surv1 and surv2 make with the x-y
plane in radians. The choice of  for surv2 presents an interest-
ing problem. Since, the antennas are highly directive, picking
a  close to 90 degrees implies that the target movement has
to be very close to the antenna to be detected. In contrast, 
of 0 degrees does not pick any movement in the x-z plane
due to Doppler shift being 0. As a compromise, we choose a
 of 45 degrees. In such a conguration, correlation signal at
surv2 is given by s2 [n] and is formed by the superimposition
(cid:88)
(cid:88)
of Doppler shifts in x-y and x-z planes, and can be represented
by the following equation, i.e.,
fdfdxz
fdfdxy

Ax[n +  ]ei2fd n +

(13)
where fdxy and fdxz contain a set of Doppler shifts in x-y and
x-z planes, respectively. Similarly, correlation signal at surv1
is given by s1 [n] and is represented by the following equation,
(cid:88)
i.e.,
fdfdxy

Ax[n +  ]ei2fd n .

s1 [n] =

s2 [n] =

(14)

Ax[n +  ]ei2fd n ,

Because of the geometry of surveillance antennas, extracted
phase information from 1 [n] also leaks into 2 [n]. In order for
s2 [n] to only track movements in x-z plane, Vi-Wi calculates
(cid:48)
2 [n] as

2 [n] = 2 [n]  1 [n],
(cid:48)

(15)



where 1 [n] and 2 [n] are phase signals calculated through
(cid:48)
Equation (8) from s1 [n] and s2 [n] respectively, and 
2 [n] is a
modied phase signal to account for information leakage from
x-y plane to x-z plane.

A. Feature Extraction
After Vi-Wi extracts phases from surveillance signals, it pro-
ceeds by extracting features in following order:


Vi-Wi determines if there is any activity in the envi-
ronment.
Vi-Wi classies the dominant activity (the activity that
results in the highest Doppler shift)
Vi-Wi extracts specic activity information.


To illustrate these steps, let us consider the phase signals
1 [n] and 2 [n] associated with surveillance signals s1 [n]
and s2 [n], against each activity. Fig. 3(a) shows that when



Tremor measurement is ');
INSERT INTO posts (postId,userId,title,body) VALUES (404,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U','similar to breathing rate measurement,
but with two main variations. First, the dominant essential
tremor frequency is in range 4-11 H z as compared to 0.5-2
H z in breathing. Second, the tremor measurements are taken
across both x-y and x-z planes instead of just x-y plane.

Accurately determining the frequency in such resolution range
is challenging. Moreover, the precise frequency estimate of
essential tremor does not provide any added value to the health
professionals. Therefore, we instead focus our experiments
on classifying tremor as either low or high. The dominant
frequency is measured using the same approach as in last
passage, and then a classication decision is made based on
whether the frequency is above or below a certain threshold.
We note that if the frequency is too large or too small, we
discard the activity as random motion.
Fall detection

Fall detection is done by locating an instance of time when
there is a spike in 2 [n] and 1 [n]
is at. We note that
fall detection can be done continuously even when Vi-Wi is
monitoring subjects breathing rate or essential tremor.

IV.

IM P LEM EN TAT ION

The implementation settings are shown in Fig. 4 and explained
below.

A. Hardware

The passive sensing system used in the experiments utilizes
USRP B200 software dened radio with an omni-directional
antenna as an access point. The access point transmits orthog-
onal frequency division multiplexed symbols at a data rate
of 3 Mb/s with a code rate of 1
2 and quadrature phase shift
keying modulation. With this conguration, the transmit power
of the WiFi source is estimated to be around -60 dBm. At the
receiver end, we have log-periodic directional antennas with
5dBi gain and 60 degree beam-width as shown in Fig. 4 on
surveillance antenna 1 and 2. The signals received at these
antennas are digitized through a Spartan 6 XC6SLX75 FPGA
and 61.44 MS/s, 12 bit ADC. For non line-of-sight experiments
(with obstacles), we use a 30 dB RF power amplier with the
transmit antenna.

B. Software

We implement Python blocks in GNU Radio for signal pro-
cessing operations. The code runs in real-time and the display
is updated on Python matplotlib library every 2 seconds.

C. Ground Truth

In our experiments, we used a pulse oximeter to establish
ground truth for breathing rate measurements. However, our
study of essential tremor was limited as we could not nd a
reliable device to estimate tremor. Therefore, we developed a
binary classication metric and determined if the tremor was
high or low. In future studies, a more reliable ground truth
could be established by using a reliable tremor monitoring
device.

Fig. 3. Tracking 1 and 2 during breathing, tremor and human fall.

a person inhales, her chest moves towards the device resulting
in a positive Doppler shift; when she inhales, her chest moves
away from the device causing a negative Doppler. Fig. 3(c) and
3(d) show variations in surveillance signals when the dominant
activity is essential tremor. In such a scenario, both 1 and 2
alternate between positive and negative Doppler shifts. Finally,
a human fall causes an aperiodic variation in 2 as shown in
Fig. 3(f).
However, there may be instances when a user moves her
limbs or makes');
INSERT INTO posts (postId,userId,title,body) VALUES (405,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U',' some other dominant motion. To deal with
such scenarios, Vi-Wi operates on a window of 20 seconds
and determines if 1 and 2 are periodic. If both are aperiodic,
we detect a random movement such as limb motion. However,
if only 2 is aperiodic, this indicates a sudden movement in
x-z plane and we detect a fall. If 1 is periodic and 2 is
aperiodic, we recognize this as breathing motion in x-y plane
only. If both are periodic, we detect tremor activity.
In the following section, we show how Vi-Wi determines the
specics of an activity once it has been classied.
Breathing Rate Measurement
Fundamentally, the breathing rate can be extracted by tak-
ing fast Fourier transform (FFT) of the phase signal and
subsequently picking the FFT peak. However, this does not
provide an accurate estimate since the frequency resolution
is quite small. Specically, for a window size of 10 seconds,
the frequency resolution is only 0.1H z or approximately 6
breaths per second. In order to improve this resolution, we
utilize a well known property that the dominant frequency of
a signal can be accurately measured by doing regression on
the phase of the complex time-domain signal [19]. Since, the
phase of complex signal 1 [n] is linear, its slope corresponds
to breathing rate estimate in H ertz .
Tremor Classication

Fig. 4. Experiment Setup. The breathing rate of the subject is tracked and
the activity is displayed on a personal computer.

Fig. 6. Accuracy comparison of health activities. Breathing rate accuracy
is determined by comparing actual breaths per minute to monitored breaths.
Tremor classication accuracy is determined the ability to classify slow (4-
7Hz) and fast (7-11Hz) tremor. In our experiments, fall detection has the
highest accuracy, while breathing has the least. Breathing measurement also
undergoes the highest drop in accuracy from LOS to NLOS environment.

Fig. 5.
Experiment settings for LOS and NLOS environments. There is a
single access point, a reference antenna and two surveillance antennas. In
NLOS setting, an additional metal block is placed to occlude the view of
target.

V. EX PER IM EN TA L EVALUAT ION

To evaluate the performance of Vi-Wi, we performed exper-
iments in a standard ofce environment with and without
obstacles. A simple metal block was used as an obstacle, and it
blocked all line of sight signals towards the receive antennas.

Fig. 6 shows Vi-Wis accuracy for different elderly-care ac-
tivities before and after an obstacle is placed in the line of
sight of receiver. We observe that Vi-Wi is able to detect falls
with the highest accuracy followed by tremor classication and
breathing rate measurement. In the presence of an obstacle, the
accuracy of all three activities considerably drops but we note
that it can be improved by increasing the APs transmit power
or using antennas with higher gains. Here, we note several
factors that may account for some of the inaccuracies in these
small scale motion measurements. First, the wavelength of the
WiFi signal is in order of centimeters and thus, slight geometry

Fig. 7. Change in breathing rate accuracy with distance. Transmit power is
set to 20 dBm. At a distance of 1 meter, the breathing rate accuracy is the
highest at 87%, and drops to');
INSERT INTO posts (postId,userId,title,body) VALUES (406,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U',' 84% when the target is 5 meters away.

changes of human body may cause additional phase variations.
Second, at each instance of time, a slightly different body part
may be detected causing deviation in phase variations. Finally,
these small scale motions are unlikely to be regular and a small
change in orientation may cause a large phase deviation.

Having xed the transmit power at 20 dBm, we validated
the performance of Vi-Wi as the distance of the target to device
changed as shown in Fig. 7. At all the distances, the accuracy
remained above 84% while the highest accuracy (87%) was
achieved at the nearest distance of 1 meter. We then asked
the participants to change their orientations and sit at slight
angles facing the receive antennas. In particular, we asked the
subjects to sit at three angles facing the antenna: 300 , 600
and 900 . At each distance, changing the sitting orientation
did not have any signicant effect on breathing rate accuracy.

Reference AntennaSurveillance Antenna-1Surveillance Antenna-2Access PointSubjectx (m)y (m)0.510.511.521.5Access PointxyplaneReference Antenna0.7 m1 m0.4 m0.5 mxzplaneSurveillance Antennas0.3 mLOS Obstaclex (m)y (m)0.511.5Access PointxyplaneReference Antenna0.7 m1 m0.4 m0.5 mxzplaneSurveillance Antennas0.3 mExperiment setting with obstacleExperiment setting without obstacleon various aspects of Vi-Wi. First, monitoring vital health
activities in presence of motion and background artefacts is
an interesting challenge. Second, additional health activities
such as human gait can be investigated. Third, research can
focus on using multiple access points for analyzing activities
of more than one target.
Beyond health, passive radar sensing offers a number of
promising future directions. By utilizing the access points
already available in buildings and intelligently assigning them
to the targets of interest, one can achieve almost ubiquitous
sensing.

[1]

care.

R E FER ENC E S
InformationWeek. Health-monitoring devices market outpaces telehealth.
http://www.informationweek.com/mobile/health-monitoring-devices-
market-outpaces-telehealth/d/d-id/1104636.
[2] M. M. Baig, H. J. Gholamhosseini, Smart health monitoring systems:
an overview of design and modeling, Journal of Medical Systems, 2013.
[3] MIT Technology Review. How cell phones are transforming health care
in Africa. https://www.technologyreview.com/s/519041/how-cell-phones-
are-transforming-health-care-in-africa/
[4] Healthcare Drive. How technologies can help the elderly age at
home. http://www.healthcaredive.com/news/how-technologies-can-help-
the-elderly-age-at-home/436386/
[5] Xinhua.
elderly
and
health
China
plans
smart
http://news.xinhuanet.com/english/2017-02/17/c 136062355.htm
[6] Vivonoetics.
dual-band.
or
single
Respiration:
http://vivonoetics.com/products/vivosense/ analyzing/respiration-single-
or-dual-band/.
[7] D. V. Forrest, The tremometer: a convenient device to measure postural
tremor from lithium and other causes, The Journal of Neuropsychiatry
'||'&'||' Clinical Neurosciences, 1990.
[8] F. Adib, Z. Kabelac, D. Katabi, R. C. Miller, 3D tracking via body radio
reections, Usenix NSDI, 2014.
[9] M. Kotaru, et. al, SpotFi: decimeter level localization using WiFi,
SIGCOMM, 2015.
[10] Y. Wang, et. al. E-eyes: device-free location-oriented activity identi-
cation using ne-grained WiFi signatures, MobiCom, 2014.
[11] GigaOm. Could a breath-monitoring headset
improve your health?
http://gigaom.com/2013/09/20/could-a-breath-monitoring-headset-
improve-your-health/.
[12] C. Guard, Wireless wrist band pulse oximeter CMS-50FW with soft-
ware for sleep study, http://www.clinicalguard.com/wireless-wristband-
pulse-oximeter-cms50fw-wi');
INSERT INTO posts (postId,userId,title,body) VALUES (407,8341,'Usman Mahmood Khan, Zain Kabir, Syed Ali Hassan School of Electrical Engineering '||'&'||' Computer Science (SEECS), National U','th-software-for-sleep-study-p-455.html.
[13] K. Ali, A. X. Liu, W. Wang, M. Shahzad, Keystroke recognition using
WiFi signals, MobiCom, 2015.
[14] Y. Wang, et. al, RF-IDraw: virtual touch screen in the air using RF
signals, SIGCOMM, 2014.
J. Qiuchi, Y. Jian, Y. Yinan, P. Bjorkholm, T. McKelvey, Detection of
breathing and heartbeat by using a simple UWB radar system, Antennas
and Propagation (EuCAP), 2014.
[16] D. Dei, et. al, Non-contact detection of breathing using a microwave
sensor, Sensors, 2009.
[17] T. Bo, et. al, Wi-Fi based passive human motion sensing for in-home
healthcare applications, Internet of Things (WF-IoT), 2015.
[18] G. Grimaldi, M. Manto, Neurological tremor: sensors, signal process-
ing and emerging applications, Sensors, 2010.
[19] A. Oppenheim, R. Schafer, J. Buck, et. al, Discrete-time signal
processing, Prentice Hall, 1989.

[15]

Fig. 8. Change in breathing rate accuracy with transmit power. Distance of
the target is set to 1 meter. The accuracy improves by around 11% when the
transmit power is changed from -30 dBm (78.5%) to 20 dBm (87%).

This is because during breathing, the chest expands in all
directions and even though the subject is facing sideways, his
chest movements can be detected. However, we noticed that
changing the subject orientation caused tremor classication
accuracy to drop signicantly. This is because unlike breathing,
tremor motion is concentrated along xed directions and as the
orientation of hand changes, Doppler shift can no longer be
monitored.
Our nal experiment was to analyze the effect of transmit
power on system accuracy as shown in Fig. 8. Our custom
designed AP offered a maximum power of -10 dBm. By
applying an external 30 dB RF power amplier, Vi-Wis
breathing rate measurement accuracy improved by 7.4%. There
were similar improvements in the accuracies of tremor and fall
activities with increase in transmit power.

A. Limitations
In this section, we discuss some of the limitations of Vi-Wi:


Vi-Wi assumes that there are no motion or background
artefacts in the environment. While Vi-Wi can work
reasonably well
in presence of background noise,
its performance is severely affected when there are
multiple movements.
Vi-Wi may be prone to detecting non-human motion.
For example,
it may mistake fall of stick in the
environment as a human fall.
Since, Vi-Wi requires a minimum signal-to-noise ratio
(SNR), it works in a limited range of 5 meters. Beyond
this distance, the accuracy of Vi-Wi drops signcantly.





V I . CONC LU S ION AND FU TUR E D IR EC T ION S
The health applications using passive radar sensing have been
largely untapped. The traditional methods to monitor breathing
rate, tremor and falls are invasive and inconvenient. The mod-
ern methods using active radars require dedicated transmitters
and high bandwidth. In contrast, Vi-Wi offers covert, license-
free and convenient health monitoring. Future works can focus

');
INSERT INTO posts (postId,userId,title,body) VALUES (408,2388,'Konstantin Avrachenkov Tejas Bodas INRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse 12rA6 F.c 127047:ir ABSTRACT','Consider a single server queue serving a multiclass popula-
tion. Some popular scheduling policies for such a system
(and of interest in this paper) are the discriminatory pro-
cessor sharing (DPS), discriminatory random order service
(DROS), generalized processor sharing (GPS) and weighted
fair queueing (WFQ). The aim of this paper is to show a
certain equivalence between these scheduling policies for the
special case when the multiclass population have identical
and exponential service requirements. In fact, we show the
equivalence between two broader classes of policies that gen-
eralize the above mentioned four policies. We specically
show that the so journ time distribution for a customer of a
particular class in a system with the DPS (GPS) scheduling
policy is a constant multiple of the waiting time distribution
of a customer of the same class in a system with the DROS
(respectively WFQ) policy.

1.

INTRODUCTION

Consider a single server queue with multiclass customers
where the customers have independent and identical service
requirements independent of their class. Suppose that there
are N customer classes and assume that the service require-
ment of a customer is exponentially distributed with rate
. Let i denote the arrival rate for a Class i customer,
i = 1 . . . N . Let PN
 and  = PN
i=1 i = , i = i
i=1 i .
Further, let pi denote a weight parameter associated with a
Class i customer. Additionally, for the purpose of stability,
assume throughout that  < . Some examples of schedul-
ing policies used in such multiclass queues are DPS, DROS,
GPS and WFQ. In this paper, we will show that the so journ
time distribution of any Class i customer with DPS (GPS)
scheduling policy is equal to the waiting time distribution
of a Class i customer in a system with DROS (respectively
WFQ) policy under the assumption of i.i.d exponential ser-
vice requirements. A quick overview of these four scheduling
policies is given below.
Scheduling policies such as GPS and DPS are variants
of the processor sharing policy and can serve multiple cus-
tomers from the system simultaneously. In case of GPS, a
separate queue is maintained for each customer class and
the total service capacity of the server is shared among cus-
tomers of the dierent classes in proportion to the weights pi .
The GPS scheduling policy is often considered as a general-

Copyright is held by author/owner(s).

ization of the head-of-line processor sharing policy (HOLPS)
as described in [6, 14, 15]. (Refer [16, 17] for details about
HOLPS). As a generalization of HOLPS, GPS maintains a
FIFO scheduling policy within each queue for a class and
only the head-of-line customers of dierent classes are al-
lowed to share the processor. The share of the server for a
head-of-line Class i customer is proportional to the weight pi
and is independent of the number of other customers in the
queue. The service rate received by the customer is precisely
pi
given by
where j = 1 if the queue has at least
PN
j=1 pj j
one class j customer and j = 0 otherwise. Refer Parekh
and Gallager [11], Zhang et al. [12] for an early analysis of
the model.
In case of DPS, the total service capacity is shared among
all the customers present in the system and not just among
the head-of-line customers of dierent classes. The share of
the server for a customer of a class is not only in proportion
to the class weight, but also depends on the number of multi-
class customers present in the queue. In particular, a Class i
pi
customer in the system is served at a rate of
where
PN
j=1 pj nj
nj denotes the number of Class j customers in the system.
The DPS system was rst introduced by Kleinrock [5] and
subsequently analyzed by several authors [13, 8, 7, 9].
The DROS and WFQ scheduling policies are also char-
acterized by an associated weight for each c');
INSERT INTO posts (postId,userId,title,body) VALUES (409,2388,'Konstantin Avrachenkov Tejas Bodas INRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse 12rA6 F.c 127047:ir ABSTRACT (part 2)','ustomer class.
However these policies are not a variant of the processor
sharing policies and hence their respective server can only
serve one customer at a time. The DROS and WFQ policies
dier in their exact rule for choosing the next customer. In
the DROS policy, the probability of choosing a customer for
service depends on the weights and the number of customers
of the dierent classes in the queue. A Class i customer is
pi
where nj denotes
thus chosen with a probability of
PN
j=1 pj nj
the number of Class j customers in the system. DROS policy
is also know as relative priority policy and was rst intro-
duced by Haviv and wan der Wal [7]. For more analysis of
this policy we refer to [3, 10]. In the WFQ policy, a separate
queue for each class is maintained and the next customer is
chosen randomly from among the head-of-line customers of
dierent classes. As in case of the GPS scheduling policy,
a FIFO scheduling policy is used within each queue for a
class. WFQ can be seen as a packetised version of GPS and
the probability of choosing a head-of-line Class i customer
pi
for service is given by
where j is as dened ear-
PN
j=1 pj j
lier. Refer Demers [14] for the detailed analysis of the WFQ
policy.
It is interesting to note that for a Class i customer, the ser-

vice rate received in DPS and the probability of being cho-
pi
sen next for service in case of DROS is given byPN
j=1 pj nj
Similarly, the service rate received in GPS and the prob-
ability of being chosen next for service in case of WFQ is
pi
. This similarity in the scheduling rules motivates
PN
j=1 pj j
us to compare the mean waiting times and so journ times
of the multiclass customers with these scheduling policies.
Having assumed identical service requirements, we speci-
cally show that the waiting time distribution of a Class i
customer in a system with DROS (WFQ) scheduling pol-
icy is  times the so journ time distribution of any Class i
customer with DPS (resp. GPS) scheduling policy. This is
a generalization of [2], where the equivalence has been es-
tablished between single-class processor sharing and random
order service discipline. The coupling technique which we
use also builds upon the technique used in [2].
The rest of the paper is organized as follows. In the next
section, we introduce a generalized notion of multiclass pro-
cessor sharing (mPS) and random order service (mROS)
policies. The DPS, GPS, DROS and WFQ policies will turn
out to be special cases of mPS and mROS. In Section 3, we
show that the mean so journ time of a Class i customer with
mPS scheduling is equivalent to the mean waiting time of
a Class i customer with mROS policy. As a special case,
this proves the mentioned equivalences among the four mul-
ticlass scheduling policies.

2. GENERALIZED MULTICLASS SCHEDUL(cid:173)
ING POLICIES

In this section, we will describe two multiclass scheduling
policies that are a generalization of policies such as DPS,
DROS, GPS and WFQ. The two policies are based on the
processor sharing and random order service mechanism and
will be labeled as mPS and mROS respectively.
The mPS scheduling policy is a multiclass processor shar-
ing policy and can serve multiple customers simultaneously.
A sepa');
INSERT INTO posts (postId,userId,title,body) VALUES (410,2388,'Konstantin Avrachenkov Tejas Bodas INRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse 12rA6 F.c 127047:ir ABSTRACT (part 3)','rate queue for each customer class is maintained and
a FIFO sequencing policy is used within each queue of a
class. The mPS scheduling policy is parameterized by a vec-
tor  of length N that characterizes the maximum number
of customers of each class that can be served simultaneously
with other customers. We shall henceforth use the nota-
tion mPS( ) where  = [1 , . . . , N ]. Here  denotes the
set of customers that can be served simultaneously. Recall
the denition that ni denotes the instantaneous number of
Class i customers in the queue, where i = 1, . . . , N . Let
i ( n) denote the number of Class i customers under service
when the conguration of total multiclass customers is n.
Then, clearly i ( n) = min (ni , i ). To lighten notation, we
shall drop the dependence on n and use only i when the
context is clear.
In other words, if ni  i , then all the
Class i customers in the queue are being served simultane-
ously for i = 1, . . . , N . However if ni > i , then only the
rst i customers of Class i in its queue are served simul-
taneously. Recall that due to the FIFO sequencing policy
within each class, only the rst i customers in the queue
are always served. For an mPS( ) scheduling policy with
a conguration of n = [n1 , . . . , nN ] multiclass customers in
the system, the service rate received by a particular Class i
pi
customer in service is given by
. When i = ,
PN
j=1 pj j
for i = 1 to N , the corresponding scheduling policy will be

denoted by mPS( ). In this case, i = min (ni , ) = ni
and therefore mPS( ) corresponds to the DPS scheduling
policy. Similarly if e = [1, . . . , 1], then mPS(e) corresponds
to the GPS scheduling policy where only the head-of-line
customers of each class can be served.
In a similar manner, we can dene the mROS( ) schedul-
ing policy where  = [1 , . . . , N ] and  denotes the set
of customers from which the subsequent customer is cho-
sen for service. As in case of the mPS policy, note that a
separate FIFO queue for each customer class is also main-
tained for the mROS system. At any given time, the rst
i = min (ni , i ) customers are candidates for being cho-
sen for service while the remaining ni  i customers have
to wait for their turn. For an mROS( ) scheduling policy
with a conguration of n = [n1 , . . . , nN ] waiting customers,
a Class i customer within the rst i customers in its queue
pi
will be chosen next for service with probabilityPN
j=1 pj j
As in case of the mPS scheduling, mROS( ) corresponds
to the DROS policy whereas mROS(e) corresponds to the
WFQ policy.

Remark 1. A policy closely related to the mPS discipline
is the limited processor sharing (LPS) policy. LPS is a single
class processor sharing policy parametrized by an integer c
where c denotes the maximum number of customers that can
be served simultaneously. Here c =  corresponds to the
processor sharing policy while c = 1 corresponds to FCFS
policy. LPS can also be viewed as a special case of the mPS
policy when there is a single service class for the arriving
customers. See [1, 18] more more details about the LPS-c
policy.

Having introduced the generalized multiclass scheduling
policies, we shall now establish a relation between the so-
journ time of a Class i customer in mPS system with the
waiting time of a Class i customer in mROS system.

3. COMPARING THE SOJOURN AND WAIT(cid:173)
ING TIME DISTRIBUTIONS IN MPS AND
MROS

The analysis in this section is inspired from that in [2]
where a ');
INSERT INTO posts (postId,userId,title,body) VALUES (411,2388,'Konstantin Avrachenkov Tejas Bodas INRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse 12rA6 F.c 127047:ir ABSTRACT (part 4)','similar result is established for the case of a sin-
gle class of population. Let n now denote a vector corre-
sponding to the number of customers of each class present
in the queueing system at an arrival instant. We have n =
(n1 , . . . , nN ) where ni denotes the number of Class i cus-
tomers at the arrival instant. Suppose PN
i=1 ni = n. Let ran-
dom variable Si ( , n) denote the conditional so journ time
experienced by an arriving Class i customer in an mP S ( )
system when it sees a conguration of n customer on ar-
rival. The corresponding unconditional random variable will
be denoted by Si ( ). We shall occasionally use the notation
mP S ( , n) to denote the mPS( ) system with n customers.
Along similar lines, let the random variable Wi ( , n) de-
note waiting time (time until chosen for service) experi-
enced by an arriving Class i customer in the mROS sys-
tem conditioned on the fact that it sees a conguration n
of waiting customers. This system will be often denoted
as mROS ( , n) and the unconditional random variable will
be denoted by Wi ( ). Let P and P denote the probability
distribution of the random variables Si ( , n) and Wi ( , n)
respectively. (The dependence of these distributions on n

have been supressed for notational convenience.) We now
state the main result of this paper.

Theorem 1. P (S i ( ) > t) = P (W i ( ) > t) for i =
1, . . . , N .

(1)

Proof. As in [2], our aim is to rst provide a coupling
(cid:16) Si ( , n), Wi ( , n)(cid:17) with the corresponding law denoted by
P such that
 Si ( , n) D= S i ( , n) and Wi ( , n) D= W i ( , n)
 P (cid:16) Si ( , n) = Wi ( , n)(cid:17) = 1
The second requirement will help us show that the two
distributions P and P are equal. This follows from the cou-
pling inequality
P  P(cid:13)(cid:13)  2P (cid:16) Si ( , n) 6= Wi ( , n)(cid:17) .
(cid:13)(cid:13)
Such a coupling is precisely obtained as follows.
Consider two tagged Class i customers X and Y that ar-
rive to a mP S ( , n) and a mROS ( , n) system respectively.
This means that at the arrival instant of customer X in
the mP S ( , n) system, there are ni Class i customers al-
ready present in the system. Similarly, at the arrival instant
of customer Y in mROS ( , n), there are ni customers of
Class i that are waiting for service in the queue. Recall that
 = [1 , . . . , N ] where i in the mPS system denotes the
number of Class i customers that are receiving service. In
the mROS system, i denotes those (waiting) Class i cus-
tomers from which the next customer could be chosen. Note
that since PN
i=1 ni = n, with the arrival of customer X, the
mP S ( , n) system has n + 1 customers. Similarly, with the
arrival of customer Y , the mROS ( , n) system has n+ 2 cus-
tomers of which one customer is in service and the remaining
n + 1 customers (including customer Y) are waiting for ser-
vice. We will now specify the rule for forming the required
coupling. Since the customers can be distinguished by their
class index and also the position in their respective queues,
we couple the n + 1 customers in mP S ( , n) with the n + 1
waiting customers in the mROS ( , n) system based on their
class and queue position. The coupling must be such that
the coupled customers belong to the same class and invari-
ably have the same queue position in their respective queues.
It goes without saying that the tagged customers X and Y
are also coupled. As in [2], we also couple the subsequent
arrivin');
INSERT INTO posts (postId,userId,title,body) VALUES (412,2388,'Konstantin Avrachenkov Tejas Bodas INRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse 12rA6 F.c 127047:ir ABSTRACT (part 5)','g customers and let D1 , D2 . . . denote i.i.d random
variables with an exponential distribution of rate . These
random variables correspond to service times of a customer
in service in mROS ( ). At the service completion epoch,
pick a pair of coupled customers randomly from the set of
 customers. The random picking is with a distribution
such that a Class i pair from the  customers is chosen with
pi
probability
. If the chosen pair belongs to Class k,
PN
j=1 pj j
then a class k customer departs from the mPS system while
such a customer is taken for service in mROS. This process
is repeated till the tagged pair (X, Y ) leaves the system.
Clearly, this joint probability space is so constructed that
the random variables Si ( , n) = Wi ( , n) Pa.s. From Eq.
(1), this implies that

S i ( , n) D= W i ( , n).

(2)

Now let random variables N mP S (resp. N mROS
) denotethe conguration of the total customers (resp. waiting cus-
tomers in case of mROS system) as seen by a Class i arrival.
The subscript 1 in N mROS
is used to indicate a busy server.We have the unconditional probabilities given by the follow-
ing.
P (S i ( ) > t) = Xn

P (N mP S = n)P (S i ( , n) > t).

(3)

P (N mROS
= n)P (W i ( , n) > t). (4)

Similarly we have
P (W i ( ) > t) = Xn
Now if suppose P (N mROS
= n) = P (N mP S = n) istrue, then from Eq. (2), the statement of the theorem follows
and this would complete the proof. In the following lemma,
= n) = P (N mP S =
we shall prove that indeed P (N mROSn).

Lemma 1. P (N mROSsuch that |n|  0.

= n) = P (N mP S = n) for n

Proof. We rst simplify the notations as follows. Let
 ( n) := P (N mP S = n) and  (1, n) := P (N mROS
= n).Let  (0, 0) denote the probability that the mROS system
has no customers and is idle. The statement of the lemma
now requires us to prove that (1, n) =  ( n). To prove this
result, consider the balance equation for the mPS system
where  shall denote the stationary invariant distribution
for the system. The assumption  <  implies that the
underlying Markov process is ergodic and hence the station-
ary distribution  is unique. For n such that |n|  0, the
detailed balance equations for the mPS( ) system are
Xi=1   i ( n)pi
j=1 pj j ( n) ! 1{| n|>0} ) ( n)PMXi=1
i 1{ni>0} ( n  ei )
Xi=1   i ( n + ei )pi
j=1 pj j ( n + ei ) !  ( n + ei ).PM
Now since
Xi=1   i ( n)pi
j=1 pj j ( n) ! = 1,PM
the balance equations can be written as

( +

=

( + 1{| n|>0} ) ( n) =
Xi=1
i1{ni >0} ( n  ei )
j=1 pj j ( n + ei ) !  ( n + ei ).
Xi=1   i ( n + ei )piPM
Similarly, the detailed balance equations for the mROS( )
system are as follows for n such that |n|  0.

(5)

+

(6)
Xi=1
( + 1{| n|>0} ) (1, n) =
i1{ni >0}  (1, n  ei )
Xi=1   i ( n + ei )pi
.!  (1, n + ei )PM
j=1 pj j ( n + ei )
Additionally, the idle system should satisfy
 (0, 0) =  (1, 0).
(7)
where  (0, 0) = 1   is the probability that the system is
empty.
Now again, the assumption  <  implies that the un-
derlying Markov process is ergodic and hence the station-
ary distribution  is also unique. Therefore to prove the
lemma, it is sucient to check if the detailed balance equa-
tions for the mROS system given by Eq. (6) are satised
when  (1, n) =  ( n).
Now from Eq. (6) and assuming that (1, n) =  ( n), we
have




Xi=1
( + 1{| n|>0} )  (1, n) 
i1{ni >0}  (1, n  ei )
Xi=1   i ( n + ei )pi
j=1 pj j ( n + ei ) !  (1, n + ei )PMXi=1
i1{ni >0}  ( n  ei )
= ( + 1{| n|>0} ) ( n) 
j=1 pj j ( n + ei ) !  ( n + ei ) = 0.
Xi=1   i ( n + ei )piPM
The last equality follows from Eq. (5) after dividing through-
out by . Similarly,
 (0, 0)   (1, 0) =  (0, 0)   (0)
=  (  (0, 0)   (0))
=  (  (0, 0)  (1  ))
= 0.
(8)
Here the third equality is from the ');
INSERT INTO posts (postId,userId,title,body) VALUES (413,2388,'Konstantin Avrachenkov Tejas Bodas INRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse 12rA6 F.c 127047:ir ABSTRACT (part 6)','fact that  (0) = (1  ) is
the probability that the mPS( ) system is empty. Clearly,
substituting  (1, n) =  ( n), satises the balance equations
for the mROS system. Since  is the unique invariant dis-
tribution, the statement of the lemma follows.

We now have the following corollary that establishes the
desired equivalence between DPS (GPS) and DROS (resp.
WFQ) scheduling policies. Note that the result is true only
for the case when all customers have identically distributed
service requirements. The equivalence result need not be
true in general when the customer classes dier in their ser-
vice requirements.

Corollary 1.

 P (S i ( ) > t) = P (W i ( ) > t)
where S i ( ) denotes the sojourn time of a Class i cus-
tomer in DPS system and W i ( ) denotes the waiting
time of a Class i customer in DROS system.

 P (S i (e) > t) = P (W i (e) > t)
where S i (e) denotes the sojourn time of a Class i cus-
tomer in GPS system and W i (e) denotes the waiting
time of a Class i customer in WFQ system.

4. ACKNOWLEDGEMENTS

The authors would like to thank Dr. Rudesindo Queija
from CWI, Amsterdam for the several discussions on this
work.

5. REFERENCES

[1] B. Avi-Itzhak and S. Haln, Expected response times
in a non-symmetric time sharing queue with a limited
number of service positions,Proceedings of ITC 12,
1988.
[2] S.C. Borst, O.J. Boxma, J.A. Morrison and R.
Nunez-Queija, The equivalence between processor
sharing and service in random order Operations
Research Letters, vol. 31, no. 4, pp. 254262, July 2003.
[3] M. Haviv and J. van der Wal, Waiting times in queues
with relative priorities, Operations Research Letters,
vol. 35, no. 5, pp. 591594, September 2007.
[4] M. Haviv and J. van der Wal, Mean so journ times for
phase-type discriminatory processor sharing systems,
European Journal of Operational Research, vol. 189, no.
2, pp. 375386, September 2008.
[5] L. Kleinrock, Time-shared systems: a theoretical
treatment. Journal of ACM, 14(2):242261, April 1967.
[6] S. Aalto, U. Ayesta, S. Borst, V. Misra and R.
Nunez-Queija, Beyond Processor Sharing, ACM
Sigmetrics Performance Evaluation Review, vol. 34, pp.
3643, 2007,
[7] M. Haviv and J. van der Wal, Equilibrium strategies
for processor sharing and random queues with relative
priorities. Probability in the Engineering and
Informational Sciences, null(4):403412, October 1997.
[8] M. Haviv and J. van der Wal, Waiting times in queues
with relative priorities. Operation Research Letters,
35:591594, 2007.
[9] J. Kim, and B. Kim,  So journ time distribution in
M/M/1 queue with discriminatory processor sharing,
Performance Evaluation, vol. 58, pp. 341365, July
2004,
[10] J. Kim, J. Kim and B. Kim,  Analysis of M/G/1
queue with discriminatory random order service policy,
Performance Evaluation, vol. 68, pp. 256270, 2011,
[11] A. K. Parekh and R. G. Gallager, A generalized
processor sharing approach to ow control in integrated
services networks: The single-node case. IEEE/ACM
Transactions on Networking, 1(3):344357, 1993.
[12] Z. Zhang, D. Towsley and J. Kurose, Statistical
analysis of generalized processor sharing scheduling
discipline. In Proceedings of SIGCOMM, pages 6877,
1994.
[13] G. Fayolle, I. Mitrani and R. Iasnogorodski, Sharing
a processor among many job classes, Journal of ACM,
vol. 27, no. 3, pp. 519532, July 1980.
[14] A. Demers, Analysis and Simulation of a Fair
Queueing Algorithm, Internetworking: Research and
Experience, vol. 1, pp. 326, 1990.

[15] A. Kumar, D. Manjunath and J. Kuri, 
Communication Networking: An Analytical Approach,
Elsevier, 2004.
[16] G. Fayolle and R. Iasnogorodski, Two Coupled
Processors: The reduction to a Riemann-Hilbert
Problem, Wahrscheinlichkeitstheorie verw Gebiete,
vol. 47, pp. 325351, 1979.
[17] J. Morrison, Head of the line p');
INSERT INTO posts (postId,userId,title,body) VALUES (414,2388,'Konstantin Avrachenkov Tejas Bodas INRIA, Sophia Antipolis LAAS(cid:173)CNRS, Toulouse 12rA6 F.c 127047:ir ABSTRACT (part 7)','rocessor sharing for
many symmetric queues with nite capacity, Queueing
Systems, vol. 14, pp. 215237, 1993.
[18] J. Zhang et al, Diusion limits of limited processor
sharing queues,The Annals of Applied Probability, vol.
21, pp. 745799, 2011.

');
INSERT INTO posts (postId,userId,title,body) VALUES (415,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs','Jayadev Misra
University of Texas at Austin, misra@utexas.edu

April 5, 2017

Abstract

This paper suggests a theomisra@utexas.edury of composable speci-
cation of concurrent programs that permits: (1) verication of program
code for a given specication, and (2) composition of the specications of
the components to yield the specication of a program. The specication
consists of both terminal properties that hold at the end of a program ex-
ecution (if the execution terminates) and perpetual properties that hold
throughout an execution. We devise (1) proof techniques for verication,
and (2) composition rules to derive the specication of a program from
those of its components. We employ terminal properties of components
to derive perpetual properties of a program and conversely. Hence, this
proof strategy is called bilateral. The compositional aspect of the theory
is important in assembling a program out of components some of whose
source code may not be available, as is increasingly the case with cross-
vendor program integration.

Keywords: Program specication, concurrent programming, verication,
composition, safety and progress properties.

1 Introduction

Four decades of intensive research has failed to yield a scalable solution to the
problem of concurrent program design and verication. While there have been
vast improvements in our understanding, the theory and practice in this area
lag considerably behind what has been achieved for sequential programs. Very
small programs, say for synchronization, are proved manually, though the proof
methods are mostly unscalable. Larger programs of practical signicance, say
cache coherence protocols, are typically proved using model checking, which
imposes size limitations. Programs from dierent vendors are rarely assembled
to run concurrently.
We believe that the problem stems from the lack of a theory of composable
specication for concurrent programs. Sequential imperative programs enjoy

such a theory, introduced by Hoare [7], in which a program is specied by a pair
of predicates, called its pre- and postcondition. The theory successfully permits:
(1) verication of program code for a given specication, and (2) composition of
the specications of the components to yield the specication of a program. A
fundamental concept is invariant that holds at specic program points, though
invariant is not typically part of the program specication. Termination of a
program is proved separately.
A specication of a concurrent program typically includes not just the pre-
and postconditions but properties that hold of an entire execution, similar to
invariants. A typical specication of a thread that requests a resource, for ex-
ample, may state that: (1) once it requests a resource the thread waits until the
resource is granted, and (2) once the resource is granted the thread will even-
tually release it. The rst property is an instance of a safety property and the
second of a progress property, see Lamport [10] and Owicki and Lamport [14].
Call the postcondition of a program for a given precondition to be a termi-
nal property. And a property that holds throughout an execution a perpetual
property. Terminal properties compose only for sequential programs, though
not for concurrent programs, and they constitute the essence of the assertional
proof method of Hoare. Safety and progr');
INSERT INTO posts (postId,userId,title,body) VALUES (416,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 2)','ess are typical perpetual properties.
This paper suggests a theory of composable specication of concurrent pro-
grams with similar goals as for sequential programs. The specication consists of
both terminal and perpetual properties. We devise (1) proof techniques to ver-
ify that program code meets a given specication, and (2) composition rules to
derive the specication of a program from those of its components. We employ
terminal properties of components to derive perpetual properties of a program
and conversely. Hence, this proof strategy is called bilateral. The compositional
aspect of the theory is important in assembling a program out of components
some of whose source code may not be available, as is increasingly the case with
cross-vendor program integration.
The Hoare-proof theory for sequential programs is known to be sound and
relatively-complete. A sound and relatively-complete proof theory for concur-
rent programs that use a very limited set of program constructs, known as
Unity, appears in Chandy and Misra [2] and Misra [11]. This paper combines
both approaches to yield a theory applicable to concurrent programs written
in conventional languages. (The soundness and relatively-completeness of this
theory has not been proved yet.)
We treat three examples of varying complexity in detail in this paper. First,
a program that implements a distributed counter is used to illustrate the various
concepts of the theory in stages through out the paper. Appendix B includes a
proof of a mutual exclusion program, a system of tightly-coupled components.
Unlike traditional proofs, it is based on composing the specications of the
components. Appendix C includes proof of a recursively-dened concurrent
program, where code of one component is only implicit, again using composable
specications. We know of no other proof technique that can be used to prove
this example program.

2 Program and Execution Model

2.1 Program Structure

The syntax for programs and its components is given below.

::= guard  body
action
f , g :: component ::= action | f [] g | seq(f0 , f1 ,    fn )
::= f
program

An action has a guard, which is a predicate, and a body which is a piece of
code. Execution of the body in a state in which the guard holds is guaranteed to
terminate; we assume that this guarantee is independently available. Details of
execution semantics is given Section 2.2. A guard that is true is often dropped.
An action without a guard is non-blocking and a guarded action blocking.
A structured component is either: (1) a join of the form f [] g where f
and g are its direct subcomponents, or (2) seq(f0 , f1 ,    fn ) where the direct
subcomponents, fi , are combined using some sequential language construct.
A join models concurrent execution, as we describe in Section 2.2. And seq
denotes any sequential language construct for which proof rules are available.
Thus, the typical constructs of sequencing, if-then-else and do-while are seq
constructs. A subcomponent of any component is either its direct subcomponent
or a subcomponent of some direct subcomponent. Note that a component is
never empty.
Join construct is commutative and associative. The syntax permits arbitrary
nesting of sequential and concurrency constructs, so, (f [] g ) ; (f  [] g  ) is a
program with f , f  , g and g  as components.
A program is a component that is meant to be executed alone.

Access rights to variables A variab');
INSERT INTO posts (postId,userId,title,body) VALUES (417,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 3)','le named in a component is accessible
to it. Variable x is local to f if f has exclusive write-access to x during any of
its executions. Therefore, any accessible variable of a component in a sequential
program is local to it. However, a local variable of f [] g may not be local to
either f or g . An accessible non-local variable x of f is shared ; x is shared with
g if x is also accessible to g . Note that x is local to f and shared in g if f has
exclusive write-access to x and g can only read x.
A program is executed alone, so, all its accessible variables are local to it.
A local predicate of f is a predicate in which every variable is a local variable
of f . Therefore, true and false are local to all components. It follows that the
value of a local predicate of f remains unchanged in any execution as long as f
does not take a step.

2.2 Program execution

A step is an instance of the execution of an action. A step of action b   is
executed as follows: evaluate b without preemption and if it is true, immediately
execute  to completion, again without preemption this is called an eective

execution of the action else the program state (and its control point) are
unaltered this is called an ineective execution. Thus, in an eective execution
of b  , b holds when the execution of  begins. Ineective execution models
busy-waiting whereby the action execution is retried at some future moment.
Execution of a join starts simultaneously for all its direct subcomponents,
and terminates when they all do. At any moment during the execution, the
program control may reside simultaneously in many of its subcomponents. Ex-
ecution rules for a seq are the traditional ones from sequential programming,
which ensures that the program control is within one direct subcomponent at
any moment. The subcomponent itself may be a join which may have multiple
control points of its own.
Initially the program control is at the entry point of the program. In any
given state the program takes a step by choosing a control point before an action
and executing, eectively or ineectively, the action. If there is no such control
point, the program has terminated. The choice of control point for a step is
arbitrary, but sub ject to the following fairness constraint: every control point is
chosen eventually for execution, so that no component is ignored forever. This
is the same constraint advocated by Dijkstra [5] in his classic paper. In contrast
to a terminated execution a deadlocked execution attempts executions of certain
actions ineectively forever, and the control resides permanently at the points
preceding each of these actions.
An execution is a sequence of steps that can not be extended.
Innite
executions do not have an end state. A nite execution either terminates or is
deadlocked, and it has an end state. It simplies the proof theory considerably
to imagine that every execution is innite, by extending each nite execution
by an innite number of stutter steps that repeat the end state forever.

2.3 Example: Distributed counter

We consider an abstraction of a distributed counter protocol in Blumofe [1].
The proof in this paper is based on a proof by Ra jeev Joshi and the one given
in Chapter 6 of Misra [11].
The protocol f that implements counter ctr is the join of a nite number
of threads, fj , given below. Each fj has local variables oldj and newj . Below,
each assignment statement and guarded statement is an action. The following
form of the if statement was introduced by Dijkstra [4]; in each execution of the
statement an alternative whose guard holds is executed without preemption.

initially ctr = 0
fj ::
initially oldj , newj = 0, 0
loop

newj := oldj + 1;
if [ ctr = oldj  ctr := newj
| ctr 6= oldj  oldj := ctr ]

forever

It is required to show that ctr behaves as a counter, that is: (1) ctr never
decreases and it increases only by 1, and (2) ctr increases eventually. Both of
these are perpetual properties. Th');
INSERT INTO posts (postId,userId,title,body) VALUES (418,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 4)','ere is no terminal property of interest since
the program never terminates.

3 Introduction to the proof theory

3.1 Specication

A specication of component f is of the form {r} f {Q
s} where r and
s are the pre- and postconditions, and Q is a set of perpetual properties. The
meaning of such a specication is as follows. For any execution of f starting in
an r-state,

1. if the execution terminates, the end state is an s-state, and

2. every property in Q holds for the execution.

We give proof rules for pre- and postconditions in the following section.
Proof rules for perpetual properties appear in subsequent sections, for safety
properties in Section 4 and for progress properties in Section 5.

Terminology Write {r} f {s} when Q is irrelevant in the discussion, and
{r} f {Q
} when s is irrelevant. Further for q  Q, write q in f  when r is
understood from the context, and just q when both f and r are understood.
An inference rule without explicit mention of f or r denotes that the rule applies
for any f and r.

Variables named in properties A property of component f includes pred-
icates that name accessible variables of f , and other variables, called free vari-
ables. A property is implicitly universally quantied over its free variables. Any
inaccessible variable named in a property denotes a free variable, and, being a
bound variable, may be renamed.

3.2 Local Annotation

Local Annotation of component f associates assertions with program points
such that each assertion holds whenever program control reaches the associated
point in every execution of any program in which f is a component. Thus, a
local annotation yields precondition for the execution of each action of f , and
valid pre- and postcondition of f in any environment. Since the execution envi-
ronment of f is arbitrary, only the predicates that are local to f are unaected
by executions of other components. Therefore, a local annotation associates
predicates local to each point of f , as explained below.
Local annotation is dened by the program structure. First, the proof rule
for an action is as follows:

{p  b}  {q}
{p} b   {q}

To construct a local annotation of f = seq(f0 , f1 ,    fn ), construct local
annotation of each fi using only the local variables of fi as well as those of
f . Then construct an annotation of f using the proof rules for seq from the
sequential program proof theory. Observe that the local variables of f are local
to each fi because in a sequential execution among the direct subcomponents
of f each fi has exclusive write-access to these variables during its execution.
To construct a local annotation of f = g [] h, construct local annotations of
each of g and h using only their local variables. Then construct an annotation
of f using the proof rule given below. Note that the proof rule is valid only
because the assertions in g and h are local to those components.

{r} g {s}
{r} h {s}
{r  r } g [] h {s  s}

Observe that a local variable of f is not necessarily local to g or h unless
they have exclusive write-access to it. Henceforth, all annotations in this paper
are local annotations.
A shortcoming of local annotation is that a variable that is local to f [] g but
shared by both f and g can not appear in a local annotation by the application
of these rules alone. The invariance meta-rule, given in Section 3.4, overcomes
this problem.

3.3 Example: Distributed Counter, contd.

Construct a local annotation of fj for the example of Section 2.3. Below, we
have labeled the actions to refer to them in subsequent proofs.

fj ::
initially oldj , newj = 0, 0
{true}
loop
{true}
j :: newj := oldj + 1;
{newj =');
INSERT INTO posts (postId,userId,title,body) VALUES (419,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 5)',' oldj + 1}
if [ j :: {newj = oldj + 1} ctr = oldj  ctr := newj {true}
| j :: {newj = oldj + 1} ctr 6= oldj  oldj := ctr
{true}
{true}
forever

3.4 Meta-rules

The following general rules apply for specications.

 (lhs strengthening; rhs weakening)

{r} f {Q
s}
r  r, s  s , Q  Q, r and s are local to f
{r } f {Q
s}

 (Conjunction; Disjunction)

{r} f {Q
{r } f {Q
{r  r } f {Q  Q
{r  r } f {Q  Q

s}
s}
s  s }
s  s }

Justications for the meta-rules The lhs strengthening and rhs weakening
rules are inspired by similar rules for Hoare-triples. Additionally, since the
properties in Q are independent, any number of them may be removed.
For the conjunction rule, let the set of executions of f starting in r-state be r-
executions, and, similarly r -executions. The r  r -executions is the intersection
of r-executions and r -executions. Therefore, the postcondition of any execution
in r  r -executions satises s  s and every property in Q or Q , justifying the
conjunction rule. The arguments for the disjunction rule are similar.

4 Safety Properties

A safety property is perpetual. We develop a safety property, co, and its special
cases, taken from Misra [11]. Safety properties are derived from local annota-
tions and/or safety properties of the subcomponents of a component.

4.1 Safety Property co

Write p co q in component f , for predicates p and q that may not be local to
f , to mean that eective execution of any action of f in a p-state establishes a
q -state. Observe that an ineective execution preserves p. Thus, given p co q :
(1) in any execution of f once p holds it continues to hold until q is established,
though q may never be established, and (2) as a composition rule, p co q holds
in component i it holds in every subcomponent of it.
For an annotated component, co is dened by the following proof rule.

{r} f {s}
For every action b   with precondition pre in the annotation :
{pre  b  p}  {q}
{r} f {p co q
s}

As an example, the statement every change in integer variable ctr can only
increment its value may be formalized as ctr = m co ctr = m  ctr = m + 1,
for all integer m.

4.2 Special cases of co

Dene special cases of co for component f : stable, constant and invariant.
Given predicate p and expression e, in any execution of f : (1) stable p means
that p remains true once it becomes true, (2) constant e that the value of
e never changes, and (3) invariant p that p is always true, including after
termination, if the program terminates. Formally, in f

stable p
 p co p
constant e  (c :: stable e = c)
invariant p  initially p and stable p

Observe that invariant true (hence, stable true) and stable false are prop-
erties of every component. A variable for which f has no write-access is constant
in f , and so is any expression constructed out of such variables.
Derived rules for co and some of its special cases, which are heavily used
in actual proofs, are given in Appendix A.1. It follows from the derived rules
that a safety property of a program is a property of all its components, and
conversely, as given by the inheritance rule below.

4.3 Meta-rules

1. (Inheritance) If any safety property (co or any of its special cases) holds
in all subcomponents of f then it holds in f . More formally, for safety
properties  ,

Given:

(i :: {ri} fi {si})
{r} f {s}

Assert:

(i :: {ri} fi {
{r} f {

s}

si})

2. (Invariance) A local invariant of a component, i.e., a local predicate that is
invariant in the component, can be substituted ');
INSERT INTO posts (postId,userId,title,body) VALUES (420,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 6)','for true, and vice versa, in
any predicate in an annotation or property of the component. In particu-
lar, for a program all variables are local, so any invariant can be conjoined
to an assertion including the postcondition.

Justications for the meta-rules
Inheritance rule is based on the fact that
if a safety property holds for all components of f it holds for f as well. Given
the proof rule at left the inheritance proof rule at right can be asserted for any
set of safety properties  .
The invariance rule is from Chandy and Misra [2] where it is called the
substitution axiom. One consequence of the rule is that a local invariant of
f [] g , that may not be a local predicate of either f or g , could be conjoined to
predicates in an annotation of f [] g . Additionally, all variables in a program
are local; so, any invariant can be substituted for true in a program.

4.4 Example: Distributed Counter, contd.

For the example of Section 2.3 we prove that ctr behaves as a counter in that
its value can only be incremented, i.e., ctr = m co ctr = m  ctr = m + 1 in
f . Using the inheritance rule, it is sucient to prove this property in every
component fj . In fj , only j may change the value of ctr; so we need only show
the following whose proof is immediate:

{ctr = m  newj = oldj + 1  ctr = oldj } ctr := newj {ctr = m  ctr = m + 1}

5 Progress Properties

We are mostly interested in progress properties of the form if predicate p holds
at any point during the execution of a component, predicate q holds eventually.
Here eventually includes the current and all future moments in the execution.
This property, called leads-to, is dened in Section 5.3 (page 12). First, we
introduce two simpler progress properties, transient and ensures. Transient is a
fundamental progress property, the counterpart of the safety property stable.
It is introduced because its proof rules are easy to justify and it can be used
to dene ensures. However, it is rarely used in program specication because
ensures is far more useful in practice. Ensures is used to dene leads-to.

5.1 Progress Property: transient

In contrast to a stable predicate that remains true once it becomes true, a
transient predicate is guaranteed to be falsied eventually. That is, predicate
p is transient in component f implies that if p holds at any point during an
execution of f , p holds then or eventually in that execution. In temporal logic
notation p is transient is written as p. Note that p may not continue to
hold after p has been falsied. Predicate false is transient because false  true,
and, hence false holds whenever false holds. Note that given p transient in f ,
p holds at the termination point of f because, otherwise, f can take no further
steps to falsify p.
The proof rules are given in Figure 5.1. Below, postf is a local predicate of
f that is initially false and becomes true only on the termination of f . Such a
predicate always exists, say, by encoding the termination control point into it.
For a non-terminating program, postf is false. Proof of postf is a safety proof.

Justications The formal justication is based on induction on the program
structure: show that the basis rule is justied and then inductively prove the
remaining rules. We give informal justications below.
In the basis rule the hypotheses guarantee that each action of f is eectively
executed whenever p holds, and that the execution establishes p. If no action
can be executed eectively, because precondition of no action holds, the program
ha');
INSERT INTO posts (postId,userId,title,body) VALUES (421,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 7)','s terminated and postf holds. Hence, p  postf , i.e.(p  postf ), hold
eventually in all cases. Note that if pre  p then pre  p is false and the

--------------------------------------------------------------------------------

 (Basis)

{r} f {s}
For every action b   of f with precondition pre :
pre  p  b
{pre  p}  {p}
{r} f {transient p  postf

s}

 (Sequencing)
{r} f {transient p  postf
{postf } g {transient p
{r} f ; g {transient p

postf }}

 (Concurrency)
{r} f {transient p
{r} f [] g {transient p

}

 (Inheritance)
(i :: {ri} fi {si })
{r} f {s}

Given:

Assert:

(i :: {ri } fi {transient p
{r} f {transient p

s}

si })

--------------------------------------------------------------------------------

Figure 1: Denition of transient

hypotheses are vacuously satised. If f never terminates, postf always holds
and p is guaranteed eventually.
The next two rules, for sequential and concurrent composition, have weaker
hypotheses. The sequencing rule is based on an observation about a sequence
of actions, ;  . To prove transient p it is sucient that  establish p
or that it execute eectively, thus establishing post , and that  establish p.
The sequencing rule generalizes this observation to components. Being a local
predicate, postf can not be falsied by any concurrently executing component,
so it holds as long as the control remains at the termination point of f .
In the concurrency rule, as a tautology g either establishes p eventually,
thus guaranteeing the desired result, or preserves p forever. In the latter case,
f establishes p since transient p holds in f .
The inheritance rule applies to a program with multiple components.
It
asserts that if the property holds in each component fi then it holds in program
f . To see this consider two cases: f is seq or join, and argue by induction on
the program structure.
For seq f : if p holds at some point before termination of f it is within exactly
one direct subcomponent fi , or will do so without changing any variable value.
For example, if control precedes execution of ifb then f0 else f1 then it will
move to a point preceding f0 or f1 after evaluation of b without changing the

10

state. Note that fi may be a join, so there may be multiple program points
within it where control may reside simultaneously, but all controls reside within
one direct subcomponent of seq f at any moment. From the hypothesis, that
component, and hence, the program establishes p eventually.
For a join, say f [] g : Consider an execution in which, say, f has not ter-
minated when p holds. From the arguments for the concurrency rule, using
that transient p in f , eventually p is established in that execution. Similar
remarks apply for all executions in which g has not terminated. And, if both
f and g have terminated, then p holds from the denition of transient for
each component.


Notes

1. The basis rule by itself is sucient to dene an elementary form of tran-
sient. However, the transient predicate then has to be extremely elaborate,
typically encoding control point of the program, so that it is falsied by
every action of the component. The other rules permit simpler predicates
to be proven transient.

2. Basis rule is the only rule that needs program code for its application,
others are derived from properties of the components, and hence, permit
specication composition.

3. It is possible that p is eventually falsied in every execution of a component
though there is no proof for transient p. To see this consider the program
f [] g in which every action of f falsies p only if for some predicate q ,
p  q holds as a precondition, and every action of g falsies p only if p  q
holds as a precondition, and neither component modies q . Clearly, p
will be falsied eventually, but this fact can not be proved as a transient
property; only p  ');
INSERT INTO posts (postId,userId,title,body) VALUES (422,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 8)','q and p  q can be shown transient. As we show later,
p leads-to p.

5.2 Progress Property: ensures

Property ensures for component f , written as p en q with predicates p and q ,
says that if p holds at any moment in an execution of f then it continues to
hold until q holds, and q holds eventually. This claim applies even if p holds
after the termination of f . For initial state predicate r, it is written formally as
{r} f {p en q
} and dened as follows:

{r} f {p  q co p  q , transient p  q
{r} f {p en q
}

We see from the safety property in the hypothesis that once p holds it con-
tinues until q holds, and from the transient property that eventually q holds.
Corresponding to each proof rule for transient, there is a similar rule for en-
sures. These rules and additional derived rules for en are given in Appendix A.3
(page 16).

11

Example: Distributed counter, contd. We prove a progress property of
the annotated program from Section 3.3, reproduced below.

fj ::
initially oldj , newj = 0, 0
{true}
loop
{true}
j :: newj := oldj + 1;
{newj = oldj + 1}
if [ j :: {newj = oldj + 1} ctr = oldj  ctr := newj {true}
| j :: {newj = oldj + 1} ctr 6= oldj  oldj := ctr
{true}
{true}
forever

Our ultimate goal is to prove that for any integer m if ctr = m at some
point during an execution of f , eventually ctr > m. To this end let auxiliary
variable nb be the number of threads fj for which ctr 6= oldj . We prove the
following ensures property, (E), that says that every step of f either increases
ctr or decreases nb while preserving ctrs value. Proof uses the inheritance rule
from Appendix A.3 (page 16). For every fj and any m and N :

ctr = m  nb = N en ctr = m  nb < N  ctr > m in fj

(E)

We use the rules for en given in Appendix A.3 (page 16). First, to prove
(E) in g ; h, for any g and h, it is sucient to show that g terminates and (E) in
h. Hence, it is sucient to show that (E) holds only for the loop in fj , because
initialization always terminates. Next, using the inheritance rule, it is sucient
to show that (E) holds only for the body of the loop in fj . Further, since
j always terminates, (E) needs to be shown only for the if statement. Using
inheritance, prove (E) for j and j .
In each case, assume the precondition
ctr = m  nb = N of if and the preconditions of j and j . The postcondition
ctr = m  nb < N  ctr > m is easy to see in each of the following cases:

j :: {ctr = m  nb = N  newj = oldj + 1  ctr = oldj }
ctr := newj
{ctr = m  nb < N  ctr > m}

j :: {ctr = m  nb = N  newj = oldj + 1  ctr 6= oldj }
oldj := ctr
{ctr = m  nb < N  ctr > m}

5.3 Progress Property: Leads-to

The informal meaning of p 7 q (read: p leads-to q) is if p holds at any point
during an execution, q holds eventually. Unlike en, p is not required to hold
until q holds.

12

Leads-to is dened by the following three rules, taken from Chandy and
Misra [2]. The rules are easy to justify intuitively.

 (basis)

p en q
p 7 q

 (transitivity)

p 7 q , q 7 r
p 7 r

 (disjunction) For any (nite or innite) set of predicates S

(p : p  S : p 7 q)
(p : p  S : p) 7 q

Derived rules for 7 are given in Appendix A.4 (page 18).
leads-to is not
conjunctive, nor does it obey the inheritance rule, so even if p 7 q holds in both
f and g it may not hold in f [] g .

Example: Distributed counter, contd. We show that for the example of
Section 2.3 the counter ctr increases without bound. The proof is actual');
INSERT INTO posts (postId,userId,title,body) VALUES (423,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 9)','ly quite
simple. We use the induction rule for leads-to given in Appendix Section A.4.2.
The goal is to show that for any integer C , true 7 ctr > C . Below, all
properties are in f .

ctr = m  nb = N en ctr = m  nb < N  ctr > m
proven in Section 5.2
ctr = m  nb = N 7 ctr = m  nb < N  ctr > m
Applying the basis rule of leads-to
ctr = m 7 ctr > m
Induction rule, use the well-founded order < over natural numbers
true 7 ctr > C , for any integer C
Induction rule, use the well-founded order < over natural numbers.

6 Related Work

The earliest proof method for concurrent programs appears in Owicki and
Gries [13]. The method works well for small programs, but does not scale
up for large ones. Further it is limited to proving safety properties only. There
is no notion of component specication and their composition. Lamport [10]
rst identied leads-to for concurrent programs as the appropriate generaliza-
tion of termination for sequential programs (progress is called liveness in that
paper). Owicki and Lamport [14] is a pioneering paper.
The rst method to suggest proof rules in the style of Hoare [7], and thus a
specication technique, is due to Jones [8, 9]. Each component is annotated as-
suming that its environment preserves certain predicates. Then the assumptions
are discharged using the annotations of the various components. The method
though is restricted to safety properties only. A similar technique for message
communicating programs was proposed in Misra and Chandy [12].

13

The approaches above are all based on specifying allowed interface behav-
iors using compositional temporal logics. Since 2000, a number of proposals[6, 3]
have instead used traditional Hoare triples and resource/ob ject invariants, but
extending state predicates to include permissions describing how locations can
be used, and suitably generalizing the Hoare rule for disjoint parallel compo-
sition. However, these proposals typically address only global safety and local
termination properties, not global progress properties (as this paper does).
A completely dierent approach is suggested in the UNITY theory of Chandy
and Misra [2], and extended in Misra [11]. A restricted language for describing
programs is prescribed. There is no notion of associating assertions with pro-
gram points. Instead, the safety and progress specication of each component
is given by a set of properties that are proved directly. The specications of
components of a program can be composed to derive program properties. The
current paper extends this approach by removing the syntactic constraints on
programs, though the safety and progress properties of UNITY are the ones
used in this paper.
One of the essential questions in these proof methods is to propose the appro-
priate preconditions for actions. In Owicki and Gries [13] theory it is postulated
and proved. In UNITY the programmer supplies the preconditions, which are
often easily available for event-driven systems. Here, we derive preconditions
that remain valid in any environment; so there can be no a');
INSERT INTO posts (postId,userId,title,body) VALUES (424,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 10)','ssertion about shared
variables. The theory separates local precondition (obtained through annota-
tion) from global properties that may mention shared variables.
The proof strategy described in the paper is bilateral in the following sense.
An invariant, a perpetual property, may be used to strengthen a postcondition,
a terminal property, using the invariance rule. Conversely, a terminal property,
postcondition postf of f , may be employed to deduce a transient predicate, a
perpetual property.
Separation logic [16] has been eective in reasonong about concurrently ac-
cessed data structures. We are studying its relationship to the work described
here.

Acknowledgment
I am truly grateful to Ernie Cohen of Amazon and Jose
Meseguer of the Univ. of Illinois at Urbana, for reading the manuscript thor-
oughly and several times, endless discussions and substantive comments that
have shaped the paper. Presenting this work at the IFIPS Working Group 2.3
and the Programming Languages lunch at the University of Texas at Austin
has sharpened my understanding; I am grateful to the students and the other
attendees, in particular to Dhananjay Ra ju.

14

A Appendix: Derived Rules

A.1 Derived Rules for co and its special cases

The derived rules for co are given in Figure 2 and for the special cases in
Figure 3. The rules are easy to derive; see Chapter 5 of Misra [11].

false co q

p co true

p co q , p

 co q



p  p

 co q  q



(conjunction)

p co q

p  p

 co q

(lhs strengthening)

p co q , p

 co q



p  p

 co q  q



(disjunction)

p co q

p co q  q

 (rhs weakening)

Figure 2: Derived rules for co

The top two rules in Figure 2 are simple properties of Hoare triples. The
conjunction and disjunction rules follow from the conjunctivity, disjunctivity
and monotonicity properties of the weakest precondition, see Dijkstra [4] and
of logical implication. These rules generalize in the obvious manner to any set
nite or innite of co-properties, because weakest precondition and logical
implication are universally conjunctive and disjunctive.
The following rules for the special cases are easy to derive from the denition
of stable , invariant and constant . Special Cases of co

 (stable conjunction, stable disjunction)
p co q , stable r
p  r co q  r
p  r co q  r

 (Special case of the above)

stable p , stable q
stable p  q
stable p  q





invariant p , invariant q
invariant p  q
invariant p  q

{r} f {stable p
{r  p} f {p}

{r} f {constant e{r  e = c} f {e = c}

 (constant formation) Any expression built out of constants is constant.

Figure 3: Derived rules for the special cases of co

15

A.2 Derived Rules for transient

 transient false.

 (Strengthening) Given transient p, transient p  q for any q .

To prove transient false use the basis rule. The proof of the strengthening
rule uses induction on the number of applications of the proof rules in deriving
transient p. The proof is a template for proofs of many derived rules for ensures
and leads-to. Consider the dierent ways by which transient p can be proved
in a component. Basis rule gives the base case of ind');
INSERT INTO posts (postId,userId,title,body) VALUES (425,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 11)','uction.

1. (Basis) In component f , p is of the form p  postf for some p . Then in
some annotation of f where action b   has the precondition pre:
(1) pre  p  b, and (2) {pre  p}  {p}.
(1) From predicate calculus pre  p  q  b, and
(2) from Hoare logic {pre  p  q}  {p}. Applying the basis rule,
transient p  q  postf , i.e., transient p  q .

2. (Sequencing) In f ; g , transient p  postf in f and transient p in g . In-
ductively, transient p  q  postf in f and transient p  q in g . Applying
the sequencing rule, transient p  q .

3. (Concurrency, Inheritance) Similar proofs.

A.3 Derived Rules for en

A.3.1 Counterparts of rules for transient

This set of derived rules correspond to the similar rules for transient. Their
proofs are straight-forward using the denition of en.

 (Basis)

{r} f {s}
For every action b   with precondition pre in the annotation :
pre  p  q  b
{pre  p  q}  {q}
{r} f {p en p  s  q

s}

 (Sequencing)

{r} f {p en p  postf  q
{postf } g {p en q
{r} f ; g {p en q
}

postf }

 (Concurrency)

p en q in f
p  q co p  q in g
p en q in f [] g

16

 (Inheritance) Assuming the proof rule at left the inheritance proof rule at
right can be asserted.

Given:

(i :: {ri } fi {si})
{r} f {s}

Assert:
(i :: {ri} fi {p en q
{r} f {p en q

s}

si})

A.3.2 Additional derived rules

The following rules are easy to verify by expanding each ensures property by its
denition, and using the derived rules for transient and co. We show one such
proof, for the PSP rule. Observe that ensures is only partially conjunctive and
not disjunctive, unlike co.

1. (implication)

p  q
p en q
Consequently, false en q and p en true for any p and q .

2. (rhs weakening)

p en q
p en q  q 

3. (partial conjunction)

p en q
p en q
p  p en q

4. (lhs manipulation)

p  q  p  p  q
p en q  p en q
Observe that p  q  p  q and p  q  p  q . So, p and q are
interchangeable in all the proof rules. As special cases, p  q en q 
p en q  p  q en q .

5. (PSP) The general rule is at left, and a special case at right using stable r
as r co r.

(PSP)

p en q
r co s
p  r en (q  r)  (r  s)

(Special case)
p en q
stable r
p  r en q  r

6. (Special case of Concurrency)

p en q in f
stable p in g
p en q in f [] g

Proof of (PSP): From the hypotheses:

transient p  q
p  q co p  q
r co s

We have to show:

17



(1)
(2)
(3)

transient p  r  (q  r)  (r  s)
p  r  (q  r)  (r  s) co p  r  q  r  r  s

(4)
(5)

First, simplify the term on the rhs of (4) and lhs of (5) to p  r  q  s.
Proof of (4) is then immediate, as a strengthening of (1). For the proof of (5),
apply conjunction to (2) and (3) to get:

p  r  q co p  s  q  s
 {expand both terms in rhs}
p  r  q co p  r  s  p  r  s  q  r  s  q  r  s
 {lhs strengthening and rhs weakening}
p  r  q  s co p  r  q  r  r  s

A.4 Derived Rules for leads-to

The rules are taken from Misra [11] where the proofs are given. The rules are
divided into two classes, lightweight and heavyweight. The former includes rules
whose validity are easily established; the latter rules are not entirely obvious.
Each application of a heavyweight rule goes a long way toward completing a
progress proof.

A.4.1 Lightweight rules
p  q
p 7 q

1. (implication)

2. (lhs strengthening, rhs weakening)
p 7 q
p  p 7 q
p 7 q  q 

3. (disjunction)

7 qi )
(i :: pi
(i :: pi ) 7 (i :: qi )

where i is quantied over an arbitrary nite or innite index set, and pi , qi
are predicates.

4. (cancellation)

p 7 q  r , r 7 s
p 7 q  s

A.4.2 Heavyweight rules

1. (impossibility)

p 7 false
2. (PSP) The general rule is at left, and a special case at right using stable r
as r co r.

18

(PSP)

p 7 q
r co s
p  r 7 (q  r)  (r  s)

(Special case)
p 7 q
stable r
p  r 7 q  r

3. (induction) Let M be a total function from program states to a');
INSERT INTO posts (postId,userId,title,body) VALUES (426,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 12)',' well-
founded set (W, ). Variable m in the following premise ranges over W .
Predicates p and q do not contain free occurrences of variable m.

(m :: p  M = m 7 (p  M  m)  q)
p 7 q

4. (completion) Let pi and qi be predicates where i ranges over a nite set.

(i ::
7 qi  b
pi
qi co qi  b
(i :: pi ) 7 (i :: qi )  b

A.4.3 Lifting Rule

This rule permits lifting a leads-to property of f to f [] g with some modi-
cations. Let x be a tuple of some accessible variables of f that includes all
variables that f shares with g . Below, X is a free variable, therefore universally
quantied. Predicates p and q name accessible variables of f and g . Clearly,
any local variable of g named in p or q is treated as a constant in f .

p 7 q in f
r  x = X co x = X  r in g
p 7 q  r in f [] g

(L)

An informal justication of this rule is as follows. Any p-state in which
r holds, q  r holds. We show that in any execution of f [] g starting in a
(p  r)-state q or r holds eventually. If r is falsied by a step of f then r
holds. Therefore, assume that every step of f preserves r. Now if any step of
g changes the value of x then it falsies r from the antecedent, i.e., r holds.
So, assume that no step of g modies x. Then g does not modify any accessible
variable of f ; so, f is oblivious to the presence of g , and it establishes q .
As a special case, we can show

(L)

p 7 q in f
p  x = M 7 q  x 6= M in f [] g

The formal proof of (L) is by induction on the structure of the proof of
p 7 q in f . See
http://www.cs.utexas.edu/users/psp/unity/notes/UnionLiftingRule.pdf
for details.

19

B Example: Mutual exclusion

We prove a coarse-grained version of a 2-process mutual exclusion program due
to Peterson [15]. The given program has a nite number of states, so it is
amenable to model-checking. In fact, model-checking is a simpler alternative
to an axiomatic proof. We consider this example primarily because this is an
instance of a tightly-coupled system where the codes of all the components are
typically considered together to construct a proof. In contrast, we construct a
composable specication of each component and combine the specications to
derive a proof.

B.1 Program

The program has two processes M and M  . Process M has two local boolean
variables, try and cs where try remains true as long as M is attempting to enter
the critical section or in it and cs is true as long as it is in the critical section;
M  has try  and cs . They both have access to a shared boolean variable turn.
It simplies coding and proof to postulate an additional boolean variable turn
that is the complement of turn, i.e., turn  turn.
The global initialization and the code for M , along with a local annota-
tion, is given below. The code of M  is the dual of M , obtained by replacing
each variable in M by its primed counterpart. Henceforth, the primed and the
unprimed versions of the same variable re duals of each other.
The unrelated computation below refers to computation preceding the
attempt to enter the critical section that does not access any of the relevant
variables. This computation may or may not terminate in any iteration.

initially cs, cs = false, false  global initialization

M : initially try = false
{try , cs}
loop
 unrelated computation that may not terminate;

{try , cs} :

try , turn := true, true;

{try , cs}

 : try   turn

 cs := true;  Enter critical section

{try , cs}
forever

 :

try , cs := false, false  Exit critical section

Given that M  is the dual of M , from any property of M obtain its dual as
a property of M  . And, for any property of M [] M  its dual is a property of
M  [] M , i.e., M [] M  , thus reducing the proof length by around half.

Remarks on the program The given program is based on a simplication
of an algorithm in Peterson [1');
INSERT INTO posts (postId,userId,title,body) VALUES (427,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 13)','5]. In the original version the assignment in 

20

may be decoupled to the sequence try := true; turn := true. The tests in 
may be made separately for each disjunct in arbitrary order. Action  may
be written in sequential order try := false; cs := false. These changes can be
easily accommodated within our proof theory by introducing auxiliary variables
to record the program control.

B.2 Safety and progress properties

It is required to show in M [] M  (1) the safety property: both M and M  are
never simultaneously within their critical sections, i.e., invariant (cs  cs ),
and (2) the progress property: any process attempting to enter its critical section
will succeed eventually, i.e., try 7 cs and try  7 cs ; we prove just try 7 cs
since its dual also holds.

B.2.1 Safety proof: invariant (cs  cs )

We prove below:

invariant cs  try in M [] M 
invariant cs  try  turn in M [] M 

(S1)
(S2)

Mutual exclusion is immediate from (S1) and (S2), as follows.

cs  cs
 {from (S1) and its dual}
cs  try  cs  try 
 {rewriting}
(cs  try )  (cs  try  )
 {from (S2) and its dual}
turn  turn
 {turn and turn are complements}
false

Proofs of (S1) and (S2) First, we show the following stable properties of
M , which constitute its safety specication, from which (S1) and (S2) follow.

stable cs  try in M
stable try  turn in M
stable cs  try   turn in M

(S3)
(S4)
(S5)

The proofs of (S3), (S4) and (S5) are entirely straight-forward, but each
property has to be shown stable for each action. We show the proofs in Table 1,
where each row refers to one of the predicates in (S3  S5) and each column
to an action. An entry in the table is either: (1) post: p claiming that since
p is a postcondition of this action the given predicate is stable, or (2) una.
meaning that the variables in the given predicate are unaected by this action
execution. The only entry that is left out is for the case that  preserves (S5):
cs  try   turn ; this is shown as follows. The guard of  can be written as

21

try   turn and execution of  aects neither try  nor turn , so try   turn
is a postcondition, and so is cs  try   turn .

(S3)
(S4)
(S5)

cs  try
try  turn
cs  try   turn




post: cs
post: try
post: cs
post: turn una.: try , turn post: try
post: cs
see text
post: cs

Table 1: Proofs of (S3), (S4) and (S5)

Now we are ready to prove (S1) and (S2). The predicates in (S1) and (S2)
hold initially because initially cs, cs = false, false. Next, we show these predi-
cates to be stable. The proof is compositional, by proving each predicate to be
stable in both M and M  . The proof is simplied by duality of the codes.

 (S1) stable cs  try in M [] M  :

stable cs  try in M
stable cs  try in M 
stable cs  try in M [] M 

, (S3)
, cs and try are constant in M 
, Inheritance rule

 (S2) stable cs  try  turn in M [] M  :

stable try  turn in M
stable cs  try  turn in M
stable cs  try  turn in M 
stable cs  try  turn in M [] M 

, (S4)
, cs constant in M
, dual of (S5)
, Inheritance rule

B.2.2 Progress proof: try 7 cs in M [] M 

First, we prove a safety property:

try  cs co try  cs in M [] M 

(S6)

To prove (S6) rst prove try cs co try  cs in M , which is entirely straight-
forward. Now variables try and cs are local to M , therefore stable try  cs in
M  , and through rhs weakening, try  cs co try  cs in M  . Using inheritance
(S6) follows.

Next we prove a progress property:

try  (try   turn ) 7 try in M [] M 

(P)

First, prove try  (try   turn ) en try in M , using the s');
INSERT INTO posts (postId,userId,title,body) VALUES (428,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 14)','equencing rule for
en ; intuitively, in a try  (try   turn )-state the program control is never at
, execution of  terminates while preserving try  (try   turn ), and execution
of  establishes try .
Using duality on (S4) get stable try   turn in M  , i.e., stable try  
turn in M  . And try is local to M , so stable try in M  . Conjoining these two

22

properties, stable try  (try   turn ) in M  . Apply the concurrency rule for
en with stable try  (try   turn ) in M  and try  (try   turn ) en try
in M to conclude that try  (try   turn ) en try in M [] M  .
Now apply the basis rule for 7 to conclude the proof of (P).



Proof of try 7 cs in M [] M  : All the properties below are in M [] M  .
Conclude from (P), using lhs strengthening,

try  try  7 try
try  turn 7 try

The main proof:

(P1)
(P2)

try   turn 7 try 
, duality applied to (P2)
try  try   turn 7 try 
, lhs strengthening
try  (try   turn) 7 try , rewriting (P) using turn  turn
try 7 try  try 
, disjunction of the above two
try 7 try  (try   try )
, rewriting the rhs
, cancellation using (P1)
try 7 try
, (S6)
try  cs co try  cs
, PSP of above two
try  cs 7 cs
try 7 cs
, disjunction with try  cs 7 cs

C Example: Associative, Commutative fold

We consider a recursively dened program f where the code of f1 is given and
fk+1 is dened to be f1 [] fk . This structure dictates that the specication sk
of fk must permit proof of (1) s1 from the code of f1 , and (2) sk+1 from s1
and sk , using induction on k . This example illustrates the usefulness of the
various composition rules for the perpetual properties. The program is not easy
to understand intuitively; it does need a formal proof.

C.1

Informal Problem Description

Given is a bag u on which  is a commutative and associative binary operation.
Dene u, fold of u, to be the result of applying  repeatedly to all pairs of
elements of u until there is a single element. It is required to replace all the
elements of u by u. Program fk , for k  1, decreases the size of u by k while
preserving its fold. That is, fk transforms the original bag u to u such that:
(1) u = u , and (2) |u| + k = |u |, provided |u | > k , where |u| is the size of
u. Therefore, execution of fn1 , where n is the size of u and n > 1, computes
a single value in u that is the fold of the initial bag.
Below, get(x) removes an item from u and assigns its value to variable x.
This operation can be completed only if u has an item. And put(x  y ), a
non-blocking action, stores x  y in u. The formal semantics of get and put are
given by the following assertions where u is constant:

23

{u = u} get(z ) {u = u  {z }}
{u = u} put(z ) {u = u  {z }}

The fold program fk for all k , k  1, is given by:

f1 = |u| > 0  get(x); |u| > 0  get(y ); put(x  y )
fk+1 = f1 [] fk , k  1

Specication and proof of safety properties appear in Section C.2, next, and
progress properties in Section C.3. Observe that

C.2 Terminal property

The relevant safety property of fk , for all k , k  1, is a terminal property:

{u = u} fk {u = u , |u| + k = |u |}

This property can not be proved from a local annotation alone because it
names the shared variable u in its pre- and postconditions. We suggest an
enhanced safety property using certain auxiliary variables.

Auxiliary variables The following auxiliary variables of fk are local to it:

1. wk : the bag of items removed from u that are, as yet, unfolded. That is,
every get from u puts a copy of the item in wk , and put(x  y ) removes x
and y from wk . Initially wk = {}.

2. nhk : the number of halted threads where a thread halts after completing a
put. A get does not aect nhk and a put increments it. Initially nhk = 0.

Introduction of any auxiliary variable auxk for fk follows a pattern: (1)
specify the initial value of auxk , (2) dene aux1 b');
INSERT INTO posts (postId,userId,title,body) VALUES (429,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 15)','y modifying the code of f1 ,
corresponding to the basis of a recursive denition, and (3) dene auxk+1 in
terms of aux1 and auxk .
We adopt this pattern for dening wk and nhk for all k , k  1. First, the
initial values of wk and nhk are {} and 0, respectively. Second, w1 and nh1 are
dened below in f1 . Third, wk+1 = w1  wk and nhk+1 = nh1 + nhk , for all k .
The modied program for f1 is as follows where h   i is an action.

f1 = |u| > 0  hget(x); w1 := w1  {x}i;
|u| > 0  hget(y ); w1 := w1  {y}i;
hput(x  y ); w1 := w1  {x, y}; nh1 := 1i

Note: The denition of auxiliary variables is problematic for nh2 , for instance;
by denition, nh2 = nh1 + nh1 . However, these are two dierent instances of
nh1 referring to the local variables of the two instances of f1 in f2 . This is not
a problem in the forthcoming proofs because the proofs always refer to indices
1, k and k + 1, and, nhk is treated as being dierent from nh1 .

24

Specication of safety property The safety specication of fk , for all k ,
k  1, is given by:

{u = u , wk = {}, nhk = 0}
fk
{constant (u  wk ), constant |u| + |wk | + nhk
wk = {}, nhk = k}

(S)

 Proof of (S) for f1 : Construct the following local annotation of f1 .

{w1 = {}, nh1 = 0}
|u| > 0  hget(x); w1 := w1  {x}i;
{w1 = {x}, nh1 = 0}
|u| > 0  hget(y ); w1 := w1  {y}i;
{w1 = {x, y}, nh1 = 0}
hput(x  y ); w1 := w1  {x, y}; nh1 := 1i
{w1 = {}, nh1 = 1}

The perpetual and terminal properties of f1 in (S) are easily shown using
this annotation and employing the semantics of get and put.

 Proof of (S) for fk+1 : by induction on k . Use the following abbreviations in
the proof.

ak  wk = {}  nhk = 0
bk  constant (u  wk ), constant |u| + |wk | + nhk
ck  wk = {}  nhk = k

{a1} f1 {b1
, from the annotation of f1
c1}
, wk , nhk constant in f1
c1}
{a1} f1 {bk+1
, inductive hypothesis
ck }
{ak } fk {bk
{ak } fk {bk+1
, w1 , nh1 constant in fk
ck }
, join proof rule on (1,2)
{a1  ak } f1 [] fk {c1  ck }
c1  ck }, inheritance on (1,2)
{a1  ak } f1 [] fk {bk+1
ck+1 }
, ak+1  a1  ak and c1  ck  ck+1
{ak+1 } fk+1 {bk+1

(1)

(2)

The terminal property {u = u} fk {u = u ,
|u| + k = |u |} follows
from (S) as follows. The initial values of (u  wk ) and |u| + |wk | + nhk are,
respectively, u and |u |, so deduce that invariant (u  wk ) = u and also
invariant |u| + |wk | + nhk = |u |. Given that fk is a program whose terminal
properties are being proved, we may apply the invariance rule of Section 4.3
with these invariants. So, deduce u = u , |u| + k = |u | as a postcondition
of fk with the given precondition.

25

C.3 Specication and proof of progress property

The relevant progress property is that if u has more than k elements initially,
fk terminates eventually. That is, |u | > k 7 nhk = k in fk . Initially |u | =
|u| + |wk | + nhk , so it is enough to prove |u| + |wk | + nhk > k 7 nhk = k .
Abbreviate |u| + |wk | + nhk > k by pk and nhk = k by qk so that for all k ,
k  1, the required progress property is:

pk 7 qk in fk

(P)

The proof of (P) is by induction on k , as shown in Sections C.3.1 and C.3.2.

C.3.1 Progress proof, p1 7 q1 in f1

We show that p1 en q1 , from which p1 7 q1 follows by applying the basis rule
of leads-to. We reproduce the annotation of f1 for easy reference.

{w1 = {}, nh1 = 0}
|u| > 0  hget(x); w1 := w1  {x}i;
{w1 = {x}, nh1 = 0}
|u| > 0  hget(y ');
INSERT INTO posts (postId,userId,title,body) VALUES (430,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 16)','); w1 := w1  {y}i;
{w1 = {x, y}, nh1 = 0}
hput(x  y ); w1 := w1  {x, y}; nh1 := 1i
{w1 = {}, nh1 = 1}

Next, prove p1 en q1 using the sequencing rule of en, from Section A.3.1.
It amounts to showing that if p1 holds before any action then the action is
eectively executed and q1 holds on completion of f1 . As shown in Section C.2
p1 is stable, and from the annotation q1 holds on completion of f1 . Therefore, it
suces to show that if p1 holds initially then every action is eectively executed.
The put action is always eectively executed. Using the given annotation, the
verication conditions for the two get actions are shown below in full:

w1 = {}  nh1 = 0  |u| + |w1 | + nh1 > 1  |u| > 0, and
w1 = {x}  nh1 = 0  |u| + |w1 | + nh1 > 1  |u| > 0

These are easily proved.

C.3.2 Progress proof, pk+1 7 qk+1 in fk+1

The main part of the proof uses the observation that every action, put and
get, reduces a well-founded metric. The metric is the pair (|u| + |wk |, |u|) and
the order relation is lexicographic. Clearly, the metric is bounded from below
because each set size is at least 0. The crux of the proof is to show that if
program fk is not terminated there is some action that can be executed, i.e.,
there is no deadlock. Henceforth, abbreviate (|u| + |wk |, |u|) by zk and any pair
of non-negative integers by n.

26

First, observe that zk can only decrease or remain the same in fk , that is,
stable zk (cid:22) n in fk , where (cid:22) is the lexicographic ordering. The proof is by
induction on k and it follows the same pattern as all other safety proofs. In f1 ,
informally, every eective get preserves |u| + |w1 | and decreases |u|, and a put
decreases |u| + |w1 |. For the proof in fk+1 : from above, stable z1 (cid:22) n in f1 , and
inductively stable zk (cid:22) n in fk . Since constant wk in f1 and constant w1 in
fk , stable zk+1 (cid:22) n in both f1 and fk . Apply the inheritance rule to conclude
that stable zk+1 (cid:22) n in fk+1 .
The progress proof of pk+1 7 qk+1 in fk+1 is based on two simpler progress
results, (P1) and (P2).
(P1) says that any execution starting from pk+1 re-
sults in the termination of either f1 or fk . And, (P2) says that once either
f1 or fk terminates the other component also terminates. The desired result,
pk+1 7 qk+1 , follows by using transitivity on (P1) and (P2).

pk+1 7 pk+1  (q1  qk ) in fk+1
pk+1  (q1  qk ) 7 qk+1 in fk+1

(P1)
(P2)

The proofs mostly use the derived rules of leads-to from Section A.4. Note
that zk+1 includes all the shared variables between f1 and fk , namely u, so that
the lifting rule can be used with zk+1 . Also note that
pk+1  (p1  pk ), pk+1  q1  pk , pk+1  qk  p1 and q1  qk  qk+1 . As shown
in Section C.2 pk is constant, hence stable, and qk is also stable in fk .

Proof of (P1) pk+1 7 pk+1  (q1  qk ) in fk+1 : Below all properties are in
fk+1 . Lifting rule refers to rule (L) of Section A.4.3. We have already shown
p1 7 q1 and, inductively, pk 7 qk .

, Lifting rule on p1 7 q1 in f1
p1  zk+1 = n 7 q1  zk+1 6= n
pk  zk+1 = n 7 qk  zk+1 6= n
, Lifting rule on pk 7 qk in fk
(p1  pk )  zk+1 = n 7 (q1  qk )  zk+1 6= n
, disjunction
(p1  pk )  zk+1 = n 7 (q1  qk )  zk+1  n
, (PSP) with stable zk+1 (cid:22) n
pk+1  (p1  pk )  zk+1 = n 7 pk+1  (q1  qk )  pk+1  zk+1  n
, conjunction with stable pk+1
pk+1  zk+1 = n 7 pk+1  zk+1  n  pk+1  (q1  qk )
, pk+1  (p1  pk )
, induction rule of leads-to

pk+1 7 pk+1  (q1  qk )

27

Proof of (P2) pk+1  (q1  qk ) 7 qk+1 in fk+1 : Below all properties are in
fk+1 .

, Lifting rule on p1 7 q1 in f1
p1  zk+1 = n 7 q1  zk+1 6= n
, conjunction with sta');
INSERT INTO posts (postId,userId,title,body) VALUES (431,1266,'Bilateral Proofs of Safety and Progress Properties of Concurrent Programs (part 17)','ble zk+1 (cid:22) n
p1  zk+1 = n 7 q1  zk+1  n
qk  p1  zk+1 = n 7 qk  q1  qk  zk+1  n
, conjunction with stable qk
pk+1  qk  p1  zk+1 = n 7 pk+1  qk  q1  pk+1  qk  zk+1  n
, conjunction with stable pk+1
pk+1  qk  zk+1 = n 7 pk+1  qk  zk+1  n  pk+1  qk  q1
, pk+1  qk  p1
, induction rule of leads-to
, rhs weakening
, similarly
, disjunction and q1  qk  qk+1

pk+1  qk 7 pk+1  q1  qk
pk+1  qk 7 q1  qk
pk+1  q1 7 q1  qk
pk+1  (q1  qk ) 7 qk+1

D References

References

[1] Robert D. Blumofe. Executing Multithreaded Programs Eciently. PhD
thesis, Massachusetts Institute Of Technology, September 1995.

[2] K. Mani Chandy and Jayadev Misra. Paral lel Program Design: A Foun-
dation. Addison-Wesley, 1988.

[3] Ernie Cohen, Michal Moskal, Wolfram Schulte, and Stephan Tobies. Local
verication of global invariants in concurrent programs. In Computer Aided
Verication, 22nd International Conference, CAV 2010, Edinburgh, UK,
July 15-19, 2010. Proceedings, pages 480494, 2010.

[4] Edsger W. Dijkstra. Guarded commands, nondeterminacy, and the formal
derivation of programs. Communications of the ACM, 8:453457, 1975.

In F. Genuys, editor,
[5] E.W. Dijkstra. Cooperating sequential processes.
Programming Languages, pages 43112. Academic Press, 1968.

[6] Thomas Dinsdale-Young, Mike Dodds, Philippa Gardner, Matthew J.
Parkinson, and Viktor Vafeiadis. Concurrent abstract predicates. In Pro-
ceedings of the 24th European Conference on Object-oriented Programming,
ECOOP10, pages 504528, Berlin, Heidelberg, 2010. Springer-Verlag.

[7] C.A.R. Hoare. An axiomatic basis for computer programming. Communi-
cations of the ACM, 12:576580,583, 1969.

[8] C. B. Jones. Specication and design of (parallel) programs. In Proceedings
of IFIP83, pages 321332. North-Holland, 1983.

28

[9] C. B. Jones. Tentative steps toward a development method for interfering
programs. TOPLAS, 5(4):596619, 1983.

[10] Leslie Lamport. Proving the correctness of multiprocess programs. IEEE
Trans. on Software Engineering, SE-3(2):125143, Mar 1977.

[11] Jayadev Misra. A Discipline of Multiprogramming. Monographs in Com-
puter Science. Springer-Verlag New York Inc., New York, 2001.

[12] Jayadev Misra and K. Mani Chandy. Proofs of networks of processes. IEEE,
SE,7(4):417426, July 1981.

[13] S. Owicki and D. Gries. Verifying properties of parallel programs: an
axiomatic approach. Communications of the ACM, 19:279285, May 1976.

[14] S. Owicki and Leslie Lamport. Proving liveness properties of concurrent
programs. ACM Transactions on Programming Languages and Systems,
4(3):455495, July 1982.

[15] G.L. Peterson. Myths about the mutual exclusion problem. Information
Processing Letters, 12(3):115116, June 1981.

[16] John C. Reynolds. Separation logic: A logic for shared mutable data struc-
tures.
In Proceedings of the 17th Annual IEEE Symposium on Logic in
Computer Science, IEEE Computer Society, Washington, DC, USA, 2002.

29

');
INSERT INTO posts (postId,userId,title,body) VALUES (432,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','I . IN TRODUC T ION
The probabilistic roadmap (PRM) [1] is a cornerstone of
robot motion planning. It is widely used in practice or as
the foundation for more complex planning algorithms. The
method is divided into two phases: the PRM graph is rst
constructed followed by, potentially multiple, shortest path
queries on this graph to solve motion planning problems.
For a single motion planning query, a feasibility checking
subroutine executed repeatedly during PRM construction
dominates run-time. However, once the PRM is constructed
it can be reused for multiple motion planning queries or
modied slightly according to minor changes in the envi-
ronment. Applicability to multi-query problems is one of the
advantages of the PRM over tree-based planners such as
Rapidly exploring Random Trees (RRT) [2] and Expansive
Space Trees (EST) [3] which are tailored to single-query
problems.
Recent efforts have focused on ne tuning various aspects
of PRM-based motion planning for real-time applications.
Highly parallelized feasibility checking using FPGAs was
recently developed in [4] to alleviate this computational bot-
tleneck during the construction phase. The sparse roadmap
spanner was introduced in [5] to reduce memory required to
store the PRM and speed up the query phase by keeping
only a sparse subgraph with near-optimality properties.
In this paper we examine the effectiveness of a landmark
based admissible heuristic for reducing the running time of
the query phase of the PRM. The landmark heuristic was
originally developed for vehicle routing problems in road
networks [6] where many shortest path queries are solved
on a single graph. In theory, any amount of time spent
preprocessing the graph is negligible in comparison to the

a The authors are with The Institute for Dynamic Systems and Control
at ETH (email: {padenb,ynager,emilio.frazzoli}@ethz.ch).
b The rst two authors made equal contributions to this work.

time spent solving shortest path queries if sufciently many
queries must be solved. This observation suggests solving
the all-pairs shortest path problem in order to answer each
routing problem in constant time with respect to graph size.
However, the memory required to store a solution to the
all-pairs shortest is prohibitive for large road networks. The
landmark heuristic provides a trade-off between memory
requirements and query times by solving a small number of
single-source shortest path problems and using their solutions
to construct an effective heuristic for a particular graph.
This investigation is inspired by the similarities between
road networks and the PRM; multiple path queries are solved
on both graphs and both graphs are, in practice, too large to
store an all pairs shortest path solution in memory.
A useful feature of the landmark heuristic is that it can be
used together with the sparse roadmap spanner and FPGA-
based collision checking for a compounded speedup over a
standard PRM implementation. Based on the results pre-
sented in this paper, we conclude that the landmark heuristic
is effective on PRM graphs; solving shortest path queries
as much as 20 times faster than Dijkstras algorithm and
twice as fast as the Euclidean distance-based heuristic in
cluttered environments. The downside to the approach is that
constructing the heuristic requires preprocessing the graph
which adds to the computation time required before the PRM
can be');
INSERT INTO posts (postId,userId,title,body) VALUES (433,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ',' used for motion planning queries.
An overview of the motion planning problem is presented
in Section II, followed by a review of the build and query
phases of the PRM method. Section III introduces the land-
mark heuristic, discusses its admissibility and the complexity
of its construction, and illustrates its utility with a simple
shortest path problem. However, to better understand the
effectiveness of the landmark heuristic in general, we con-
struct randomized environments with a quantiable degree
of clutter and run numerous motion planning queries on
these environments to obtain the average case performance.
The environment construction and experimental results are
presented in Section IV. In Section V we evaluate the
landmark heuristic on a simulation of the Kinova Jaco robotic
manipulator and nd the landmark heuristic to be effective on
realistic robot models. Lastly, we conclude with a discussion
of our experimental observations in Section VI.

I I . MOT ION P LANN ING PROB L EM
The following optimal motion planning problem will be
addressed: Let Xfree be an open, bounded subset of Rd , and
 the set of continuous curves from [0, 1] to Rn . Then let
free be the subset of  whose image is contained in Xfree .
The cost objective is a function c :   [0, ) that assigns a

cost to each curve in Rd . The cost function must be additive
in the sense that if two curves 1,2   satisfy 1 ([0, 1]) 
2 ([0, 1]) then c(1 )  c(2 ).
An individual motion planning query on Xfree consists of
nding a curve    free from an initial state x0  Xfree to
a goal state xg  Xfree . That is,   (0) = x0 and   (1) = xg .
The subset of curves in free which satisfy these additional
endpoint constraints are denoted sol . In addition to nding a
curve in sol , we would like a curve   which approximately
minimizes the cost objective,

(1)

(c( )) + ,

c(  ) < inf
sol
for a xed  > 0. An approximate minimization is often used
for two reasons: the rst is that the problem may not admit
a minimum, and second, without further assumptions on the
cost objective and geometry of Xfree there are no practical
techniques available for obtaining exact solutions when they
exist.

A. Probabilistic Roadmaps

The set free has innite dimension so the conventional
approach to obtaining approximate solutions to motion plan-
ning problems is to construct a graph on Xfree whose vertices
are points in Xfree . To avoid confusion with curves on Xfree ,
a path is a sequence of vertices in a graph {xi } such that
(xi , xi+1 ) is an edge in the graph. Curves in free are
approximated using paths in the graph by associating each
edge of the graph with the line segment between the two
vertices making up that edge. The PRM method falls into
this category of approximations to free .
The PRM method [7] is a popular variation of the
PRM because it generates a sparse graph with the following
property: if x0 and xg belong to a connected subset of Xfree ,
then for any xed  > 0, the probability that the PRM graph
contains a curve   free satisfying

(c( )) + ,
c( ) < inf
sol
(cid:107) (0)  x0(cid:107) < ,
(cid:107) (1)  xg (cid:107) < ,

(2)

converges to 1 as the number of vertices is increased.

B. Graph Construction Phase

The construction phase of the PRM method is summa-
rized in Algorithm 1. The nearest(r, x, VPRM ) subroutine
returns the points v  VPRM \ {x} such that (cid:107)x v(cid:107) < r . The
subroutine sample(Xfree ) in Algorithm 1 returns a randomly
sampled point from the uniform distribution supported on
Xfree . The subroutine collisionFree(x, v) returns true if
the line segment connecting x to v is an element of free
and false otherwise. In reference to line 2 of Algorithm 1,
 ');
INSERT INTO posts (postId,userId,title,body) VALUES (434,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','is the Legesgue measure on Rd , and B1 (0) is the ball of
radius 1 centered at 0.

(cid:17) (cid:16) log(n)
(cid:17)(cid:17)1/d
(cid:16)
(cid:16) (Xfree )
Algorithm 1 PRM
1: VPRM  ; EPRM  
2: r =
(2 + 2/d)(B1 (0))
3: for i = 1, . . . , n
VPRM  VPRM  {sample(Xfree )}
4:
5: for x  VPRM
U  nearest(r, x, VPRM )
6:
for v  U \ {x}
7:
if collisionFree(x, v)
8:
EPRM = EPRM  {(x, v)}
9:
10: return (VPRM , EPRM )

C. Motion Planning Query Phase
After construction, paths in the graph (VPRM , EPRM ) can
be used to solve motion planning queries. One subtlety is that
the probability of x0 and xg being present in the PRM graph
is zero. There are a number of practical ways to resolve this
issue, but to keep the exposition as concise as possible we
will simply select the nearest vertex x0  VPRM to x0 and
xg  VPRM to xg as an approximation in light of (2).
Once initial and nal states x0 and xg are selected, the
motion planning query reduces to a shortest path problem
on the PRM graph with edge weights determined by the
cost of the line segments between vertices of the graph.
Algorithm 2 summarizes the A algorithm for nding a
shortest path in the PRM graph from x0 to xg . The function
parent : VPRM  VPRM  {NULL} is used to keep track of
the shortest path from x0 to each vertex examined by the
algorithm. Initially, parent maps all vertices of the graph
to NULL, but is redened in each iteration of the algorithm as
shorter paths from x0 to vertices in the graph are found. The
function label : VPRM  [0, ] maps each vertex to the
cost of the shortest known path reaching that vertex from x0 .
The function label initially maps all vertices to , but is
updated at each iteration with the cost of newly discovered
paths.
A set Q of vertices represents a priority queue. The
distinguishing feature of the A algorithm is the ordering of
vertices in the priority queue according to the labeled cost of
the vertex plus a heuristic estimate of the remaining cost to
reach the goal h : VPRM  [0, ]. The subroutine pop(Q)
returns a vertex x  Q such that
x  argmin
{label( ) + h( )}The heuristic h is called admissible if it never overestimates
the cost to reach the goal from a particular vertex. The A
algorithm is guaranteed to return the shortest path from x0
if the heuristic in equation (3) is admissible.
The pathToRoot subroutine returns the sequence of
vertices {vi}i=1,...,N , terminating at vN = x0 , generated by
the recursion

(3)

vi+1 = parent(vi ),
v1 = xg ,
If pathToRoot is evaluated in Algorithm 2, then its output
is a shortest path from x0 to xg .

(4)

For graphs with nonnegative edge-weights the heuristic
h(x) = 0 for all x  VPRM is clearly admissible. In this
special case, the A algorithm is equivalent to Dijkstras
algorithm. However, the more closely h underestimates the
optimal cost from each vertex to xg the fewer iterations
required by the A algorithm to nd the shortest path from
x0 to xg . Therefore, it is desirable to use a heuristic which
estimates the optimal cost to reach the goal as closely as
possible.
Algorithm 2 The A algorithm
1: Q  x0 ;
2: label( x0 )  0
3: while Q (cid:54)= 
v  pop(Q)
4:
if v = xg
5:
return pathToRoot(xg )
6:
S  neighbors(v)
7:
for w  S
8:
if label(v) + cost(v , w) < label(w)
9:
label(w)  label(v) + cost(v , w)
10:
parent(w)  v
11:
Q  Q  {w}
12:
13: return NO SOLUTION

(5)

c( ) =

When the cost functional is simply the length of the path,
as in equation (5), the canonical heuristic is the Euclidean
distance between x0 to xg which is the length of the optimal
(cid:90) 1
path in the absence of obstacles.
(cid:107) (cid:48) (t)(cid:107)2 dtThe Euclidean distance heuristic is specic to shortest path
objectives, and may not be admissible for cost functionals
other than (5).
I I I . TH E LANDMARK H EUR I ST IC
The landmark heuristic is tailored to a particular graph and
requires preprocessing the graph before it can be used in the
A algorithm. The resulting heuristic is admi');
INSERT INTO posts (postId,userId,title,body) VALUES (435,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','ssible regardless
of cost functional and environment making it a very general
approach to obtaining an admissible heuristic.
The idea behind the landmark heuristic is as follows: Let
d : VPRM  VPRM  [0, ] be the function which returns
the cost of a shortest path from one vertex of the graph
to another; taking the value  if no path exists. It follows
from the denition that d satises the triangle inequality.
Consider a vertex xl  VPRM that will represent a landmark.
Rearranging the triangle inequality with xl and xg yields
|d(x, xl )  d(xl , xg )|  d(x, xg ) x  VPRM .
(6)
Thus, the left hand side of (6) is a lower bound on cost of
the shortest path to xg . While computing d explicitly would
require solving the all-pairs shortest path problem, only the
solution to the single-source shortest path problem from xl
is required to evaluate (6).
When the lower bound in (6) is evaluated at a vertex
x that lies on or near to the shortest path from xl to xg

Fig. 1. Geometric illustration showing how the triangle inequality can be
rearranged to obtain a lower bound on the minimum cost path from x to
xg .

or vice-versa it provides a surprisingly close estimate of
the minimum cost path from x to xg . Figure 1 illustrates
this lower bound. However, obtaining an effective heuristic
for all origin-destination pairs requires having a collection
of landmarks Vl  VPRM . The landmark heuristic then
leverages (6) for each landmark:
{|d(x, xl )  d(xl , xg )|}.

h(x, xg ) = max
xlVl
To simplify the analysis presented in this paper, each
landmark is an i.i.d. random variable selected from the
uniform distribution on VPRM . However, other selection rules
can be used to improve the heuristic.

(7)

A. Complexity of the Landmark Heuristic
Generating the function d(, xl ) for an individual land-
mark requires solving a single-source shortest path problem
which can be accomplished with Dijkstras algorithm in
O(|VPRM | log(|VPRM |)) time1 where |  | denotes the car-
dinality of a set. Thus, the time complexity of constructing
the heuristic is in O(|Vl |  |VPRM | log(|VPRM |)). From this
observation it is clear that this heuristic is only useful in
instances where the number of motion planning queries that
will be evaluated on the PRM graph will be greater than |Vl |
since this many shortest path queries can be solved in the
time required to construct the heuristic. Then evaluating the
landmark heuristic (7) requires looking up the optimal cost
to a landmark 2  |Vl | times so the complexity of (7) is linear
in the number of landmarks.
Storing the cost of the shortest path to each vertex from
a landmark for use in (7) requires O(|VPRM |) memory per
landmark for a total memory requirement in O(|Vl |  |VPRM |).
The next question is how many landmarks should be used?
A natural choice is to select a xed fraction of the PRM
vertices to be landmarks. That is, |Vl | =   |VPRM | for
some constant . This results in O(|VPRM |2 ) space required
to store the heuristics lookup tables in memory. However,
with just than 16 landmarks, the landmark heuristic has been
observed to speed up routing queries by a factor of 9 to 16
on city to continent-scale road networks. On a PRM with a
shortest path objective, this observation can be made precise
as stated in the next result.

1 This assumes the PRM graph is constructed using Algorithm 1 which
has O(|VPRM | log(|VPRM |) edges [7].

Fig. 2. Shortest path from x0 (red circle) to xg (green circle) on a PRM
graph. Paths are computed with the Euclidean distance as a heuristic (left)
and the landmark-based heuristic (right). Colored markers represent vertices
examined in each search with color indicating the relative cost to reach that
vertex.
Lemma 1. If the number of landmarks relative to the number
of vertices is given by |Vl | =   |VPRM | for   (0, 1], t');
INSERT INTO posts (postId,userId,title,body) VALUES (436,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','hen
(8)
h(x, xg ) = d(x, xg ),
lim
|VPRM |

almost surely.

The proof can be found in the appendix. With increasing
graph size and an arbitrarily small fraction of vertices as-
signed to landmarks, the landmark heuristic will converge to
the solution of the all-pairs shortest path problem.

B. Demonstration of the Landmark Heuristic
To demonstrate the advantages of using the landmark
heuristic, it was compared with Dijkstras algorithm and A
with the Euclidean distance heuristic in a bug-trap environ-
ment. A PRM was constructed in the bug trap environment
according to Algorithm 1 with a density of 1000 vertices
per unit area for a total of 69, 272 vertices. The Landmark
heuristic was then constructed with 100 landmarks (0.14%
of vertices) obtained by randomly sampling from the vertices
of the graph.
Figure 2 shows the environment and vertices expanded by
the A algorithm using Euclidean distance as a heuristic and
the landmark heuristic. The A algorithm with Euclidean
distance heuristic required 58, 145 iterations and 351ms to
nd the shortest path; a marginal difference in performance
in comparison to the 69, 180 iterations and 334ms required
by Dijkstras algorithm. In contrast, the A algorithm with
the landmark heuristic required only 3, 338 iterations and
49ms to nd the shortest path.
The results of this demo can be reproduced with the
implementation of the landmark heuristic available in [8].

IV. EVALUAT ION IN RANDOM I Z ED ENV IRONM EN T S
Environments with randomly placed obstacles provides a
simple and easily reproducible benchmark for motion plan-
ning algorithms [9], [10]. In this paper, the degree of clutter
in these randomly generated environments is quantied as
the probability of the line segment connecting two randomly
sampled points being contained in Xfree .

Fig. 3. A PRM graph on a randomized environment with P(clear) =
0.5.
A. Environment Generation
A Poisson forest with intensity  of circular obstacles with
radius r is used as a random environment This is simulated
over a sample window S = [1, 1]2 by sampling the number
of obstacles N from the Poisson distribution

fN (n) =

(9)

((S ))n e(S )
n!
and then placing these obstacles randomly by sampling
from uniform distribution on S . The subset of S occupied
by the circular obstacles is denoted Xobs . Then we select
Xfree = [0.5, 0.5]2 \Xobs . Embedding Xfree in S simplies
subsequent calculations by eliminating boundary effects of
the sample window.
Let Z1 and Z2 be independent random variables with the
uniform distribution on Xfree , and let clear denote the
event that the line segment connecting Z1 and Z2 remains
in Xfree .
The next derivation relates the obstacle intensity  to the
marginal probability P(clear). Observe that a line segment
intersects a circular obstacle of radius r if and only if the
circle of radius r swept along this line segment contains the
obstacle center. If the obstacle is placed by sampling from
the uniform distribution on S , the probability of collision is
simply the ratio of the swept area of the circle along the
line segment and the area of S . Thus, conditioned on the
number of obstacles N and the points Z1 , Z2 , the probability
(cid:18) (S )  r2  2r(cid:107)Z1  Z2(cid:107)
(cid:19)N
of clear is
P(clear|N , Z1 , Z2 ) =(S )
(10)
Then the marginal probability P(clear) for a given obstacle
intensity  can be calculated by combining (9) and (10) to
obtain
(cid:82)
(cid:82)
(cid:80)
P (clear) =
Xfree
Xfree
nN

P (clear|n,z1 ,z2 )fN (n)
((Xfree ))2

(dz1 )(dz2 ).
(11)

In all of the numerical experiments of the next section
random environments with obstacle radius r = 0.05 were
used.

B. Numerical Experiments
Experiments were designed to evaluate how the effective-
ness of the landma');
INSERT INTO posts (postId,userId,title,body) VALUES (437,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','rk heuristic varies with with the parameter
P(clear) and to validate Lemma 1. To facilitate obtaining
the results in a reasonable time, experiments were run in
parallel on the central high-performance cluster EULER
(Erweiterbarer, Umweltfreundlicher, Leistungsfhiger ETH-
Rechner) of ETH Zrich. Each compute node consists of
two 12-Core Intel Xeon E5-2680 processors with clock rates
varying between 2.5-3.5 GHz.
In the rst set of trials a single random environment
was sampled with P(clear) = 0.05. Three PRM graphs
were constructed on this environment with 40,000, 60,000
and 80,000 vertices. On each PRM, 700 landmark heuris-
tics were constructed, 100 each for landmark quantities
|Vl |  {10, 30, 50, 70, 90, 110, 130}. Then for each landmark
heuristic, a random shortest path query is solved using A
with the landmark heuristic.
In the second set of trials, 20 logarithmically spaced
values for the parameter P(clear) from 0.01 to 1.0 were
selected. For each of these values 100 random environments
were generated according to the construction outlined in
Section IV-A. A PRM with 100,000 vertices per unit area
was constructed on each environment with 100, 000 ver-
tices per unit area. Then for each PRM, the 9 landmark
heuristics were constructed with landmark quantities |Vl | 
{10, 30, 50, 70, 90, 110, 130, 150, 170}. Finally, for each of
the 9 landmark heuristics, 100 shortest path queries were
evaluated on each PRM using A .

C. Results
The rst experiment, summarized in Figure 4, revealed
how the effectiveness of the landmark heuristic varied with
the fraction of vertices assigned to landmarks as well as
with varying graph sizes. We observed a rapid reduction in
iterations required to nd a solution relative to Dijkstras
algorithm with just 0.2% of vertices assigned to landmarks.
Secondly, the number of iterations required to nd a solution
with A relative to that of Dijkstras algorithm decreased
with increasing graph size. This validates Lemma 1 since
the number of iterations required by A decreases with an
improving estimate of the optimal cost to reach the goal.
In the second experiment we observed that the effective-
ness of the Euclidean distance heuristic rapidly diminishes
with increasing clutter, while the the landmark heuristic was
much less sensitive to P(clear). This is summarized in
Figure 5 where the landmark heuristic reduced the number of
iterations required to nd a solution by a factor greater than
20 in highly cluttered environments whereas the Euclidean
distance heuristic reduced the number of iterations by less
than a factor of 3.
This experiment also showed the diminishing returns of
increasing the number of landmarks in terms of iteration

Fig. 4.
Effectiveness of the landmark heuristic increases with both the
fraction of vertices assigned to landmarks and the number of vertices in the
graph.

Fig. 5.
Effectiveness of the landmark heuristic in comparison to the
Euclidean distance heuristic with varying degrees of clutter quantied by
P(clear).
time. Recall th');
INSERT INTO posts (postId,userId,title,body) VALUES (438,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','at evaluating the landmark heuristic in (7)
required checking the triangle equality for each landmark.
In Figure 6 the average running time of the A algorithm
with the landmark heuristic reaches a minimum with with
50 landmarks.

0%0.1%0.2%0.3%0.4%0.5%%Landmarks0.000.050.100.150.200.25AiterationsDijkstraiterations40000vertices60000vertices80000vertices102101100P(clear)0.000.050.100.150.200.250.30AiterationsDijkstraiterationsA10LandmarksA30LandmarksA50LandmarksA90LandmarksA130LandmarksA170LandmarksAEuclidianobjectives were considered for this problem, a shortest
path objective and a minimum mechanical work objective.
Motivation for using the shortest path objective is that the
Euclidean distance is available as an admissible heuristic. On
the other hand, minimizing the mechanical work required
to execute the motion is a more natural objective that is
likely similar to the motion a human would use for the
task. The drawback to the latter objective is that there is
no obvious heuristic to inform the A search. Since the
landmark heuristic is admissible regardless of the objective
it was applicable for this objective.
A 100, 000 vertex PRM was constructed followed by the
construction of a landmark heuristic with 50 landmarks. A
minimum work path was computed in 48 iterations and
7.5ms using the landmark heuristic while Dijkstras algo-
rithm required 25, 207 iterations and 209.4ms. A shortest
path was computed in 36 iterations and 5.6ms using the
landmark heuristic while using the Euclidean distance re-
quired 1, 238 iterations and 14.4ms. Figure 7 illustrates the
minimum energy motion that was computed.

V I . CONCLU S ION
The landmark heuristic is well known in the vehicle
routing literature where it has been shown to reduce shortest
path query times by a factor of 9 to 16 on city to continent-
scale road networks. Multi-query applications of the PRM in
robot motion planning have striking similarities with vehicle
routing problems in road networks in that shortest path
queries are evaluated repeatedly on a large graph. The goal
of this investigation was to evaluate the effectiveness of the
landmark heuristic in robotic motion planning applications.
Since the heuristic is based on preprocessing the PRM
graph, our hypothesis was that its effectiveness would be
independent of how densely cluttered the environment was
a useful feature for complex planning tasks.
To make this evaluation, we constructed a randomized
environment parameterized by the probability that the line
segment between two random points did not intersect obsta-
cles. The average case relative performance of the landmark
heuristic relative to the Euclidean distance heuristic was then
measured through numerous randomized trials. Additionally,
the performance of the landmark heuristic was evaluated on
a manipulator arm model in a realistic planning scenario.
The landmark heuristic was empirically observed to be
less sensitive to environment clutter than the Euclidean
distance heuristic. For the range of parameters evaluated,
the query times were red');
INSERT INTO posts (postId,userId,title,body) VALUES (439,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','uced by a factor of 5 to 20 in
comparison to Dijkstras algorithm. Secondly, a theoretical
analysis showed that, with a xed fraction of PRM vertices
assigned to be landmarks, the landmark heuristic converges
to the optimal cost between any origin-destination pair with
increasing graph size. This analysis was then validated in our
experimental results.
The landmark heuristic is an effective heuristic for query-
ing large PRM graphs. In particular, it is more effective than
the Euclidean distance heuristic in all but nearly obstacle
free problem instances. However,
the preprocessing time

Fig. 6. Running time of shortest path queries using the landmark heuristic
and Euclidean distance heuristic normalized by the running time using
Dijkstras algorithm.

Fig. 7.
Still frames of the minimum mechanical work motion generated
by the Jaco robotic arm when using a PRM with 100000 vertices and A
with the landmark heuristic.
V. ROBOT MAN I PU LATOR EXAM PL E

To demonstrate suitability of the landmark-based heuristic
for realistic manipulator models, we use a model of the six
degree of freedom Jaco manipulator by Kinova Robotics.
To simulate a complex planning task the arm must nd a
collision free motion through a window terminating with
the end effector near the ground to simulate reaching for
an object.
The landmark heuristic was implemented in the Open
Motion Planning Library (OMPL) [11] and the problem was
solved using the MoveIt [12] software tool. Two planning

102101100P(clear)0.000.050.100.150.200.250.300.35AquerytimeDijkstraquerytimeA10LandmarksA30LandmarksA50LandmarksA90LandmarksA130LandmarksA170LandmarksAEuclidianrequired to construct the heuristic makes it only suitable
for multi-query applications where the heuristic will be used
repeatedly on the same graph. A valuable direction for future
investigation would be an efcient update to the heuristic
when small changes are made to the PRM as a result of
changes in the workspace.

for constants ,   (0, 1) that depend only on the dimension
d of Xfree . One can readily verify that
this expression
converges to zero as |VPRM |  . Thus, the probability
Br/2 (zi )(cid:1) has a landmark as a neighbor almost surely
(cid:0)(cid:83)
that there is at least one landmark in each Br/2 (zi ) for
i  K converges 1. It follows that every vertex x  VPRM 
as |VPRM |  . Therefore, for at least one landmark l , the
iK
optimal cost from x to l satises d(x, l )  r . Thus,
{|d(x, xl )  d(xl , xg )|}
h(x, xg ) = max
xlVl
 d(x, l )  d(xg , l )
Expanding d(x, l ) with the triangle inequality between x,
l , and xg yields
h(x, xg )  d(x, xg )  d(xg , xn )  d(xg , xn )
(18)
 d(x, xg )  2r
Combining (6) and (17) we have d(x, xg )  2r  h(x, xg ) 
d(x, xg ), and since r  0 as |VPRM |   we obtain
on (cid:83)
h(x, xg ) = d(x, xg ),
lim
|VPRM |
Br/2 (zi ). The desired result (8) then follows in light
iK
of (14).

(17)

(19)

R E FER ENC E S
[1] L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Overmars, Prob-
abilistic roadmaps for path planning in high-dimensional conguration
spaces, IEEE ');
INSERT INTO posts (postId,userId,title,body) VALUES (440,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','transactions on Robotics and Automation, vol. 12, no. 4,
pp. 566580, 1996.
[2] S. M. LaValle and J. J. Kuffner, Randomized kinodynamic planning,
The International Journal of Robotics Research, vol. 20, no. 5,
pp. 378400, 2001.
[3] D. Hsu, J.-C. Latombe, and R. Motwani, Path planning in expansive
conguration spaces, in Robotics and Automation, 1997. Proceed-
ings., 1997 IEEE International Conference on, vol. 3, pp. 27192726,
IEEE, 1997.
[4] S. Murray, W. Floyd-Jones, Y. Qi, D. Sorin, and G. Konidaris, Robot
motion planning on a chip, in Robotics: Science and Systems, 2016.
[5] J. D. Marble and K. E. Bekris, Asymptotically near-optimal planning
with probabilistic roadmap spanners, IEEE Transactions on Robotics,
vol. 29, no. 2, pp. 432444, 2013.
[6] A. V. Goldberg and C. Harrelson, Computing the shortest path: A
search meets graph theory, in Proceedings of the sixteenth annual
ACM-SIAM symposium on Discrete algorithms, pp. 156165, Society
for Industrial and Applied Mathematics, 2005.
[7] S. Karaman and E. Frazzoli, Sampling-based algorithms for optimal
motion planning, The International Journal of Robotics Research,
vol. 30, no. 7, pp. 846894, 2011.
[8] B. Paden, Y. Nager, and E. Frazzoli, Landmark guided probabilistic
roadmap queries, 2017. Available at: https://github.com/
bapaden/Landmark_Guided_PRM/releases/tag/v0.
[9] J. D. Gammell, S. S. Srinivasa, and T. D. Barfoot, Batch informed
trees (bit*): Sampling-based optimal planning via the heuristically
guided search of implicit random geometric graphs, in 2015 IEEE In-
ternational Conference on Robotics and Automation (ICRA), pp. 3067
3074, IEEE, 2015.
[10] S. Karaman and E. Frazzoli, High-speed ight in an ergodic forest, in
Robotics and Automation (ICRA), 2012 IEEE International Conference
on, pp. 28992906, IEEE, 2012.
[11] I. A. Sucan, M. Moll, and L. E. Kavraki, The Open Motion Planning
Library, IEEE Robotics '||'&'||' Automation Magazine, vol. 19, pp. 7282,
December 2012. http://ompl.kavrakilab.org.
[12] I. A. Sucan and S. Chitta, Moveit!, 2016.

(14)

(15)

A P P END IX
The proof of Lemma 1 requires some additional notation.
The symbol  denotes the Lebesgue measure on Rd so that
the uniform probability measure of a measurable subset S
of Xfree is given by (S )/(Xfree ). Since each landmark
is an i.i.d. random variable with the uniform distribution on
} can be viewed as
Xfree , the set of landmarks {l1 , ..., l|Vl |
a random variable on the product space, denoted X |Vl |
free . The
probability of li  Si for subsets Si of Xfree is given by the
product measure m:
(cid:1) = m (cid:0)S1  ...  S|Vl |
(cid:1)
P (cid:0){l1 , ...l|Vl |
= (cid:81)|Vl |
}  S1  ...  S|Vl |
(Si )
i=1
(Xfree )
(12)
Next, an -net on Xfree is a subset {z1 , ..., zk } of Xfree
1) Xfree  (cid:83)M
such that
i=1 B (zi ),
2) B/2 (zi )  B/2 (zj ) =  i (cid:54)= j .
Based on these two properties it is clear that the number of
points k making up an -net on Xfree is bounded by
 k  (Xfree )
(Xfree )
(13)
(B ())
(B/2 ())Observe that not every B (zi )  Xfree is convex since it
may intersect the boundary of Xfree . The the index set K 
{1, ..., k} will identify open balls of the -net which have a
convex intersection with Xfree . As the -net becomes ner,
a greater fraction of points will lie on the interior of Xfree
(cid:91)
with a distance to the boundary greater than  so
lim
0
iK
Proof (Lemma 1). Consider the -net described above with
 = r/2, half the connection radius of the PRM. Note that
every vertex in Br/2 (zi ) is connected by a line segment for
i  K.
the landmarks {l1 , ..., l|Vl |
} 
The probability that
(cid:17)
(cid:16)
(l1 , ..., l|Vl | )  (cid:83)
Br/2 (zi ) =  for some i  K can be written as
 (cid:80)
r/2 (zi ))|Vl |
(B c(cid:17)|Vl |
(Br/');
INSERT INTO posts (postId,userId,title,body) VALUES (441,8825,'Brian Padena,b , Yannik Nagera,b , and Emilio Frazzolia 12rA6 O.c 168047:ir Abstract A landmark based heuristic is investigated ','4 )  (cid:16)
iK
r/2 (zi ))|Vl | )
m((B c
iK
 (Xfree )
1  (Br/2 (zi ))
(Xfree )
By inserting the expression for r in Algorithm 1 and replac-
ing |Vl | with   |VPRM |, the last expression in (15) simplies
(cid:18)
(cid:19)|VPRM |
to
1    log(|VPRM |)
|VPRM |

|VPRM |
log(|VPRM |)

B (zi ) = Xfree .

,

(16)



');
INSERT INTO posts (postId,userId,title,body) VALUES (442,4464,'The impact of random actions on opinion dynamics','Amir Leshemand Anna Scaglione

April 7, 2017

Abstract

Opinion dynamics have fascinated researchers for centuries. The
ability of societies to learn as well as the emergence of irrational herd-
ing are equally evident. The simplest example is that of agents that
have to determine a binary action, under peer pressure coming from
the decisions observed. By modifying several popular models for opin-
ion dynamics so that agents internalize actions rather than smooth
estimates of what other people think, we are able to prove that almost
surely the actions nal outcome remains random, even though actions
can be consensual or polarized depending on the model. This is a the-
oretical conrmation that the mechanism that leads to the emergence
of irrational herding behavior lies in the loss of nuanced information
regarding the privately held beliefs behind the individuals decisions.

Mathematical models of herding phenomena prove the old saying that
actions speak louder than words. A classical result shows that sequential
Bayesian learners give rise to dysfunctional information cascades because of
the overwhelming eect of the actions that are observed. In this work we
demonstrate that a variety of popular opinion dynamics models, modied so
that the agents internalize the eect of random actions rather than smooth
estimates their peers beliefs, create herding, polarized groups or do not
converge to a xed belief. Because of the generality of our analysis, our
work cements the notion that actions are poor representations of private
information and lead to unpredictable social herding phenomena.
There is a vast literature on the sub ject of opinion dynamics, aimed at
modeling how beliefs propagate across a social fabric. In the late 1700s [1]
Corresponding Author: Amir Leshem, Faculty of Engineering, Bar-Ilan University,
Ramat-Gan, 52900, Israel. Phone: +972-35317409. email: leshema@ biu.ac.il.
Dept. of EE Arizona State University.

rst noted that averaging individuals beliefs would, by the law of large num-
bers, lead to a noise free average opinion and precise estimate in the limit [1],
a concept later popularized broadly in the early 1900s [2]. The models used
to derive these insights are very similar to the Bayesian multi-agent models
that are pervasive in signal processing [3, 4, 5]. While in signal processing
decision criteria are often based on minimizing the expected Bayesian risk,
in economics, the decision model of rational agents has been long modeled
as maximizing the expected utility in the Bayesian sense. Furthermore, a
substantial body of the economics literature assumes that agents reveal im-
perfectly their private beliefs through their utility maximizing action. The
seminal paper by Bikhchandani, Hirshleifer and Welch (BHW) [6] (see also
[7]) highlighted how pathologies, called informational cascades, seem to de-
velop when agents make a zero-one decisions (for example, zero is bad
and one is good), instead of averaging continuous values as in [2]. The
BHW paper highlights the signicant impact of processing discrete actions,
as opposed to continuous belief values, on the nal behavior of the group.
This impact leads to what is often referred to as herding behavior [8]. Other
opinion diusions models and interactions of agents in social networks are
reviewed in e.g. [9, 10, 11, 12, 13, 14].
In the last forty years, a urry of models for opinion dynamics has
emerged in the statistical');
INSERT INTO posts (postId,userId,title,body) VALUES (443,4464,'The impact of random actions on opinion dynamics (part 2)',' physics literature [12], inspired by ideas aimed
at predicting macroscopic behavior that emerges from simple random par-
ticles interactions. Among them the voter model, introduced in [15], is one
of the simplest non-ergodic opinion diusion models that leads to herding.
This more closely resemble the zero-one interactions in [6] since the agents
have two discrete opinions and copy at random one of their neighbors. This
model was later mapped onto that of random walkers that coalesce upon
encounter by Liggett [16, 17] which made it analytically tractable. An in-
teresting variant is the one introduced by Mobilia in [18] where, using a
mean-eld approach, the author studied the eect of having stubborn agents
in the network that, by not changing their opinion, attempt to inuence the
nal equilibrium point. In recent work [19, 20, 21] re-examined this question
and analytically characterized its behavior by generalizing the approach in-
troduced by Liggett and providing a solution for the strategic placement of
stubborn agents in the voter model. Stubborn agents were also considered
by Jia et al. [22]
Szna jd and coauthors in [23] considered a dierent discrete opinion dy-
namic in which it takes two agents to convince their neighbors of the cor-
rectness of their opinion. By contrast, Galam in [24], proposed a variant in
which the opinions of the agents switched to the prevalent opinion (a ma-

jority rule) in their neighborhood. Since Szna jd dynamics are a special case
of the general sequential probabilistic model (GPM) in [25], only two dier-
ent phases are possible: either consensus, or coexistence of the two opinions
in equal proportions. Interestingly, in the presence of stubborn agents with
asynchronous updates, opinion uctuations exist and convergence is not cer-
tain [26].
Continuous belief models and updates, even in the presence of non-
linearities (e.g. bounded condence) are also a popular way to model so-
cial dynamics of opinions. A popular form is the Hegselmann-Krause (HK)
model (or the bounded condence (BC) model) [27, 28], where opinions are
represented by a real number and are updated synchronously, by averaging
all the agents opinions that dier by less than a condence level  (set to 1
in [28]). As long as the network is connected, the agents reach a consensus
[29] with the average belief in the network. A randomized variant of the HK
was introduced by DeGroot in [30] and studied in e.g.
[31, 32, 33, 34] in
which two randomly chosen neighbors meet and adjust their opinions only
when their opinion distance is below a threshold. These models lead either
to consensus or polarization, but opinions do converge.
Note that in the economics literature, due to the Bayesian update rule,
the agents have a continuous belief even if their action and decision variables
are discrete (the probability mass function of the decision variable given the
private information and history of the actions that were observed). Gener-
alizing the analysis in [6] to arbitrary social graphs is both intractable and
perhaps futile, given the ample evidence that social agents are not rational
[35].
In statistical physics the acronym CODA (continuous opinions discrete
actions) was coined much later in [36, 37]. This class of models follows the
general idea from economics of assuming that the agents internalize an es-
timate of the beliefs of the others obtained by observing their actions. But
contrary to the BHW model, CODA models postulate simple mechanisms of
opinion contagion, without oering signicant detail on the interpretation of
the models and providing mostly numerical characterization of their asymp-
totic behavior. The strong guarantees and mathematical characterizations
of the limiting behavior is completely known only for the simple chain of
sequential interactio');
INSERT INTO posts (postId,userId,title,body) VALUES (444,4464,'The impact of random actions on opinion dynamics (part 3)','ns treated in [6].
Our main results is to oer strong theoretical guarantees that the same
emergent behavior of random herding in the BHW model holds more broadly.
Our work supports the argument that it is the discrete and random nature
of what the agents internalize about the beliefs held by their neighbors that
produces the formation of random herds, i.e. herds whose nal decision out-

come that cannot be predicted with certainty and is, potentially, nefariously
wrong.

1 Consensus under the social pressure of actions

We assume each agent has only two options to chose from, and we label them
choice 0 and choice 1. Assume these choices are made by ipping a coin that
is biased by a belief variable xn (t), generating an action an (t)  {0, 1}. For
each n, an (t) is a Bernoulli random variable with probability xn (t).
To capture the agents tendency to conform to social norms, we assume
that the agents perturb their belief xn (t) by moving towards the weighted
mean of their neighbors actions. The agents update their belief as a convex
combination of the frequency of action 1 over their neighborhood and their
xn (t + 1) = (1  )xn (t) +  (cid:80)N
prior belief, i.e.:
k=1 wnk ak (t)
We also require that (cid:80)
where wnk is a weight function corresponding to the relative weight that
agent n gives to the action of agent k if (n, k) is and edge and zero otherwise.
k wn,k = 1. We require that the matrix W dened
by: will be the transition matrix of an irreducible Markov chain but we
do not require wn,n = 0 since it might be that agent n is inuenced by its
own action (even though it is random and determined by its internal belief
xn (t). Basic results from spectral graph theory show that this condition is
equivalent to having a network that has a single component, where we have
weighted edges between agents n, k whenever wn,k (cid:54)= 0. Note that xn (t) is
always an number in [0, 1] and assuming that not all xn (0) are identically 0 or
identically 1, we may assume without loss of generality that they are strictly
between 0 and 1 (since this will happen with probability 1 in nite time by
the averaging property). Introducing the vectors x(t) = (x1 (t), . . . , xN (t))T
and a(t) = (a1 (t), . . . , aN (t)) we note that:
E{a(t)|x(t)} = x(t),

(1)

(2)

and

Cov{a(t)|x(t)} = diag {v1 (t), ..., vn (t)}
(3)
where vj (t) = xj (t)(1  xj (t)) is the variance of agent j at time t. Further-
N(cid:89)
more,
n=1

n (t)(1  xn (t))(1an ) .
xan

P r (a(t) = a|x(t)) =

(4)

The value of  determines the inuence of the actions on the belief of each
agent1 . Let us dene also the matrix W which has elements wm,n and dene
W = (1  )I + W.
(5)
We note that for all 0    1 both W and W are stochastic matrices
with the same right and left eigenvectors, and spectral radius 1. Let  be
the left eigenvector of W corresponding to the largest eigenvalue 1 (nor-
malized such that 1T  = 1) and remember that the corresponding right
eigenvector is 1. By the Perron-Frobenius theorem we have  >> 0, i.e. all
its components are strictly positive. Denoting by
J (cid:44) 1T ,

(6)

By standard theory of stochastic matrices for all  the spectral radius of
W  J 0 < (W  J) < 1 and WJ = J.
Introducing the vectors
x(t) = (x1 (t), . . . , xN (t))T and a(t) = (a1 (t), . . . , aN (t)), we can also note
that (1) can be written in vector form as follows:
x(t + 1) = (1  )x(t) + Wa(t)

(7)

From (7) it is straightforward to prove the following:
Lemma 1 In general E{x(t + 1)|x(t)} = Wx(t) and E{Jx(t + 1)|x(t)} =
Jx(t).

The proof of the lemma is provided in the supplementary material. This
lemma claries the connection with the classic opinion dynamics model in-
trodu');
INSERT INTO posts (postId,userId,title,body) VALUES (445,4464,'The impact of random actions on opinion dynamics (part 4)','ced by DeGroot [30], which is identical to our model in expectation.
The weights in the matrix W in the DeGroot model typically areinterpreted
as being the trust individuals place on each other. The average dynamics are
also a classic example of the so called Average Consensus Gossiping (AGC)
algorithms [38]. For a connected network is well known that these dynamics
lead to consensus on the state. Note however (7) is not deterministic and
the fact that actions are discrete leads to very dierent dynamics than AGC
as the following lemma shows:

Lemma 2 The network has only two stationary states: x(t) = 1 and x(t) =
0.

1We assume that  is independent of n for notational simplicity, but all arguments go
through even when n depends on the player.

Proof: It is clear that in both cases, x(t) = 0 or x(t) = 1, if we evaluate (4)
for x(t) = 0 P r (a(t) = 0|0) = 1 and x(t) = 1 P r (a(t) = 1|1) = 1. This
implies (7) leads to a xed point x(t + 1) = x(t). Any other belief level will
place a non-zero probability in all a(t + 1)  {0, 1}N therefore making it
possible to deviate from the previous belief for every  > 0.
Interestingly, the fact that there are stationary states, does not suce
to prove that the beliefs indeed converge to the stationary states. This
convergence is the main result. Denote the expected value of x(t) by x (t):
x (t) = E{x(t)}.

(8)

Let the network weighted sample average of the beliefs according to the
stationary distribution  at time t be:

q(t) = T x(t).

(9)

The next lemma leverages results in the AGC literature [38]:

(10)

Lemma 3 The expected value in (8) is such that
t+ x (t) = E{q(0)}1.
lim
The simple proof of this lemma is relegated to the supplementary material
for brevity. While this property is not surprising, what is interesting is the
following result:
t xn (t)  {0, 1}(cid:17)
(cid:16)
Theorem 1 The mechanism in (1) leads to herding, i.e.
n,
lim
Moreover, the limit is identical to al l agents, i.e., either al l agents end up
with belief 0 or al l of them end up with belief 1.

= 1.

P r

The rst part of the proof of this lemma is similar to that used in [39] albeit
for a dierent opinion diusion model to show that the process converges
to a random variable with probability 1. However, in contrast to [39], our
model leads to herding while their model leads almost surely to consensus
on the true value of the parameter. The second part resorts to the Lesbegue
dominated convergence theorem to prove that the limiting random variable
must be equal to either 0 or 1:
Proof: We know that the only stationary belief levels are x = 0 or x = 1,
since for every other vector there is a positive probability that the average

action will cause a deviation up to , however, in contrast to the voters
model this is insucient, since when the initial state has all beliefs strictly
between 0 and 1, at any given stage the beliefs will be in the open interval
(0, 1) since they are convex combinations of the current belief with a num-
ber in [0, 1] and convergence is not achieved in nite time. Since W is a
stochastic matrix. Let q(t) = T x(t) be the average belief level in the net-
work according to the stationary distribution');
INSERT INTO posts (postId,userId,title,body) VALUES (446,4464,'The impact of random actions on opinion dynamics (part 5)',' of W . We will show that q(t)
is a martingale process with respect to the beliefs. To that end we observe
that:
q(t + 1) = T x(t + 1) = T [(1  )x(t) + (W)a(t)] .
where a(t) is the action vector at times t. Computing the expectation of
both sides conditioned on x(t) we observe that:
E {q(t + 1)|x(t)} = T Wx(t) = T x(t) = q(t),

(12)

(11)

since  is a left eigenvector of W with an eigenvalue equal to 1 and by
the Perron-Frobenius theorem all its elements are strictly positive, since the
network is connected (which is equivalent to the Markov chain dened by
W is irreducible. This implies that q(t) is a martingale with respect to
the sigma-algebra determined by the sequence of beliefs x(t). By denition
q(t) is a bounded sequence, as the weighted mean of a vector with elements
between 0 and 1. By the martingale convergence theorem the sequence q(t)
must converge to a random variable q with probability 1. We need to show
that q is almost surely either 0 or 1,
Consider now for t > 1 the sequence q(t) = q(t)  q(t  1), i.e. the
martingale dierence sequence. For each t by the martingale property,
E {q(t)} = 0.
(13)
Since  is a probability vector and for all n, t 0  xn (t)  1 we have that
for all n, t |q(t)| < 1 as well as |q(t)| < 1. Therefore, by the almost sure
convergence of q(t)

lim
t var(q(t)) = 0, a.s.

(14)

which implies that also
t var(q(t)|q(t  1)) = 0, a.s.
lim
Similarly q(t) < 1 and converges almost surely to 0. Hence by the Lesbegue
dominated convergence theorem q(t) converges to 0 in the mean square

(15)

var(q(t)|q(t  1)) = E (cid:8)q2 (t)|q(t  1)(cid:9)
sense. If we express explicitly the conditional variance of q(t)
= 2 (cid:80)N
= 2 (cid:80)N
n=1 2
nvar(an (t))
nxn (t)(1  xn (t)).
n=1 2
Since for all n, n > 0 the MS convergence implies that
 n.
t xn (t)(1  xn (t)) = 0
lim
Simple probabilistic computation shows that either for all n

lim
t xn (t) = 0 or

lim
t xn (t) = 1.

(16)

(17)

(18)

In fact, if for some j, n which are connected, xn (t) converges to 1 while
xj (t) converges to 0, since the actions are independent, there will be with
probability 1 innitely many deviations of at least min {|Wn,k | : wn,k (cid:54)= 0} of
both xn (t) and xj (t), contradicting the convergence of both xn (t) and xj (t).
This implies that herding must be achieved with probability 1, which
ends the proof of the main theorem.
It should be noted that while the limit variables xn () have values
0, 1, each initial condition has a dierent probability of converging to 0 or
1, which implies that what outcomes prevails, wrong or right, is random
and therefore social pressure can lead to umpredictable collective behavior,
in spite of the initial information the agents have available to forge their
opinion. We would like to determine its distribution given an initial value
or a distribution of initial values. The next corollary will show that xn () is
a binomial random variable with mean q(0). This implies that the variance
of xn () is q(0) (1  q(0)). In fact, the combination of the lemma above
and the main theorem leads to the following corollary:

Corollary 1 Let x(0) be the agents initial belief level vector. The prob-
ability that the agents herd to action an (t) = 1 is q(0) = 1T x(0), i.e.
limt+ P r(xn (t) = 1) = q(0).
Proof: Since, as t  +, the network can only be in one of the two xed
points x(t) = 1 or x(t) = 0, the probability that it will endup on one of
these two states is, as indicated in Lemma 3, equal to the average initial
belief.
Interestingly, no matter what the initial state is, we cannot predict the
outcome of the herding for sure. No matter how close the population belief

is to 0 or 1 initially, it is always possible that they will herd towards the op-
posite action. If it is indeed true that the social pressure mechanism is based
on internalizing the act');
INSERT INTO posts (postId,userId,title,body) VALUES (447,4464,'The impact of random actions on opinion dynamics (part 6)','ions of ones peers, we cannot predict accurately the
outcome of the social dynamics. Applying this to market behavior, where
people favor one product over the other, this indicates a quality brand that
has high probability of capturing a large percentage of the market if indi-
viduals were to choose in isolation, may actually fail and other rms with
worse products have a ghting chance to capture it. Furthermore, we can see
that even in large markets driven only by the consumers repeated actions,
a winner takes al l cascade eventually happens, and one product wins the
ma jority of the market share irrespective of the initial individual evidence
to the quality of the product. This may not always be the case however.
In the next section we generalize our analysis rst to random interactions
and show the same type of herding phenomenon happens as well in that
context. We also show that when social pressure is mitigated by mistrust
for what deviates excessively from ones prior belief, then the society may
split in multiple herds.

(19)

1.1 Randomized interactions
A simple variation of our model is one that captures random interactions
xn (t + 1) = (1  )xn (t) +  (cid:80)N
among the agents. In this case (1) becomes
k=1 wnk (t)ak (t)
where the weights wnk (t) are non zero only for those individuals who ob-
serve and are aected by their peers actions in that particular epoch. We
assume wnk (t) are random processes independent from the actions the nodes
performed, which continue to be modeled as in the previous section. Cor-
respondingly, we can dene the random matrices W(t) and W (t) as be-
fore.
Interestingly this variation of the model hardly changes the result
and random herding ensues also in this case almost surely. Let us now
assume that these random matrices are ergodic processes and denote by
W = E{W(t)}. If all nodes will speak innitely often with peers and the
average W = E{W(t)} is a stochastic matrix with eigenvalue 1 with multi-
plicity 1, then we can easily generalize the proof of our main theorem, using
the same denition of  as before:

Corollary 2 The statement of Theorem 1 holds unchanged if the interac-
tions are random and W = E{W(t)}.

Proof:

In this case we dene:

q(t) = T x(t)

In this case, like before:
E {q(t + 1)|x(t)} = T Wx(t) = T x(t) = q(t),

(20)

(21)

but now the average is also with respect to the process W(t). Nonetheless
everything else from this point on follows the steps of the proof of Theorem
1.

2 Bounded condence models

2.1 Bounded condence under the inuence of actions

An interesting generalization of our model is the update with bounded-
condence, where the nth agent updates happen only where the obser-
vations are suciently close to the agent own disposition. A natural option
is to analyze a model similar to the HK model proposed in [27] for continu-
ous opinion dynamics. In the HK model, agents mix their belief only with
agents whose belief suciently close to their own. In our case, agents update
their belief only if the empirical distribution of the actions of its neighbors
is suciently close to xn (t). We introduce the function:
(x) = xu(|x|   )

(22)

in which the condence threshold  < 1 and u(x) is the Heaviside (i.e. the
unit step) function. The agents update with bounded condence in its
(cid:32) N(cid:88)
(cid:33)
neighbors is modeled as:
wnk ak (t)  xn (t)
xn (t + 1) = xn (t) + 
k=1
We note right away that (0  0) = (1  1) = 0, which implies that x(t) = 1
(cid:33)
(cid:32)(cid:12)(cid:12)(cid:12) N(cid:88)
(cid:12)(cid:12)(cid:12)x(t)
(cid:12)(cid:12)(cid:12) > 
or x(t) = 0 are, once again xed points. Let us denote by:
wnk ak (t)  xn (t)
k=1
n (x(t)) = (1  Pn (x(t))).

Pn (x(t)) = P

(25)

Let:

(23)

(24)

10

We can dene a matrix W(x(t)) in the same way as (5), except that we
replace  with a diagonal matrix with diagonal elements n (x(t)).
It is
straightforward to see that
E{x(t + 1)|x(t)}');
INSERT INTO posts (postId,userId,title,body) VALUES (448,4464,'The impact of random actions on opinion dynamics (part 7)',' = W(x(t))x(t)

which are smooth non-linear dynamics equivalent to the HK model. In the
action-based HK dynamics a range of stationary beliefs are possible and, also
the beliefs may never converge even in distribution. In fact, next we provide
an example where the action-based HK model exhibits much more complex
behavior than the polarization phenomenon observed in the classical HK
model. Let us illustrate this fact with an example. Consider a network with
four nodes, with an initial beliefs vector x = (0, 0.45, 0.55, 1)T , a condence
 .
 0
threshold  = 0.25 and a mixing matrix:00.25 0.250.5
W =
0.50.25 0.2500
(cid:26)x1 (t)+u(|1x1 (t)|   )(1x1 (t)) a2 (t) = 1
In this case the dynamics of the four nodes are as follows. For the rst node:
(cid:26)x1 (t) = 0
x1 (t)u(x1 (t)   )x1 (t)
a2 (t) = 0
a2 (t) = 1
(1)x1 (t) = 0 a2 (t) = 0

x1 (t + 1) =

(26)

(27)

(28)

and the reason why x1 (t) = 0 is that we assumed that x1 (0) = 0 and all
later values remain unchanged. Similarly, without giving a proof we can
show that x4 (t) = 1 (just by symmetry 1  x4 (t) behaves like x1 (t)). For
node number two, when update is activated, the node has a new state that
is a convex combination of x2 (t) with either 0.25 or 0.5. Therefore:
(1  )x2 (t) +  0.25  x2 (t + 1)  (1  )x2 (t) +  0.5

(29)

and, since the node starts at x2 (0) = 0.45, the subsequent values will have
to remain conned in the interval (0.25, 0.5). For similar reasons x3 (t) will
remain in the interval (0.5, 0.75). However, they both will change randomly.
In fact, x2 (t) will change due to a3 (t) and viceversa x3 (t) will change based
on a2 (t). Therefore no convergence in distribution is attained and the opin-
ions will continue to uctuate indenitely in the intervals discussed above.
This is shown numerically in the next section in Fig. 4.

11

2.2 Reinforcement model with random pairwise interactions

We propose an alternative model for the update. We assume that if agents
n observe at random one of its neighbors, say the kth neighbor, make the
same decision, agent n will adjust the belief as follows:
xn (t + 1) = xn (t) +  [an (t)  ak (t)](an (t)  xn (t)),

(30)

where  [x] is the Kronecker delta function.
We view this model as a mechanism that reinforces behavior, because it
tends to decrease or increase the belief based on the frequency with which
ones action is repeated in the neighborhood. In this case the only xed points
for the dynamics x have integer entries, but unlike either (12) or (21), in this
case the network may never coalesce in taking a single action, but opposite
decisions may persist as well. The expectation of these dynamics given a
random pair (n, k) is:
E{xn (t + 1)|x(t), (n, k)} = xn (t)[1  (1  xn (t))(1  xk (t))]
 xn (t).

(31)

Since this holds for each random choice of (n, k) we observe that from the
previous equations, taking the expectation also over the random choices of
(n, k) it is clear that the state is a sub-martingale:
E{xn (t + 1)|x(t)}  xn (t)

(32)

Next we prove the following lemma:
t xn (t)  {0, 1}(cid:17)
(cid:16)
Theorem 2 The mechanism in (30) leads to
n,
lim
but not necessarily herding, which in turn means society wil l be polarized in
general.

= 1.

P r

Proof: The argument follow');
INSERT INTO posts (postId,userId,title,body) VALUES (449,4464,'The impact of random actions on opinion dynamics (part 8)','s a similar line of reasoning as our previous
theorem. Because of the dominated convergence theorem the state xn (t)
must converge in the mean square sense. We can see then that the sequence
ynk (t) =  [an (t)  ak (t)](an (t)  xn (t)) must go to zero in the mean square
sense, because each pair is selected at random innitely often. This can
happen in only two cases, either the conditional variance of an (t)  xn (t)
goes to zero or  [an (t)  ak (t)] = 0 with probability one. If we assume the

12

rst is not true, then the second implies that the independent actions of the
random pair of agents are the same, which can only be if they have equal
probability xn (t) = xk (t)  {0, 1}; while this cannot happen in nite time
it can happen at the limit.
If this does not happen then the variance of
an (t) in the limit must be zero which again implies that in the limit the
probability converges to either 1 or 0. Both lead to the same conclusion,
which proves the theorem.

3 Simulated experiments

3.1 Consensus under the social pressure of actions

To demonstrate the herding and clustering phenomenon we proved to be
true in Section 1, we performed several Monte-Carlo trials.
In the rst
experiments we randomly picked a random sample graph shown in Figure
1.

Figure 1: The random graph used for simulations.

Then we picked randomly 5000 initial vectors x(0) whose entries are
independent and drawn to have a mean  n E{xn (0)} = p0 varying from 0.2
to 0.8 and chose uniform weights wkn = 1/deg(k), where deg(k) denotes the
degree of node k (the graph is undirected).
Figure 2 shows the histogram of the limit for various random realizations

13

-2-1.5-1-0.500.511.52-2.5-2-1.5-1-0.500.511.522.512345678910Figure 2: Histograms of the nal belief for p0 = 0.2 and p0 = 0.6.

of the process for initial belief levels of p0 = 0.2, 0.6. Figure 3 shows clearly
that the actions are Bernoulli random variables whose parameter is correctly
predicted by the theorem.

3.2 Action-based HK model

In Figure 4 we simulated the dynamics discussed in Section 2 pertaining
four nodes with W in (26). Note that there is no agreement but also no
convergence of belief, in contrast to the classical HK model and the claims
made in Section 2 are corroborated by the simulation results. Furthermore
the random process of the opinions of the agents is very complex, as one can
notice from the complex non linear dynamics of the mean. This shows that
it is much harder to predict a form of polarization because the random pro-
cesses x2 (t), x3 (t) are governed by non-linear stochastic dynamics that are
Markovian but non homogeneous, whose trends are dicult to manipulate
mathematically to obtain optimum forecasts. Either than resorting to the
bounds we mentioned, accurate predictions become intractable, even when
the initial conditions are known.

3.3 Reinforcement model

In Section 2 we introduced a new model in which nodes move their belief
when their action is identical to one of their neighbors, selected at random.
In this case we showed that the network in Figure 1, with uniform initial
belief equal to 0.5, converges to possibly multiple herds. This is illustrated
in our three numerical simulation of the dynamics of the beliefs in Figure 5
where we can clearly observe the emergence of herding towards belief 1 and

14

00.20.40.60.8100.10.20.30.40.50.60.70.80.91p0=0.200.20.40.60.8100.10.20.30.40.50.60.70.80.91p0=0.6Figure 3: Mean and variance of the limit point as a function of p0 .

belief 0 by all nodes in the top and bottom gures respectively, as well as
the emergence of polarized groups, whose belief is 1 or 0, in the middle plot.
This corroborate the statement in Theorem 2.

4 Conclusions

In this paper we examined an opinion diusion model in which agents update
their belief based on their neighbors empirical distribution of the actions.
The use of the observed actions leads to herding unlike ');
INSERT INTO posts (postId,userId,title,body) VALUES (450,4464,'The impact of random actions on opinion dynamics (part 9)','continuous state
updates when neither bounded condence nor stubborn agents are present
in the system showing that the trend observed in the BHW model extends
to other settings.

5 Appendix

Proof of Lemma 3

We now prove prove Lemma 3 Taking the expectation on both sides of 7,
we obtain:

x (t + 1) = (1  )E{x(t)} + WE{E{a(t)|x(t)}}
tx (0)
= Wx (t) = W

(33)

15

0.10.20.30.40.50.60.70.80.900.51E{xn(0)}=p0E{xn()}  simulationtheoretical0.10.20.30.40.50.60.70.80.90.050.10.150.20.25E{xn(0)}=p0Var(xn())  simulationtheoreticalFigure 4: The state of the four node network with W in (26).

which is the AGC update that converges to consensus in the limit. In fact

t+ Wt = J
lim

(34)

and

Jx (0) = 1T x (0)
(35)
which proves the claim, since E{q(0)} = T x (0). In addition to the lemmas
and theorems in the main paper, the following result holds

Lemma 4 In general

and

E{x(t + 1)|x(t)} = Wx(t)

E{Jx(t + 1)|x(t)} = Jx(t).

Proof:
E{x(t + 1)|x(t)} = (1  )x(t) + WE{a(t + 1)|x(t)}
= (1  )x(t) + Wx(t) = Wx(t).

(36)

(37)

(38)

16

012345678910x 10400.10.20.30.40.50.60.70.80.91  x1(t)x2(t)x3(t)x4(t)Figure 5: Three experiments of the dynamics of the agents beliefs with same
initial beliefs that converge to herding (top 0 and bottom 1) and polarization
(middle).

The second claim follows easily since JW = J. This shows that in expec-
tation the belief follows the same dynamics as the DeGroot model [30].

Simulations of herding under random pairwise interactions

We also tested the dynamics where each time two random nodes average
their actions, discussed in the Randomized interactions section, and use
the averaged action to update their mutual beliefs. Figure 6 describes the
random graph used. In this case we describe the course of two dynamics
of the population, both where all nodes had an initial belief of 0.9, i.e. all
nodes were very inclined to take action 1. The top Figure 7 presents a
dynamics converging to 1. We present the lowest belief, the highest belief
and the mean belief. As can be seen when the dynamics converge to 1 this
happens quite rapidly, because of the initial state of all nodes. The bottom
of Figure 7 presents another instance of the dynamics, where initial beliefs
were identical to the previous case, but the limiting value of the herding was
0. With the initial conditions dened in the simulation, this occurs only 10%
of the simulations. Still this is surprising, as it would be expected that the

17

initial belief will have signicant impact on the results of the herding. As
predicted by the main theorem, the direction of the herding is unexpected,
and governed by the random actions of the agents.

Figure 6: 10 nodes networks used for the asynchronous dynamics.

18

References

[1] de Caritat (marquis de Condorcet) JAN (1785) Essai sur lapplication
de lanalyse `a la probabilite des decisions rendues `a la pluralitee des
voix. (De lImprimerie royale).

[2] Galton F (1907) Vox populi. Nature 75:450451.

[3] Vikram Krishnamurthy ONG, Hamdi M (2014) Interactive sensing and
decision making in social networks. Foundations and Trends in Signal
Processing 7(1-2):1196.

[4] Krishnamurthy V, Poor HV (2014) A tutorial on interactive sensing in
social networks.

[5] Sayed AH (2014) Adaptation, l');
INSERT INTO posts (postId,userId,title,body) VALUES (451,4464,'The impact of random actions on opinion dynamics (part 10)','earning, and optimization over networks.
Foundations and Trends in Machine Learning 7(4-5):311801.

[6] Bikhchandani S, Hirshleifer D, Welch I (1992) A theory of fads, fashion,
custom, and cultural change as informational cascades in Journal of
Political Economy. Vol. 100, pp. 9921026.

[7] Lohmann S (1994) Dynamics of informational cascades: The monday
demonstrations in leipzig, east germany, 1989-1991 in World Politics.
Vol. 47, pp. 42101.

[8] Chamley C (2004) Rational herds: Economic models of social learning.
(Cambridge University Press).

[9] Jackson M (2008) Social and Economic Networks. (Princeton University
Press).

[10] Easley D, Kleinberg J (2010) Networks, Crowds, and Markets: Reason-
ing About a Highly Connected World. (Cambridge University Press).

[11] Acemoglu D, Dahleh M, Lobel I, Ozdaglar A (2010) Bayesian learning
in social networks in Review of Economic Studies. Vol. 78, pp. 1201
1236.

[12] Castellano C, Fortunato S, Loreto V (2009) Statistical physics of social
dynamics. Reviews of modern physics 81(2):591.

[13] Krishnamurthy V, Poor HV (2013) Social learning and bayesian games
in multiagent signal processing: How do local and global decision mak-
ers interact? IEEE Signal Processing Magazine 30(3):4357.

19

[14] Friedkin NE (2006) A structural theory of social inuence. (Cambridge
University Press) Vol. 13.

[15] Cliord P, Sudbury A (1973) A model for spatial conict. Biometrika
60(3):581588.

[16] Liggett TM (1985) Particle systems.

[17] Holley RA, Liggett TM (1975) Ergodic theorems for weakly interacting
innite systems and the voter model. The annals of probability pp.
643663.

[18] Mobilia M (2003) Does a single zealot aect an innite group of voters?
Physical review letters 91(2):028701.

[19] Yildiz E, Ozdaglar A, Acemoglu D, Scaglione A (2010) The voter model
with stubborn agents extended abstract in Communication, Control,
and Computing (Al lerton), 2010 48th Annual Al lerton Conference on.
(IEEE), pp. 11791181.

[20] Yildiz E, Acemoglu D, Ozdaglar A, Saberi A, Scaglione A (2011) Dis-
crete opinion dynamics with stubborn agents. SSRN eLibrary.

[21] Yildiz E, Ozdaglar A, Acemoglu D, Saberi A, Scaglione A (2013) Bi-
nary opinion dynamics with stubborn agents. ACM Transactions on
Economics and Computation 1(4):19.

[22] Jia P, MirTabatabaei A, Friedkin NE, Bullo F (2015) Opinion dynamics
and the evolution of social power in inuence networks. SIAM review
57(3):367397.

[23] Szna jd-Weron K, Szna jd J (2000) Opinion evolution in closed commu-
nity. International Journal of Modern Physics C 11(06):11571165.

[24] Galam S (2002) Minority opinion spreading in random geometry. The
European Physical Journal B-Condensed Matter and Complex Systems
25(4):403406.

[25] Galam S (2005) Local dynamics vs. social mechanisms: A unifying
framework. EPL (Europhysics Letters) 70(6):705.

[26] Acemoglu D, Como G, Fagnani F, Ozdaglar A (2013) Opinion uctua-
tions and disagreement in social networks. Mathematics of Operations
Research 38(1):127.

20

[27] Hegselmann R, Krause U (2002) Opinion dynamics and bounded con-
dence models, analysis and simulations in Jounral of Articial Societies
and Social Simulation. Vol. 5.

[28] Blondel V, Hendrickx J, Tsitsiklis J (2009) On krauses multi-agent con-
sensus model with state-dependent connectivity in IEEE Trans. Auto.
Control. Vol. 54, pp. 25862597.

[29] Tsitsiklis J (1984) Ph.D. thesis (Dept. of Electrical Engineering and
Computer Science, M.I.T., Boston, MA).

[30] DeGroot M (1974) Reaching a consensus in Journal of American
Statistcal Association. Vol. 69, pp. 118121.

[31] Deuant G, Neau D, Amblard F, Weisbuch G (2000) Mixing beliefs
among interacting agents in Adv. Compl. Syst. Vol. 3, pp. 8798.

[32] Weisbuch G, Deuant G, Amblard F, Nadal J (2001) Interacting agents
and continuous opinions dynamics in Heterogeneous agents');
INSERT INTO posts (postId,userId,title,body) VALUES (452,4464,'The impact of random actions on opinion dynamics (part 11)',', interac-
tions, and economic performance.

[33] Weisbuch G (2004) Bounded condence and social networks in The Eu-
ropean Physical Journal B - Condensed Matter and Complex Systems.
Vol. 38, pp. 339343.

[34] Li L, Scaglione A, Swami A, Zhao Q (2013) Consensus, polarization
and clustering of opinions in social networks. Selected Areas in Com-
munications, IEEE Journal on 31(6):10721083.

[35] Kahneman D (2003) Maps of bounded rationality: Psychology for be-
havioral economics. American economic review pp. 14491475.

[36] Martins AC (2008) Continuous opinions and discrete actions in opin-
ion dynamics problems.
International Journal of Modern Physics C
19(04):617624.

[37] Martins AC, Pereira CdB, Vicente R (2009) An opinion dynamics model
for the diusion of innovations. Physica A: Statistical Mechanics and
its Applications 388(15):32253232.

[38] Dimakis A, Kar S, Moura J, Rabbat M, Scaglione A (2010) Gossip
algorithms for distributed signal processing. Proceedings of the IEEE
98(11):1847 1864.

21

[39] Jadbabaie A, Molavi P, Sandroni A, Tahbaz-Salehi A (2012) Non-
bayesian social learning. Games and Economic Behavior 76(1):210225.

22

Figure 7: Opinion dynamics herding to 1 (top) and to 0 (bottom), xn (0) =
0.9, n.

23

0501001502002503000.40.50.60.70.80.91txn(t)0500100015002000250030003500400000.10.20.30.40.50.60.70.80.91txn(t)');
INSERT INTO posts (postId,userId,title,body) VALUES (453,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu','Graham Neubig
Language Technologies Institute
Carnegie Mellon University
gneubig@cs.cmu.edu
12rA6 L.c 166047:ir
Abstract

We consider the problem of parsing natu-
ral language descriptions into source code
written in a general-purpose programming
language like Python.
Existing data-
driven methods treat this problem as a lan-
guage generation task without considering
the underlying syntax of the target pro-
gramming language.
Informed by previ-
ous work in semantic parsing, in this pa-
per we propose a novel neural architecture
powered by a grammar model to explicitly
capture the target syntax as prior knowl-
edge. Experiments nd this an effective
way to scale up to generation of complex
programs from natural language descrip-
tions, achieving state-of-the-art results that
well outperform previous code generation
and semantic parsing approaches.

IntroductionEvery programmer has experienced the situation
where they know what they want to do, but do
not have the ability to turn it into a concrete im-
plementation. For example, a Python programmer
may want to sort my list in descending order,
but not be able to come up with the proper syn-
tax sorted(my list, reverse=True) to real-
ize his intention. To resolve this impasse, it is
common for programmers to search the web in
natural language (NL), nd an answer, and mod-
ify it into the desired form (Brandt et al., 2009,
2010). However,
this is time-consuming, and
thus the software engineering literature is ripe
with methods to directly generate code from NL
descriptions, mostly with hand-engineered meth-
ods highly tailored to specic programming lan-
guages (Balzer, 1985; Little and Miller, 2009;
Gvero and Kuncak, 2015).

In parallel, the NLP community has developed
methods for data-driven semantic parsing, which
attempt to map NL to structured logical forms ex-
ecutable by computers. These logical forms can be
general-purpose meaning representations (Clark
and Curran, 2007; Banarescu et al., 2013), for-
malisms for querying knowledge bases (Tang and
Mooney, 2001; Zettlemoyer and Collins, 2005;
Berant et al., 2013) and instructions for robots or
personal assistants (Artzi and Zettlemoyer, 2013;
Quirk et al., 2015), among others. While these
methods have the advantage of being learnable
from data, compared to the programming lan-
guages (PLs) in use by programmers, the domain-
specic languages targeted by these works have a
schema and syntax that is relatively simple.
Recently, Ling et al. (2016) have proposed a
data-driven code generation method for high-level,
general-purpose PLs like Python and Java. This
work treats code generation as a sequence-to-
sequence modeling problem, and introduce meth-
ods to generate words from character-level mod-
els, and copy variable names from input descrip-
tions. However, unlike most work in semantic
parsing, it does not consider the fact that code has
to be well-dened programs in the target syntax.
In this work, we propose a data-driven syntax-
based neural network model tailored for genera-
tion of general-purpose PLs like Python.
In or-
der to capture the strong underlying syntax of the
PL, we dene a model that transduces an NL state-
ment into an Abstract Syntax Tree (AST; Fig. 1(a),
 2) for the target PL. ASTs can be deterministi-
cally generated for all well-formed programs us-
ing standa');
INSERT INTO posts (postId,userId,title,body) VALUES (454,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 2)','rd parsers provided by the PL, and thus
give us a way to obtain syntax information with
minimal engineering. Once we generate an AST,
we can use deterministic generation tools to con-
vert the AST into surface code. We hypothesize
that such a structured approach has two benets.

Production Rule
Explanation
Role
Call (cid:55) expr[func] expr*[args] keyword*[keywords] Function Call (cid:46) func: the function to be invoked (cid:46) args: arguments list
(cid:46) keywords: keyword arguments list
If (cid:55) expr[test] stmt*[body] stmt*[orelse]
(cid:46) test: condition expression (cid:46) body: statements inside
the If clause (cid:46) orelse: elif or else statements
For (cid:55) expr[target] expr*[iter] stmt*[body]
(cid:46) target: iteration variable (cid:46) iter: enumerable to iterate
stmt*[orelse]
over (cid:46) body: loop body (cid:46) orelse: else statements
FunctionDef (cid:55) identier[name] arguments*[args]
Function Def. (cid:46) name: function name (cid:46) args: function arguments
stmt*[body]
(cid:46) body: function body

If Statement

For Loop

Table 1: Example production rules for common Python statements (Python Software Foundation, 2016)

First, we hypothesize that structure can be used
to constrain our search space, ensuring generation
of well-formed code. To this end, we propose a
syntax-driven neural code generation model. The
backbone of our approach is a grammar model
( 3) which formalizes the generation story of a
derivation AST into sequential application of ac-
tions that either apply production rules ( 3.1), or
emit terminal tokens ( 3.2). The underlying syn-
tax of the PL is therefore encoded in the grammar
model a priori as the set of possible actions. Our
approach frees the model from recovering the un-
derlying grammar from limited training data, and
instead enables the system to focus on learning the
compositionality among existing grammar rules.
Xiao et al. (2016) have noted that this imposition
of structure on neural models is useful for seman-
tic parsing, and we expect this to be even more im-
portant for general-purpose PLs where the syntax
trees are larger and more complex.
Second, we hypothesize that structural informa-
tion helps to model information ow within the
neural network, which naturally reects the recur-
sive structure of PLs. To test this, we extend a
standard recurrent neural network (RNN) decoder
to allow for additional neural connections which
reect the recursive structure of an AST ( 4.2).
As an example, when expanding the node (cid:63) in
Fig. 1(a), we make use of the information from
both its parent and left sibling (the dashed rectan-
gle). This enables us to locally pass information
of relevant code segments via neural network con-
nections, resulting in more condent predictions.
Experiments ( 5) on two Python code gener-
ation tasks show 11.7% and 9.3% absolute im-
provements in accuracy against the state-of-the-art
system (Ling et al., 2016). Our model also gives
competitive performance on a standard semantic
parsing benchmark.
2 The Code Generation Problem
Given an NL description x, our task is to generate
the code snippet c in a modern PL based on the in-

tent of x. We attack this problem by rst generat-
ing the underlying AST. We dene a probabilistic
grammar model of generating an AST y given x:
p(y |x). The best-possible AST y is then given by
p(y |x).
(1)
y = arg maxy is ');
INSERT INTO posts (postId,userId,title,body) VALUES (455,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 3)','then deterministically converted to the corre-
sponding surface code c.1 While this paper uses
examples from Python code, our method is PL-
agnostic.
Before detailing our approach, we rst present
a brief introduction of the Python AST and its
underlying grammar. The Python abstract gram-
mar contains a set of production rules, and an
AST is generated by applying several production
rules composed of a head node and multiple child
nodes. For instance, the rst rule in Tab. 1 is
used to generate the function call sorted() in
Fig. 1(a). It consists of a head node of type Call,
and three child nodes of type expr, expr* and
keyword*, respectively. Labels of each node are
noted within brackets.
In an AST, non-terminal
nodes sketch the general structure of the target
code, while terminal nodes can be categorized into
two types: operation terminals and variable ter-
minals. Operation terminals correspond to basic
arithmetic operations like AddOp.Variable termi-
nal nodes store values for variables and constants
of built-in data types2 . For instance, all terminal
nodes in Fig. 1(a) are variable terminal nodes.
3 Grammar Model
Before detailing our neural code generation
method, we rst introduce the grammar model at
its core. Our probabilistic grammar model denes
the generative story of a derivation AST. We fac-
torize the generation process of an AST into se-
quential application of actions of two types:
 A P PLYRU LE[r] applies a production rule r to
the current derivation tree;

1We use astor library to convert ASTs into Python code.
2 bool, float, int, str.

(2)

Figure 1: (a) the Abstract Syntax Tree (AST) for the given example code. Dashed nodes denote terminals. Nodes are labeled
with time steps during which they are generated. (b) the action sequence (up to t14 ) used to generate the AST in (a)
example, in Fig. 1(b), the rule Call (cid:55) expr. . .
 G ENTOKEN[v ] populates a variable terminal
expands the frontier node Call at time step t4 , and
node by appending a terminal token v .
its three child nodes expr, expr* and keyword*
are added to the derivation.
A P PLYRU LE actions grow the derivation AST
by appending nodes. When a variable terminal
node (e.g., str) is added to the derivation and be-
comes the frontier node, the grammar model then
switches to G ENTOK EN actions to populate the
variable terminal with tokens.
Unary Closure Sometimes, generating an AST
requires applying a chain of unary productions.
For instance, it takes three time steps (t9  t11 )
to generate the sub-structure expr* (cid:55) expr (cid:55)
Name (cid:55) str in Fig. 1(a). This can be effectively
reduced to one step of A P P LYRU LE action by tak-
ing the closure of the chain of unary productions
and merging them into a single rule: expr* (cid:55)
str. Unary closures reduce the number of actions
needed, but would potentially increase the size of
the grammar.
In our experiments we tested our
model both with and without unary closures ( 5).
3.2 GENTOKEN Actions
Once we reach a frontier node nft that corresponds
to a variable type (e.g., str), G ENTOK EN actions
are used to ll this node with values. For general-
purpose PLs like Python, variables and constants
have values with one or multiple tokens. For in-
stance, a node that stores the name of a function
(e.g., sorted) has a single token, while a node
that denotes a string constant (e.g., a=hello
world) could have multiple tokens. Our model
copes with both scenarios by ring G ENTOKEN
actions at one or more time steps. At each time

Fig. 1(b) shows the generation process of the');
INSERT INTO posts (postId,userId,title,body) VALUES (456,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 4)',' tar-
get AST in Fig. 1(a). Each node in Fig. 1(b) in-
dicates an action. Action nodes are connected by
solid arrows which depict the chronological order
of the action ow. The generation proceeds in
depth-rst, left-to-right order (dotted arrows rep-
resent parent feeding, explained in  4.2.1).
Formally, under our grammar model, the prob-
T(cid:89)
ability of generating an AST y is factorized as:
p(y |x) =
p(at |x, a<t ),
t=1
where at is the action taken at time step t, and a<t
is the sequence of actions before t. We will explain
how to compute Eq. (2) in  4. Put simply, the
generation process begins from a root node at t0 ,
and proceeds by the model choosing A P PLYRU LE
actions to generate the overall program structure
from a closed set of grammar rules, then at leaves
of the tree corresponding to variable terminals, the
model switches to G ENTOKEN actions to gener-
ate variables or constants from the open set. We
describe this process in detail below.
3.1 A PPLYRULE Actions
A P P LYRU LE actions generate program structure,
expanding the current node (the frontier node at
time step t: nft ) in a depth-rst,
left-to-right
traversal of the tree. Given a xed set of produc-
tion rules, A P P LYRU LE chooses a rule r from the
subset that has a head matching the type of nft ,
and uses r to expand nft by appending all child
nodes specied by the selected production. As an

the current frontier variable node. A special </n>
token is used to close the node. The grammar
model then proceeds to the new frontier node.
Terminal tokens can be generated from a pre-
dened vocabulary, or be directly copied from the
input NL. This is motivated by the observation
that the input description often contains out-of-
vocabulary (OOV) variable names or literal values
that are directly used in the target code. For in-
stance, in our running example the variable name
my list can be directly copied from the the input
at t12 . We give implementation details in  4.2.2.
4 Estimating Action Probabilities
We estimate action probabilities in Eq. (2) using
attentional neural encoder-decoder models with an
information ow structured by the syntax trees.
4.1 Encoder
For an NL description x consisting of n words
{wi}n
i=1 ,
the encoder computes a context sen-
sitive embedding hi for each wi using a bidi-
rectional Long Short-Term Memory (LSTM) net-
work (Hochreiter and Schmidhuber, 1997), simi-
lar to the setting in (Bahdanau et al., 2014). See
supplementary materials for detailed equations.
4.2 Decoder
The decoder uses an RNN to model the sequential
generation process of an AST dened as Eq. (2).
Each action step in the grammar model naturally
grounds to a time step in the decoder RNN. There-
fore, the action sequence in Fig. 1(b) can be in-
terpreted as unrolling RNN time steps, with solid
arrows indicating RNN connections. The RNN
maintains an internal state to track the generation
process ( 4.2.1), which will then be used to com-
pute action probabilities p(at |x, a<t ) ( 4.2.2).
4.2.1 Tracking Generation States
Our implementation of the decoder resembles a
vanilla LSTM, with additional neural connections
(parent feeding, Fig. 1(b)) to reect the topological
structure of an AST. The decoders internal hidden
state at time step t, st , is given by:
(3)
st = fLSTM ([at1 : ct : pt : nft ], st1 ),
where fLSTM () is the LSTM update function.
[:] denotes vector concatenation. st will then be
used to compute action probabilities p(at |x, a<t )
in Eq. (2). Here, at1 is the embedding of the');
INSERT INTO posts (postId,userId,title,body) VALUES (457,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 5)',' pre-
vious action. ct is a context vector retrieved from

Figure 2: Illustration of a decoder time step (t = 9)
input encodings {hi} via soft attention. pt is a
vector that encodes the information of the parent
action. nft denotes the node type embedding of
3 . Intuitively, feeding
the current frontier node nft
the decoder the information of nft helps the model
to keep track of the frontier node to expand.
Action Embedding at We maintain two action
embedding matrices, WR and WG . Each row in
WR (WG ) corresponds to an embedding vector
for an action A P PLYRU LE[r] (G ENTOKEN[v ]).
Context Vector ct The decoder RNN uses soft at-
tention to retrieve a context vector ct from the in-
put encodings {hi} pertain to the prediction of the
current action. We follow Bahdanau et al. (2014)
and use a Deep Neural Network (DNN) with a sin-
gle hidden layer to compute attention weights.
Parent Feeding pt Our decoder RNN uses ad-
ditional neural connections to directly pass infor-
mation from parent actions. For instance, when
computing s9 , the information from its parent ac-
tion step t4 will be used. Formally, we dene the
parent action step pt as the time step at which
the frontier node nft is generated. As an exam-
ple, for t9 , its parent action step p9 is t4 , since
nf9 is the node (cid:63), which is generated at t4 by the
A P PLYRU LE[Call(cid:55). . .] action.
We model parent
information pt from two
sources: (1) the hidden state of parent action spt ,
and (2) the embedding of parent action apt . pt is
the concatenation. The parent feeding schema en-
ables the model to utilize the information of par-
ent code segments to make more condent predic-
tions. Similar approaches of injecting parent in-
formation were also explored in the S EQ2TR EE
model in Dong and Lapata (2016)4 .
3We maintain an embedding for each node type.
4 S EQ2TR EE generates tree-structured outputs by condi-

h1h2sortmy_listh3h4h5indescendings9p9a8nf9GenToken[</n>]ApplyRule[Call]ParentStates4a4+expr*s8ApplyRuleGenTokentypeof?order......nf9nonterminalvariableterminalembeddingofnodetypeembeddingofc94.2.2 Calculating Action Probabilities
In this section we explain how action probabilities
p(at |x, a<t ) are computed based on st .
A PPLYRULE The probability of applying rule r
as the current action at is given by a softmax5 :
p(at = A P PLYRU LE[r]|x, a<t ) =
softmax(WR  g(st ))
(cid:124)  e(r)
(4)
where g() is a non-linearity tanh(W  st + b), and
e(r) the one-hot vector for rule r .
GENTOKEN As in  3.2, a token v can be gener-
ated from a predened vocabulary or copied from
the input, dened as the marginal probability:
p(at = G ENTOK EN[v ]|x, a<t ) =
p(gen|x, a<t )p(v |gen, x, a<t )
+ p(copy|x, a<t )p(v |copy, x, a<t ).
The selection probabilities p(gen|) and p(copy|)
are given by softmax(WS  st ).
The prob-
ability of generating v from the vocabulary,
p(v |gen, x, a<t ), is dened similarly as Eq. (4),
except that we use the G ENTOK EN embedding
matrix WG , and we concatenate the context vector
ct with st as input. To model the');
INSERT INTO posts (postId,userId,title,body) VALUES (458,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 6)',' copy probability,
we follow recent advances in modeling copying
mechanism in neural networks (Gu et al., 2016;
Jia and Liang, 2016; Ling et al., 2016), and use a
pointer network (Vinyals et al., 2015) to compute
the probability of copying the i-th word from the
input by attending to input representations {hi}:
(cid:80)n
p(wi |copy, x, a<t ) =
exp((hi , st , ct ))i(cid:48)=1 exp((hi(cid:48) , st , ct ))
where () is a DNN with a single hidden layer.
Specically, if wi is an OOV word (e.g., my list,
which is represented by a special <unk> token in
encoding), we directly copy the actual word wi to
the derivation.

4.3 Training and Inference
Given a dataset of pairs of NL descriptions xi and
code snippets ci , we parse ci into its AST yi and
decompose yi into a sequence of oracle actions un-
der the grammar model. The model is then op-
timized by maximizing the log-likelihood of the
oracle action sequence. At inference time, we
use beam search to approximate the best AST y
in Eq. (1). See supplementary materials for the
pseudo-code of the inference algorithm.
tioning on the hidden states of parent non-terminals, while
our parent feeding uses the states of parent actions.
5We do not show bias terms for all softmax equations.

Dataset
Train
Development
Test
Avg. tokens in description
Avg. characters in code
Avg. size of AST (# nodes)

HS
533
66
66
39.1
360.3
136.6

D JANGO I F TTT
16,000
77,495
5,171
1,000
758
1,805
7.4
14.3
41.1
62.2
7.0
17.2

Statistics of Grammar
w/o unary closure
# productions
# node types
terminal vocabulary size
Avg. # actions per example
w/ unary closure
# productions
# node types
Avg. # actions per example

100
61
1361
173.4

222
96
6733
20.3

100
57
141.7

237
92
16.4

1009
8285.0





Table 2: Statistics of datasets and associated grammars

5 Experimental Evaluation
5.1 Datasets and Metrics
HEARTHSTONE (HS) dataset (Ling et al., 2016)
is a collection of Python classes that implement
cards for the card game HearthStone. Each card
comes with a set of elds (e.g., name, cost, and
description), which we concatenate to create the
input sequence. This dataset is relatively difcult:
input descriptions are short, while the target code
is in complex class structures, with each AST hav-
ing 137 nodes on average.
D JANGO dataset (Oda et al., 2015) is a collection
of lines of code from the Django web framework,
each with a manually annotated NL description.
Compared with the HS dataset where card imple-
mentations are somewhat homogenous, examples
in D JANGO are more diverse, spanning a wide va-
riety of real-world use cases like string manipula-
tion, IO operations, and exception handling.
I FTTT dataset (Quirk et al., 2015) is a domain-
specic benchmark that provides an interest-
ing side comparison. Different from HS and
D JANGO which are in a general-purpose PL, pro-
grams in I FT TT are written in a domain-specic
language used by the IFTTT task automation
App. Users of the App write simple instruc-
tions (e.g., If Instagram.AnyNewPhotoByYou
Then Dropbox.AddFileFromURL) with NL de-
scriptions (e.g., Autosave your Instagram photos
to Dropbox). Each statement inside the If or
Then clause consists of a channel (e.g., Dropbox)
and a function (e.g., AddFileFromURL)6 . This

6Like Beltagy and Quirk (2016), we strip function param-

simple structure results in much more concise
ASTs (7 nodes on average). Because all examples
are created by ordinary Apps users, the dataset
is highly noisy, with input NL very loosely con-
nected to target ASTs. The authors thus provide a
high-quality ltered test set, where each example
is veried by at least three annotators. We use this
set for evaluation. Also note I F TTTs grammar has
more productions (Tab. 2), but this does not imply
that its grammar is more complex. This is because');
INSERT INTO posts (postId,userId,title,body) VALUES (459,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 7)','
for HS and D JANGO terminal tokens are generated
by G ENTOK EN actions, but for I FT TT, all the code
is generated directly by A P P LYRU LE actions.
Metrics As is standard in semantic parsing, we
measure accuracy, the fraction of correctly gen-
erated examples. However, because generating an
exact match for complex code structures is non-
trivial, we follow Ling et al. (2016), and use token-
level BLEU-4 with as a secondary metric, dened
as the averaged BLEU scores over all examples.7
5.2 Setup
Preprocessing All
input descriptions are tok-
enized using N LTK. We perform simple canoni-
calization for D JANGO, such as replacing quoted
strings in the inputs with place holders. See sup-
plementary materials for details. We extract unary
closures whose frequency is larger than a thresh-
old k (k = 30 for HS and 50 for D JANGO).
Conguration The size of all embeddings is 128,
except for node type embeddings, which is 64.
The dimensions of RNN states and hidden layers
are 256 and 50, respectively. Since our datasets are
relatively small for a data-hungry neural model,
we impose strong regularization using recurrent
dropouts (Gal and Ghahramani, 2016), together
with standard dropout layers added to the inputs
and outputs of the decoder RNN. We validate the
dropout probability from {0, 0.2, 0.3, 0.4}. For
decoding, we use a beam size of 15.
5.3 Results
Evaluation results for Python code generation
tasks are listed in Tab. 3. Numbers for our sys-

eters since they are mostly specic to users.
7These two metrics are not ideal: accuracy only measures
exact match and thus lacks the ability to give credit to seman-
tically correct code that is different from the reference, while
it is not clear whether BLEU provides an appropriate proxy
for measuring semantics in the code generation task. A more
intriguing metric would be directly measuring semantic/func-
tional code equivalence, for which we present a pilot study
at the end of this section (cf. Error Analysis). We leave ex-
ploring more sophisticated metrics (e.g. based on static code
analysis) as future work.

Retrieval System
Phrasal Statistical MT
Hierarchical Statistical MT
NM T
S EQ2TR EE
S EQ2TR EEUNK
L PN
Our system
Ablation Study
 frontier embed.
 parent feed.
 copy terminals
+ unary closure
 unary closure

HS

D JANGO

ACC
0.0
0.0
0.0
1.5
1.5
13.6
4.5
16.2

16.7
10.6
3.0

10.1

B LEU
62.5
34.1
43.2
60.4
53.4
62.8
65.6
75.8

75.8
75.7
65.7

74.8



BL EU
18.6
47.6
35.9
63.4
44.6
58.2
77.6
84.5

83.8
84.3
61.7
83.3

ACC
14.7
31.5
9.5
45.1
28.9
39.4
62.3
71.6

70.7
71.5
32.3
70.3



Table 3: Results on two Python code generation tasks.
Results previously reported in Ling et al. (2016).

tems are averaged over three runs. We compare
primarily with two approaches:
(1) Latent Pre-
dictor Network (L PN), a state-of-the-art sequence-
to-sequence code generation model (Ling et al.,
2016), and (2) S EQ2TR EE, a neural semantic pars-
ing model (Dong and Lapata, 2016). S EQ2TR EE
generates trees one node at a time, and the tar-
get grammar is not explicitly modeled a priori,
but implicitly learned from data. We test both
the ');
INSERT INTO posts (postId,userId,title,body) VALUES (460,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 8)','original S EQ2TR EE model released by the au-
thors and our revised one (S EQ2TR EEUNK) that
uses unknown word replacement to handle rare
words (Luong et al., 2015). For completeness,
we also compare with a strong neural machine
translation (NM T) system (Neubig, 2015) using a
standard encoder-decoder architecture with atten-
tion and unknown word replacement8 , and include
numbers from other baselines used in Ling et al.
(2016). On the HS dataset, which has relatively
large ASTs, we use unary closure for our model
and S EQ2TR EE, and for D JANGO we do not.
System Comparison As in Tab. 3, our model
registers 11.7% and 9.3% absolute improvements
over L PN in accuracy on HS and D JANGO. This
boost in performance strongly indicates the impor-
tance of modeling grammar in code generation.
For the baselines, we nd L PN outperforms oth-
ers in most cases. We also note that S EQ2TR EE
achieves a decent accuracy of 13.6% on HS , which
is due to the effect of unknown word replacement,
since we only achieved 1.5% without it. A closer

8For NM T, we also attempted to nd the best-scoring syn-
tactically correct predictions in the size-5 beam, but this did
not yield a signicant improvement over the NM T results in
Tab. 3.

Figure 3: Performance w.r.t reference AST size on D JANGO

Figure 4: Performance w.r.t reference AST size on HS

comparison with S EQ2TR EE is insightful for un-
derstanding the advantage of our syntax-driven ap-
proach, since both S EQ2TR EE and our system out-
put ASTs: (1) S EQ2TR EE predicts one node each
time step, and requires additional dummy nodes
to mark the boundary of a subtree. The sheer num-
ber of nodes in target ASTs makes the prediction
process error-prone. In contrast, the A P PLYRU LE
actions of our grammar model allows for gener-
ating multiple nodes at a single time step. Em-
pirically, we found that in HS, S EQ2TR EE takes
more than 300 time steps on average to generate a
target AST, while our model takes only 170 steps.
(2) S EQ2TR EE does not directly use productions
in the grammar, which possibly leads to grammat-
ically incorrect ASTs and thus empty code out-
puts. We observe that the ratio of grammatically
incorrect ASTs predicted by S EQ2TR EE on HS
and D JANGO are 21.2% and 10.9%, respectively,
while our system guarantees grammaticality.
Ablation Study We also ablated our best-
performing models to analyze the contribution of
each component. frontier embed. removes the
frontier node embedding nft from the decoder
RNN inputs (Eq. (3)). This yields worse results on
D JANGO while gives slight improvements in ac-
curacy on HS. This is probably because that the
grammar of HS has fewer node types, and thus
the RNN is able to keep track of nft without de-
pending on its embedding. Next, parent feed.
removes the parent feeding mechanism. The ac-
curacy drops signicantly on HS, with a marginal
deterioration on D JANGO. This result is interest-
ing because it suggests that parent feeding is more
important when the ASTs are larger, which will
be the case when handling more complicated code
generation tasks like HS . Finally, removing the
pointer network (copy terminals) in G ENTO -

CHANN EL FU LL TR EE

Classical Methods
posclass (Quirk et al., 2015)
LR (Beltagy and Quirk, 2016)
Neural Network Methods
NM T
NN (Beltagy and Quirk, 2016)
S EQ2TR EE (Dong and Lapata, 2016)
Doubly-Recurrent NN
(Alvarez-Melis and Jaakkola, 2017)
Our system
 parent feed.
 frontier embed.

81.4
88.8

87.7
88.0
8');
INSERT INTO posts (postId,userId,title,body) VALUES (461,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 9)','9.7
90.1

90.0
89.9
90.1

71.0
82.5

77.7
74.3
78.4
78.2

82.0
81.1
78.7

Table 4: Results on the noise-ltered I F TTT test set of >3
agree with gold annotations (averaged over three runs), our
model performs competitively among neural models.

K EN actions gives poor results, indicating that it
is important to directly copy variable names and
values from the input.
The results with and without unary closure
demonstrate that, interestingly, it is effective on
HS but not on D JANGO. We conjecture that this is
because on HS it signicantly reduces the number
of actions from 173 to 142 (c.f., Tab. 2), with the
number of productions in the grammar remaining
unchanged.
In contrast, D JANGO has a broader
domain, and thus unary closure results in more
productions in the grammar (237 for D JANGO
vs. 100 for HS), increasing sparsity.
Performance by the size of AST We further in-
vestigate our models performance w.r.t. the size
of the gold-standard ASTs in Figs. 3 and 4. Not
surprisingly, the performance drops when the size
of the reference ASTs increases. Additionally, on
the HS dataset, the BLEU score still remains at
around 50 even when the size of ASTs grows to
200, indicating that our proposed syntax-driven
approach is robust for long code segments.
Domain Specic Code Generation Although this
is not the focus of our work, evaluation on I F TTT
brings us closer to a standard semantic parsing set-
ting, which helps to investigate similarities and
differences between generation of more compli-
cated general-purpose code and and more limited-
domain simpler code. Tab. 4 shows the results,
following the evaluation protocol in (Beltagy and
Quirk, 2016) for accuracies at both channel and
full parse tree (channel + function) levels. Our
full model performs on par with existing neu-
ral network-based methods, while outperforming
other neural models in full tree accuracy (82.0%).
This score is close to the best classical method
(LR), which is based on a logistic regression

01020304050Reference AST Size (# nodes)0.00.20.40.60.81.0PerformanceBLEUacc50100150200250Reference AST Size (# nodes)0.00.20.40.60.81.0PerformanceBLEUaccinput <name> Brawl </name> <cost> 5 </cost> <desc>
Destroy all minions except one (chosen randomly)
</desc> <rarity> Epic </rarity> ...
pred. class Brawl(SpellCard):
def
init (self):
super(). init (Brawl, 5, CHARACTER CLASS.
WARRIOR, CARD RARITY.EPIC)
def use(self, player, game):
super().use(player, game)
targets = copy.copy(game.other player.minions)
targets.extend(player.minions)
for minion in targets:
minion.die(self)

ref.

minions = copy.copy(player.minions)
minions.extend(game.other player.minions)
if len(minions) > 1:
survivor = game.random choice(minions)
for minion in minions:
if minion is not survivor: minion.die(self)

input join app cong.path and string locale into a le
path, substitute it for localedir.
pred. localedir = os.path.join(
app config.path, locale) 
input self.plural is an lambda function with an argument
n, which returns result of boolean expression n not
equal to integer 1
pred. self.plural = lambda n: len(n) 
ref.
self.plural = lambda n: ');
INSERT INTO posts (postId,userId,title,body) VALUES (462,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 10)','int(n!=1)

Table 5: Predicted examples from HS (1st) and D JANGO.
Copied contents (copy probability > 0.9) are highlighted.

model with rich hand-engineered features (e.g.,
brown clusters and paraphrase). Also note that the
performance between NM T and other neural mod-
els is much closer compared with the results in
Tab. 3. This suggests that general-purpose code
generation is more challenging than the simpler
I FT TT setting, and therefore modeling structural
information is more helpful.
Case Studies We present output examples in
Tab. 5. On HS, we observe that most of the
time our model gives correct predictions by lling
learned code templates from training data with ar-
guments (e.g., cost) copied from input. However,
we do nd interesting examples indicating that the
model learns to generalize beyond trivial copy-
ing. For instance, the rst example is one that our
model predicted wrong  it generated code block
A instead of the gold B (it also missed a function
denition not shown here). However, we nd that
the block A actually conveys part of the input in-
tent by destroying all, not some, of the minions.
Since we are unable to nd code block A in the
training data, it is clear that the model has learned
to generalize to some extent from multiple training
card examples with similar semantics or structure.
The next two examples are from D JANGO. The
rst one shows that the model learns the usage
of common API calls (e.g., os.path.join), and

how to populate the arguments by copying from
inputs. The second example illustrates the dif-
culty of generating code with complex nested
structures like lambda functions, a scenario worth
further investigation in future studies. More exam-
ples are attached in supplementary materials.
Error Analysis To understand the sources of er-
rors and how good our evaluation metric (exact
match) is, we randomly sampled and labeled 100
and 50 failed examples (with accuracy=0) from
D JANGO and HS, resp. We found that around 2%
of these examples in the two datasets are actually
semantically equivalent. These examples include:
(1) using different parameter names when dening
a function; (2) omitting (or adding) default values
of parameters in function calls. While the rarity of
such examples suggests that our exact match met-
ric is reasonable, more advanced evaluation met-
rics based on statistical code analysis are denitely
intriguing future work.
For D JANGO, we found that 30% of failed
cases were due to errors where the pointer net-
work failed to appropriately copy a variable name
into the correct position. 25% were because the
generated code only partially implementated the
required functionality.
10% and 5% of errors
were due to malformed English inputs and pre-
processing errors, respectively. The remaining
30% of examples were errors stemming from mul-
tiple sources, or errors that could not be easily cat-
egorized into the above. For HS, we found that
all failed card examples were due to partial imple-
mentation errors, such as the one shown in Table 5.

6 Related Work

Code Generation and Analysis Most existing
works on code generation focus on generating
code for domain specic languag');
INSERT INTO posts (postId,userId,title,body) VALUES (463,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 11)','es (DSLs) (Kush-
man and Barzilay, 2013; Raza et al., 2015; Man-
shadi et al., 2013), with neural network-based ap-
proaches recently explored (Parisotto et al., 2016;
Balog et al., 2016). For general-purpose code gen-
eration, besides the general framework of Ling
et al. (2016), existing methods often use language
and task-specic rules and strategies (Lei et al.,
2013; Raghothaman et al., 2016). A similar line
is to use NL queries for code retrieval (Wei et al.,
2015; Allamanis et al., 2015). The reverse task of
generating NL summaries from source code has
also been explored (Oda et al., 2015; Iyer et al.,
2016). Finally, there are probabilistic models of

source code (Maddison and Tarlow, 2014; Nguyen
et al., 2013). The most relevant work is Allama-
nis et al. (2015), which uses a factorized model
to measure semantic relatedness between NL and
ASTs for code retrieval, while our model tackles
the more challenging generation task.
Semantic Parsing Our work is related to the
general topic of semantic parsing, where the tar-
get logical forms can be viewed as DSLs. The
parsing process is often guided by grammatical
formalisms like combinatory categorical gram-
mars (Kwiatkowski et al., 2013; Artzi et al.,
2015), dependency-based syntax (Liang et al.,
2011; Pasupat and Liang, 2015) or task-specic
formalisms (Clarke et al., 2010; Yih et al., 2015;
Krishnamurthy et al., 2016; Misra et al., 2015; Mei
et al., 2016). Recently, there are efforts in design-
ing neural network-based semantic parsers (Misra
and Artzi, 2016; Dong and Lapata, 2016; Nee-
lakantan et al., 2016; Yin et al., 2016). Several
approaches have be proposed to utilize grammar
knowledge in a neural parser, such as augmenting
the training data by generating examples guided
by the grammar (Kocisk y et al., 2016; Jia and
Liang, 2016). Liang et al. (2016) used a neu-
ral decoder which constrains the space of next
valid tokens in the query language for question
answering. Finally, the structured prediction ap-
proach proposed by Xiao et al. (2016) is closely
related to our model in using the underlying gram-
mar as prior knowledge to constrain the genera-
tion process of derivation trees, while our method
is based on a unied grammar model which jointly
captures production rule application and terminal
symbol generation, and scales to general purpose
code generation tasks.

7 Conclusion
This paper proposes a syntax-driven neural code
generation approach that generates an abstract
syntax tree by sequentially applying actions from
a grammar model. Experiments on both code gen-
eration and semantic parsing tasks demonstrate the
effectiveness of our proposed approach.

Acknowledgment
We are grateful to Wang Ling for his generous help
with L PN and setting up the benchmark. We also
thank Li Dong for helping with S EQ2TR EE and
insightful discussions.

References
Miltiadis Allamanis, Daniel Tarlow, Andrew D. Gor-
don, and Yi Wei. 2015. Bimodal modelling of
In Proceedings
source code and natural language.
of ICML. volume 37.

David Alvarez-Melis and Tommi S. Jaakkola. 2017.
Tree-structured decoding with doubly recurrent neu-
ral networks. In Proceedings of ICLR.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage CCG semantic parsing with AMR.
In Proceedings of EMNLP.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transaction of ACL 1(1).

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014.
Neural machine translation by
CoRR
jointly learning to align and translate.
abs/1409.0473.

Matej Balog, Alexander L. Gaunt, Marc Brockschmidt,
Sebastian Nowozin, and Daniel Tarlow. 2016.
Deepcoder: Learning to write programs. CoRR
abs/1611.01989.

Robert Balzer. 1985. A 15 year perspective on au-
IEEE Trans. Software Eng.
tomatic programming.
11(11).

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griftt, Ulf Hermjakob, Kevin
Knight, Phi');
INSERT INTO posts (postId,userId,title,body) VALUES (464,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 12)','lipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, LAW-ID@ACL.

I. Beltagy and Chris Quirk. 2016.
Improved seman-
tic parsers for if-then statements. In Proceedings of
ACL.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of EMNLP.

Joel Brandt, Mira Dontcheva, Marcos Weskamp, and
Scott R. Klemmer. 2010. Example-centric program-
ming: integrating web search into the development
environment. In Proceedings of CHI .

Joel Brandt, Philip J. Guo, Joel Lewenstein, Mira
Dontcheva, and Scott R. Klemmer. 2009. Two stud-
ies of opportunistic programming: interleaving web
foraging, learning, and writing code. In Proceedings
of CHI .

Stephen Clark and James R. Curran. 2007. Wide-
coverage efcient statistical parsing with CCG and
log-linear models. Computational Linguistics 33(4).

James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
worlds response. In Proceedings of CoNLL.

Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. In Proceedings of ACL.

Greg Little and Robert C. Miller. 2009. Keyword pro-
gramming in java. Autom. Softw. Eng. 16(1).

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Incorporating copying mechanism in
Li. 2016.
In Proceedings of
sequence-to-sequence learning.
ACL.

Tihomir Gvero and Viktor Kuncak. 2015. Interactive
In Proceedings
synthesis using free-form queries.
of ICSE.

Sepp Hochreiter and J urgen Schmidhuber. 1997. Long
short-term memory. Neural Computation 9(8).

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
In Proceedings of
using a neural attention model.
ACL.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of ACL.

Tom as Kocisk y, G abor Melis, Edward Grefenstette,
Chris Dyer, Wang Ling, Phil Blunsom,
and
Karl Moritz Hermann. 2016. Semantic parsing with
In Pro-
semi-supervised sequential autoencoders.
ceedings of EMNLP.

Jayant Krishnamurthy, Oyvind Tafjord, and Aniruddha
Kembhavi. 2016. Semantic parsing to probabilistic
programs for situated question answering. In Pro-
ceedings of EMNLP.

Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unication to generate regular expressions
from natural language. In Proceedings of NAACL.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013.
Scaling semantic
parsers with on-the-y ontology matching. In Pro-
ceedings of the EMNLP.

Tao Lei, Fan Long, Regina Barzilay, and Martin C. Ri-
nard. 2013. From natural language specications to
program input parsers. In Proceedings of ACL.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2016. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. CoRR abs/1611.00020.

Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tom as Kocisk y, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
In Proceedings of
networks for code generation.
ACL');
INSERT INTO posts (postId,userId,title,body) VALUES (465,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 13)','.

Thang Luong,
Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015. Addressing
the rare word problem in neural machine translation.
In Proceedings of ACL.

Chris J. Maddison and Daniel Tarlow. 2014. Structured
In Pro-
generative models of natural source code.
ceedings of ICML. volume 32.

Mehdi Hafezi Manshadi, Daniel Gildea, and James F.
Allen. 2013. Integrating programming by example
and natural language programming. In Proceedings
of AAAI .

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. Listen, attend, and walk: Neural mapping
of navigational instructions to action sequences. In
Proceedings of AAAI .

Dipendra K. Misra and Yoav Artzi. 2016. Neural shift-
In Proceedings of
reduce CCG semantic parsing.
EMNLP.

Dipendra Kumar Misra, Kejia Tao, Percy Liang, and
Ashutosh Saxena. 2015. Environment-driven lex-
In Pro-
icon induction for high-level instructions.
ceedings of ACL.

Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever.
Inducing latent pro-
2016. Neural programmer:
In Proceedings of
grams with gradient descent.
ICLR.

Graham Neubig. 2015.
lamtram: A toolkit for lan-
guage and translation modeling using neural net-
works. http://www.github.com/neubig/lamtram.

Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh
Nguyen, and Tien N. Nguyen. 2013. A statistical
semantic language model for source code. In Pro-
ceedings of ACM SIGSOFT .

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015.
Learning to generate
pseudo-code from source code using statistical ma-
chine translation (T). In Proceedings of ASE.

Emilio Parisotto, Abdel-rahman Mohamed, Rishabh
Singh, Lihong Li, Dengyong Zhou, and Pushmeet
Kohli. 2016. Neuro-symbolic program synthesis.
CoRR abs/1611.01855.

Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
Proceedings of ACL.

Python Software Foundation. 2016. Python abstract
grammar. https://docs.python.org/2/library/ast.html.

Chris Quirk, Raymond J. Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In Proceedings of ACL.

Mukund Raghothaman, Yi Wei, and Youssef Hamadi.
2016.
SWIM: synthesizing what i mean: code
search and idiomatic snippet synthesis. In Proceed-
ings of ICSE.

Mohammad Raza, Sumit Gulwani, and Natasa Milic-
Frayling. 2015. Compositional program synthesis
In Proceed-
from natural language and examples.
ings of IJCAI .

Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of ECML.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Proceedings of NIPS.

Yi Wei, Nirupama Chandrasekaran, Sumit Gul-
and Youssef Hamadi. 2015.
wani,
Build-
ing
bing
developer
assistant.
Techni-
cal
https://www.microsoft.com/en-
report.
us/research/publication/building-bing-developer-
assistant/.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
mantic parsing. In Proceedings of ACL.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In Proceedings of ACL.

Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao.
2016. Neural enquirer: Learning to query tables in
natural language. In Proceedings of IJCAI .

Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form structured clas-
sication with probabilistic categorial grammars. In
Proceedings of UAI .

quoted string literals (e.g., verbose name is a
string cache entry). We therefore replace quoted
strings with indexed placeholders using regular
expression. After dec');
INSERT INTO posts (postId,userId,title,body) VALUES (466,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 14)','oding, we run a post-
processing step to replace all placeholders with
their actual values.
(2) For descriptions with
cascading variable reference (e.g., call method
self.makekey), we append after the whole variable
name with tokens separated by .
(e.g., append
self and makekey after self.makekey). This gives
the pointer network exibility to copy either par-
tial or whole variable names.
Generate Oracle Action Sequence To train our
model, we generate the gold-standard action se-
quence from reference code. For I FT TT, we sim-
ply parse the ofcially provided ASTs into se-
quences of A P P LYRU LE actions. For HS and
D JANGO, we rst convert the Python code into
ASTs using the standard ast module. Values
inside variable terminal nodes are tokenized by
space and camel case (e.g., ClassName is tok-
enized to Class and Name). We then traverse the
AST in pre-order to generate the reference action
sequence according to the grammar model.

D Additional Decoding Examples
We provide extra decoding examples from the
D JANGO and HS datasets, listed in Table 6 and Ta-
ble 7, respectively. The model heavily relies on the
pointer network to copy variable names and con-
stants from input descriptions. We nd the source
of errors in D JANGO is more diverse, with most
incorrect examples resulting from missing argu-
ments and incorrect words copied by the pointer
network. Errors in HS are mostly due to partially
or incorrectly implemented effects. Also note that
the rst example in Table 6 is semantically cor-
rect, although it was considered incorrect under
our exact-match metric. This suggests more ad-
vanced evaluation metric that takes into account
the execution results in future studies.

Supplementary Materials
A Encoder LSTM Equations

Suppose the input natural language description x
consists of n words {wi}n
i=1 . Let wi denote the
embedding of wi . We use two LSTMs to process
x in forward and backward order, and get the se-
quence of hidden states {(cid:126)hi}n
i=1 and { (cid:126)hi}n
i=1 in
the two directions:
(cid:126)hi = f 
LSTM (wi , (cid:126)hi1 )
(cid:126)hi = f 
LSTM (wi , (cid:126)hi+1 ),
where f 
LSTM and f 
LSTM are standard LSTM up-
date functions. The representation of the i-th
word, hi , is given by concatenating (cid:126)hi and (cid:126)hi .

B Inference Algorithm

Given an NL description, we approximate the best
AST y in Eq. 1 using beam search. The inference
procedure is listed in Algorithm 1.
We maintain a beam of size K . The beam is
initialized with one hypothesis AST with a single
root node (line 2). At each time step, the decoder
enumerates over all hypotheses in the beam. For
each hypothesis AST, we rst nd its frontier node
nft (line 6). If nft is a non-terminal node, we col-
lect all syntax rules r with nft as the head node
If nft is a variable
to the actions set (line 10).
terminal node, we add all terminal tokens in the
vocabulary and the input description as candidate
actions (line 13). We apply each candidate action
on the current hypothesis AST to generate a new
hypothesis (line 15). We then rank all newly gen-
erated hypotheses and keep the top-K scored ones
in the beam. A complete hypothesis AST is gener-
ated when it has no frontier node. We then convert
the top-scored complete AST int');
INSERT INTO posts (postId,userId,title,body) VALUES (467,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 15)','o the surface code
(lines 18-19).
We remark that our inference algorithm can
be implemented efciently by expanding multi-
ple hypotheses (lines 5-16) simultaneously using
mini-batching on GPU.

C Dataset Preprocessing
Infrequent Words We replace word types whose
frequency is lower than d with a special <unk>
token (d = 3 for D JANGO, 3 for HS and 2 for
I FT TT).
Canonicalization We perform simple canonical-
ization for the D JANGO dataset:
(1) We ob-
serve that
input descriptions often come with

Algorithm 1: Inference Algorithm
Input
: NL description x
Output: code snippet c
1 call Encoder to encode x
2 Q = {y0 (root )}
3 for time step t do
Q(cid:48) = foreach hypothesis yt  Q donft = FrontierNode(yt )A = if nft is non-terminal thenforeach production rule r with nft as the head node doA = A  {A P P LYRU LE[r ]}
10
11
foreach terminal token v do
12
A = A  {G ENTOKEN[v ]}
13
foreach action at  A do
14
y (cid:48)
t = ApplyAction(yt , at )
15
Q(cid:48) = Q(cid:48)  {y (cid:48)
t }
16
Q = top-K scored hypotheses in Q(cid:48)
17
18 y = top-scored complete hypothesis AST
19 convert y to surface code c
20 return c

else

(cid:46) Initialize a beam of size K

(cid:46) Initialize the set of candidate actions

(cid:46) A P P LYRU LE actions for non-terminal nodes

(cid:46) G ENTOKEN actions for variable terminal nodes

input for every i in range of integers from 0 to length of result, not included
pred. for i in range(0, len(result)): 
ref. for i in range(len(result)):
input call the function blankout with 2 arguments: t.contents and B, write the result to out.
ref. out.write(blankout(t.contents, B))
pred. out.write(blankout(t.contents, B)) 
pred. code list.append(foreground[v]) 
ref. code list.append(foreground[v])
input zip elements of inner result and inner args into a list of tuples, for every i item and i args in the result
ref. for i item, i args in zip(inner result,
pred. for i item, i args in zip(inner result,
inner args): 
inner args):
input activate is a lambda function which returns None for any argument x.
ref. activate = lambda x: None
pred. activate = lambda x: None 
input if elt is an instance of Choice or NonCapture classes
ref. if isinstance(elt, (Choice, NonCapture)):
pred. if isinstance(elt, Choice): 
input get translation function attribute of the object t, call the result with an argument eol message, substitute the result for
result.
pred. translation function = getattr(t,
ref. result = getattr(t, translation function)(
translation function) 
eol message)
input for every s in strings, call the function force text with an argument s, join the results in a string, return the result.
pred. return .join(force text(s)) 
ref. return .join(force text(s) for s in strings)
input for every p in parts without the rst element
pred. for p in p[1:]: 
ref. for p in parts[1:]:
input call the function get language, split the result by -, substitute the rst element of the result for base lang.
ref. base lang = get language().split()[0]
pred. base lang = get language().split()[0] 

Table 6: Predicted examples from D JANGO dataset. Copied contents (copy probability > 0.9) are highlighted

input <name> Burly Rockjaw Trogg </name> <cost> 5 </cost> <attack> 3 </attack> <defense> 5 </defense>
<desc> Whenever your opponent casts a spell, gain 2 Attack. </desc> <rarity> Common </rarity> ...
pred. class BurlyRockjawTrogg(MinionCard):
init (self):
def
super(). init (Burly Rockjaw Trogg, 4, CHARACTER CLASS.ALL, CARD RARITY.COMMON)
def create minion(self, player):
return Minion(3, 5, effects=[Effect(SpellCast(player=EnemyPlayer()),
ActionTag(Give(ChangeAttack(2)), SelfSelector()))]) 

input <name> Maexxna </name> <cost> 6 </cost> <attack> 2 </attack> <defense> 8 </defense> <desc> Destroy
any minion damaged by this minion. </desc> <rarity> Legenda');
INSERT INTO posts (postId,userId,title,body) VALUES (468,5569,'Pengcheng Yin Language Technologies Institute Carnegie Mellon University pcyin@cs.cmu.edu (part 16)','ry </rarity> ...
pred. class Maexxna(MinionCard):
init (self):
def
super(). init (Maexxna, 6, CHARACTER CLASS.ALL, CARD RARITY.LEGENDARY,
minion type=MINION TYPE.BEAST)
def create minion(self, player):
return Minion(2, 8, effects=[Effect(DidDamage(), ActionTag(Kill(),
TargetSelector(IsMinion())))]) 

input <name> Hellre </name> <cost> 4 </cost> <attack> -1 </attack> <defense> -1 </defense> <desc> Deal 3
damage to ALL characters. </desc> <rarity> Free </rarity> ...
pred. class Hellfire(SpellCard):
init (self):
def
super(). init (Hellfire, 4, CHARACTER CLASS.WARLOCK, CARD RARITY.FREE)

def use(self, player, game):
super().use(player, game)
for minion in copy.copy(game.other player.minions):
minion.damage(player.effective spell damage(3), self) 
ref. class Hellfire(SpellCard):
init (self):
def
super(). init (Hellfire, 4, CHARACTER CLASS.WARLOCK, CARD RARITY.FREE)

def use(self, player, game):
super().use(player, game)
targets = copy.copy(game.other player.minions)
targets.extend(game.current player.minions)
targets.append(game.other player.hero)
targets.append(game.current player.hero)
for minion in targets:
minion.damage(player.effective spell damage(3), self)
reason Partially implemented effect: only deal 3 damage to opponents characters
input <name> Darkscale Healer </name> <cost> 5 </cost> <attack> 4 </attack> <defense> 5 </defense> <desc>
Battlecry: Restore 2 Health to all friendly characters. </desc> <rarity> Common </rarity> ...
pred. class DarkscaleHealer(MinionCard):
init (self):
def
super(). init (Darkscale Healer, 5, CHARACTER CLASS.ALL,
CARD RARITY.COMMON, battlecry=Battlecry(Damage(2),
CharacterSelector(players=BothPlayer(), picker=UserPicker())))

def create minion(self, player):
return Minion(4, 5) 
ref. class DarkscaleHealer(MinionCard):
init (self):
def
super(). init (Darkscale Healer, 5, CHARACTER CLASS.ALL,
CARD RARITY.COMMON, battlecry=Battlecry(Heal(2), CharacterSelector()))

def create minion(self, player):
return Minion(4, 5)
reason Incorrect effect: damage 2 health instead of restoring. Cast effect to all players instead of friendly players only.

Table 7: Predicted card examples from HS dataset. Copied contents (copy probability > 0.9) are highlighted.

');
INSERT INTO posts (postId,userId,title,body) VALUES (469,8943,'Li-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen Research Center for Information Technology Innovation, Aca','1. INTRODUCTION

The popularity of a song can be measured a posteriori ac-
cording to statistics such as the number of digital downloads,
playcounts, listeners, or whether the song has been listed in
the Billboard Chart once or multiple times. However, for mu-
sic producers and artists, it would be more interesting if song
popularity can be predicted a priori before the song is ac-
tually released. For music streaming service providers, an
automatic function to identify emerging trends or to discover
potentially interesting but not-yet-popular artists is desirable
to address the so-called long tail of music listening [1]. In
academia, researchers are also interested in understanding the

This work was partially supported by the Ministry of Science and Tech-
nology of Taiwan under Contracts 104-2221-E-001-029-MY3 and 105-2221-
E-001-019-MY2.

factors that make a song popular [2,3]. This can be formulated
as a pattern recognition problem, where the task is to gener-
alize observed association between song popularity measure-
ments and feature representation characterizing the songs in
the training data to unseen songs [4].
Our literature survey shows that this automatic hit song
prediction task has been approached using mainly two differ-
ent information sources: 1) internal factors directly relating
to the content of the songs, including different aspects of au-
dio properties, song lyrics, and the artists; 2) external factors
encompassing social and commercial inuences (e.g. concur-
rent social events, promotions or album cover design).
The majority of previous work on the internal factors of
song popularity are concerned with the audio properties of
music. The early work of Dhanaraj and Logan [4] used sup-
port vector machine to classify whether a song will appear
in music charts based on latent topic features computed from
audio Mel-frequency cepstral coefcients (MFCC) and song
lyrics. Following this work, Pachet et al. [5] employed a large
number of audio features commonly used in music informa-
tion retrieval (MIR) research and concluded that the features
they used are not informative enough to predict hits, claiming
that hit song science is not yet a science. Ni et al. [6] took
a more optimistic stand, showing that certain audio features
such as tempo, duration, loudness and harmonic simplicity
correlate well with the evolution of musical trends. How-
ever, their work analyzes the evolution of hit songs [79],
rather than discriminates hits from non-hits. Fan et al. [10]
performed audio-based hit song prediction of music charts in
mainland China and UK and found that Chinese hit song pre-
diction is more accurate than the UK version. Purely lyric-
based hit song prediction was relatively unexplored, except
for the work presented by Singhi and Brown [11].
On the other hand, on external factors, Salganik et al. [12]
showed that the song itself has relatively minor role than the
social inuences for deciding whether a song can be a hit.
Zangerla et al. [13] used Twitter posts to predict future charts
and found that Twitter posts are helpful when the music charts
of the recent past are available.
To our best knowledge, despite its recent success in var-
ious pattern recognition problems, deep learning techniques
have not be employed for hit song prediction. In particular,

in speech and');
INSERT INTO posts (postId,userId,title,body) VALUES (470,8943,'Li-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen Research Center for Information Technology Innovation, Aca',' music signal processing, convolutional neural
network (CNN) models have exhibited remarkable strength in
learning task-specic audio features directly from data, out-
performing models based on hand-crafted audio features in
many prediction tasks [1416].
We are therefore motivated to extend previous work on
audio-based hit song prediction by using state-of-the-art
CNN-based models, using either the primitive,
low-level
mel-spectrogram directly as the input for feature learning, or
a more advanced setting [17] that exploits an external mu-
sic auto-tagging dataset [18] for extracting high-level audio
features. Moreover, instead of using music charts, we use a
collection of user listening data from KKBOX Inc., a leading
music streaming service provider in East Asia. We formulate
hit song prediction as a regression problem and test how we
can predict the popularity of Chinese and Western Pop music
among Taiwanese KKBOX users, whose mother tongue is
Mandarin. Therefore, in addition to testing whether deep
models outperform shallow models in hit song prediction, we
also investigate how the culture origin of songs affects the
performance of different CNN models.

2. DATASET

Because we are interested in discriminating hits and non-hits,
we nd it informative to use the playcounts a song receives
over a period of time from streaming services to dene song
popularity and formulate a regression problem to predict song
popularity. In collaboration with KKBOX Inc., we obtain a
subset of user listening records contributed by Taiwanese lis-
teners over a time horizon of one year, from Oct. 2012 to
Sep. 2013, involving the playcounts of close to 30K users
for around 125K songs. Based on the language metadata pro-
vided by KKBOX, we compile a Mandarin subset featuring
Chinese Pop songs and a Western subset comprising of songs
sung mainly in English. There are more songs in the West-
ern subset but the Mandarin songs receive more playcounts
on average, for Mandarin is the mother tongue of Taiwanese.
The following steps are taken to gain insights into the data
and for data pre-processing. First, as the songs in our dataset
are released in different times, we need to check whether we
have to compensate for this bias, for intuitively songs released
earlier can solicit more playcounts. We plot in Fig. 1 the
average playcounts of songs released in different time peri-
ods, where Q1 denotes the rst three months starting from
Oct. 2012 and Q1 the most recent three months before Oct.
2012, etc. The y-axis is in log scale but the actual values are
obscured due to a condentiality agreement with KKBOX.
From the dash lines we see that the average playcounts from
different time periods seem to be within a moderate range in
the log scale for both subsets, exempting the need to compen-
sate for the time bias by further operations.
Second, we dene the hit score of a song according to the
multiplication of its playcount in log scale and the number of

Fig. 1. The average playcounts (in log scale) of songs released
in different time periods.

Fig. 2. The distribution of hit scores (see Section 2 for deni-
tion) ');
INSERT INTO posts (postId,userId,title,body) VALUES (471,8943,'Li-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen Research Center for Information Technology Innovation, Aca','in the (left) whole and (right) test sets.

users (also in log scale) who have listened to the song. We
opt for not using the playcounts only to measure song pop-
ularity because it is possible that the playcount of a song is
contributed by only a very small number of users.
Third, to make our experimental results on the two subsets
comparable, we sample the same amount of 10K songs in our
experiment for both subsets. These songs are those with the
highest playcounts within the subset. It can be seen from Fig.
2 that the distributions of hit scores of the sampled songs are
similar. The solid lines in Fig. 1 show that after this sampling
the time bias among the sampled songs remains moderate.
Finally, we randomly split the songs to have 8K, 1K, and
1K songs as the training, validation, and test data for each
of the subsets. Although it may be more interesting to split
the songs according to their release dates so as to learn from
the past and predict the future, we leave this as a future work.
Our focus here is to study whether deep models perform better
than shallow models in audio-based hit song prediction.
The scale and the time span of the dataset are deemed
appropriate for this study. Unlike previous work on musical
trend analysis that may involve more than ten years worth of
data (e.g. [6], [19]), for the purpose of our work we want to
avoid changes in public music tastes and therefore it is better
to use listening records collected within a year.

3. METHODS

We formulate hit song prediction as a regression problem and
train either shallow or deep neural network models for pre-
dicting the hit scores. Given the audio representation xn for
each song n in the training set, the objective is to optimize the
parameters  of our model f () by minimizing the squared er-
(cid:80)
ror between the ground truth yn and our estimate, expressed
n (cid:107)yn  f (xn )(cid:107)2
as min
2 . As described below, a total
number of six methods are considered, All of them are im-
plemented based on the lasagne library [20], and the model
settings such as learning rate update strategy, dropout rate,
and numbers of feature maps per layer are empirically tuned
by using the validation set.

3.1. Method 1 (m1): LR

As the simplest method, we compute 128-bin log-scaled mel-
spectrograms [21] from the audio signals and take the mean
and standard deviation over time, leading to a 256-dim feature
vector per song. The feature vectors are used as the input to
a single-layer shallow neural network model, which is effec-
tively a linear regression (LR) model. The mel-spectrograms
are computed by short-time Fourier transform with 4,096-
sample, half-overlapping Hanning windows, from the middle
60-second segment of each song, which is sampled at 22 kHz.
In lasagne, we can implement the LR model by a dense layer.

3.2. Method 2 (m2): CNN

Going deeper, we use the mel-spectrograms directly as the in-
put, which is a 128 by 646 matrix for there are 646 frames
per song, to a CNN model. Our CNN model consists of two
early convolutional layers, with respectively 128-by-4 and 1-
by-4 convolutional kernels, and three late convolutional lay-
ers, which all has 1-by-1 convolutional kernels. Unlike usual
CNN models, we do not use fully connected layers in the lat-
ter half of our model for such fully convolutional model has
been shown more effective for music [14, 17, 22].

3.3. Method 3 (m3): inception CNN

The idea of inception was introduced in GoogLeNet for visual
problems [23]. It uses multi-scale kernels to learn features.
We make an audio version of');
INSERT INTO posts (postId,userId,title,body) VALUES (472,8943,'Li-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen Research Center for Information Technology Innovation, Aca',' it by adding two more parallel
early convolutional layers with different sizes: 132-by-8 and
140-by-16, as illustrated in the bottom-right corner of Fig. 3.
To combine the output of these three kernels by concatena-
tion, the input mel-spectrogram needs to be zero-padded.

3.4. Method 4 (m4): JYnet (a CNN model) + LR

While generic audio features such as mel-spectrogram may be
too primitive to predict hits, we employ a state-of-art music
auto-tagging system referred to as the JYnet [17] to compute

Fig. 3. Architecture of the investigated CNN models.

high-level tag-based features. JYnet is another CNN model
that also takes the 128-bin log-scaled mel-spectrograms as the
input, but the model is trained to make tag prediction using
the MagnaTagATune dataset [18]. The output is the activa-
tion scores of 50 music tags, including genres, instruments,
and other performing related tags such as male vocal, female
vocal, fast and slow. From the output of JYnet (i.e. 50-dim
tag-based features), we learn another LR model for predicting
hit scores, as illustrated in the bottom-left corner of Fig. 3.

3.5. Methods 5 and 6 (m5 '||'&'||' m6): Joint Training

We also try to combine (m4) with (m2) or (m3) to exploit
information in both the mel-spectrograms and tags, leading to
(m5) and (m6). Instead of simply combining the results of
the two models f1 () and f2 () being combined, we add
another layer on top of them for joint training, as illustrated
(cid:88)
in Fig. 3. The learning objective becomes:
(cid:107)yn  wf1 (xn )  (1  w)f2 (xn )(cid:107)2
2 ,
min
w,1 ,2

(1)

where w determines their relative weight.
In this way, we
can optimize the model parameters of both models jointly.
However, when method 4 is used in joint training we only
update the parameters of its LR part, as JYnet is treated as an
external, pre-trained model in our implementation.

4. EXPERIMENTAL RESULTS

We train and evaluate the two data subsets separately. For
evaluation, the following four metrics are considered:
 Recall@100: Treating the 100 songs (i.e. 10%) with
the highest hit scores among the 1,000 test songs as the
hit songs, we rank all the test songs in descending order
of the predicted hit scores and count the number of hit
songs that occur in the top 100 of the resulting ranking.
 nDCG@100: normalized discounted cumulative gain
(nDCG) is another popular measure used in ranking

Method
recall
(m1) audio+LR
0.1900
(m2) audio+CNN
0.2300
(m3) audio+inception CNN 0.2500
(m4) tag+LR
0.2400
(m5) (m2)+(m4)
0.2500
(m6) (m3)+(m4)
0.3000

Table 1. Accuracy of Hit Song Prediction
Mandarin subset
Kendall
nDCG
0.1679
0.1997
0.2334
0.1806
0.2286
0.2369
0.1671
0.2372
0.2018
0.2558
0.2927
0.2665

Spearman
0.2480
0.2678
0.3374
0.2473
0.2971
0.3894

recall
0.1400
0.1300
0.1800
0.2000
0.1800
0.2100

Western subset
Kendall
nDCG
0.0674
0.1271
0.1294
0.1031
0.1093
0.1989
0.0918
0.1774
0.1300
0.1791
0.2413
0.1341

Spearman
0.1002
0.1564
0.1636
0.1372
0.1941
0.1996

problems [24]. It is computed in a way similar to re-
call@100, but the positions of recalled hit songs in the
ranking list are taken into account.
 Kendalls  : we directly compare the ground truth and
predicted rankings of the test songs in hit scores (with-
out dening which songs are hit songs) and compute a
value that is based on the number of correctly and in-
correctly ranked pairs [25].
 Spearmans : the rank correlation coefcient (consid-
ering the relative rankings but not the a');
INSERT INTO posts (postId,userId,title,body) VALUES (473,8943,'Li-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen Research Center for Information Technology Innovation, Aca','ctual hit scores)
between the ground truth and predicted rankings.
The result is shown in Table 1, which is obtained by aver-
aging the result of 10 repetition of each method. The follow-
ing observations can be made. First, by comparing the result
of (m1), (m2) and (m3), we see that better result in most of
the four metrics is obtained by using deeper and more compli-
cated models for both subsets. This suggests the effectiveness
of deep structures for this task. Furthermore, by comparing
the result of the two subsets, we see that audio-based hit song
prediction is easier for the Mandarin subset, conrming the
ndings of Fan et al. [10].
Second, as both (m1) and (m4) use LR for prediction, by
comparing their result we see that the tag-based method (m4)
outperforms the simple audio-based method (m1) in all the
four metrics for the Western subset, demonstrating the effec-
tiveness of the JYnet tags. This is however not the case for
the Mandarin subset for Kendalls  and Spearmans .
Third, from the result of (m5) and (m6), we see that the
joint learning structure can further improve the result for both
subsets. The best result is obtained by (m6) in all metrics.
To gain insights, we employ JYnet to assign genre labels
to all the test songs and examine the distribution of genres in
the top-50 hit songs determined by either automatic models or
the ground truth. For each song, we pick the genre label that
has the strongest activation as predicted by JYnet. The result-
ing genre distributions are shown in Fig. 4. We see from the
result of ground truth that the Western hits have more diverse
genres. The predominance of Pop songs in the Mandarin
subset might explain why 1) hit song prediction in this subset
is easier and 2) (m4) alone cannot improve  and . More-
over, for the Western subset, we see that the genre distribu-
tion of (m4) is more diverse than that of (m3), despite that

Fig. 4. The predominate tags (predicted by JYnet) for the
top-50 hit songs determined by different methods for the (top)
Mandarin and (bottom) Western subsets. From left to right:
(a) the tag-based model (m4), (b) the audio-based model
(m3), (c) the hybrid model (m6), and (d) the ground truth.

(m3) achieves slightly higher nDCG and Spearmans . This
might imply that the ability to match the genre distribution of
the ground truth is another important performance indicator.

5. CONCLUSION

In this paper, we have introduced state-of-the-art deep learn-
ing techniques to the audio-based hit song prediction prob-
lem. Instead of aiming at classifying hits from non-hits, we
formulate it as a regression problem. Evaluations on the lis-
tening data of Taiwanese users of a streaming company called
KKBOX conrms the superiority of deep structures over shal-
low structures in predicting song popularity. Deep structures
are in particular important for Western songs, as simple shal-
low models may not capture the rich acoustic and genre di-
versity exhibited in Western hits. For future work, we hope
to understand what our neural network models actually learn,
to compare against more existing methods (preferably using
the same datasets), and to investigate whether our models can
predict future charts or emerging trends.

6. REFERENCES

[1] H. Silk, R. Santos-Rodriguez, C. Mesnage, T. De Bie,
and M. McVicar,
Data science for the detection of
emerging music styles, EPSRC, pp. 46, 2014.

[2] S. McClary, Studying popular music, vol. 10, 1991.

[3] P. D. Lopes, Innovation and diversity in the popular
music industry, 1969 to 1990, American Sociological
Review, vol. 57, no. 1, pp. 56, 1992.

[4] R. Dhanaraj and B. Logan, Automat');
INSERT INTO posts (postId,userId,title,body) VALUES (474,8943,'Li-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen Research Center for Information Technology Innovation, Aca','ic prediction of
hit songs, in Proceedings of International Society for
Music Information Retrieval, pp. 1115, 2005.

[5] F. Pachet and P. Roy, Hit song science is not yet a sci-
ence, in Proceedings of International Society for Music
Information Retrieval, pp. 355360, 2008.

[6] Y. Ni and R. Santos-Rodriguez, Hit song science once
again a science, International Workshop on Machine
Learning and Music, pp. 23, 2011.

[7] R. M. MacCallum, M. Mauch, A. Burt, and A. M. Leroi,
Evolution of music by public choice, in Proceedings
of the National Academy of Sciences, vol. 109, no. 30,
pp. 1208112086, 2012.

[8] M. Mauch, R. M. MacCallum, M. Levy, and A. M.
Leroi, The evolution of popular music: USA 1960-
2010, Royal Society Open Science, vol. 2, no. 5, pp.
150081, 2015.

[9] J. Serr `a, A. Corral, M. Bogu n a, M. Haro, and J. L. Ar-
cos, Measuring the evolution of contemporary western
popular music, Scientic Reports, vol. 2, pp. 16, 2012.

[10] J. Fan and M. Casey, Study of Chinese and UK hit
songs prediction, 10th International Symposium on
Computer Music Multidisciplinary Research (CMMR),
pp. 640652, 2013.

[11] A. Singhi and D. G. Brown, Hit song detection using
lyric features alone, in Proceedings of International
Society for Music Information Retrieval, 2014.

[12] M. J. Salganik, P. S. Dodds, and D. J. Watts, Experi-
mental study of inequality and cultural market, Science,
vol. 311, no. 5762, pp. 854856, 2006.

[13] E. Zangerle, M. Pichl, B. Hupfauf, and G. Specht, Can
microblogs predict music charts ? an analysis of the
relationship between # nowplaying tweets and music
charts, in Proceedings of International Society for Mu-
sic Information Retrieval, 2016.

[14] K. Choi, G. Fazekas, and M. Sandler, Automatic tag-
ging using deep convolutional neural networks, in Pro-
ceedings of International Society for Music Information
Retrieval, 2016.

[15] O. Abdel-Hamid, A.-R. Mohamed, H. Jiang, L. Deng,
G. Penn, and D. Yu, Convolutional neural networks for
speech recognition, IEEE/ACM Transactions on Au-
dio, Speech, and Language Processing, pp. 15331545,
2014.

[16] S. Dieleman and B. Schrauwen, End-to-end learning
in IEEE International Conference
for music audio,
on Acoustics, Speech and Signal Processing, 2014, pp.
69646968.

[17] J.-y. Liu and Y.-h. Yang, Event localization in Music
auto-tagging, in Proceedings of the 24th ACM interna-
tional conference on Multimedia, 2016.

[18] E. Law, K. West, M. I. Mandel, M. Bay, and J. S.
Downie, Evaluation of algorithms using games: the
case of music tagging, in Proceedings of International
Society for Music Information Retrieval, 2009.

[19] S. Kinoshita, T. Ogawa, and M. Haseyama, Popular
music estimation based on topic model using time infor-
mation and audio features, IEEE 3rd Global Confer-
ence on Consumer Electronics (GCCE), pp. 102103,
2014.

[online]
[20] lasagne,
https://lasagne.
readthedocs.org/en/latest/.

[21] S. Dieleman and B. Schrauwen, Multiscale approaches
to music audio feature learning, in Proceedings of In-
ternational Society for Music Information Retrieval, pp.
116121, 2013.

[22] E. Shelhamer, J. Long, and T. Daeedll, Fully convolu-
tional networks for semantic segmentation, Computer
Vision and Pattern Recognition (CVPR), 2015.

[23] C. Szegedy, W. Liu, Y. Jia, and P. Sermanet,
Go-
ing deeper with');
INSERT INTO posts (postId,userId,title,body) VALUES (475,8943,'Li-Chia Yang , Szu-Yu Chou , Jen-Yu Liu , Yi-Hsuan Yang , Yi-An Chen Research Center for Information Technology Innovation, Aca',' convolutions, arXiv preprint arXiv:
1409.4842, 2014.

[24] Y. Wang, L. Wang, Y. Li, D. He, T.-Y. Liu, and W. Chen,
A theoretical analysis of NDCG ranking measures, in
Proceedings of the Annual Conference on Learning The-
ory, pp. 130, 2013.

[25] M. Kendall and J. D. Gibbons, Rank correlation meth-
ods, vol. 3, Oxford University Press, 1990.

');
INSERT INTO posts (postId,userId,title,body) VALUES (476,2482,'of Systems of Linear Equations ','Gennadi.I.Malaschonok
Tambov State University, 392622 Tambov, Russia
e-mail: malaschonok@math-univ.tambov.su

Abstract

New solution method for the systems of linear equations in commutative
integral domains is proposed. Its complexity is the same that the complexity
of the matrix multiplication.

Introduction

One of the rst results in the theory of computational complexity is the
Strassen discovery of the new algorithm for matrix multiplication [1]. He
changed the classical method with the complexity O(n3 ) for the new algo-
rithm with the complexity O(nlog2 7 ). This method may be used for a matrix
in any commutative ring. He used matrix multiplication for the computation
of the inverse matrix, of the determinant of a matrix and for the solution of
the systems of linear equations over an arbitrary eld with the complexity
O(nlog2 7 ).
Many authors improved this result. There is known now an algorithm
of matrix multiplication with the complexity O(n2,37 ) (see D.Coppersmith,
S.Winograd [2]).
We have another situation with the problems of the solution of systems
of linear equations and of the determinant computation in the commutative
rings. Dodgson [3] proposed a method for the determinant computation and
the solution of systems of linear equations over the ring of integer numbers
with the complexity O(n3 ). During this century this result was improved and
generalized for arbitrary commutative integral domain due to Bareis [4] and
the author (see [5]  [8]). But the complexity is still O(n3 ).
There is proposed the new solution method for the systems of linear equa-
tions in integral domains. Its complexity is the same that the complexity of
the matrix multiplication in integral domain.

This paper vas published in: Computational Mathematics (A. Sydow Ed, Proceedings of the
15th IMACS World Congress, Vol.
I, Berlin, August 1997 ), Wissenschaft '||'&'||' Technik Verlag,
Berlin 1997, 475480. No part of this materials may be reproduced, stored in retrieval system, or
transmitted, in any form without prior permission of the copyright owner.

Let

aij xj = aim ,

m1
Xj=1
be the system of linear equations with extended coecients matrix

i = 1, 2, . . . , n

A = (aij ), i = 1, . . . , n, j = 1, . . . , m.

xj =

, j = 1, . . . , n,

whose coecients are in integral domain R: A  Rnm .
The solution of such system may be written according to Cramers rule
jm  Pm1
p=n+1 xp nj pwhere xp , p = n + 1, . . . , m, are free variables and n 6= 0.
n = |aij |,
i = 1, . . . , n, j = 1, . . . , n, - denote the corner minors of the matrix A of
order n, n
ij - denote the minors obtained by a substitution of the column
j of the matrix A instead of the column i in the minors n , i = 1, . . . , n,
j = n + 1, . . . , m. So we need to construct the algorithm of computation of
the minor n and the matrix G = ( n
ij ), i = 1, . . . , n, j = n + 1, n + 2, . . . , m.
That means that we must make the reduction of the matrix A to the
diagonal form

In denotes the unit matrix of order n.

A  (n In , G).

2 Recursive Algorithm

For the extended coecients matrix A we shall denote:




 the matrix, formed by the surrounding of the submatrix of an order k  1
in the upper left corner by row i and column j ,

a1j
a1,k1
  
a2j
a2,k1
  
...
...
. . .
   ak1,k1 ak1,j
  
ai,k1
aij

a12
a11
a22
a21
...
...
ak1,1 ak1,2
ai1
ai2

Ak
ij =

ij = det Ak
ak
ij ,

ij = aij , 0 = 1, k = ak
kk , k
a1
ij  the determinant of the matrix, that is
received from the matrix Ak
kk after the substitution of the column i by the
column j .
ij and ak
We shall use the minors k
ij for the construction of the matricez
r');
INSERT INTO posts (postId,userId,title,body) VALUES (477,2482,'of Systems of Linear Equations  (part 2)','+1,k+1 ap
ap

r+1,k+2
r+2,k+1 ap
ap

r+2,k+2
...
...
ap
ap
l,k+2
l,k+1

   ap
r+1,c
   ap
r+2,c
...
. . .
ap
  
l,c

Ar,l,(p)
k ,c =




and

Gr,l,(p)
k ,c =
r+1,k+1r+2,k+1
...l,k+1





 R(lr)(ck) , 0  k < n, k < c  n, 0  r < m, r < l  m,
r+1,k+2r+2,k+2
...l,k+2

  
  
. . .
  
r+1,cr+2,c
...l,c

, Ar,l,(p)
Gr,l,(p)
k ,c
k ,c
1  p  n.
We shall describe one recursive step, that makes the following reduction
of the matrix A to the diagonal form

where

A  ( l Ilk , G)

A = Ak ,l,(k+1)
k ,c

G = Gk ,l,(l)
l,c

0  k < c  m, k < l  n, l < c. Note that if k = 0, l = n and c = m then
we get the solution of the system.
We can choose the arbitrary integer number s: k < s < l and write the
matrix A as the following:
A = (cid:18) A1
A2 (cid:19)
where A1 = Ak ,s,(k+1)
- the upper part of the matrix A consists of the s  k
k ,c
rows and A2 = As,l,(k+1)
- the lower part of the matrix A.
k ,c

2.1 The rst step

As the next recurcive step we make the following reduction of the matrix
A1  R(sk)(ck) to the diagonal form

A1  (s Isk , G1
2 ),

2 = Gk ,s,(s)
where G1
s,c

2.2 The second step

We write the matrix A2 in the following way:

1 , A2
A2 = (A2
2 )

2 = As,l,(k+1)
1 = As,l,(k+1)
consists of the rst s  k columns and A2
where A2
s,c
k ,s
consists of the last c  s columns of the matrix A2 .
2 = As,l,(s+1)
The matrix A2
is obtained from the matrix identity (see the
s,c
proof in the next section):

k  A2
1  G1
2  A2
2 = s  A2
2 .

The minors k must not equal zero.

2.3 The third step

As the next recurcive step we make the following reduction of the matrix
A2
2  R(ls)(cs) to the diagonal form

2  ( l Ils , G2
A2
2 ),

2 = Gs,l,(l)
where G2
l,c

2.4 The fourst step

We write the matrix G1
2 in the following way:

2 , G1
2 = (G1
G1
2 )

2 = Gk ,s,(s)
2 = Gk ,s,(s)
consists of the rst l  s columns and G1
where G1
l,c
s,l
consists of the last c  l columns of the matrix G1
2 .
2 = Gk ,s,(l)
The matrix G1
is obtained from the matrix identity (see the
l,c
proof in the next section):

s  G1
2  G2
2 =  l  G1
2  G1
2 .

The minors s must not equal zero.
So we get
G = (cid:18) G1
2 (cid:19)G2

and  l .

2.5 Representation of the one recursive step

We can represent one recursive step as the following reduction of the matrix
A:
2 (cid:19) 2 (cid:18) s Isk G1
A2 (cid:19) 1 (cid:18) s Isk G1
A = (cid:18) A1
2 (cid:19) 32
A2
A2
A21
G1
G1
3 (cid:18) s Isk G1
2 (cid:19) 4 (cid:18)  l Isk2 (cid:19) = (  l Ilk2G2
G2
 l Ils
 l Ils0

G )

3 The Proof of the Main Identities

3.1 The rst matrix identity

The second step of the algorithm is based on the following matrix identity:

k As,l,(s+1)
s,c

= sAs,l,(k+1)
s,c

 As,l,(k+1)
k ,s

 Gk ,s,(s)
s,c

So we must prove the next identities for the matrix elements

ij = s ak+1
k as+1
ij 
Xp=k+1

ak+1
ip

 s
pj ,

i = s + 1, . . . , l; j = s + 1, . . . , c.
ij denote the minors that will stand in the place of the minors k after
Let k
the replacement the row i by the row j . An expansion of the determinant ak+1
ij
according to the column j is the following
0
...0
riarj

ak+1
ij = k aij 
    
...
. . .
     k
ki
Xr=1
Therefore we can write the next matrix identity


0
1

...
...0
1i k2i


a11
a12
  
a1,s
a1j
a21
a22
  
a2,s
a2j


...
...
...
...
. . .
as,1
as,2
  
as,s
as,j
');
INSERT INTO posts (postId,userId,title,body) VALUES (478,2482,'of Systems of Linear Equations  (part 3)','ak+1
ak+1
   ak+1
ak+1
i1
i2
i,s
ij
Note that ak+1
ip = 0 for p  k . Finaly we decompose the determinant of
the right matrix according to the last row and write the determinant identity
correspondingly to this matrix identity.
00
...
...0
0 k

  
  
. . .
  
  

 As+1
ij =

3.2 The second matrix identity

The fourth step of the algorithm bases on the matrix identity

sGk ,s,(l)
l,c

=  lGk ,s,(s)
l,c

 Gk ,s,(s)
s,l

 Gs,l,(l)
l,c

So we must prove the next identities for the matrix elements:

ij =  l s
s  l
ij 
Xp=s+1

ip   lpj ,

i = k + 1, . . . , s; j = l + 1, . . . , c.
j,i denote the algebraic adjunct of element aj,i in the matrix As
Let  s
s,s . An
expansion of the determinant s
ip according to the column i is the following
ip =

 s
q iaqp
Xq=1
Therefore we can write the next matrix identity:




    
...
. . .
      s
s,i

0   
0   
...
. . .
0   
0   
0
... s
1i
1
... s
2i
0
...0
0
...0

Al+1
ij =

=



a1j
   a1,l
a12
a11
a2j
   a2,l
a22
a21


...
...
...
...
. . .
al,j
  
al,l
al,1 al,2ss
  
ij
i2
i1
i,l
ii = s . So to nish the proof we must
ip = 0 for p  s and s
Note that s
decompose the determinant of the right matrix according to the last row and
write the determinant identity correspondingly to this matrix identity.

4 Evaluation of Operations Number

Let us have a method for matrix multiplications with the complexity M (n) =
O(n2+ ), then for multiplication of two matrixes of order l  n and n  c we
need M (l  n, n  c) = O(lcn ) operations. Let us denote by S (n, m) the
complexity of the recursive algorithm for the matrix A  Rnm .
If in the rst recursive step upper submatrix consists of the s rows, 1 
s < n, then

S (n, m) = S (s, m) + M ((n  s)  s, s  (m  s))+

+S (n  s, m  s) + M (s  (n  s), (n  s)  (m  n)) + O(nm).

For a matrix with k rows we can choose the arbitrary s : 1  s  k  1.
If the process of partition is dichotomous, and the number of rows in the
upper and lower submatrixes is the same in every step, then S (2n, m) satises
the recursive inequality:

S (2n, m) = S (n, m) + M (n  n, n  (m  n)) + S (n, m  n)+

+M (n  n, n  (m  2n)) + O(nm)  2S (n, m) + 2O(mn+1 ).

So we have

S (2n, m)  nS (2, m) +

(log2 n)1
Xi=0

O((
2i )+1m)2i+1 =

= nS (2, m) +
1  2 O((n  1)nm)

S (2n, m)  O(mn+1 ).

And nally

On the other hand

S (2n, m) > M (n  n, n  (m  n)) = O(mn+1 ).

Therefore

S (2n, m) = O(mn+1 ).

So the complexity of this algorithm is the same that the complexity of the
matrix multiplication. In particular for m = n + 1 we have

S (n, n + 1) = O(n2+ )

It means that the solution of the system of linear equations needs (ac-
curate to the constant multiplier) the same number of operations that the
multiplication of two matrixes needs.
We can get the exact number of operations, that are necessary for the
solution of the system of linear equations of order n  m, in the case when on
every step upper submatrix is no less then lower submatrix and the number
of rows in upper submatrix is some power of 2.
Let F (s, s,  ) = M (( s)s, s(s))+M (s( s), ( s)( )),
then we obtain S (n, m):
2k , m  2k (

n/2k 
log2 n
(F');
INSERT INTO posts (postId,userId,title,body) VALUES (479,2482,'of Systems of Linear Equations  (part 4)',' (2k , n  2k 
Xi=1
Xk=1
Let n = 2p . If we use simple matrix multiplications with complexity n3 than
we obtain

F (2k1 , 2k1 , m  (i 1)2k ))
2k  1))+

Anm = (6n2m  4n3  6nm + 3n2 + n)/6,
Mnm = (6n2m  4n3 + (6nm  3n2 ) log2 n  6nm + 4n)/6,
Dnm = ((6nm  3n2 ) log2 n  6nm  n2 + 6m + 3n  2)/6.
Here we denote by Anm , Mnm , Dnm the numbers of additions/subtractions,
multiplications and divisions, and take into account that (6nm  2n2  6m +
2)/6 divisions in the second step are divisions by 0 = 1, so they do not exist
in Dnm .
For m=n+1 we obtain

An,n+1 = (2n3 + 3n2  5n)/6,

Mn,n+1 = (2n3 + (3n2 + 6n) log2 n  2n)/6,
Dn,n+1 = (3n2 log2 n  7n2 + 6n log2 n + 3n + 4)/6.
The general quantity of multiplication and division operations is about n3/3.
We can compare these results with one-pass algorithm, that was the best
n,n+1 = (2n3 + 3n2  5n)/6, M O
of all known algorithms (see [8]): AO
n,n+1 =
n,n+1 = (n3  7n + 6)/6, the general quantity of
(n3 + 2n2  n  2)/2, DO
multiplication and division operations is about 2n3/3.
If we use Strassens matrix multiplications with complexity nlog2 7 than
we can obtain for n = 2p the general quantity of multiplication and division
operations

M DS
n,m = n2 (log2 n  5/3) + 7/15nlog2 7 + (m  n)(n2 + 2n log2 n  n) + 6/5n



log2 n12i (8i  7i ){
Xi=1
For m = n + 1, n = 2p we get

m  n
2i

  (
2i  2)

m  n  2i+1 (m  n)/2i+1 
2i

}.

M DS
n,n+1 =
15

nlog2 7 + n2 (log2 n 
3

) + n(2 log2 n +
5

).

5 Example

Let us consider next system over the integer numbers= 










5.1 Reduction of the matrix A1 = A02(1)
05
onal form

1 1 12 01 2
0 0
30
x1
x2
x3
x4
41

to the diag-

We make the next reduction:
05  (2 I2 , G02(2)
A02(1)
25

5.1.1

5.1.2

5.1.3

5.1.4

05  (1 I1 , G01(1)
A01(1)
15

) = (3; 1, 1, 1, 4)

01 G01(1)
15  A12(1)
15 = 1A12(1)
0A12(2)
15 =
= 3(2, 0, 1, 4)  (1)(1, 1, 1, 4) = (5, 1, 4, 8),

0  1.

15  (2 I1 , G12(2)
A12(2)
25

) = (5; 1, 4, 8)

1G01(2)
25 = 2G01(1)
25  G01(1)
12 G12(2)
25 =
= 5(1, 1, 4)  (1)(1, 4, 8) = (6, 9, 12)
G01(2)
25 = (2, 3, 4)

Finally we obtain

(2 I2 , G02(2)
25

2 3 4
) = (cid:18) 5 0;
8 (cid:19)
0 5; 1
2 = A24(3)
5.2 Computation of the matrix A2
25
0A24(3)
25 = 2A24(1)
25  A24(1)
02 G02(2)
25 =
0 2
= 5  (cid:18) 2
2 1 (cid:19)  (cid:18) 0 1
1 0 (cid:19) (cid:18) 2 31= (cid:18) 11 4 18
13 9 (cid:19)25 = (cid:18) 11 4 18
13 9 (cid:19)
0  1; A24(3)
4
8 (cid:19) =

2 = A24(3)
5.3 Reduction of the matrix A2
25
agonal form

to the di-

We make the next reduction:

25  (4 I2 G24(4)
A24(3)
45

5.3.1

5.3.2

5.3.3

5.3.4

25  (3 I1 , G23(3)
A23(3)
35

) = (11; 4, 18)

23 G23(3)
35  A34(3)
35 = 3A34(4)
2A34(4)
35 =
= 11(13, 9)  (2)(4, 18) = (135, 135)

A34(4)
35 = (27, 27)

35  (4 I1 , G34(4)
A34(4)
45

) = (27, 27)

34 G34(4)
45  G23(3)
45 = 4G23(3)
3G23(4)
45 =
= 27(18)  (4)(27) = 594, G23(4)
45 = (54)

Finally, in step (3) we obtain

(4 I2 , G24(4)
45

) = (cid:18) 27
0; 54
27; 27 (cid:19)

2 = G02(4)
5.4 Computation of the matrix G1
45
24 G24(4)
2G02(4)
45 = 4G02(2)
45  G02(2)
45 =
= 27 (cid:18) 4
8 (cid:19)  (cid:18) 2 3
4 (cid:19) (cid:18) 54
27 (cid:19) = (cid:18) 135
270 (cid:19)45 = (cid:18) 27
54 (cid:19)
G02(4)
The solution of the system is the following:

4 = 27; G04(4)
45 =

27
54
54
27







5.5 Representation of the rst recursive step

We can represent the rst recursive step as the following
01
0
2 35 10 210
2 1
2 3
5 00 5 10 54
0 0
27
27 270 0

A 1





6 Conclusion

2










5 0
2 3
0 5 14
11 4 18
0 0
0 0 2
13 9



270
54
0 54
27 27




270
0
270
0
27
The described algorithm for the solution of the systems of linear equations
over the integral domain includes the known one-pass method and the method
of forward and back-up procedures [8]. If in every recursive step the partition
of the matrix is such that the upper submatrix c');
INSERT INTO posts (postId,userId,title,body) VALUES (480,2482,'of Systems of Linear Equations  (part 5)','onsists only of one row then
it is the method of forward and back-up procedures. If the lower submatrix
consists only of one row in every step then it is the one-pass method.
If the process of partition is dichotomous and the numbers of rows in the
upper and lower submatrixes are equal in every step, then the complexity of
the solution has the same order O(n2+ ) as the complexity of matrix multi-
plication.
The computation of the matrix determinant and the computation of the
adjugate matrix have the same complexity.
This method may be used in any commutative ring if the corner minors
k , k = 1, 2, . . . , n, do not equal zero and are not zero divisors.

References

[1] V. Strassen. Gaussian Elimination is not optimal. Numerische
Mathematik, 1969, 13, 354356.

[2] D. Coppersmith, S. Winograd. in Proc. 19th Annu ACM Symp.
on Theory of Comput., 1987, 16.

[3] C.L. Dodgson. Condensation of determinants, being a new and
brief method for computing their arithmetic values. Proc. Royal
Soc. Lond., 1866, A.15, 150155.

[4] E.N. Bareiss. Sylvesters identity and multistep integer-preserving
Gaussian elimination. Math. Comput., 1968, 22, 565578.

[5] G.I. Malaschonok. Solution of a system of linear equations in an
integral domain. USSR Journal of Computational Mathematics
and Mathematical Physics, 1983, 23, 14971500.

10

[6] G.I. Malaschonok. On the solution of a linear equation system
over commutative ring. Math. Notes of the Acad. Sci. USSR,
1987, 42, N4, 543548.

[7] G.I. Malaschonok. A new solution method for linear equation
systems over the commutative ring. In Int. Algebraic Conf., The-
ses on the ring theory, algebras and modules. Novosibirsk, 1989,
8283.

[8] G.I. Malaschonok. Algorithms for the solution of systems of linear
equations in commutative rings. In Eective Methods in Algebraic
Geometry, Edited by T. Mora and C. Traverso, Progress in Math-
ematics 94, Birkhauser, Boston-Basel-Berlin, 1991, 289298.

11

');
INSERT INTO posts (postId,userId,title,body) VALUES (481,5583,'via simulation relations','F. Chen  , S. Haesaert  , A. Abate  , and S. Weiland 

 Department of Electrical Engineering
Eindhoven University of Technology, Eindhoven, The Netherlands
 Department of Computer Science
University of Oxford, Oxford, United Kingdom
12rA6 Y.c 126047:ir
Abstract: The analysis of industrial processes, modelled as descriptor systems,
is often
computationally hard due to the presence of both algebraic couplings and dierence equations
of high order. In this paper, we introduce a control renement notion for these descriptor
systems that enables analysis and control design over related reduced-order systems. Utilising
the behavioural framework, we extend upon the standard hierarchical control renement for
ordinary systems and allow for algebraic couplings inherent to descriptor systems.

Keywords: Descriptor systems, simulation relations, control renement, behavioural theory.

1. INTRODUCTION

Complex industrial processes generally contain algebraic
couplings in addition to dierential (or dierence) equa-
tions of high order. These systems, referred to as descriptor
systems (Kunkel and Mehrmann, 2006; Dai, 1989), are
commonly used in the modelling of mechanical systems.
The presence of algebraic equations, or couplings, together
with large state dimensions renders numerical simulation
and controller design challenging. Instead model reduction
methods (Antoulas, 2005) can be applied to replace the
systems with reduced order ones. Even though most meth-
ods have been developed for systems with only ordinary
dierence equations, recent research also targets descriptor
systems (Cao et al., 2015).
In this paper, we newly target the use of descriptor systems
of reduced order for the veriable design of controllers.
A rich body of literature on verication and formal con-
troller synthesis exists for systems solely composed of
dierence equations. This includes the algorithmic design
of certiable (hybrid) controllers and the verication of
pre-specied requirements (Tabuada, 2009; Kloetzer and
Belta, 2008). Usually, these methods rst reduce the orig-
inal, concrete systems to abstract systems with nite or
smaller dimensional state spaces over which the verica-
tion or controller synthesis can be run. A such controller
obtained for the abstract system can be rened over the
concrete system leveraging the existence of a similarity re-
lation, e.g., an (approximate) simulation relation, between
the two systems (Tabuada, 2009; Girard and Pappas,
2011). For the application of these relations in control
problems, a hierarchical control framework is presented
by (Girard and Pappas, 2009). Currently, the control syn-
thesis over descriptor systems cannot be dealt with in this
fashion due to the presence of algebraic equations.

The presence of similarity relations between descrip-
tor systems has also been a topic under investigation

in (Megawati and Van der Schaft, 2015). This work on
similarity relations deals with continuous-time descriptor
systems that are unconstrained and non-deterministic, and
focuses on the conditions for bisimilarity and on the con-
struction of similarity relations. Instead in this work, we
specically consider the control renement problem for
discrete-time descriptor systems via simulation relations
within a behavioural framework, such that properties ver-
ied over the future behaviour of the abstract system are
also veried over the concrete controlled system. Within
the behavioural theory (Willems and Polderman, 2013),
a formal distinction is made between a system (its be-
haviour) and its representations, enabling us to investi-
gate descriptor systems and renement control problems
without having to directly deal with their inherent anti-
causality.
In the next section, we dene the notion of dynamical
systems and control within a behavioural framework and
use it to formalise the control renement problem. Subse-
quently, Section 3 is dedicated to the exact control rene-
ment for descriptor systems and contains the mai');
INSERT INTO posts (postId,userId,title,body) VALUES (482,5583,'via simulation relations (part 2)','n results
of the paper. The last section closes with the conclusions.

2. THE BEHAVIOURAL FRAMEWORK

2.1 Discrete-time descriptor systems

As introduced by (Willems and Polderman, 2013), we
dene dynamical systems as follows.
Denition 1. A dynamical system  is dened as a triple
 = (T, W, B)
with the time axis T, the signal space W, and the behaviour
B  WT . 

In this denition, WT denotes the collection of all time-
dependent functions w : T  W. The set of tra jectories
or time-dependent functions given by B represents the

tra jectories that are compatible with the system. This set
is referred to as the behaviour of the system (Willems
and Polderman, 2013). Generally, the representation of the
behaviour of a dynamical system by equations, such as a
set of ordinary dierential equations, state space equations
and transfer functions, is non-unique. Hence we distinguish
a dynamical system (its behaviour) from the mathematical
equations used to represent its governing laws.

We consider dynamical systems evolving over discrete-time
(T := N = {0, 1, 2, . . .}) that can be represented by a
combination of linear dierence and algebraic equations.
The dynamics of such a linear discrete-time descriptor
system (DS) are dened by the tuple (E , A, B , C ) as
Ex(t + 1) = Ax(t) + Bu(t),
y (t) = C x(t),
with the state x(t)  X = Rn , the input u(t)  U = Rp ,
and the output y (t)  Y = Rk and t  N. Further, E , A 
Rnn , B  Rnp and C  Rkn are constant matrices and
we presume that rank(B ) = p and rank(C ) = k .

(1)

We say that a tra jectory w = (u, x, y ), with w : N  (U 
X  Y), satises (1) if for all t  N the equations in
(1) evaluated at u(t), x(t), x(t + 1), y (t) hold. Then the
collection of all tra jectories w denes the ful l behaviour,
or equivalently the input-state-output behaviour as
Bi/s/o := {(u, x, y )  (U  X  Y)N | (1) is satised}. (2)
The variable x is considered as a latent variable, therefore
the manifest, or equivalently the input-output behaviour
associated with (1) is dened by
Bi/o:= {(u, y )|x  XN s.t. (u, x, y )  Bi/s/o}.
If E is non-singular, we refer to the corresponding dynam-
ical system as a non-singular DS. In that case, we can
transform (1) into standard state space equations, as
x(t + 1) = Ax(t) + Bu(t),
y (t) = C x(t),
with A = E1A, B = E1B . Further Bi/s/o as in (2) is
{(u, x, y )  (U  X  Y)N | (u, x, y ) s.t. (3) holds}.
Similarly, if E is non-singular, Bi/o can be dened by (3).

(3)

The tuple with dynamics (1) denes a dynamical system
 evolving over the combined signal space W = U  X  Y
with behaviour B := Bi/s/o given in (2). Similarly, for W
restricted to input-output space, the tuple (N, U Y, Bi/o )
denes the manifest or induced dynamical system.

We are specically interested in the behaviour initialised
at t = 0 with a given set of initial states X0  X. For this,
we say that a tra jectory w : N  (U  X  Y) is initialised
with X0 if (1) holds and x(0) = x0  X0 . Such a tra jectory,
initialised with x0  X0 , is also called the continuation
of x0 . We refer to the collection of initialised tra jectories
related to X0 as the initialised behaviour Binit
i/s/o . This
allows us');
INSERT INTO posts (postId,userId,title,body) VALUES (483,5583,'via simulation relations (part 3)',' to formalise our denition of the descriptor
system evolving over N.
Denition 2. (Discrete-time descriptor systems (DS)). A
(discrete-time) descriptor system is dened as a dynamical
system  initialised with X0 , whose behaviour can be
represented by the combination of algebraic equations and
dierence equations given in (1), that is

 := (T, W, B) = (N, U  X  Y, Binit
i/s/o )

(4)

with

 the time axis T := N = {0, 1, 2, . . .},
 the full signal space W := U  X  Y, and
 the initialised behaviour 1
Binit
i/s/o = {w  WN |w = (u, x, y ) s.t. (1)
and s.t. x(0) = x0  X0 }.

2.2 Control of descriptor systems

Controller synthesis amounts to synthesising a system c ,
called a controller, which, after interconnection with ,
restricts the behaviour B of  to desirable (or controlled)
tra jectories. Thus, in the behavioural framework, control
is dened through interconnections (or via variable sharing
as specied next), rather than based on the causal trans-
mission of signals or information, as in classical system the-
ory. Let 1 = (T, C1 W, B1) and 2 = (T, C2W, B2 ) be
two dynamical systems. Then, as depicted in Fig. 1a and
dened in (Willems and Polderman, 2013), the intercon-
nection of 1 and 2 over W, denoted by  = 1 w 2
with the shared variable w  W, yields the dynamical
system  = (T, C1  C2  W, B) with B = {(c1 , c2 , w) :
T  C1  C2  W | (c1 , w)  B1 , (c2 , w)  B2 }.

c1

c2

2

Bc

Bc

(a) The interconnected system 
obtained via the shared variables
w in W between dynamical sys-
tems 1 and 2 with signal spaces
C1  W and C2  W.

(b) The controlled behaviour
Bc = B  Bc is given as
the intersection of the behaviours
of the dynamical system  and
its controller c .

Fig. 1. The left gure (a) portrays the general interconnec-
tion of two dynamical systems. In gure (b), the more
specic case of behavioural intersection for a system
and its controller is depicted.

Observe that w  WT contains the signals shared by
both 1 and 2 , while c1  CT
1 only belongs to 1 and
c2  CT
2 only belongs to 2 . So, in the interconnected
system, the shared variable w satises the laws of both
B1 and B2 . Note that it is always possible to trivially
extend the signal spaces of 1 and 2 (and the associated
behaviour) such that a full interconnection structure is
obtained, that is, such that both C1 and C2 are empty and
the behaviour of the interconnected system is B = B1 
B2 . Hence, a full interconnection of  = (T, W, B ) and
c = (T, W, Bc ) is simply  w c = (T, W, B 
Bc ), with the intersection of the behaviours, denoted by
Bc , as portrayed in Fig. 1b. That is, interconnection
and intersection are equivalent in full interconnections.

Further, we dene a well-posed controller c for  as
follows.
Denition 3. Consider a dynamical system  = (T, W, B),
with initialised behaviour as dened in (4). We say that a
system c = (T, W, Bc ) is a wel l-posed controller for  if
the following conditions are satised:

1 In the sequel the indexes init and i/s/o will be dropped.

(1) Bc := B  Bc 6= {};
(2) For every initial state x0  X0 , there exists a unique
continuation in Bc .

Denote with C() the collection of all well-posed con-
trollers for .

We want a controller that accepts any initial state of
the system. This is formalised in the second condition
by requiring that for any initial state of , there exists a
unique continuation in Bc . We elucidate the properties
of a well-posed linear controller as follows.
Example 1. For a system  as in (1), consider a control ler
c , which is a DS, and has dynamics given as
(5)
Ecx(t + 1) = Acx(t) + Bcu(t),
with Ec , Ac  Rncn and Bc  Rncp . Suppose that the
control ler shares the variables u and x with the system .
That is, w = (u, x). The interconnected system  w c
y');
INSERT INTO posts (postId,userId,title,body) VALUES (484,5583,'via simulation relations (part 4)','ields the state evolutions of the combined system as
Bc(cid:21) u(t),
Ac(cid:21) x(t) + (cid:20) B
Ec(cid:21) x(t + 1) = (cid:20) A
(cid:20) E
and can be rewritten to
u(t) (cid:21) = (cid:20) A
Ac(cid:21) x(t).
(cid:20) E B
Ec Bc(cid:21) (cid:20)x(t + 1)
If for any x(t)  X, there exists a pair (x(t + 1), u(t)) such
that (7) holds, then this implies that for any initial state
x0  X0 of  there exists a continuation in the control led
behaviour. In addition, if the pair (x(t + 1), u(t)) is unique
for any x(t)  X, then this continuation is unique and we
say that c  C(). This existence and uniqueness of the
pairs (x(t + 1), u(t)) depends on the solutions of the matrix
equality (7). We use the classical results on the solutions
of matrix equalities (cf. (Abadir and Magnus, 2005)) to
conclude that the rst wel l-posedness condition is satised
if and only if
rank (cid:18)(cid:20) E B
Ec Bc(cid:21)(cid:19) = rank (cid:18)(cid:20) E B A
Ec Bc Ac(cid:21)(cid:19) .
If in addition,

(6)

(7)

(8)

rank (cid:18)(cid:20) E B A
Ec Bc Ac(cid:21)(cid:19) = n + p,
then the second condition is also satised and c  C().

(9)

Of interest is the design of well-posed controllers sub ject
to specications over the future output behaviour of the
controlled system. We thus consider specications dened
over the output space. In order to analyse the output
behaviour, we introduce a pro jection map. For B  (W1 
W2 )T we denote with W2 a pro jection given as
2 |w1  WT
W2 (B) := {w2  WT
1 s.t. (w1 , w2 )  B}.

We focus here on nding a controller c for a given dynam-
ical system  such that the output behaviour Y (Bc )
of the interconnected system satises some specications.

2.3 Exact control renement '||'&'||' problem statement

Let us refer to the original DS that represents the real
physical system as the concrete DS. It is for this system
that we would like to develop a well-posed controller.
Recall that the DS is a dynamical system  with dynamics

(E , A, B , C ) as in (1) and initialised with X0 . A well-posed
controller for  is referred to c  C(). The controlled
concrete system is the interconnected system w c with
the shared variables w = (u, x).

Now, we consider a simpler DS a , related to the concrete
DS , with dynamics given as (Ea , Aa , Ba , Ca ) and ini-
tialised with Xa0 . We assume that the synthesis of a well-
posed controller ca for a is substantially easier than for
. We refer to this simpler system a as the abstract DS,
and we note that its signals take values ua(t), xa (t), ya (t)
with xa (t)  Xa = Rm , ua(t)  Ua = Rq , ya (t)  Ya =
Y = Rk and t  N. With respect to the concrete system,
the abstract DS is generally a reduced-order system. The
controlled abstract system a wa ca is the intercon-
nected system with the shared variables wa = (ua , xa ).

If we assume that we can compute a well-posed controller
for the abstract system, then the control synthesis problem
reduces to a control renement problem.
Denition 4. (Exact control renement). Let a and  be
the abstract and concrete DS, respectively. We say that
controller c  C() renes the controller ca  C(a ) if
Y (Bc )  Y (Ba ca ).
Then we formalise the exact control renement problem.
Problem 1.
Let a and  be the abstract and concrete
DS, respectively. For any ca  C(a ), rene ca to c ,
s.t. c  C() and Y (Bc )  Y (Baca ).
In the next section, we will show that the existence of
a solution to this problem hinges on certain conditions
involving similarity relations between the concrete and
abstract DS. For this, we will rst introduce simulation
relations to formally characterise');
INSERT INTO posts (postId,userId,title,body) VALUES (485,5583,'via simulation relations (part 5)',' this similarity.

3. EXACT CONTROL REFINEMENT

3.1 Similarity relations between DS

We give the notion of simulation relation as dened in
(Tabuada, 2009) for transition systems and applied to
pairs of DS 1 and 2 that share the same output space
Y1 = Y2 = Y.

Denition 5. Let 1 and 2 be two DS with respective
dynamics (E1 , A1 , B1 , C1 ) and (E2 , A2 , B2 , C2 ) over state
spaces X1 and X2 . A relation R  X1  X2 is called a
simulation relation from 1 to 2 , if (x1 , x2 )  R,
(1) for all (u1 , x+
1 )  U1  X1 sub ject to
E1 x+
1 = A1 x1 + B1u1
there exists (u2 , x+
2 )  U2  X2 sub ject to
E2 x+
2 = A2 x2 + B2u2
1 , x+
such that (x+
2 )  R, and

(2) we have C1x1 = C2x2 .

We say that 1 is simulated by 2 , denoted by 1 (cid:22) 2 , if
there exists a simulation relation R from 1 to 2 and if in
addition x10  X10 , x20  X20 such that (x10 , x20 )  R.

We call R  X1  X2 a bisimulation relation between 1
and 2 , if R is a simulation relation from 1 to 2 and
its inverse R1  X2  X1 is a simulation relation from

2 to 1 . We say that 1 and 2 are bisimilar, denoted
by 1 = 2 , if 1 (cid:22) 2 w.r.t. R and 2 (cid:22) 1 w.r.t. R1 .
Simulation relations as dened above are transitive. Let
R12 and R23 be simulation relations respectively, from 1
to 2 and from 2 to 3 . Then a simulation relation from
1 to 3 is given as a composition of R12 and R23 , namely
R12 R23 = {(x1 , x3 ) | x2 : (x1 , x2 )  R12(x2 , x3 )  R23}.
We also have that 1 (cid:22) 2 and 2 (cid:22) 3 implies 1 (cid:22) 3
and, in addition, 1 = 2 and 2 = 3 implies 1 = 3 .
Simulation relations have also implications on the prop-
erties of the output behaviours of the two systems. More
precisely, if a system is simulated by another system then
this implies output behaviour inclusion. This follows from
Proposition 4.9 in (Tabuada, 2009) and is formalised next.
Proposition 6. Let 1 and 2 be two DS with simulation
relations as dened in Denition 5. Then,
1 (cid:22) 2 = Y (B1 )  Y (B2 ),
1 = 2 = Y (B1 ) = Y (B2 ).
Simulation relations can also be used for the controller
design for deterministic systems such as nonsingular DS
(Tabuada, 2009; Fainekos et al., 2007; Girard and Pappas,
2009). This will be used in the next subsection, where we
consider the exact control renement for non-singular DS.
After that, we introduce a transformation of a singular
DS to an auxiliary nonsingular DS representation, referred
to as a driving variable (DV) system. The exact control
renement problem is then solved based on the introduced
notions.

3.2 Control renement for non-singular DS

Let us consider the simple case where the concrete and
abstract systems of interest are given with non-singular
dynamics. For these systems, the existence of a simulation
relation also implies the existence of an interface function
(Girard and Pappas, 2009), which is formulated as follows.
Denition 7. (Interface). Let 1 and 2 be two non-
singular DS dened over the same output space Y with a
simulation relation R from 1 to 2 . A mapping F : U1 
X1  X2 7 U2 is an interface related to R, if (x1 , x2 )  R
and for all u1  U1 , u2 := F (u1 , x1 , x2 )  U2 is such that
(x+
1 , x+
2 )  R with
1 = A1x1 + B1 u1 and x+
x+
2 = A2x2 + B2 u2 .
It follows from Denition 5 that there exists at least one
interface related to R if two deterministic, or non-singular
systems are in a simulation relation. As such we can solve
the exact renement problem as follows.
Theorem 8. Let 1 and 2 be two non-singular DS de-
ned over the same output space Y with dynamics
(I , A1 , B1 , C1 ) and (I , A2 , B2 , C2 ), which are initialised
with X10 and X20 , respectively. If there exists a rel');
INSERT INTO posts (postId,userId,title,body) VALUES (486,5583,'via simulation relations (part 6)','ation
R  X1  X2 such that

(1) R is a simulation relation from 1 to 2 , and
(2) x20  X20 , x10  X10 s.t. (x10 , x20 )  R,

then for any controller c1  C(1 ), there exists a
controller c2  C(2 ) that is an exact control renement
for c1 and thus achieves with
Y (B2c2 )  Y (B1c1 ).

Proof. Since R is a simulation relation from 1 to 2 ,
there exists an interface function F : U1  X1  X2  U2
as given in Denition 7, cf (Tabuada, 2009; Girard and
Pappas, 2009). Additionally, due to (2) there exists a map,
F0 : X20  X10 such that for all x20  X20 it holds that
(F0 (x20 ), x20 )  R.
Next, we construct the controller c2 that achieves exact
control renement for c1 as
c2 := (1 w1 c1 ) w1 F ,
where w1 = (u1 , x1 ) and where F := (N, W, BF ) is a
dynamical system taking values in the combined signal
space with
BF := {(x1 , u1 , x2 , u2 )  W|x10 = F0 (x20 ) and
u2 = F (x1 , u1 , x2 )}.
The dynamical system c2 is a well-posed controller for
2 with 2 w2 c2 sharing w2 = (u2 , x2 ). Denote with
B2c2 the behaviour of the controlled system, then due
to the construction of F it follows that B2c2 is non-
empty and x20  X20 , x10  X10 such that (x10 , x20 ) has
a unique continuation in B2c2 . Furthermore it holds
that Y (B2 c2 )  Y (B1c1 ). 

The design of the controller c2 that achieves exact control
renement for c1 is similar to that in (Tabuada, 2009),
which also holds in the behavioural framework.

3.3 Driving variable systems

Since it is dicult to control and analyse a DS directly, we
develop a transformation to a system representation that
is in non-singular DS form and is driven by an auxiliary
input. We refer to this non-singular DS as the driving
variable (DV) system (Weiland, 1991). We investigate
whether the DS and the obtained DV system are bisimilar
and behaviourally equivalent. Let us rst introduce with
a simple example the apparent non-determinism or anti-
causality in the DS. Later-on, we show the connections
between a DS and its related DV system.
Example 2. Consider the DS with dynamics (E , A, B , C )
dened as
0.5 iT
0 0 0 i, A = h 1 0 0
1 i, C = h 0
0 0 1 i, B = h 1
E = h 1 0 00.2
0 0 1
0 1 0
and x(t) = [x1 (t) x2 (t) x3 (t)]T . In this case, the input
u(t) = x3 (t) is constrained by the third state component.
Now the state trajectories of (10) can be found as fol lows:

(10)

 for a given input sequence u : N  U, we have
x2 (t) = u(t)u(t+1), and thus we can use this anti-
causal relation of the DS to nd the corresponding
state trajectories;
 alternatively, we can al low the next state x2 (t + 1)
to be freely chosen, and for arbitrary state x2 (t),
the equations (10) impose constraints on the input
sequence that is, therefore, no longer free as u(t) =
x3 (t).

We embrace the latter, non-deterministic interpretation of
the DS.

This non-determinism can be characterised by introducing
an auxiliary driving input of a so-called DV system. We

(11)

reorganise the state evolution of (1). For simplicity we omit
the time index in x(t) and u(t) and denote x(t + 1) as x+
M (cid:20)x+
u (cid:21) = Ax,
where M = [E B ]. For any x, we notice that the pairs
(u, x+ ) are non-unique due to the non-determinism related
to x+ . If M has full row rank, then it has a right inverse.
This always holds when the DS is reachable (cf. Denition
2-1.1 (Dai, 1989)). In that case we can characterise the
non-determinism as follows. Let M + be a right inverse of
M such that M M + = I and N be a matrix such that
im N = ker M and N T N = I . Then all pairs (u, x+ ) that
are compatible with state x in (11) are parametrised as
(cid:20)x+
u (cid:21) = M +Ax + N s,
where s is a free variable. We now claim that all transitions
(x, u, x+ ) in (12) for some variable s');
INSERT INTO posts (postId,userId,title,body) VALUES (487,5583,'via simulation relations (part 7)',' satisfy (11). To see
this, multiply M on both sides of (12) to regain (11). Now
assume that there exists a tuple (x, u, x+ ) satisfying (11)
that does not satisfy (12). Then there exists an s and a
vector z 6= 0 that is not an element of the kernel of M and
such that the right side of (12) becomes M +Ax + N s +
z . Multiplying again with M , we infer that there is an
additional non-zero term M z and that (11) cannot hold.
In conclusion any transition of (11) is also a transition of
(12) and vice versa.
Example 3. [Example 2: contd] For the DS of Example 2,
the related DV system DV is developed as
x(t + 1) = h 1 0 1
0 1 1 ix(t) + h 0
0 is(t)0 0 0
u(t) = [ 0 0 1 ]x(t)
y (t) = [ 0 0.2 0.5 ]x(t).

(12)

(13)

As indicated by (13), the input u(t) is a function of
the state trajectory. The non-determinism of x2 (t + 1) is
characterised by s(t) for which the auxiliary input s can
be freely selected.

(14)

Let us now formalise the notion of a driving variable
representation. We associate a driving variable repre-
sentation with any given DS (1) by dening a tuple
(Ad , Bd , Cu , Du , C ) and setting
(cid:20)Ad
Cu (cid:21) = M +A, (cid:20)Bd
Du (cid:21) = N ,
where N  R(n+p)p has orthonormal columns, that is
N T N = I . For any given DS, this tuple denes the driving
variable system DV = (N, W, BDV ), which maintains
the same set of initial states X0 and has dynamics
x(t + 1) = Adx(t) + Bds(t)
u(t) = Cu x(t) + Du s(t)
y (t) = C x(t),
thereby yielding the initialised behaviour
BDV := {w  WN |w =(u, x, y ), s  SN
s.t. (15) and x0  X0}.
Next, we propose the following assumption for DS, which
will be used in the sequel to develop our main results.
Assumption 1. The given DS  is a dynamical system
with dynamics (E , A, B , C ) such that M = [E B ] has
full row rank.

(15)

The relationship between a DS and its related DV system
is characterised as follows.
Theorem 9. Let the DS  be given as in (1) satisfying
Assumption 1 and let DV = (N, W, BDV ) be dened as
in (15). Then
(1)  and DV are bisimilar, that is,  = DV ,
(2)  and DV have equal behaviour, i.e., BDV = B ,
(3)  and DV have equal output behaviour, that is,
Y (B ) = Y (BDV ).

Proof. For the rst statement (1), we dene the diagonal
relation as I := {(x, x) | x  X}. Then I is a bisimulation
relation between  and DV , because by construction their
state evolutions can be matched, hence stay in I ; and they
share the same output map. In addition, since they have
the same set of initial states it follows that  = DV .
The second part (2) follows immediately from the deriva-
tion of DV , because by construction all the transitions
in  can be matched by those of DV and vice versa, in
addition, they have the same output map. Hence, they
share the same signal space (U  X  Y) and we can
conclude that  and DV have equal behaviour.
Additionally, we have that (2) implies (3); via Proposition
6 also (1) implies (3). 

3.4 Main result: exact control renement for DS

Based on the results developed in the previous subsections,
we now derive the solution to the exact control renement
problem in Problem 1. More precisely, sub ject to the
assumption that there exists ');
INSERT INTO posts (postId,userId,title,body) VALUES (488,5583,'via simulation relations (part 8)','a simulation relation R from
a to , for which in addition holds that x0  X0 , xa0 
Xa0 s.t. (xa0 , x0 )  R, we show that for any ca  C(a ),
there exists a controller c for  that renes ca such that
c  C() and Y (Bc )  Y (Baca ).

In the case of Assumption 1, we construct DV systems
DV and DVa for the respective DS systems  and a
as a rst step. For these systems, we develop the following
results on exact control renement:

i) The exact control renement for the DV systems:
DVa  C(DVa ), cDV  C(DV ), s.t.
Y (cid:0)BDVc
DVa (cid:1);
DV (cid:1)  Y (cid:0)BDVaii) The exact control renement from a to DVa :
ca  C(a ), c
DVa  C(DVa ), s.t.
Y (cid:0)Baca (cid:1) = Y (cid:0)BDVa
iii) The exact control renement from DV to :DV  C(DV ), c  C(), s.t.
Y (cid:16)BDVc
DV (cid:17) = Y (Bc ) .
It will be shown that the combination of the elements
i)iii) also implies the construction of the exact control
renement for the concrete and abstract DS.

DVa (cid:1);
i) Exact control renement for the DV systems. From
Theorem 9, we know that  = DV and a = DVa
with respective diagonal relations I := {(x, x)|x  X} and
Ia := {(xa , xa )|xa  Xa}. Hence as depicted in Fig. 2 and
based on the transitivity of simulation relations, we also
derive that R is a simulation relation from DVa to DV .

 = DV ,

w.r.t. I

DV
(xa0 , x0 )  R

R = Ia  R  I
(xa0 , x0 )  R

a

= DVa ,

w.r.t. Ia

DVa

(16)

Fig. 2. Connection between DS and DV systems for the
exact control renement.
Since the DV systems DV and DVa share the same
initial states as the respective DS  and a , it also holds
that x0  X0 , xa0  Xa0 s.t. (xa0 , x0 )  R. According
to Theorem 8, we know that we can do exact control
renement, that is, we have shown
DVa  C(DVa ), cDV  C(DV ), s.t.
DV (cid:1)  Y (cid:0)BDVa
DVa (cid:1).
Y (cid:0)BDVcii) Exact control renement from a to DVa . Denote
with DVa the abstract DV system related to a , with dy-
namics (Ada , Bda , Cua , Dua , Ca ) and initialised with Xa0 .
We rst derive the static function Sa mapping transitions
of a to the auxiliary input sa of DVa . From the denition
of DV systems, we can also derive the transitions of DVa
indexed with a, which is similar to the derivation of (12).
(cid:20)x+
ua (cid:21) = M +a Aaxa + Na sa .
Multiplying N T
a on both sides of (16), Sa is derived as
a (cid:20)x+
ua (cid:21)N T
a M +
Sa : sa = Sa (x+
a , ua , xa ) = N Ta Aa xa . (17)
Sa maps the state evolutions of a wa ca to the auxiliary
input sa for DVa , where wa = (ua , xa ). Now, we consider
the exact control renement from the abstract DS to the
abstract DV system.
Theorem 10. Let a be the abstract DS with dynamics
(Ea , Aa , Ba , Ca ) satisfying the condition of Assumption 1
and let DVa be its related DV system with dynamics
(Ada , Bda , Cua , Dua , Ca ) such that both systems are ini-
tialised with Xa0 . Then, for any ca  C(a ), there exists
a controller c
DVa  C(DVa ) that is an exact control
renement for ca as dened in Denition 4 with
DVa (cid:1).
Y (cid:0)Baca (cid:1) = Y (cid:0)BDVaProof. Denote with xa and xd
a the state variables of a
and DVa , respectively. Next, we construct the controllerDVa that achieves exact control renement for ca asDVa := (a wa ca ) wa Sa ,
where wa = (ua , xa ) and where Sa := (N, W, BSa ) is a
dynamical system with
a , sa )  W|xa0 = xd
BSa := {(xa , ua , xd
a0 and
sa = Sa (x+
a , ua , xa )}.
The dynamical system c
DVa is a well-posed controller for
a c
a = (sa , xd
DVa sharing wd
DVa with DVa wd
a ). Denote
the behaviour of the controlled system.
with BDVaDVa

By construction, we know t');
INSERT INTO posts (postId,userId,title,body) VALUES (489,5583,'via simulation relations (part 9)','hat the set of the behaviour
is non-empty and there is a unique continuation for any
xd
a0  Xa0 . Further based on the construction of Sa , the
behaviour is such that xd
a (t) = xa (t), t  N. Additionally,
since a and DVa share the same set of initial states Xa0 ,
it holds that Y (cid:0)Baca (cid:1) = Y (cid:16)BDVa
DVa (cid:17) . The proof is actually constructive in the design of the
controller c
DVa that achieves exact control renement for
ca .

iii) Exact control renement from DV to . Now, we
consider the exact control renement from DV to .
Suppose we are given a well-posed controller c
DV for DV ,
which shares the free variable s and the state variable x
with DV . We want to design a well-posed controller for
 over w = (u, x), for which we consider the dynamical
system C = (N, W, B) over the signal space W = U 
X  S, the behaviour of which can be dened by
d x(t + 1) = B T
d Adx(t) + B T
B T
d Bds(t)
u(t) = Cu x(t) + Dus(t).

(18)

(19)

(20)

Then the dynamics of the interconnected system  w C
as a function of x and s is derived as
d Bd(cid:21) s(t).
d Ad (cid:21) x(t) + (cid:20) BDu
d (cid:21) x(t + 1) = (cid:20)A + BCu
(cid:20) E
B T
B T
B T
Note that A + BCu = EAd and BDu = EBd by
multiplying M = [E B ] on the left-hand side of the two
equations in (14). Therefore, (19) is simplied to
d (cid:21) Bds(t).
d (cid:21) Adx(t) + (cid:20) E
d (cid:21) x(t + 1) = (cid:20) E
(cid:20) E
B T
B T
B T
Furthermore (cid:2)E T Bd (cid:3)T
has full column rank because the
matrix (cid:2)M T N (cid:3)T
is square and has full rank. Hence
(cid:2)E T Bd (cid:3)T
has a left inverse and the dynamics of  w C
in (20) can be simplied as
x(t + 1) = Adx(t) + Bd s(t),
which is exactly the same as the state evolutions of DV
as shown in (15). Next we construct c := C wd c
DV
with wd = (s, xd ) and it is a well-posed controller for .
This allows us to state the following theorem regarding the
control renement from DV to .
Theorem 11. Let  be the concrete DS with dynamics
(E , A, B , C ) satisfying Assumption 1 and let DV be its
related DV system with dynamics (Ad , Bd , Cu , Du , C ) such
that both systems are initialised with X0 . Then, for anyDV  C(DV ), there exists a controller c  C()
that is an exact control renement for c
DV as dened in
Denition 4 with
Y (cid:16)BDVc
DV (cid:17) = Y (Bc ) .
Proof. Denote with x and xd the state variables of the
 and DV , respectively. Next, we construct the controller
c that achieves exact control renement for c
DV as
c := C wd c
DV ,
where wd = (s, xd ) and the dynamics of C is dened as
(18). Then, we can show that the dynamical system c is a
well-posed controller for . Based on the analysis of (20),
it is shown that  w C = DV with w = (u, x), then we

can derive  w c = DV wd c
DV . Therefore, we can
DV (cid:1) = Y (cid:0)Bc (cid:1)
conclude c  C() with Y (cid:0)BDVc
immediately follows from c
DV  C(DV ). 

Exact control renement for descriptor systems. We can
now argue that there exists exact control renement from
a to , as stated in the following result.
Theorem 12. Consider two DS a (abstract,
initialised
with Xa0 ) and  (concrete, initialised with X0 ) satisfying
Assumption 1 and let R be a simulation relation from a
to , for which in addition holds that x0  X0 , xa0 
Xa0 s.t. (xa0 , x0 )  R. Then, for any ca  C(a ), there
exists a controller c  C() such that
Y (Bc )  Y (cid:0)Baca (cid:1) .
Proof. Based on Assumption 1, we rst construct DV
and DVa . Then to prove this we need to construct the
exact control renement. This can be done based on
the subsequent control renements given in Theorem 10,
Theorem 8 and Theorem 11. 

Theorem 12 claims the existence of such controller c that
achieves exact control renement for ca . More precisely,
');
INSERT INTO posts (postId,userId,title,body) VALUES (490,5583,'via simulation relations (part 10)','we have shown in the proof that the rened controller c
is constructive, which provides the solution to Problem 1.

To elucidate how such an exact control renement is
constructed, we consider the following example.
Example 4. [Example 2,3: contd] Consider the DS of
Example 2 and its related DV system (cf. Example 3)
such that both systems are initialised with X0 = {x0 |
x0  [1, 1]3  R3}. According to Silverman-Ho algo-
rithm (Dai, 1989), we can select an abstract DS a =
(Ea , Aa , Ba , Ca ) that is the minimal realisation of  and
is initialised with Xa0 = R2 , in addition
0.2 ]T .
0 ], Ca = [ 0.7
Ea = [ 0 0
1 0 ], Aa = [ 1 0
0 1 ], Ba = [ 1
Similarly, the related DV system DVa of a is given as
0 0 ]xa (t) + (cid:2) 0
1 (cid:3)sa (t)
xa (t + 1) = [ 0 1
ua (t) = [ 1 0 ]xa (t)
ya (t) = [ 0.7 0.2 ]xa (t).

(21)

Subsequently,
R := {(xa , x) | xa = Hx, xa  Xa , x  X}
is a simulation relation from a to  with
H = (cid:2) 0 0 1
0 1 1 (cid:3).
This can be proved through verifying the two properties of
Denition 5. In addition, the condition x0  X0 , xa0 
Xa0 s.t. (xa0 , x0 )  R holds. According to Theorem 12,
we can rene any ca  C(a ) to attain a wel l-posed
control ler c for  that solves Problem 1 as fol lows: Dene
ca  C(a ) with dynamics as
[ 1 1 ]xa (t + 1) = [ 0.5 0.5 ]xa (t) + ua (t).
The control led system a wa ca is derived as
xa (t + 1) = (cid:2) 00.5 0.5 (cid:3)xa (t)
ya (t) = [ 0.7 0.2 ]xa (t),
with wa = (ua , xa ) and ua (t) = [ 1 0 ]xa (t). Then a wa
ca is stable. According to Theorem 10, we derive the map
Sa for DVa as sa (t) = [ 0 1 ]xa (t + 1) = [ 0.5 0.5 ]xa (t).
Next, the related interface from DVa to DV is developed

as s(t) = sa (t)  [ 0 1 1 ]x(t). According to Theorem 11,
we derive the wel l-posed control ler c as
[ 0 1 0 ]x(t + 1) = [ 0 1 1 ]x(t) + [ 0.5 0.5 ]xa (t)
u(t) = [ 0 0 1 ]x(t),
and the interconnected system  w c with w = (u, x), is
derived as
x(t + 1) = h 1 0 10 1 1 ix(t) + h 0
0 ixa (t)
0 1 1
0.5 0.5y (t) = [ 0 0.2 0.5 ]x(t).
Since (xa , x)  R, that is xa = Hx,  w c can be
simplied by replacing xa (t):
x(t + 1) = h 1 00 1 1 ix(t)
0 0.5 1
y (t) = [ 0 0.2 0.5 ]x(t).
Final ly, c  C() and Y (Bc )  Y (cid:0)Baca (cid:1) are
achieved.
4. CONCLUSION

In this paper, we have developed a control renement
procedure for discrete-time descriptor systems that is
largely based on the behavioural theory of dynamical
systems and the theory of simulation relations among
dynamical systems. Our main results provide complete
solutions of the control renement problem for this class
of discrete-time systems.

The exact control renement that has been developed
in this work also opens the possibilities for approximate
control renement notions, to be coupled with approxi-
mate similarity relations: these promise to leverage general
model reduction techniques and to provide more freedom
for the analysis and control of descriptor systems.

The future research includes a comparison of the control
renement approach for descriptor systems to results in
perturbation theory, as well as control renement for
nonlinear descriptor systems.

REFERENCES

Abadir, K.M. and Magnus, J.R. (2005). Matrix algebra.
Cambridge University Press.
Antoulas, A.C. (2005). Approximation of large-scale dy-
namical systems. SIAM.
Cao, X., Saltik, M., and Weiland, S. (2015). Hankel model
reduction for descriptor systems.
In 2015 54th IEEE
CDC, 46684673.
Dai, L. (1989). Singular control systems. Springer-Verlag
New York, Inc.
Fainekos, G.E., Girard, A., and Pappas, G.J. (2007). Hi-
erarchical synthesis of hybrid controllers from tempo-
ral logic specications.
In International Workshop on
HSCC, 203216.
Girard, A. and Pappas, G.J. (2009). Hierarchical control
system design using approximate simulation. Automat-
ica, 45(2), 566571.
Girard, A. and Pappas, G.J. (2011). Approximate bisim-
ulation: A bridge between computer science and control
theory. European Journal of Control, 17(5), 568578.
Kloetzer, M. and Belta, C. (2008). A fully');
INSERT INTO posts (postId,userId,title,body) VALUES (491,5583,'via simulation relations (part 11)',' automated
framework for control of linear systems from temporal
logic specications. IEEE Transactions on Automatic
Control, 53(1), 287297.

Kunkel, P. and Mehrmann, V.L. (2006). Dierential-
algebraic equations: analysis and numerical solution.
European Mathematical Society.
Megawati, N.Y. and Van der Schaft, A. (2015). Bisimula-
tion equivalence of DAE systems. arXiv:1512.04689.
Tabuada, P. (2009). Verication and control of hybrid
systems: a symbolic approach.
Springer Science '||'&'||'
Business Media.
Van der Schaft, A. (2004). Equivalence of dynamical sys-
tems by bisimulation. IEEE transactions on automatic
control, 49(12), 21602172.
Weiland, S. (1991). Theory of approximation and dis-
turbnace attenuation for linear systems. University of
Groningen.
Willems, J.C. and Polderman, J.W. (2013). Introduction
to mathematical systems theory: a behavioral approach,
volume 26. Springer Science '||'&'||' Business Media.

');
